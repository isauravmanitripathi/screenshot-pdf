{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "CHAPTER 1",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "The Machine Learning Landscape",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/7b71b69d-64b0-4206-8ce0-54f868126db9.png",
                            "./screenshots-images-2/chapter_1/section_1/e36cb872-ba86-464b-b29e-aad9471f045d.png",
                            "./screenshots-images-2/chapter_1/section_1/0121e0ae-467a-42ef-b291-d3bce4cb7f7b.png",
                            "./screenshots-images-2/chapter_1/section_1/2d2d06b5-4581-4dc1-9f6d-7f6b3840d45d.png",
                            "./screenshots-images-2/chapter_1/section_1/a7dea87f-3f00-4ac2-9d8f-53742876b577.png",
                            "./screenshots-images-2/chapter_1/section_1/9039ec2b-cfba-4789-a43e-15bf21657062.png",
                            "./screenshots-images-2/chapter_1/section_1/ba19b210-662d-416f-8b2e-14fe7ca03391.png",
                            "./screenshots-images-2/chapter_1/section_1/b72c53f8-180f-45d9-abdf-a329fb98dc38.png",
                            "./screenshots-images-2/chapter_1/section_1/d17c86f4-c4f9-4c50-b64f-6c8a7367b1e8.png",
                            "./screenshots-images-2/chapter_1/section_1/12a3dcbc-7f91-4224-8de5-179884b32ffa.png",
                            "./screenshots-images-2/chapter_1/section_1/3963b81b-0621-4f9e-8728-e32a91c3a57f.png",
                            "./screenshots-images-2/chapter_1/section_1/44f1ab19-d1b7-4244-8a36-6efc3cd5c769.png",
                            "./screenshots-images-2/chapter_1/section_1/2a0377f0-1629-4bb9-970b-dc8fe16a1717.png",
                            "./screenshots-images-2/chapter_1/section_1/08386a53-b1ab-4fcd-8606-871c9c36564d.png",
                            "./screenshots-images-2/chapter_1/section_1/eff8059c-5827-4e77-844a-acbe825fa381.png",
                            "./screenshots-images-2/chapter_1/section_1/6bb4fb24-a1c1-40a1-9993-37be63416f50.png",
                            "./screenshots-images-2/chapter_1/section_1/c27e708a-a3cc-472e-9710-a1f3e32285a2.png",
                            "./screenshots-images-2/chapter_1/section_1/459c9d5d-8dff-4045-ac49-91c656b5bda4.png",
                            "./screenshots-images-2/chapter_1/section_1/47ddab51-60e1-410b-b293-818fa06a397a.png",
                            "./screenshots-images-2/chapter_1/section_1/24985a2f-9998-4624-99c8-d6aec0a24493.png",
                            "./screenshots-images-2/chapter_1/section_1/f26da41e-3cf2-41c3-83c9-2e3950f51a31.png",
                            "./screenshots-images-2/chapter_1/section_1/694caf71-095e-4f5d-86b4-36e67a84c42e.png",
                            "./screenshots-images-2/chapter_1/section_1/68aac27e-22a6-4329-b941-09b23a0b3779.png",
                            "./screenshots-images-2/chapter_1/section_1/6a649c03-877e-4069-b873-4c82f1fd9309.png",
                            "./screenshots-images-2/chapter_1/section_1/f41811bf-1fff-4b72-b8f3-2594d897e4f8.png",
                            "./screenshots-images-2/chapter_1/section_1/7d78ce2d-fb1b-497e-9258-fd24302c4fee.png",
                            "./screenshots-images-2/chapter_1/section_1/613c84a4-ab37-43ff-bd4c-f708d23c6b33.png",
                            "./screenshots-images-2/chapter_1/section_1/0227c281-9287-4217-bfc3-c26f855e8fc8.png",
                            "./screenshots-images-2/chapter_1/section_1/a7dc69bb-ecd3-4835-b8d1-d8cb4f1b5786.png",
                            "./screenshots-images-2/chapter_1/section_1/ffe101c9-381f-4bd1-a0e8-ce62ec54639c.png",
                            "./screenshots-images-2/chapter_1/section_1/19e4eee2-017c-4e71-80fa-bf4c556cd901.png",
                            "./screenshots-images-2/chapter_1/section_1/3c863eb1-5bf0-47ae-b03e-8732a3bc88d8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "With Early Release ebooks, you get books in their earliest form\u2014\nthe author's raw and unedited content as he or she writes\u2014so you\ncan take advantage of these technologies long before the official\nrelease of these titles. The following will be Chapter 1 in the final\nrelease of the book.\n\nWhen most people hear \u201cMachine Learning,\u201d they picture a robot: a dependable but-\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\njust a futuristic fantasy, it's already here. In fact, it has been around for decades in\nsome specialized applications, such as Optical Character Recognition (OCR). But the\nfirst ML application that really became mainstream, improving the lives of hundreds\nof millions of people, took over the world back in the 1990s: it was the spam filter.\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\n(it has actually learned so well that you seldom need to flag an email as spam any-\nmore). It was followed by hundreds of ML applications that now quietly power hun-\ndreds of products and features that you use regularly, from better recommendations\nto voice search.\n\nWhere does Machine Learning start and where does it end? What exactly does it\nmean for a machine to learn something? If I download a copy of Wikipedia, has my\ncomputer really \u201clearned\u201d something? Is it suddenly smarter? In this chapter we will\nstart by clarifying what Machine Learning is and why you may want to use it.\n\nThen, before we set out to explore the Machine Learning continent, we will take a\nlook at the map and learn about the main regions and the most notable landmarks:\nsupervised versus unsupervised learning, online versus batch learning, instance-\nbased versus model-based learning. Then we will look at the workflow of a typical ML\nproject, discuss the main challenges you may face, and cover how to evaluate and\nfine-tune a Machine Learning system.\n\nThis chapter introduces a lot of fundamental concepts (and jargon) that every data\nscientist should know by heart. It will be a high-level overview (the only chapter\nwithout much code), all rather simple, but you should make sure everything is\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let's\nget started!\n\nIf you already know all the Machine Learning basics, you may want\nto skip directly to Chapter 2. If you are not sure, try to answer all\nthe questions listed at the end of the chapter before moving on.\n\nWhat Is Machine Learning?\n\nMachine Learning is the science (and art) of programming computers so they can\nlearn from data.\n\nHere is a slightly more general definition:\n\n[Machine Learning is the] field of study that gives computers the ability to learn\nwithout being explicitly programmed.\n\u2014Arthur Samuel, 1959\n\nAnd a more engineering-oriented one:\n\nA computer program is said to learn from experience E with respect to some task T\nand some performance measure P, if its performance on T, as measured by P, improves\nwith experience E.\n\n\u2014Tom Mitchell, 1997\n\nFor example, your spam filter is a Machine Learning program that can learn to flag\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\n(nonspam, also called \u201cham\u201d) emails. The examples that the system uses to learn are\ncalled the training set. Each training example is called a training instance (or sample).\nIn this case, the task T is to flag spam for new emails, the experience E is the training\ndata, and the performance measure P needs to be defined; for example, you can use\nthe ratio of correctly classified emails. This particular performance measure is called\naccuracy and it is often used in classification tasks.\n\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\nnot suddenly better at any task. Thus, it is not Machine Learning.\n\nWhy Use Machine Learning?\n\nConsider how you would write a spam filter using traditional programming techni-\nques (Figure 1-1):\n\n1. First you would look at what spam typically looks like. You might notice that\nsome words or phrases (such as \u201c4U,\u201d \u201ccredit card,\u201d \u201cfree,\u201d and \u201camazing\u201d) tend to\ncome up a lot in the subject. Perhaps you would also notice a few other patterns\nin the sender's name, the email\u2019s body, and so on.\n\n2. You would write a detection algorithm for each of the patterns that you noticed,\nand your program would flag emails as spam if a number of these patterns are\ndetected.\n\n3. You would test your program, and repeat steps 1 and 2 until it is good enough.\n\nFigure 1-1. The traditional approach\n\nSince the problem is not trivial, your program will likely become a long list of com-\nplex rules\u2014pretty hard to maintain.\n\nIn contrast, a spam filter based on Machine Learning techniques automatically learns\nwhich words and phrases are good predictors of spam by detecting unusually fre-\nquent patterns of words in the spam examples compared to the ham examples\n(Figure 1-2). The program is much shorter, easier to maintain, and most likely more\naccurate.\n\nFigure 1-2. Machine Learning approach\n\nMoreover, if spammers notice that all their emails containing \u201c4U\u201d are blocked, they\nmight start writing \u201cFor U\u201d instead. A spam filter using traditional programming\ntechniques would need to be updated to flag \u201cFor U\u201d emails. If spammers keep work-\ning around your spam filter, you will need to keep writing new rules forever.\n\nIn contrast, a spam filter based on Machine Learning techniques automatically noti-\nces that \u201cFor U\u201d has become unusually frequent in spam flagged by users, and it starts\nflagging them without your intervention (Figure 1-3).\n\nFigure 1-3. Automatically adapting to change\n\nAnother area where Machine Learning shines is for problems that either are too com-\nplex for traditional approaches or have no known algorithm. For example, consider\nspeech recognition: say you want to start simple and write a program capable of dis-\ntinguishing the words \u201cone\u201d and \u201ctwo.\u201d You might notice that the word \u201ctwo\u201d starts\nwith a high-pitch sound (\u201cI\u201d), so you could hardcode an algorithm that measures\nhigh-pitch sound intensity and use that to distinguish ones and twos. Obviously this\ntechnique will not scale to thousands of words spoken by millions of very different\n\npeople in noisy environments and in dozens of languages. The best solution (at least\ntoday) is to write an algorithm that learns by itself, given many example recordings\nfor each word.\n\nFinally, Machine Learning can help humans learn (Figure 1-4): ML algorithms can be\ninspected to see what they have learned (although for some algorithms this can be\ntricky). For instance, once the spam filter has been trained on enough spam, it can\neasily be inspected to reveal the list of words and combinations of words that it\nbelieves are the best predictors of spam. Sometimes this will reveal unsuspected cor-\nrelations or new trends, and thereby lead to a better understanding of the problem.\n\nApplying ML techniques to dig into large amounts of data can help discover patterns\nthat were not immediately apparent. This is called data mining.\n\nSolution\n\nlterate if needed }---\n\nUnderstand the\nproblem better\n\nFigure 1-4. Machine Learning can help humans learn\n\n\u2018To summarize, Machine Learning is great for:\n\n+ Problems for which existing solutions require a lot of hand-tuning or long lists of\nrules: one Machine Learning algorithm can often simplify code and perform bet-\nter.\n\n+ Complex problems for which there is no good solution at all using a traditional\napproach: the best Machine Learning techniques can find a solution.\n\n+ Fluctuating environments: a Machine Learning system can adapt to new data.\n\n+ Getting insights about complex problems and large amounts of data.\n\nTypes of Machine Learning Systems\n\nThere are so many different types of Machine Learning systems that it is useful to\nclassify them in broad categories based on:\n\n+ Whether or not they are trained with human supervision (supervised, unsuper-\nvised, semisupervised, and Reinforcement Learning)\n\n+ Whether or not they can learn incrementally on the fly (online versus batch\nlearning)\n\n\u00ab Whether they work by simply comparing new data points to known data points,\nor instead detect patterns in the training data and build a predictive model, much\nlike scientists do (instance-based versus model-based learning)\n\nThese criteria are not exclusive; you can combine them in any way you like. For\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net-\nwork model trained using examples of spam and ham; this makes it an online, model-\nbased, supervised learning system.\n\nLet\u2019s look at each of these criteria a bit more closely.\n\nSupervised/Unsupervised Learning\n\nMachine Learning systems can be classified according to the amount and type of\nsupervision they get during training. There are four major categories: supervised\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn-\ning.\n\nSupervised learning\n\nIn supervised learning, the training data you feed to the algorithm includes the desired\nsolutions, called labels (Figure 1-5).\n\nTraining set\n\nOw New instance\n\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n\n\nA typical supervised learning task is classification. The spam filter is a good example\nof this: it is trained with many example emails along with their class (spam or ham),\nand it must learn how to classify new emails.\n\nAnother typical task is to predict a target numeric value, such as the price of a car,\ngiven a set of features (mileage, age, brand, etc.) called predictors. This sort of task is\ncalled regression (Figure 1-6).' To train the system, you need to give it many examples\nof cars, including both their predictors and their labels (i.e., their prices).\n\nIn Machine Learning an attribute is a data type (e.g., \u201cMileage\u201d),\nwhile a feature has several meanings depending on the context, but\ngenerally means an attribute plus its value (e.g., \u201cMileage =\n15,000\u201d). Many people use the words attribute and feature inter-\nchangeably, though.\n\nValue\n\nfe)\noO 20%?\n\nO.0 OO eke)\noe) 2% 0\u00b0\nfe)\n\neke) %\n12) Value?\n\nFeature 1\n\nNew instance\n\nFigure 1-6. Regression\n\nNote that some regression algorithms can be used for classification as well, and vice\nversa. For example, Logistic Regression is commonly used for classification, as it can\noutput a value that corresponds to the probability of belonging to a given class (e.g.,\n20% chance of being spam).\n\n1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\nfact that the children of tall people tend to be shorter than their parents. Since children were shorter, he called\nthis regression to the mean. This name was then applied to the methods he used to analyze correlations\nbetween variables.\n\nHere are some of the most important supervised learning algorithms (covered in this\nbook):\n\n+ k-Nearest Neighbors\n\n\u00ab Linear Regression\n\n+ Logistic Regression\n\n+ Support Vector Machines (SVMs)\n\n+ Decision Trees and Random Forests\n\n+ Neural networks?\n\nUnsupervised learning\n\nIn unsupervised learning, as you might guess, the training data is unlabeled\n(Figure 1-7). The system tries to learn without a teacher.\n\nTraining set\n\nFigure 1-7. An unlabeled training set for unsupervised learning\n\nHere are some of the most important unsupervised learning algorithms (most of\nthese are covered in Chapter 8 and Chapter 9):\n+ Clustering\n\u2014 K-Means\n\u2014 DBSCAN\n\u2014 Hierarchical Cluster Analysis (HCA)\n+ Anomaly detection and novelty detection\n\u2014 One-class SVM\n\n\u2014 Isolation Forest\n\n2 Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann\nmachines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining.\n\n+ Visualization and dimensionality reduction\n\n\u2014 Principal Component Analysis (PCA)\n\n\u2014 Kernel PCA\n\n\u2014 Locally-Linear Embedding (LLE)\n\n\u2014 t-distributed Stochastic Neighbor Embedding (t-SNE)\n+ Association rule learning\n\n\u2014 Apriori\n\n\u2014 Eclat\n\nFor example, say you have a lot of data about your blog's visitors. You may want to\nrun a clustering algorithm to try to detect groups of similar visitors (Figure 1-8). At\nno point do you tell the algorithm which group a visitor belongs to: it finds those\nconnections without your help. For example, it might notice that 40% of your visitors\nare males who love comic books and generally read your blog in the evening, while\n20% are young sci-fi lovers who visit during the weekends, and so on. If you use a\nhierarchical clustering algorithm, it may also subdivide each group into smaller\ngroups. This may help you target your posts for each group.\n\nFigure 1-8. Clustering\n\nVisualization algorithms are also good examples of unsupervised learning algorithms:\nyou feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep-\nresentation of your data that can easily be plotted (Figure 1-9). These algorithms try\nto preserve as much structure as they can (e.g., trying to keep separate clusters in the\ninput space from overlapping in the visualization), so you can understand how the\ndata is organized and perhaps identify unsuspected patterns.\n\nFigure 1-9. Example of a t-SNE visualization highlighting semantic clusters\u2019\n\nA related task is dimensionality reduction, in which the goal is to simplify the data\nwithout losing too much information. One way to do this is to merge several correla-\nted features into one. For example, a car\u2019s mileage may be very correlated with its age,\nso the dimensionality reduction algorithm will merge them into one feature that rep-\nresents the car\u2019s wear and tear. This is called feature extraction.\n\nIt is often a good idea to try to reduce the dimension of your train-\ning data using a dimensionality reduction algorithm before you\nfeed it to another Machine Learning algorithm (such as a super-\nvised learning algorithm). It will run much faster, the data will take\nup less disk and memory space, and in some cases it may also per-\nform better.\n\nYet another important unsupervised task is anomaly detection\u2014for example, detect-\ning unusual credit card transactions to prevent fraud, catching manufacturing defects,\nor automatically removing outliers from a dataset before feeding it to another learn-\ning algorithm. The system is shown mostly normal instances during training, so it\nlearns to recognize them and when it sees a new instance it can tell whether it looks\n\n3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds,\nand so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual-\nization of the semantic word space\u201d\n\nlike a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar\ntask is novelty detection: the difference is that novelty detection algorithms expect to\nsee only normal data during training, while anomaly detection algorithms are usually\nmore tolerant, they can often perform well even with a small percentage of outliers in\nthe training set.\n\nFeature 2\nNew instances\n\nAnomaly x :\ne\n\nFeature 1\n\nFigure 1-10. Anomaly detection\n\nFinally, another common unsupervised task is association rule learning, in which the\ngoal is to dig into large amounts of data and discover interesting relations between\nattributes. For example, suppose you own a supermarket. Running an association rule\non your sales logs may reveal that people who purchase barbecue sauce and potato\nchips also tend to buy steak. Thus, you may want to place these items close to each\nother.\n\nSemisupervised learning\n\nSome algorithms can deal with partially labeled training data, usually a lot of unla-\nbeled data and a little bit of labeled data. This is called semisupervised learning\n(Figure 1-11).\n\nSome photo-hosting services, such as Google Photos, are good examples of this. Once\nyou upload all your family photos to the service, it automatically recognizes that the\nsame person A shows up in photos 1, 5, and 11, while another person B shows up in\nphotos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all\nthe system needs is for you to tell it who these people are. Just one label per person,*\nand it is able to name everyone in every photo, which is useful for searching photos.\n\n4 That's when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\nmixes up two people who look alike, so you need to provide a few labels per person and manually clean up\nsome clusters.\n\nFigure 1-11. Semisupervised learning\n\nMost semisupervised learning algorithms are combinations of unsupervised and\nsupervised algorithms. For example, deep belief networks (DBNs) are based on unsu-\npervised components called restricted Boltzmann machines (RBMs) stacked on top of\none another. RBMs are trained sequentially in an unsupervised manner, and then the\nwhole system is fine-tuned using supervised learning techniques.\n\nReinforcement Learning\n\nReinforcement Learning is a very different beast. The learning system, called an agent\nin this context, can observe the environment, select and perform actions, and get\nrewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It\nmust then learn by itself what is the best strategy, called a policy, to get the most\nreward over time. A policy defines what action the agent should choose when it is ina\ngiven situation.\n\ni >\nEnvironment 2 ? Agent \u00b0 Observe\n\nSelect action\nusing policy\n\n\u00a9 Action:\n\nGet reward\nor penalty\n\nUpdate policy\n(learning step)\nIterate until an\n\nfc] optimal policy is\nfound\n\nFigure 1-12. Reinforcement Learning\n\nFor example, many robots implement Reinforcement Learning algorithms to learn\nhow to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\nat the game of Go. It learned its winning policy by analyzing millions of games, and\nthen playing many games against itself. Note that learning was turned off during the\ngames against the champion; AlphaGo was just applying the policy it had learned.\n\nBatch and Online Learning\n\nAnother criterion used to classify Machine Learning systems is whether or not the\nsystem can learn incrementally from a stream of incoming data.\n\nBatch learning\n\nIn batch learning, the system is incapable of learning incrementally: it must be trained\nusing all the available data. This will generally take a lot of time and computing\nresources, so it is typically done offline. First the system is trained, and then it is\nlaunched into production and runs without learning anymore; it just applies what it\nhas learned. This is called offline learning.\n\nIf you want a batch learning system to know about new data (such as a new type of\nspam), you need to train a new version of the system from scratch on the full dataset\n(not just the new data, but also the old data), then stop the old system and replace it\nwith the new one.\n\nFortunately, the whole process of training, evaluating, and launching a Machine\nLearning system can be automated fairly easily (as shown in Figure 1-3), so even a\n\nbatch learning system can adapt to change. Simply update the data and train a new\nversion of the system from scratch as often as needed.\n\nThis solution is simple and often works fine, but training using the full set of data can\ntake many hours, so you would typically train a new system only every 24 hours or\neven just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre-\ndict stock prices), then you need a more reactive solution.\n\nAlso, training on the full set of data requires a lot of computing resources (CPU,\nmemory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\nyou automate your system to train from scratch every day, it will end up costing you a\nlot of money. If the amount of data is huge, it may even be impossible to use a batch\nlearning algorithm.\n\nFinally, if your system needs to be able to learn autonomously and it has limited\nresources (e.g., a smartphone application or a rover on Mars), then carrying around\nlarge amounts of training data and taking up a lot of resources to train for hours\nevery day is a showstopper.\n\nFortunately, a better option in all these cases is to use algorithms that are capable of\nlearning incrementally.\n\nOnline learning\n\nIn online learning, you train the system incrementally by feeding it data instances\nsequentially, either individually or by small groups called mini-batches. Each learning\nstep is fast and cheap, so the system can learn about new data on the fly, as it arrives\n(see Figure 1-13).\n\nFigure 1-13. Online learning\n\nOnline learning is great for systems that receive data as a continuous flow (e.g., stock\nprices) and need to adapt to change rapidly or autonomously. It is also a good option\n\nif you have limited computing resources: once an online learning system has learned\nabout new data instances, it does not need them anymore, so you can discard them\n(unless you want to be able to roll back to a previous state and \u201creplay\u201d the data). This\ncan save a huge amount of space.\n\nOnline learning algorithms can also be used to train systems on huge datasets that\ncannot fit in one machine's main memory (this is called out-of-core learning). The\nalgorithm loads part of the data, runs a training step on that data, and repeats the\nprocess until it has run on all of the data (see Figure 1-14).\n\nOut-of-core learning is usually done offline (ie., not on the live\nsystem), so online learning can be a confusing name. Think of it as\nincremental learning.\n\na7, ge\n\n\u201cLots* of data\n\nStudy the\nproblem\n\nFigure 1-14. Using online learning to handle huge datasets\n\nOne important parameter of online learning systems is how fast they should adapt to\nchanging data: this is called the learning rate. If you set a high learning rate, then your\nsystem will rapidly adapt to new data, but it will also tend to quickly forget the old\ndata (you don't want a spam filter to flag only the latest kinds of spam it was shown).\nConversely, if you set a low learning rate, the system will have more inertia; that is, it\nwill learn more slowly, but it will also be less sensitive to noise in the new data or to\nsequences of nonrepresentative data points (outliers).\n\nA big challenge with online learning is that if bad data is fed to the system, the sys-\ntem\u2019s performance will gradually decline. If we are talking about a live system, your\nclients will notice. For example, bad data could come from a malfunctioning sensor\non a robot, or from someone spamming a search engine to try to rank high in search\n\nresults. To reduce this risk, you need to monitor your system closely and promptly\nswitch learning off (and possibly revert to a previously working state) if you detect a\ndrop in performance. You may also want to monitor the input data and react to\nabnormal data (e.g., using an anomaly detection algorithm).\n\nInstance-Based Versus Model-Based Learning\n\nOne more way to categorize Machine Learning systems is by how they generalize.\nMost Machine Learning tasks are about making predictions. This means that given a\nnumber of training examples, the system needs to be able to generalize to examples it\nhas never seen before. Having a good performance measure on the training data is\ngood, but insufficient; the true goal is to perform well on new instances.\n\nThere are two main approaches to generalization: instance-based learning and\nmodel-based learning.\n\nInstance-based learning\n\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\ncreate a spam filter this way, it would just flag all emails that are identical to emails\nthat have already been flagged by users\u2014not the worst solution, but certainly not the\nbest.\n\nInstead of just flagging emails that are identical to known spam emails, your spam\nfilter could be programmed to also flag emails that are very similar to known spam\nemails. This requires a measure of similarity between two emails. A (very basic) simi-\nlarity measure between two emails could be to count the number of words they have\nin common. The system would flag an email as spam if it has many words in com-\nmon with a known spam email.\n\nThis is called instance-based learning: the system learns the examples by heart, then\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\nthem), using a similarity measure. For example, in Figure 1-15 the new instance\nwould be classified as a triangle because the majority of the most similar instances\nbelong to that class.\n\nFeature 2\n\nFeature 1\nFigure 1-15. Instance-based learning\nModel-based learning\n\nAnother way to generalize from a set of examples is to build a model of these exam-\n\nples, then use that model to make predictions. This is called model-based learning\n(Figure 1-16).\n\nFeature 2 Model \\\nA A \u2018. Oo\nA A oO\nA WN\nA A a0\nA\n\nWN instance\n\nFeature 1\n\nFigure 1-16. Model-based learning\n\nFor example, suppose you want to know if money makes people happy, so you down-\nload the Better Life Index data from the OECD's website as well as stats about GDP\nper capita from the IMF's website. Then you join the tables and sort by GDP per cap-\nita. Table 1-1 shows an excerpt of what you get.\n\n\nTable 1-1. Does money make people happier?\nCountry GDP per capita (USD) Life satisfaction\n\nHungary 12,240 49\nKorea 27,195 58\nFrance 37,675 65\nAustralia 50,962 73\nUnited States 55,805 72\n\nLet\u2019s plot the data for a few random countries (Figure 1-17).\n\n10\nc\n6 8\nEs} . i \u201c3\n& 6 te % 2 of\n2 ee\nOo 4 US.\nBH Australia\n2 France\n5 Korea\n\nHungary\n\n0 + + Y + r\n0 10000 20000 30000 40000 50000 60000\nGDP per capita\n\nFigure 1-17. Do you see a trend here?\n\nThere does seem to be a trend here! Although the data is noisy (i.e., partly random), it\nlooks like life satisfaction goes up more or less linearly as the country\u2019s GDP per cap-\nita increases. So you decide to model life satisfaction as a linear function of GDP per\ncapita. This step is called model selection: you selected a linear model of life satisfac-\ntion with just one attribute, GDP per capita (Equation 1-1).\n\nEquation 1-1. A simple linear model\nlife_satisfaction = 4 + 4, x GDP_per_capita\n\nThis model has two model parameters, 0, and @,.\u00b0 By tweaking these parameters, you\ncan make your model represent any linear function, as shown in Figure 1-18.\n\nLife satisfaction\n\n0 + 1 + ,\n\u00a9 10000 20000 30000 40000 50000 60000\nGDP per capita\n\nFigure 1-18. A few possible linear models\n\nBefore you can use your model, you need to define the parameter values @, and @,.\nHow can you know which values will make your model perform best? To answer this\nquestion, you need to specify a performance measure. You can either define a utility\nfunction (or fitness function) that measures how good your model is, or you can define\na cost function that measures how bad it is. For linear regression problems, people\ntypically use a cost function that measures the distance between the linear model's\npredictions and the training examples; the objective is to minimize this distance.\n\nThis is where the Linear Regression algorithm comes in: you feed it your training\nexamples and it finds the parameters that make the linear model fit best to your data.\n\nThis is called training the model. In our case the algorithm finds that the optimal\nparameter values are @, = 4.85 and 6, = 4.91 x 10\u00b0.\n\nNow the model fits the training data as closely as possible (for a linear model), as you\ncan see in Figure 1-19.\n\nLife satisfaction\n\n0 . . . .\n0 10000 20000 30000 40000 50000 60000\nGDP per capita\n\nFigure 1-19. The linear model that fits the training data best\n\nYou are finally ready to run the model to make predictions. For example, say you\nwant to know how happy Cypriots are, and the OECD data does not have the answer.\nFortunately, you can use your model to make a good prediction: you look up Cyprus\u2019s\nGDP per capita, find $22,587, and then apply your model and find that life satisfac-\ntion is likely to be somewhere around 4.85 + 22,587 x 4.91 x 10\u00b0 = 5.96.\n\nTo whet your appetite, Example 1-1 shows the Python code that loads the data, pre-\npares it, creates a scatterplot for visualization, and then trains a linear model and\nmakes a prediction.\u201d\n\nExample 1-1. Training and running a linear model using Scikit-Learn\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport pandas as pd\n\nimport sklearn.linear_model\n\n# Load the data\n\noecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\n\ngdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\nencoding='latini', na_values=\"n/a\")\n\n# Prepare the data\n\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\nX = np.c_[country_stats[\"GDP per capita\"]]\n\ny = np.c_[country_stats[\"Life satisfaction\"]]\n\n# Visualize the data\ncountry_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\nplt.show()\n\n# Select a linear model\nmodel = sklearn.linear_model.LinearRegression()\n\n# Train the model\nmodel.fit(X, y)\n\n# Make a prediction for Cyprus\nX_new = [[22587]]  # Cyprus\u2018 GDP per capita\nprint(model.predict(X_new)) # outputs [[ 5.96242338]]\n\n6 The prepare_country_stats() function's definition is not shown here (see this chapter's Jupyter notebook if\nyou want all the gory details). It's just boring Pandas code that joins the life satisfaction data from the OBCD\nwith the GDP per capita data from the IMF.\n\n\nIf all went well, your model will make good predictions. If not, you may need to use\nmore attributes (employment rate, health, air pollution, etc.), get more or better qual-\nity training data, or perhaps select a more powerful model (e.g., a Polynomial Regres-\nsion model).\n\nIn summary:\n\nThis is what a typical Machine Learning project looks like. In Chapter 2 you will\n\nIf you had used an instance-based learning algorithm instead, you\nwould have found that Slovenia has the closest GDP per capita to\nthat of Cyprus ($20,732), and since the OECD data tells us that\nSlovenians\u2019 life satisfaction is 5.7, you would have predicted a life\nsatisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\ntwo next closest countries, you will find Portugal and Spain with\nlife satisfactions of 5.1 and 6.5, respectively. Averaging these three\nvalues, you get 5.77, which is pretty close to your model-based pre-\ndiction. This simple algorithm is called k-Nearest Neighbors regres-\nsion (in this example, k = 3).\n\nReplacing the Linear Regression model with k-Nearest Neighbors\n\nregression in the previous code is as simple as replacing these two\nlines:\n\nimport sklearn.linear_model\n\nmodel = sklearn.linear_model.LinearRegression()\nwith these two:\n\nimport sklearn.neighbors\nmodel = sklearn.neighbors.KkNeighborsRegressor (n_neighbors=3)\n\nYou studied the data.\nYou selected a model.\n\nYou trained it on the training data (ie., the learning algorithm searched for the\nmodel parameter values that minimize a cost function).\n\nFinally, you applied the model to make predictions on new cases (this is called\ninference), hoping that this model will generalize well.\n\nexperience this first-hand by going through an end-to-end project.\n\nWe have covered a lot of ground so far: you now know what Machine Learning is\nreally about, why it is useful, what some of the most common categories of ML sys-\ntems are, and what a typical project workflow looks like. Now let's look at what can go\n\nwrong in learning and prevent you from making accurate predictions.\n\nMain Challenges of Machine Learning\n\nIn short, since your main task is to select a learning algorithm and train it on some\ndata, the two things that can go wrong are \u201cbad algorithm\u201d and \u201cbad data\u201d Let's start\nwith examples of bad data.\n\nInsufficient Quantity of Training Data\n\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\nsay \u201capple\u201d (possibly repeating this procedure a few times). Now the child is able to\nrecognize apples in all sorts of colors and shapes. Genius.\n\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn-\ning algorithms to work properly. Even for very simple problems you typically need\nthousands of examples, and for complex problems such as image or speech recogni-\ntion you may need millions of examples (unless you can reuse parts of an existing\nmodel).\n\nThe Unreasonable Effectiveness of Data\n\nIn a famous paper published in 2001, Microsoft researchers Michele Banko and Eric\nBrill showed that very different Machine Learning algorithms, including fairly simple\nones, performed almost identically well on a complex problem of natural language\ndisambiguation\u2018 once they were given enough data (as you can see in Figure 1-20).\n\n1.00\n\nMillions of Words\n\nFigure 1-20. The importance of data versus algorithms\u00ae\n\nAs the authors put it: \u201cthese results suggest that we may want to reconsider the trade-\noff between spending time and money on algorithm development versus spending it\non corpus development.\u201d\n\nThe idea that data matters more than algorithms for complex problems was further\npopularized by Peter Norvig et al. in a paper titled \u201cThe Unreasonable Effectiveness\nof Data\u201d published in 2009.\"\u00b0 It should be noted, however, that small- and medium-\nsized datasets are still very common, and it is not always easy or cheap to get extra\ntraining data, so don't abandon algorithms just yet.\n\n\nNonrepresentative Training Data\n\nIn order to generalize well, it is crucial that your training data be representative of the\nnew cases you want to generalize to. This is true whether you use instance-based\nlearning or model-based learning.\n\nFor example, the set of countries we used earlier for training the linear model was not\nperfectly representative; a few countries were missing. Figure 1-21 shows what the\ndata looks like when you add the missing countries.\n\n10\n\nBrazil = Mexico Chile Czech Republic\n\nc\no . - .\n\u00a9 6 were \\\nDd eee . -\n2\no 4\nbe Norway Switzerland Luxembourg\n2\n5 2\n0 + r + -\n0 20000 40000 60000 80000 100000\n\nGDP per capita\n\nFigure 1-21. A more representative training sample\n\nIf you train a linear model on this data, you get the solid line, while the old model is\nrepresented by the dotted line. As you can see, not only does adding a few missing\ncountries significantly alter the model, but it makes it clear that such a simple linear\nmodel is probably never going to work well. It seems that very rich countries are not\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\nsome poor countries seem happier than many rich countries.\n\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\naccurate predictions, especially for very poor and very rich countries.\n\nIt is crucial to use a training set that is representative of the cases you want to general-\nize to. This is often harder than it sounds: if the sample is too small, you will have\nsampling noise (i.e., nonrepresentative data as a result of chance), but even very large\nsamples can be nonrepresentative if the sampling method is flawed. This is called\nsampling bias.\n\nInstead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest\u2019s\nsampling method:\n\n+ First, to obtain the addresses to send the polls to, the Literary Digest used tele-\nphone directories, lists of magazine subscribers, club membership lists, and the\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\nRepublican (hence Landon).\n\n+ Second, less than 25% of the people who received the poll answered. Again, this\nintroduces a sampling bias, by ruling out people who don't care much about poli-\ntics, people who don't like the Literary Digest, and other key groups. This is a spe-\ncial type of sampling bias called nonresponse bias.\n\nHere is another example: say you want to build a system to recognize funk music vid-\neos. One way to build your training set is to search \u201cfunk music\u201d on YouTube and use\nthe resulting videos. But this assumes that YouTube's search engine returns a set of\nvideos that are representative of all the funk music videos on YouTube. In reality, the\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\nyou will get a lot of \u201cfunk carioca\u201d videos, which sound nothing like James Brown).\nOn the other hand, how else can you get a large training set?\n\nPoor-Quality Data\n\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\nquality measurements), it will make it harder for the system to detect the underlying\npatterns, so your system is less likely to perform well. It is often well worth the effort\nto spend time cleaning up your training data. The truth is, most data scientists spend\na significant part of their time doing just that. For example:\n\n+ Ifsome instances are clearly outliers, it may help to simply discard them or try to\nfix the errors manually.\n\n+ If some instances are missing a few features (e.g., 5% of your customers did not\nspecify their age), you must decide whether you want to ignore this attribute alto-\ngether, ignore these instances, fill in the missing values (e.g., with the median\nage), or train one model with the feature and one model without it, and so on.\n\nIrrelevant Features\n\nAs the saying goes: garbage in, garbage out. Your system will only be capable of learn-\ning if the training data contains enough relevant features and not too many irrelevant\nones. A critical part of the success of a Machine Learning project is coming up with a\ngood set of features to train on. This process, called feature engineering, involves:\n\n+ Feature selection: selecting the most useful features to train on among existing\nfeatures.\n\n+ Feature extraction: combining existing features to produce a more useful one (as\nwe saw earlier, dimensionality reduction algorithms can help).\n\n+ Creating new features by gathering new data.\n\nNow that we have looked at many examples of bad data, let\u2019s look at a couple of exam-\nples of bad algorithms.\n\nOverfitting the Training Data\n\nSay you are visiting a foreign country and the taxi driver rips you off. You might be\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\nsomething that we humans do all too often, and unfortunately machines can fall into\nthe same trap if we are not careful. In Machine Learning this is called overfitting: it\nmeans that the model performs well on the training data, but it does not generalize\nwell.\n\nFigure 1-22 shows an example of a high-degree polynomial life satisfaction model\nthat strongly overfits the training data. Even though it performs much better on the\ntraining data than the simple linear model, would you really trust its predictions?\n\n10\n\nLife satisfaction\n\n0 20000 40000 60000 80000 100000\nGDP per capita\n\nFigure 1-22. Overfitting the training data\n\nComplex models such as deep neural networks can detect subtle patterns in the data,\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\nwill not generalize to new instances. For example, say you feed your life satisfaction\nmodel many more attributes, including uninformative ones such as the country\u2019s\nname. In that case, a complex model may detect patterns like the fact that all coun-\ntries in the training data with a w in their name have a life satisfaction greater than 7:\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\n\nare you that the W-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\nthis pattern occurred in the training data by pure chance, but the model has no way\nto tell whether a pattern is real or simply the result of noise in the data.\n\nOverfitting happens when the model is too complex relative to the\namount and noisiness of the training data. The possible solutions\nare:\n\n+ To simplify the model by selecting one with fewer parameters\n(e.g., a linear model rather than a high-degree polynomial\nmodel), by reducing the number of attributes in the training\ndata or by constraining the model\n\n+ To gather more training data\n\n+ To reduce the noise in the training data (e.g., fix data errors\nand remove outliers)\n\nConstraining a model to make it simpler and reduce the risk of overfitting is called\nregularization. For example, the linear model we defined earlier has two parameters,\n9, and 6,. This gives the learning algorithm two degrees of freedom to adapt the model\nto the training data: it can tweak both the height (@,) and the slope (6,) of the line. If\nwe forced 6, = 0, the algorithm would have only one degree of freedom and would\nhave a much harder time fitting the data properly: all it could do is move the line up\nor down to get as close as possible to the training instances, so it would end up\naround the mean. A very simple model indeed! If we allow the algorithm to modify 0,\nbut we force it to keep it small, then the learning algorithm will effectively have some-\nwhere in between one and two degrees of freedom. It will produce a simpler model\nthan with two degrees of freedom, but more complex than with just one. You want to\nfind the right balance between fitting the training data perfectly and keeping the\nmodel simple enough to ensure that it will generalize well.\n\nFigure 1-23 shows three models: the dotted line represents the original model that\nwas trained with a few countries missing, the dashed line is our second model trained\nwith all countries, and the solid line is a linear model trained with the same data as\nthe first model but with a regularization constraint. You can see that regularization\nforced the model to have a smaller slope, which fits a bit less the training data that the\nmodel was trained on, but actually allows it to generalize better to new examples.\n\n~-- Linear model on all data\nPS Od Linear model on partial data\n\u2014 Regularized linear model on partial data\n\nLife satisfaction\n\n0 20000 40000 60000 80000 100000\nGDP per capita\n\nFigure 1-23. Regularization reduces the risk of overfitting\n\nThe amount of regularization to apply during learning can be controlled by a hyper-\nparameter. A hyperparameter is a parameter of a learning algorithm (not of the\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\nto training and remains constant during training. If you set the regularization hyper-\nparameter to a very large value, you will get an almost flat model (a slope close to\nzero); the learning algorithm will almost certainly not overfit the training data, but it\nwill be less likely to find a good solution. Tuning hyperparameters is an important\npart of building a Machine Learning system (you will see a detailed example in the\nnext chapter).\n\nUnderfitting the Training Data\n\nAs you might guess, underfitting is the opposite of overfitting: it occurs when your\nmodel is too simple to learn the underlying structure of the data. For example, a lin-\near model of life satisfaction is prone to underfit; reality is just more complex than\nthe model, so its predictions are bound to be inaccurate, even on the training exam-\nples.\n\nThe main options to fix this problem are:\n\n+ Selecting a more powerful model, with more parameters\n+ Feeding better features to the learning algorithm (feature engineering)\n\n+ Reducing the constraints on the model (e.g., reducing the regularization hyper-\nparameter)\n\nStepping Back\n\nBy now you already know a lot about Machine Learning. However, we went through\nso many concepts that you may be feeling a little lost, so let\u2019s step back and look at the\nbig picture:\n\n\u00ab Machine Learning is about making machines get better at some task by learning\nfrom data, instead of having to explicitly code rules.\n\n+ There are many different types of ML systems: supervised or not, batch or online,\ninstance-based or model-based, and so on.\n\n+ Ina ML project you gather data in a training set, and you feed the training set to\na learning algorithm. If the algorithm is model-based it tunes some parameters to\nfit the model to the training set (i.e., to make good predictions on the training set\nitself), and then hopefully it will be able to make good predictions on new cases\nas well. If the algorithm is instance-based, it just learns the examples by heart and\ngeneralizes to new instances by comparing them to the learned instances using a\nsimilarity measure.\n\n+ The system will not perform well if your training set is too small, or if the data is\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\nout). Lastly, your model needs to be neither too simple (in which case it will\nunderfit) nor too complex (in which case it will overfit).\n\nThere's just one last important topic to cover: once you have trained a model, you\ndon't want to just \u201chope\u201d it generalizes to new cases. You want to evaluate it, and fine-\ntune it if necessary. Let's see how.\n\nTesting and Validating\n\nThe only way to know how well a model will generalize to new cases is to actually try\nit out on new cases. One way to do that is to put your model in production and moni-\ntor how well it performs. This works well, but if your model is horribly bad, your\nusers will complain\u2014not the best idea.\n\nA better option is to split your data into two sets: the training set and the test set. As\nthese names imply, you train your model using the training set, and you test it using\nthe test set. The error rate on new cases is called the generalization error (or out-of-\nsample error), and by evaluating your model on the test set, you get an estimate of this\nerror. This value tells you how well your model will perform on instances it has never\nseen before.\n\nIf the training error is low (i.e., your model makes few mistakes on the training set)\nbut the generalization error is high, it means that your model is overfitting the train-\ning data.\n\nIt is common to use 80% of the data for training and hold out 20%\nfor testing. However, this depends on the size of the dataset: if it\ncontains 10 million instances, then holding out 1% means your test\nset will contain 100,000 instances: that\u2019s probably more than\nenough to get a good estimate of the generalization error.\n\nHyperparameter Tuning and Model Selection\n\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi-\ntating between two models (say a linear model and a polynomial model): how can\nyou decide? One option is to train both and compare how well they generalize using\nthe test set.\n\nNow suppose that the linear model generalizes better, but you want to apply some\n\nregularization to avoid overfitting. The question is: how do you choose the value of\nthe regularization hyperparameter? One option is to train 100 different models using\n100 different values for this hyperparameter. Suppose you find the best hyperparame-\nter value that produces a model with the lowest generalization error, say just 5% error.\n\nSo you launch this model into production, but unfortunately it does not perform as\nwell as expected and produces 15% errors. What just happened?\n\nThe problem is that you measured the generalization error multiple times on the test\nset, and you adapted the model and hyperparameters to produce the best model for\nthat particular set. This means that the model is unlikely to perform as well on new\ndata.\n\nA common solution to this problem is called holdout validation: you simply hold out\npart of the training set to evaluate several candidate models and select the best one.\nThe new heldout set is called the validation set (or sometimes the development set, or\ndev set). More specifically, you train multiple models with various hyperparameters\non the reduced training set (i.e., the full training set minus the validation set), and\nyou select the model that performs best on the validation set. After this holdout vali-\ndation process, you train the best model on the full training set (including the valida-\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\nthe test set to get an estimate of the generalization error.\n\nThis solution usually works quite well. However, if the validation set is too small, then\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\nmistake. Conversely, if the validation set is too large, then the remaining training set\nwill be much smaller than the full training set. Why is this bad? Well, since the final\nmodel will be trained on the full training set, it is not ideal to compare candidate\nmodels trained on a much smaller training set. It would be like selecting the fastest\nsprinter to participate in a marathon. One way to solve this problem is to perform\nrepeated cross-validation, using many small validation sets. Each model is evaluated\nonce per validation set, after it is trained on the rest of the data. By averaging out all\nthe evaluations of a model, we get a much more accurate measure of its performance.\nHowever, there is a drawback: the training time is multiplied by the number of valida-\ntion sets.\n\nData Mismatch\n\nIn some cases, it is easy to get a large amount of data for training, but it is not per-\nfectly representative of the data that will be used in production. For example, suppose\nyou want to create a mobile app to take pictures of flowers and automatically deter-\nmine their species. You can easily download millions of pictures of flowers on the\nweb, but they won't be perfectly representative of the pictures that will actually be\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\npictures (i.e., actually taken with the app). In this case, the most important rule to\nremember is that the validation set and the test must be as representative as possible\nof the data you expect to use in production, so they should be composed exclusively\nof representative pictures: you can shuffle them and put half in the validation set, and\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\nsets). After training your model on the web pictures, if you observe that the perfor-\nmance of your model on the validation set is disappointing, you will not know\nwhether this is because your model has overfit the training set, or whether this is just\ndue to the mismatch between the web pictures and the mobile app pictures. One sol-\nution is to hold out part of the training pictures (from the web) in yet another set that\nAndrew Ng calls the train-dev set. After the model is trained (on the training set, not\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\nthe model is not overfitting the training set, so if performs poorly on the validation\nset, the problem must come from the data mismatch. You can try to tackle this prob-\nlem by preprocessing the web images to make them look more like the pictures that\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\nmodel performs poorly on the train-dev set, then the model must have overfit the\ntraining set, so you should try to simplify or regularize the model, get more training\ndata and clean up the training data, as discussed earlier.\n\nNo Free Lunch Theorem\n\nA model is a simplified version of the observations. The simplifications are meant to\ndiscard the superfluous details that are unlikely to generalize to new instances. How-\never, to decide what data to discard and what data to keep, you must make assump-\ntions. For example, a linear model makes the assumption that the data is\nfundamentally linear and that the distance between the instances and the straight line\nis just noise, which can safely be ignored.\n\nIn a famous 1996 paper,'! David Wolpert demonstrated that if you make absolutely\nno assumption about the data, then there is no reason to prefer one model over any\nother. This is called the No Free Lunch (NFL) theorem. For some datasets the best\n\n\nmodel is a linear model, while for other datasets it is a neural network. There is no\nmodel that is a priori guaranteed to work better (hence the name of the theorem). The\nonly way to know for sure which model is best is to evaluate them all. Since this is not\npossible, in practice you make some reasonable assumptions about the data and you\nevaluate only a few reasonable models. For example, for simple tasks you may evalu-\nate linear models with various levels of regularization, and for a complex problem you\nmay evaluate various neural networks.\n\n",
                        "extracted-code": "",
                        "gpt-text-processed": "With Early Release ebooks, you receive the author's unedited and preliminary writings, allowing you to engage with cutting-edge technologies long before the official publications are available. Below is the content that will constitute Chapter 1 in the final edition of the book.\n\nWhen the term \u201cMachine Learning\u201d is mentioned, many envision a robot\u2014perhaps a reliable assistant or, for some, a menacing Terminator. However, Machine Learning transcends mere science fiction; it is already a significant aspect of our lives. In reality, it has been utilized in specialized applications like Optical Character Recognition (OCR) for decades. The first Machine Learning application to break into the mainstream and improve the lives of millions was the spam filter, which gained prominence in the 1990s. While it may not be an advanced AI like Skynet, this technology has effectively learned and, as a result, users rarely have to manually flag emails as spam anymore. Since then, a plethora of Machine Learning applications have emerged, seamlessly enhancing products and services, from personalized recommendations to voice search capabilities.\n\nYou might wonder about the boundaries of Machine Learning. What does it actually mean for a machine to learn? If I download Wikipedia onto my computer, has it acquired knowledge? Is it now somehow \u201csmarter\u201d? This chapter aims to clarify what Machine Learning is and explores its potential applications.\n\nBefore we embark on a journey through the landscape of Machine Learning, we will familiarize ourselves with the foundational concepts: the distinctions between supervised and unsupervised learning, the differences between online and batch learning, and the contrasts between instance-based and model-based learning. We will also delve into the typical workflow of a Machine Learning project, highlight key challenges you might encounter, and discuss methods for evaluating and fine-tuning a Machine Learning system.\n\nThis chapter introduces essential concepts and terminology that every data scientist should grasp. It provides a high-level overview\u2014devoid of substantial code\u2014so it\u2019s crucial that you have a thoroughly clear understanding before progressing to the subsequent chapters. So, get comfortable and let\u2019s dive in!\n\nIf you are well-versed in the basics of Machine Learning, you might consider skipping ahead to Chapter 2. If you're uncertain, attempt to address all the questions at the end of this chapter before advancing.\n\n### What Is Machine Learning?\n\nMachine Learning encompasses the science (and art) of programming computers to learn from data.\n\nHere\u2019s a more broad definition:\n\n**Machine Learning is the field of study that empowers computers to learn without being explicitly programmed.**  \n\u2014Arthur Samuel, 1959\n\nAnd a more engineering-based definition:\n\n**A computer program learns from experience E related to a task T and a performance measure P if its performance on T, as indicated by P, improves with experience E.**  \n\u2014Tom Mitchell, 1997\n\nAs an illustration, consider your spam filter, which learns to identify spam by analyzing examples of spam emails (which users have flagged) alongside those of legitimate emails (commonly referred to as \u201cham\u201d). The data utilized for training is known as the training set, and each individual data point is termed a training instance or sample. Here, the task T is to flag spam in new emails, the experience E consists of the training data, and the performance measure P can be defined as the proportion of correctly classified emails. This measure, accuracy, is frequently adopted in classification tasks.\n\nConversely, if you merely download Wikipedia, your computer may hold vast amounts of data, but it does not become adept at any specific task. Hence, that process does not qualify as Machine Learning.\n\n### Why Use Machine Learning?\n\nLet\u2019s reflect on creating a spam filter using traditional programming methods (see Figure 1-1):\n\n1. Initially, you would analyze typical characteristics of spam emails. You might identify common words or phrases (like \u201c4U,\u201d \u201ccredit card,\u201d \u201cfree,\u201d and \u201camazing\u201d) in the subject lines and look for discernible patterns in sender names or email bodies.\n\n2. Next, you would develop a detection algorithm for each pattern noted, and your program would flag emails as spam if it identified a significant number of these patterns.\n\n3. Then, you would test your program, revising it through steps 1 and 2 until it meets your satisfaction.\n\n![Traditional Approach](Figure_1-1)\n\nDue to the problem's complexity, the resulting program may morph into an extensive list of intricate rules, which can be cumbersome to maintain.\n\nConversely, a spam filter employing Machine Learning techniques autonomously learns which terms and phrases serve as indicators of spam by identifying patterns in the spam messages, contrasting them with the ham examples (see Figure 1-2). This leads to a more concise and maintainable program that is likely more accurate.\n\n![Machine Learning Approach](Figure_1-2)\n\nFurthermore, if spammers realize that emails containing \u201c4U\u201d are being filtered, they might pivot to using \u201cFor U\u201d instead. A traditional algorithm would require updates to include this phrase as spam. Continuous updates would be necessary as spammers adapt. \n\nIn contrast, a Machine Learning-based spam filter would automatically recognize \u201cFor U\u201d as it gains frequency in spam reports from users, promptly flagging it without needing manual intervention (see Figure 1-3).\n\n![Automatically Adapting to Change](Figure_1-3)\n\nMachine Learning also excels in addressing problems that are either too complex for traditional programming methods or lack known algorithms. Take speech recognition as an example: suppose you wish to develop a program that can distinguish between the numbers \u201cone\u201d and \u201ctwo.\u201d You might observe that \u201ctwo\u201d starts with a higher pitch sound, so you could implement an algorithm to quantify pitch intensity to differentiate them. However, this approach does not scale easily with thousands of words spoken by a diverse group of individuals in chaotic acoustic environments with numerous languages. The more effective solution (for the present) would be to devise an algorithm that learns independently through numerous audio recordings of each word.\n\nLastly, Machine Learning can enhance human understanding (see Figure 1-4): ML algorithms provide insights into what they've learned (even though some algorithms may complicate this process). For example, when a spam filter has enough training on spam data, it's possible to review which terms and combinations of terms it identifies as being strong spam indicators. This exploration can unearth hidden correlations or emerging patterns, granting a deeper insight into the problem at hand.\n\nBy leveraging Machine Learning techniques to explore extensive datasets, one might uncover hidden patterns that were not immediately obvious. This process is known as data mining.\n\n*Solution*\n\nIterate if needed.\n\n*Understand the problem better*\n\n![Machine Learning Can Help Humans Learn](Figure_1-4)\n\n**In summary, Machine Learning is highly effective for:**\n\n- **Problems requiring extensive manual tuning or cumbersome sets of rules:** A single Machine Learning algorithm can consolidate code and enhance performance.\n- **Complex issues with no viable traditional solutions:** Advanced Machine Learning techniques can provide effective resolutions.\n- **Dynamic environments:** Machine Learning systems can adjust to incoming data.\n- **Gaining insights into intricate problems and copious data.**\n\n### Types of Machine Learning Systems\n\nThe diverse landscape of Machine Learning systems warrants classification into broad categories based on several criteria:\n\n- **Human supervision during training:** This includes supervised, unsupervised, semi-supervised, and Reinforcement Learning.\n- **Incremental learning capability:** Distinctions between online and batch learning.\n- **Method of learning:** Whether by comparing new data points with known ones or identifying patterns in training data to formulate predictive models (akin to a scientist\u2019s approach): instance-based versus model-based learning.\n\nThese classifications aren\u2019t mutually exclusive; various combinations are possible. For instance, a cutting-edge spam filter may learn incrementally through an online, model-based, supervised learning framework incorporating deep neural networks trained on spam and ham data.\n\nLet\u2019s delve into each classification criterion further.\n\n### Supervised vs. Unsupervised Learning\n\nMachine Learning systems are categorized based on the level and type of supervision applied during training. The main classifications are:\n\n- **Supervised Learning**\n- **Unsupervised Learning**\n- **Semi-supervised Learning**\n- **Reinforcement Learning**\n\n#### Supervised Learning\n\nIn supervised learning, the training dataset provided to the algorithm includes the desired outcomes or labels (see Figure 1-5).\n\n![Labeled Training Set for Supervised Learning](Figure_1-5)\n\nA common supervised learning task is classification; a spam filter serves as an ideal example. It is trained by utilizing example emails, categorized as either spam or ham, and learns to classify new emails accordingly.\n\nAnother frequent scenario is predicting a numeric value (e.g., car prices) based on specific features like mileage, age, and brand, which are designated as predictors. This scenario typifies regression (see Figure 1-6). To train the system effectively, numerous car examples, including both predictors and their respective prices as labels, need to be provided.\n\nIn Machine Learning, an attribute refers to a data type (for instance, \u201cMileage\u201d), whereas a feature encompasses a broader interpretation, typically combining an attribute with its value (e.g., \u201cMileage = 15,000\u201d). While many use the terms attribute and feature interchangeably, subtle distinctions do exist.\n\n![Regression](Figure_1-6)\n\nNotably, some regression methods can also function for classification tasks and vice-versa. For example, **Logistic Regression** is often employed for classification since it yields a probability value for class membership (e.g., a 20% chance of being categorized as spam).\n\n#### Key Supervised Learning Algorithms:\n\n- k-Nearest Neighbors\n- Linear Regression\n- Logistic Regression\n- Support Vector Machines (SVMs)\n- Decision Trees and Random Forests\n- Neural Networks\n\n#### Unsupervised Learning\n\nConversely, unsupervised learning training data is not labeled (see Figure 1-7). The system learns independently, without guidance.\n\n![Unlabeled Training Set for Unsupervised Learning](Figure_1-7)\n\nSome prominent unsupervised learning algorithms are covered in Chapters 8 and 9:\n\n- **Clustering:** \n  - K-Means\n  - DBSCAN\n  - Hierarchical Cluster Analysis (HCA)\n- **Anomaly Detection:**\n  - One-Class SVM\n  - Isolation Forest\n- **Visualization and Dimensionality Reduction:**\n  - Principal Component Analysis (PCA)\n  - Kernel PCA\n  - Locally-Linear Embedding (LLE)\n  - t-distributed Stochastic Neighbor Embedding (t-SNE)\n- **Association Rule Learning:**\n  - Apriori\n  - Eclat\n\nFor example, suppose you have a dataset detailing your blog visitors. You may opt to implement a clustering algorithm to identify groups of similar visitors (see Figure 1-8). The algorithm autonomously discerns visitor categories without being told whom each visitor belongs to. For instance, it might reveal that a significant portion of visitors are comic book enthusiasts visiting during the evenings, while another segment comprises young sci-fi fans visiting on weekends. If utilizing a hierarchical clustering algorithm, it may further dissect these groups into smaller subsets, aiding in tailoring your content more effectively.\n\n![Clustering](Figure_1-8)\n\nVisualization algorithms exemplify unsupervised learning as well: they process intricate, unlabeled data and yield 2D or 3D visual representations, facilitating plotting (see Figure 1-9). These algorithms aim to preserve the structure of the data (e.g., keeping distinct clusters in the input space from converging in the visual representation) to reveal how the data is organized, potentially uncovering hidden trends.\n\n![Example of a t-SNE Visualization Highlighting Semantic Clusters](Figure_1-9)\n\nDimensionality reduction seeks to simplify datasets while retaining as much information as possible. One method involves merging related features. For instance, vehicle mileage could correlate with age, allowing a dimensionality reduction algorithm to integrate them into a single feature representing the vehicle's overall wear and tear\u2014this process is referred to as feature extraction.\n\nBefore feeding a Machine Learning algorithm (like supervised learning) with your training data, it's often beneficial to apply dimensionality reduction, which can enhance processing speed, reduce memory and disk space requirements, and sometimes improve performance.\n\nAnother vital unsupervised task is anomaly detection\u2014this can involve identifying unusual credit card transactions to mitigate fraud, catching production flaws, or removing outliers from a dataset prior to analyzing it with another learning algorithm. During training, the system predominantly sees typical instances and learns to recognize them. When encountering a new instance, it can assess if it appears normal or if it is likely an anomaly (refer to Figure 1-10). The closely related concept of novelty detection is characterized by expecting solely normal training data, while anomaly detection algorithms typically exhibit tolerance, often performing adequately even with a small percentage of outliers in the training dataset.\n\n![Anomaly Detection](Figure_1-10)\n\nLastly, association rule learning aims to sift through expansive datasets to identify intriguing relationships between attributes. For instance, if you operate a supermarket, applying an association rule to your sales logs may reveal that customers buying barbecue sauce and potato chips are also likely to purchase steak, suggesting a strategic positioning of these items in your store.\n\n#### Semi-Supervised Learning\n\nSome algorithms can function with partially labeled training data, generally comprising a substantial amount of unlabeled data alongside a smaller labeled subset. This methodology is termed semi-supervised learning (see Figure 1-11).\n\nApplications such as Google Photos serve as practical examples of this approach. After uploading family photos, the service autonomously recognizes that person A appears in several images while person B appears in others. This clustering represents the unsupervised component, and subsequently, all that is required from the user is to label these individuals. Providing just a single label per person allows the system to identify everyone in the images, an asset for photo searches.\n\n![Semi-Supervised Learning](Figure 1-11)\n\nMost semi-supervised learning algorithms blend aspects of both supervised and unsupervised learning methods. For example, deep belief networks (DBNs) utilize unsupervised components known as restricted Boltzmann machines (RBMs), arranged in layers. These RBMs are trained in sequence without supervision, followed by comprehensive fine-tuning using supervised learning techniques.\n\n### Reinforcement Learning\n\nReinforcement Learning represents a distinct category, involving a learning system (called an agent) that can observe its environment, make and execute decisions, and receive feedback in the form of rewards or penalties (see Figure 1-12). The agent must independently learn to devise a strategy (or policy) that maximizes its expected rewards over time. This policy dictates which action the agent should undertake given a specific scenario.\n\n![Reinforcement Learning](Figure 1-12)\n\nRobots commonly employ Reinforcement Learning algorithms to master tasks like walking. A notable example is DeepMind's AlphaGo, which captured headlines after defeating world champion Ke Jie in Go back in May 2017. AlphaGo learned its winning strategies by examining millions of games and engaging in self-play. Importantly, when playing against the champion, it relied solely on the strategy it had previously mastered.\n\n### Batch vs. Online Learning\n\nAnother criterion for classifying Machine Learning systems is whether the model can learn incrementally from a stream of incoming data.\n\n#### Batch Learning\n\nIn batch learning, the system cannot incrementally learn; it requires training on the entire dataset. This process usually demands substantial time and computational power, commonly performed offline. After training, the system is deployed for practical use and ceases to learn, applying its trained knowledge. This is referred to as offline learning.\n\nIf you need to update a batch learning system with fresh data (e.g., new spam message types), you must retrain the system entirely on the complete dataset (old and new data) before replacing the existing setup.\n\nFortunately, the entire process of training, validating, and launching a Machine Learning system can be automated efficiently (illustrated in Figure 1-3), allowing even batch learning systems to adapt. Just update your dataset and retrain as necessary.\n\nThis method is straightforward and typically effective; however, training on extensive datasets can take significant time, and new systems may only be retrained every 24 hours or even weekly. If rapid adaptation to changing data is essential (e.g., stock price predictions), a more reactive method is required.\n\nFurthermore, extensive dataset training imposes an increased demand for computational resources (CPU, memory, disk space, disk I/O, network I/O, etc.). Automating daily retraining with a vast dataset can be costly and, in some instances, infeasible.\n\nIf your system must learn autonomously and has resource constraints (e.g., a smartphone app or a robotic rover), transporting massive training datasets and consuming extensive training hours daily can be impractical.\n\nFortunately, a more suitable solution in such scenarios is to utilize algorithms adept at learning incrementally.\n\n#### Online Learning\n\nIn online learning, models are updated incrementally as they are fed data instances one at a time or in small batches (mini-batches). Each learning step is quick and resource-light, enabling the system to learn about new data on the fly (see Figure 1-13).\n\n![Online Learning](Figure 1-13)\n\nOnline learning is excellent for systems receiving continuous data flows (like stock prices) that must adjust promptly or autonomously. It is also advantageous with limited computing resources because, once new data instances are processed, they can be discarded, conserving memory and storage (unless you wish to maintain a record for potential data replays).\n\nOnline learning algorithms can accommodate large datasets that do not fit into an individual machine's memory (known as out-of-core learning). This method involves loading data in segments, training on those segments sequentially until the entire dataset has been fully ingested (see Figure 1-14).\n\nOut-of-core learning typically occurs offline\u2014not running within the live system\u2014hence why the term \"online learning\" might be misleading. It's best viewed as a form of incremental learning.\n\n![Using Online Learning to Handle Huge Datasets](Figure 1-14)\n\nA critical aspect of online learning systems is determining the adaptation speed to changing data, known as the learning rate. A high learning rate allows rapid adaption but can lead to quickly forgetting older data (not ideal for spam filters that should recognize various spam types). Conversely, a low learning rate results in slower learning, making the system less susceptible to noise from new data or sequences with nonrepresentative data points (outliers).\n\nOne substantial hurdle in online learning is the impact of poor data quality. If subpar data is introduced to the system, its performance will steadily decline\u2014potentially noticeable to users in a live environment. Examples include erroneous sensor inputs on robots or attempts to manipulate search engine rankings through spamming. To mitigate this risk, diligent system monitoring is essential, alongside the ability to halt learning and revert to prior versions should performance slip. Employing anomaly detection algorithms can also guard against insufficiently vetted input data.\n\n### Instance-Based vs. Model-Based Learning\n\nAn additional method for categorizing Machine Learning systems is based on their generalization techniques. The primary goal of most Machine Learning tasks is making predictions. Given multiple training examples, the system must generalize to unfamiliar instances. While obtaining favorable performance metrics on training data is beneficial, the ultimate goal is to excel on new inputs.\n\nTwo main generalization strategies exist: instance-based learning and model-based learning.\n\n#### Instance-Based Learning\n\nPossibly the simplest learning form involves direct memorization. For instance, a spam filter could be designed to flag emails identifying those identical to previously marked spam\u2014not an optimal approach.\n\nInstead of merely recognizing identical spam messages, the spam filter could be programmed to detect those closely mirroring spam emails. This necessitates defining similarity measures between emails. A basic approach could involve counting the shared words. Emails will be flagged when they share a significant number of words with known spam.\n\nThis process is termed instance-based learning, where the system learns by memorizing examples and generalizes to new information by comparing it to these learned instances (or a subset thereof). For instance, in Figure 1-15, a new instance is classified as a triangle based on the majority of similar past instances belonging to that category.\n\n![Instance-Based Learning](Figure 1-15)\n\n#### Model-Based Learning\n\nAn alternative approach to generalization involves developing a model from the provided examples and using that model to make predictions. This is called model-based learning (refer to Figure 1-16).\n\n![Model-Based Learning](Figure 1-16)\n\nAs an example, let's explore whether financial prosperity correlates to personal happiness. You could retrieve the Better Life Index data from the OECD and GDP per capita statistics from the IMF, joining and sorting the tables by GDP per capita. Table 1-1 illustrates a snippet of the resulting dataset.\n\n**Table 1-1: Does Money Make People Happier?**\n\n| Country       | GDP per Capita (USD) | Life Satisfaction |\n|---------------|-----------------------|-------------------|\n| Hungary       | 12,240                | 4.9               |\n| Korea         | 27,195                | 5.8               |\n| France        | 37,675                | 6.5               |\n| Australia     | 50,962                | 7.3               |\n| United States  | 55,805                | 7.2               |\n\nTo visually analyze the data, we can graph the life satisfaction against GDP for several countries (see Figure 1-17).\n\n![Do You See a Trend Here?](Figure 1-17)\n\nA noticeable trend appears! Though the data fluctuates (containing random elements), it seems that life satisfaction tends to rise in line with GDP per capita. Thus, you could model life satisfaction as a linear function of GDP per capita. This step is known as model selection: you opt for a linear model correlating life satisfaction with a single attribute, GDP per capita (Equation 1-1).\n\n**Equation 1-1: A Simple Linear Model**\n\n**life_satisfaction = 4 + 4.91 x GDP_per_capita**\n\nThis model consists of two parameters: \u03b20 and \u03b21. By adjusting these parameters, you can represent various linear functions, as depicted in Figure 1-18.\n\n![A Few Possible Linear Models](Figure 1-18)\n\nBefore employing your model, you need to identify the best parameter values for \u03b20 and \u03b21. To do this, a performance measure must be specified\u2014a utility function gauging the model's efficacy or a cost function quantifying its discrepancies. For linear regression issues, a common cost function assesses the distance between the model's predictions and the training examples, with the objective being minimization of this distance.\n\nHere\u2019s where the Linear Regression algorithm comes into play: it receives your training examples and determines the parameters that allow the linear model to fit as accurately as possible to the dataset.\n\nThis process aligns with training the model. For this case, the algorithm derives optimum parameter values of \u03b20 = 4.85 and \u03b21 = 4.91 \u00d7 10.\n\nNow, your model fits the training data as closely as possible (typical for a linear model), as seen in Figure 1-19.\n\n![The Linear Model That Fits the Training Data Best](Figure 1-19)\n\nYou are now prepared to utilize the model for predictions. For example, if you wish to ascertain the happiness level of Cypriots, but the OECD data does not provide this information, you can employ your model to predict: you consult Cyprus\u2019s GDP per capita (which is $22,587), apply the model, and discover that life satisfaction is expected to approximate 4.85 + 22,587 \u00d7 4.91 \u00d7 10 = 5.96.\n\nAs a preview, Example 1-1 showcases Python code that loads the data, prepares it, visualizes it through a scatterplot, trains a linear model, and conducts a prediction. \n\n**Example 1-1: Training and Running a Linear Model Using Scikit-Learn**\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\n\n# Load the data\noecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\ngdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\", thousands=', delimiter='\\t',\n                             encoding='latin-1', na_values=\"n/a\")\n\n# Prepare the data\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\nX = np.c_[country_stats[\"GDP per capita\"]]\ny = np.c_[country_stats[\"Life satisfaction\"]]\n\n# Visualize the data\ncountry_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\nplt.show()\n\n# Select a linear model\nmodel = sklearn.linear_model.LinearRegression()\n\n# Train the model\nmodel.fit(X, y)\n\n# Make a prediction for Cyprus\nX_new = [[22587]]  # Cyprus\u2019 GDP per capita\nprint(model.predict(X_new))  # outputs [[ 5.96242338]]\n```\n\nIf everything has functioned correctly, your model should yield solid predictions. If it doesn't, you may need to incorporate additional features (employment rates, health metrics, pollution levels, etc.), enhance the quality and quantity of training data, or perhaps select a more advanced model (e.g., a polynomial regression model).\n\n### Summary:\n\nThis is a snapshot of what a standard Machine Learning project entails. In Chapter 2, you'll experience this process firsthand through a comprehensive, step-by-step project.\n\nIf you\u2019d used an instance-based learning approach, you would have determined that Slovenia\u2019s GDP per capita is closest to Cyprus\u2019s at $20,732. With Slovenian life satisfaction rated at 5.7, you would have used this figure to estimate Cyprus\u2019s life satisfaction. Broadening the perspective to include Portugal and Spain, with satisfaction rates of 5.1 and 6.5, respectively, averaging these values would yield a prediction of 5.77, quite close to the earlier model-based prediction. This straightforward methodology is referred to as k-Nearest Neighbors regression (where k = 3).\n\n**Replacing the Linear Regression Model with k-Nearest Neighbors Regression**\n\nSwitching from the Linear Regression model in the earlier code to k-Nearest Neighbors regression is as easy as altering these lines:\n\n```python\nimport sklearn.linear_model\nmodel = sklearn.linear_model.LinearRegression()\n```\nto:\n\n```python\nimport sklearn.neighbors\nmodel = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n```\n\nYou analyzed the data. \nYou chose a model. \nYou trained it on the training data (where the learning algorithm searched for model parameters that minimize a cost function). \nFinally, you applied the model to make predictions on new cases (referred to as inference), hoping it will generalize effectively.\n\nWe have traversed considerable terrain thus far: you now understand what Machine Learning encompasses, its utility, several common classifications of Machine Learning systems, and a typical workflow involved in projects. Next, we will examine potential pitfalls in the learning process that may hinder accurate predictions.\n\n### Main Challenges of Machine Learning\n\nUltimately, your primary task is to select a learning algorithm and train it on a dataset. The two prevalent issues you might encounter involve either a \u201cbad algorithm\u201d or \u201cbad data.\u201d Let's first look at examples of subpar data.\n\n#### Insufficient Quantity of Training Data\n\nTo teach a child to recognize an apple, it suffices to point out an apple while labeling it as such (possibly repeating this several times). The child subsequently learns to identify apples of varying shapes and colors. \n\nIn contrast, Machine Learning systems often need more data; generally, thousands of examples are required for even straightforward tasks. Complex problems, such as image or speech recognition, may need millions of examples unless existing model parts can be reused.\n\n#### The Unreasonable Effectiveness of Data\n\nA notable study published in 2001 by Microsoft researchers Michele Banko and Eric Brill demonstrated that various Machine Learning algorithms\u2014some quite basic\u2014performed almost equally well in a complex natural language disambiguation task once sufficient data was provided (illustrated in Figure 1-20).\n\n![Importance of Data vs. Algorithms](Figure_1-20)\n\nTheir findings suggest a reevaluation of the tradeoff between resources allocated to algorithm enhancement versus corpus expansion.\n\nThe assertion that data supersedes algorithm optimization in complex scenarios was further popularized by Peter Norvig et al. in a 2009 paper entitled \"The Unreasonable Effectiveness of Data.\" Nevertheless, it remains pertinent that small- and medium-sized datasets are still widespread\u2014acquiring additional training data is not always feasible or affordable, so don't wholly discount algorithm development yet.\n\n#### Nonrepresentative Training Data\n\nTo generalize successfully, it\u2019s vital that your training data accurately reflects the new cases for prediction. This principle holds regardless of whether you apply instance-based or model-based learning methodologies.\n\nFor example, the dataset used earlier for creating the linear model was not entirely comprehensive; several countries were unrepresented. Figure 1-21 illustrates the changes upon supplementing the missing countries.\n\n![A More Representative Training Sample](Figure_1-21)\n\nTraining a linear model on the complete dataset yields a solid line, while the prior model is represented by a dotted line. Supplementing a few countries significantly alters the model and suggests that the original, simplistic approach likely won't yield accurate predictions for countries at both extremes of wealth. \n\nUtilizing a nonrepresentative dataset produces a model less likely to yield accurate results, particularly for nations that are very poor or affluent.\n\nIt's crucial to deploy a dataset that reflects the characteristics of the cases you wish to generalize; achieving this can be more challenging than it appears. A small sample may lead to sampling noise (non-representative data resulting from chance), but even a large sample can be skewed if sampled improperly\u2014a phenomenon known as sampling bias.\n\nFor instance, a notorious error in polling occurred during the 1936 U.S. presidential election, when the Literary Digest predicted a win for the Republican candidate, Alf Landon. In reality, Franklin D. Roosevelt won 62% of the votes. The problem stemmed from the Digest\u2019s flawed sampling method:\n\n- First, the method used telephone directories and subscriber lists that disproportionately included affluent individuals, likely to vote Republican.\n- Second, the poll resulted in less than a 25% response rate, further skewing the results by excluding those indifferent to politics or critical of the Digest.\n\nIn another scenario, if you aimed to construct a system to identify funk music videos, searching \"funk music\" on YouTube would yield a dataset. However, such results may favor popular artists, potentially leading to biased training data.\n\n#### Poor-Quality Data\n\nTraining datasets laden with inaccuracies, anomalies, and noise reduce the system\u2019s ability to recognize patterns, resulting in diminished performance. Investing time in cleaning your training data is often well worth the effort; indeed, a significant portion of a data scientist's time typically goes toward this task. Some potential actions include:\n\n- Discarding seemingly inaccurate instances or manually correcting errors where feasible.\n- Addressing missing features (e.g., 5% of customers lack age information) necessitating decisions: ignore the attribute entirely, omit these instances, input missing values through methods such as median imputation, or train separate models, one including the attribute and one without.\n\n#### Irrelevant Features\n\nThe principle of \"garbage in, garbage out\" applies here\u2014your model's learning capability hinges on the inclusion of relevant features while minimizing irrelevant inputs. A crucial aspect of any Machine Learning project is feature engineering, including:\n\n- **Feature Selection:** Identifying the most beneficial features for training.\n- **Feature Extraction:** Merging existing features to create more valuable insights.\n- **New Feature Creation:** Gathering new data to enhance the dataset.\n\nHaving examined instances of inadequate data, we can now address a few examples of ineffective algorithms.\n\n#### Overfitting the Training Data\n\nImagine you're traveling in a foreign country and have a negative experience, leading you to generalize that all taxis in that area are untrustworthy. Machines can easily fall into similar traps if caution is not observed, a phenomenon known as overfitting: where a model performs well on training data yet fails to generalize beyond it.\n\nFigure 1-22 depicts a high-degree polynomial life satisfaction model that is strongly overfitting the training data. While its performance exceeds that of a simple linear model on the training data, can its predictions truly be trusted?\n\n![Overfitting the Training Data](Figure_1-22)\n\nSophisticated models like deep neural networks can discern intricate patterns, but if the training data is noisy or insufficiently extensive (leading to sampling noise), the model is likely to learn patterns reflective of the noise rather than genuine trends. For instance, if you include an unnecessary attribute like the country name in your life satisfaction model, it might discover a spurious correlation\u2014that all countries in your dataset with a \"w\" in their name have a satisfaction level exceeding 7 (e.g., New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5)). How reliably can you assert that this \"w-satisfaction\" rule applies to other countries like Rwanda or Zimbabwe? Such patterns likely appeared by chance during training, placing the model in a perilous position to indiscriminately generalize findings.\n\nOverfitting arises when a model's complexity exceeds the quantity and quality of training data. Potential resolutions include:\n\n- Opting for a simpler model with fewer parameters (e.g., selecting a linear model instead of a complex polynomial model), decreasing the number of attributes in the training data, or constraining the parameters.\n- Acquiring additional training data.\n- Reducing noise present in the training data (e.g., correcting errors and excluding outliers).\n\nRegularization constrains model complexity, mitigating overfitting risks. For instance, let\u2019s say the linear model mentioned earlier had two parameters, \u03b20 and \u03b21. This permits two degrees of freedom to adapt the model to the dataset: the algorithm can modify both the intercept (\u03b20) and the slope (\u03b21). If you set \u03b21 = 0, it limits the algorithm to only one degree of freedom, making it exceptionally challenging to adjust the model correctly since it can only elevate or lower the line to fit the training instances, often centering it around the mean\u2014a remarkably simplistic model! Conversely, permitting the algorithm to modify \u03b20 while forcing it to be kept minimal allows the learning process to maintain more than one but fewer than two degrees of freedom, leading to a model that is more straightforward than with two degrees of freedom, yet complex enough to encapsulate essential patterns. Striking a balance between perfect training data fit and sufficient simplicity to generalize effectively is essential.\n\nFigure 1-23 displays three models: the dotted line illustrates the previous model trained on incomplete data, the dashed line represents the updated model trained with all data, and the solid line showcases the linear model trained on the part of the dataset but subjected to a regularization constraint. This constraint noticeably reduces the model's slope, resulting in a slightly less accurate fit to the training data, yet enhances its overall ability to generalize to new examples.\n\n![Regularization Reduces the Risk of Overfitting](Figure_1-23)\n\nThe degree of regularization applied during the learning process can be governed by a hyperparameter. Unlike model parameters that shift during training, hyperparameters remain constant and must be specified before the training process begins. Setting a hyperparameter for excessive regularization yields a nearly flat model (close to zero slope)\u2014while it effectively prevents overfitting, the model will likely struggle to identify optimal solutions. Tuning such hyperparameters becomes a pivotal aspect of crafting a Machine Learning system, as demonstrated in the forthcoming chapter.\n\n#### Underfitting the Training Data\n\nConversely, underfitting arises when a model lacks the requisite complexity to grasp the underlying data structure. For instance, employing a simplistic linear model for life satisfaction can result in underfitting due to the intricacy of the relationship involved, yielding erroneous predictions\u2014even on training data.\n\nStrategies to remedy underfitting include:\n\n- Deploying a more sophisticated model that contains additional parameters.\n- Enhancing the feature set provided to the Machine Learning algorithm (feature engineering).\n- Reducing restrictions imposed on the model (like loosening the regularization hyperparameter).\n\n### Stepping Back\n\nAt this point, you must have amassed significant insights into Machine Learning. However, given the extensive range of concepts covered, you may find it useful to take a moment and survey the broader landscape:\n\n- Machine Learning revolves around enabling machines to improve at specified tasks by learning from data, rather than relying solely on explicit coding of rules.\n- Various types of ML systems exist, including supervised and unsupervised categories, as well as distinctions between batch and online methodologies, and instance-based versus model-based learning.\n- In a Machine Learning project, you assemble a training dataset and feed it into a learning algorithm. If a model-based algorithm is utilized, the system adjusts parameters to align the model with the training data, aiming for effective predictions on new inputs. If employing an instance-based learning algorithm, the machine memorizes existing instances and generalizes to novel examples via similarity metrics.\n- System performance will falter if the training dataset is too small, non-representative, noisy, or cluttered with irrelevant features (adhering to the principle of \"garbage in, garbage out\"). Moreover, the model itself must balance between simplicity (which might lead to underfitting) and complexity (risking overfitting).\n\nThe final key topic on our agenda pertains to the evaluation of your model once trained, ensuring it doesn\u2019t merely operate under hopeful generalization assumptions, but is instead rigorously evaluated and fine-tuned as required. Let\u2019s delve into this aspect.\n\n### Testing and Validating\n\nThe most reliable method for assessing how a model generalizes to new situations is to subject it to trials on previously unseen cases. One straightforward approach is to launch the model into production and observe its performance. While effective, if the model performs poorly, user complaints can be detrimental.\n\nA better strategy presents itself in subdividing your data into two segments: a training set and a test set. Training models on the training set and subsequently evaluating them on the test set enables an error analysis on new data, termed generalization error or out-of-sample error. Evaluating the model against the test set helps approximate this error level, ultimately estimating how the model performs on completely new instances.\n\nWhen training errors are low (the model makes few mistakes based on the training data) but generalization errors are elevated, it signals the model is overfitting to the training data.\n\nIn practice, 80% of the dataset is typically allocated for training and 20% set aside for testing. However, this can vary based on dataset size; for instance, in a dataset containing 10 million instances, withholding just 1% provides a test set of 100,000 instances\u2014likely ample to yield a solid generalization error estimate.\n\n#### Hyperparameter Tuning and Model Selection\n\nEvaluating a model may seem straightforward using a test set. Consider the situation where you face a choice between two models (say, a linear and a polynomial model); how do you discern which to utilize? One option is to train both models and analyze their respective generalization capability using the test set.\n\nLet\u2019s suppose the linear model outperforms but you desire to implement regularization to combat overfitting. The next challenge arises\u2014how to determine the optimal value of the regularization hyperparameter? One approach would involve training multiple models while varying this hyperparameter across a range of values\u2014let\u2019s say you identify the best one resulting in a model with a low error rate, possibly achieving just 5%.\n\nAfter deploying this model, its performance may disappoint, producing higher error rates, at 15%. What occurred to elicit this disconnect?\n\nThe dilemma is that multiple evaluations of the generalization error were performed on the test set, leading to adaptations tailored solely to that dataset, making the model less likely to generalize to new cases.\n\nTo safeguard against this risk, the method of holdout validation comes into play: setting aside a portion of the training set allows process evaluations across multiple potential models, enabling selection of the best performing one. This reserved subset is titled the validation set (or development set, occasionally abbreviated to dev set). In practice, numerous models are trained on a reduced training set (inclusive of the full training set minus the validation set), and the most successful on the validation set is chosen. After determining the best model through this validation process, it is retrained using the entire training dataset (also incorporating the validation set) to culminate in the final model. This final model is then assessed against the test set to gauge its generalization error.\n\nThis method generally yields favorable results; however, an excessively small validation set risks imprecise evaluations, potentially leading to suboptimal model selections. Conversely, should the validation set be disproportionately large, the remaining training data becomes relatively small. Why does this matter? It becomes problematic because the final model is assessed using the entire training data\u2014thus comparing models trained on different dataset sizes is impractical. This scenario likens selecting an elite sprinter for a marathon.\n\nOne way to remedy this complication is through repeated cross-validation, wherein multiple small validation sets are produced. Each model is evaluated individually for its performance against a validation set after being trained using the remaining data. Through averaging evaluations across multiple validations, a more accurate representation of performance can be derived. However, this strategy may entail significant increases in training time based on validation set counts.\n\n### Data Mismatch\n\nIn select situations, acquiring an abundance of training data proves straightforward, yet it may not accurately reflect the production data. As an example, if you aim to develop an app capable of photographing flowers for species identification, you may effortlessly collect a plethora of flower pictures from the internet. However, those images may not effectively represent photos taken with a smartphone app. Suppose only 10,000 photos accurately represent those captured through the app; it\u2019s essential to ensure the validation and test datasets are as reflective as possible to the production data, constituted solely by these representative examples. By randomizing and segmenting these into validation and test datasets, you prevent duplicates or similarities.\n\nUpon training your model using the internet-sourced images, a lackluster validation set may yield disappointing results; this outcome leaves uncertainty regarding whether the model overfitted the training set or if the data mismatch caused the decline. One potential solution involves reserving part of the training pictures from the web into an additional set known as the train-dev set, as proposed by Andrew Ng. Once your model has trained using the primary training set (not including the train-dev set), evaluating on the latter will clarify model performance: effective results indicate no overfitting, while inadequate performance on the validation set implies data mismatch. Strategies can thus be implemented to adjust preprocessing of web-sourced images, aligning them more closely with photos the app would capture, followed by retraining the model. If the model performs poorly against the revision, indications suggest overfitting occurred within the training set\u2014 necessitating simplification, regularization, or enhancements in data acquisition and quality, as previously discussed.\n\n### No Free Lunch Theorem\n\nModels represent simplified interpretations of observed data. Such simplifications are intended to eliminate extraneous elements unlikely to generalize beyond specific instances. However, to decide which data to retain and which to discard, you must form assumptions. For example, a linear model presupposes that underlying data is fundamentally linear, and that the variance between instances and the related linear approximation is negligible noise.\n\nIn a seminal 1996 study, David Wolpert articulated that if no assumptions are made about the data, then no particular model could be favored above another\u2014this is termed the No Free Lunch (NFL) theorem. Various datasets may benefit from distinct optimal models, with some favoring linear approaches and others favoring neural networks. Thus, a universally superior model is non-existent; the only method to ascertain the best fit is to evaluate numerous models. Since exhaustive evaluations aren't practical, reasonable assumptions about the data guide the selection of a narrower field of candidate models. For basic tasks, you may consider linear models incorporating diverse regularization levels, while complex challenges might necessitate exploring different neural network architectures."
                    }
                ]
            }
        ]
    }
}