{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "Core PyTorch",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "Core PyTorch",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/434fe9e9-6a5b-47e2-839d-46b4478fef1d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "\\ Vi scome to the first part of this book. This is where we'll take our first\n\nsteps with PyTorch, gaining the fundamental skills needed to understand its\nanatomy and work out the mechanics of a PyTorch project.\n\nIn chapter 1, we'll make our first contact with PyTorch, understand what it is\nand what problems it solves, and how it relates to other deep learning frame-\nworks. Chapter 2 will take us on a tour, giving us a chance to play with models\nthat have been pretrained on fun tasks. Chapter 3 gets a bit more serious and\nteaches the basic data structure used in PyTorch programs: the tensor. Chapter 4\nwill take us on another tour, this time across ways to represent data from differ-\nent domains as PyTorch tensors. Chapter 5 unveils how a program can learn\nfrom examples and how PyTorch supports this process. Chapter 6 provides the\nfundamentals of what a neural network is and how to build a neural network\nwith PyTorch. Chapter 7 tackles a simple image classification problem with a\nneural network architecture. Finally, chapter 8 shows how the same problem can\nbe cracked in a much smarter way using a convolutional neural network.\n\nBy the end of part 1, we'll have what it takes to tackle a real-world problem\nwith PyTorch in part 2.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 2,
                "chapter_name": "Introducing deep\nlearning and the\nPyTorch Library",
                "chapter_path": "./screenshots-images-2/chapter_2",
                "sections": [
                    {
                        "section_id": 2.1,
                        "section_name": "Introducing deep\nlearning and the\nPyTorch Library",
                        "section_path": "./screenshots-images-2/chapter_2/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_1/6805bc28-9071-41cb-8586-33fffe83b78d.png",
                            "./screenshots-images-2/chapter_2/section_1/bdd4696d-e71e-4829-a724-1ddbc138b5fc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The poorly defined term artificial intelligence covers a set of disciplines that have\nbeen subjected to a tremendous amount of research, scrutiny, confusion, fantasti-\ncal hype, and sci-fi fearmongering. Reality is, of course, far more sanguine. It would\nbe disingenuous to assert that today\u2019s machines are learning to \u201cthink\u201d in any\nhuman sense of the word. Rather, we've discovered a general class of algorithms\n\nthat are able to approximate complicated, nonlinear processes very, very effectively,\nwhich we can use to automate tasks that were previously limited to humans.\n\nFor example, at https://talktotransformer.com, a language model called GPT-2\ncan generate coherent paragraphs of text one word at a time. When we fed it this very\nparagraph, it produced the following:\n\nNext we're going to feed in a list of phrases from a corpus of email addresses, and see if the\nprogram can parse the lists as sentences. Again, this is much more complicated and far more\ncomplex than the search at the beginning of this post, but hopefully helps you understand the\nbasics of constructing sentence structures in various programming languages.\n\nThat's remarkably coherent for a machine, even if there isn\u2019t a well-defined thesis\nbehind the rambling.\n\nEven more impressively, the ability to perform these formerly human-only tasks is\nacquired through examples, rather than encoded by a human as a set of handcrafted\nrules. In a way, we\u2019re learning that intelligence is a notion we often conflate with self\nawareness, and self-awareness is definitely not required to successfully carry out these\nkinds of tasks. In the end, the question of computer intelligence might not even be\nimportant. Edsger W. Dijkstra found that the question of whether machines could\nthink was \u201cabout as relevant as the question of whether Submarines Can Swim.\u201d!\n\nThat general class of algorithms we're talking about falls under the AI subcategory\nof deep learning, which deals with training mathematical entities named deep neural net-\nworks by presenting instructive examples. Deep learning uses large amounts of data to\napproximate complex functions whose inputs and outputs are far apart, like an input\nimage and, as output, a line of text describing the input; or a written script as input\nand a natural-sounding voice reciting the script as output; or, even more simply, asso-\nciating an image of a golden retriever with a flag that tells us \u201cYes, a golden retriever is\npresent.\u201d This kind of capability allows us to create programs with functionality that\nwas, until very recently, exclusively the domain of human beings.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.2,
                        "section_name": "The deep learning revolution",
                        "section_path": "./screenshots-images-2/chapter_2/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_2/546ecdf0-b176-476e-be08-c45ef41d8da0.png",
                            "./screenshots-images-2/chapter_2/section_2/09d3b9a6-0729-4e3a-829d-82b08c707945.png",
                            "./screenshots-images-2/chapter_2/section_2/28faa156-1483-4b69-b154-1c6d8b82d0f7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The deep learning revolution\n\nTo appreciate the paradigm shift ushered in by this deep learning approach, let\u2019s take\na step back for a bit of perspective. Until the last decade, the broader class of systems\nthat fell under the label machine learning relied heavily on feature engineering. Features\nare transformations on input data that facilitate a downstream algorithm, like a classi-\nfier, to produce correct outcomes on new data. Feature engineering consists of com-\ning up with the right transformations so that the downstream algorithm can solve a\ntask. For instance, in order to tell ones from zeros in images of handwritten digits, we\nwould come up with a set of filters to estimate the direction of edges over the image,\nand then train a classifier to predict the correct digit given a distribution of edge\ndirections. Another useful feature could be the number of enclosed holes, as seen in a\nzero, an eight, and, particularly, loopy twos.\n\nDeep learning, on the other hand, deals with finding such representations auto-\nmatically, from raw data, in order to successfully perform a task. In the ones versus\nzeros example, filters would be refined during training by iteratively looking at pairs\nof examples and target labels. This is not to say that feature engineering has no place\nwith deep learning; we often need to inject some form of prior knowledge in a learn-\ning system. However, the ability of a neural network to ingest data and extract useful\nrepresentations on the basis of examples is what makes deep learning so powerful.\nThe focus of deep learning practitioners is not so much on handcrafting those repre-\nsentations, but on operating on a mathematical entity so that it discovers representa-\ntions from the training data autonomously. Often, these automatically created\nfeatures are better than those that are handcrafted! As with many disruptive technolo-\ngies, this fact has led to a change in perspective.\n\nOn the left side of figure 1.1, we see a practitioner busy defining engineering fea-\ntures and feeding them to a learning algorithm; the results on the task will be as good\nas the features the practitioner engineers. On the right, with deep learning, the raw\ndata is fed to an algorithm that extracts hierarchical features automatically, guided by\nthe optimization of its own performance on the task; the results will be as good as the\nability of the practitioner to drive the algorithm toward its goal.\n\nFEATURES\npeer \\\nLEARNING | LEARNING &\nMACHINE Jey 1 MACHINE\n|\n| OUTCOME \\\nREPRESENTATIONS 42\nTHE PARADIGM SHIFT\n\n\\\n1\nouTcome = 4.\nI\n\nFigure 1.1 Deep learning exchanges the need to handcraft features for an increase in data and\ncomputational requirements.\n\nStarting from the right side in figure 1.1, we already get a glimpse of what we need to\nexecute successful deep learning:\n\n= We need a way to ingest whatever data we have at hand.\n\n= We somehow need to define the deep learning machine.\n\n= We must have an automated way, training, to obtain useful representations and\nmake the machine produce desired outputs.\n\nThis leaves us with taking a closer look at this training thing we keep talking about.\nDuring training, we use a criterion, a real-valued function of model outputs and refer-\nence data, to provide a numerical score for the discrepancy between the desired and\nactual output of our model (by convention, a lower score is typically better). Training\nconsists of driving the criterion toward lower and lower scores by incrementally modi-\nfying our deep learning machine until it achieves low scores, even on data not seen\nduring training.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.3,
                        "section_name": "PyTorch for deep learning",
                        "section_path": "./screenshots-images-2/chapter_2/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_3/a5e1bc5a-ad3c-4930-8400-b9293e8d4619.png",
                            "./screenshots-images-2/chapter_2/section_3/900941c6-beca-4e2a-8460-0fa8b5575eb7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "PyTorch for deep learning\n\nPyTorch is a library for Python programs that facilitates building deep learning proj-\nects. It emphasizes flexibility and allows deep learning models to be expressed in idi-\nomatic Python. This approachability and ease of use found early adopters in the\nresearch community, and in the years since its first release, it has grown into one of\nthe most prominent deep learning tools across a broad range of applications.\n\nAs Python does for programming, PyTorch provides an excellent introduction to\ndeep learning. At the same time, PyTorch has been proven to be fully qualified for use\nin professional contexts for real-world, high-profile work. We believe that PyTorch\u2019s\nclear syntax, streamlined API, and easy debugging make it an excellent choice for\nintroducing deep learning. We highly recommend studying PyTorch for your first\ndeep learning library. Whether it ought to be the last deep learning library you learn\nis a decision we leave up to you.\n\nAtits core, the deep learning machine in figure 1.1 is a rather complex mathemat-\nical function mapping inputs to an output. To facilitate expressing this function,\nPyTorch provides a core data structure, the tensor, which is a multidimensional array\nthat shares many similarities with NumPy arrays. Around that foundation, PyTorch\ncomes with features to perform accelerated mathematical operations on dedicated\nhardware, which makes it convenient to design neural network architectures and train\nthem on individual machines or parallel computing resources.\n\nThis book is intended as a starting point for software engineers, data scientists, and\nmotivated students fluent in Python to become comfortable using PyTorch to build\ndeep learning projects. We want this book to be as accessible and useful as possible,\nand we expect that you will be able to take the concepts in this book and apply them\nto other domains. To that end, we use a hands-on approach and encourage you to\nkeep your computer at the ready, so you can play with the examples and take them a\nstep further. By the time we are through with the book, we expect you to be able to\n\ntake a data source and build out a deep learning project with it, supported by the\nexcellent official documentation.\n\nAlthough we stress the practical aspects of building deep learning systems with\nPyTorch, we believe that providing an accessible introduction to a foundational deep\nlearning tool is more than just a way to facilitate the acquisition of new technical skills.\nIt is a step toward equipping a new generation of scientists, engineers, and practi-\ntioners from a wide range of disciplines with working knowledge that will be the back-\nbone of many software projects during the decades to come.\n\nIn order to get the most out of this book, you will need two things:\n\n= Some experience programming in Python. We're not going to pull any punches\non that one; you'll need to be up on Python data types, classes, floating-point\nnumbers, and the like.\n\n= A willingness to dive in and get your hands dirty. We'll be starting from the\nbasics and building up our working knowledge, and it will be much easier for\nyou to learn if you follow along with us.\n\nDeep Learning with PyTorch is organized in three distinct parts. Part 1 covers the founda-\ntions, examining in detail the facilities PyTorch offers to put the sketch of deep learn-\ning in figure 1.1 into action with code. Part 2 walks you through an end-to-end project\ninvolving medical imaging: finding and classifying tumors in CT scans, building on\nthe basic concepts introduced in part 1, and adding more advanced topics. The short\npart 3 rounds off the book with a tour of what PyTorch offers for deploying deep\nlearning models to production.\n\nDeep learning is a huge space. In this book, we will be covering a tiny part of that\nspace: specifically, using PyTorch for smaller-scope classification and segmentation\nprojects, with image processing of 2D and 3D datasets used for most of the motivating\nexamples. This book focuses on practical PyTorch, with the aim of covering enough\nground to allow you to solve real-world machine learning problems, such as in vision,\nwith deep learning or explore new models as they pop up in research literature. Most,\nif not all, of the latest publications related to deep learning research can be found in\nthe arXiV public preprint repository, hosted at https://arxiv.org.\u201d\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.4,
                        "section_name": "Why PyTorch?",
                        "section_path": "./screenshots-images-2/chapter_2/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_4/01ac6c8a-9cb4-4de7-aac9-763363791ca8.png",
                            "./screenshots-images-2/chapter_2/section_4/8a6b857a-d610-4249-a86a-47bb07f84282.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Why PyTorch?\n\nAs we've said, deep learning allows us to carry outa very wide range of complicated tasks,\nlike machine translation, playing strategy games, or identifying objects in cluttered\nscenes, by exposing our model to illustrative examples. In order to do so in practice, we\nneed tools that are flexible, so they can be adapted to such a wide range of problems,\nand efficient, to allow training to occur over large amounts of data in reasonable times;\nand we need the trained model to perform correctly in the presence of variability in the\ninputs. Let\u2019s take a look at some of the reasons we decided to use PyTorch.\n\nPyTorch is easy to recommend because of its simplicity. Many researchers and prac-\ntitioners find it easy to learn, use, extend, and debug. It\u2019s Pythonic, and while like any\ncomplicated domain it has caveats and best practices, using the library generally feels\nfamiliar to developers who have used Python previously.\n\nMore concretely, programming the deep learning machine is very natural in\nPyTorch. PyTorch gives us a data type, the Tensor, to hold numbers, vectors, matrices,\nor arrays in general. In addition, it provides functions for operating on them. We can\nprogram with them incrementally and, if we want, interactively, just like we are used to\nfrom Python. If you know NumPy, this will be very familiar.\n\nBut PyTorch offers two things that make it particularly relevant for deep learning:\nfirst, it provides accelerated computation using graphical processing units (GPUs),\noften yielding speedups in the range of 50x over doing the same calculation on a\nCPU. Second, PyTorch provides facilities that support numerical optimization on\ngeneric mathematical expressions, which deep learning uses for training. Note that\nboth features are useful for scientific computing in general, not exclusively for deep\nlearning. In fact, we can safely characterize PyTorch as a high-performance library\nwith optimization support for scientific computing in Python.\n\nA design driver for PyTorch is expressivity, allowing a developer to implement com-\nplicated models without undue complexity being imposed by the library (it\u2019s not a\nframework!). PyTorch arguably offers one of the most seamless translations of ideas\ninto Python code in the deep learning landscape. For this reason, PyTorch has seen\nwidespread adoption in research, as witnessed by the high citation counts at interna-\ntional conferences.*\n\nPyTorch also has a compelling story for the transition from research and develop-\nment into production. While it was initially focused on research workflows, PyTorch\nhas been equipped with a high-performance C++ runtime that can be used to deploy\nmodels for inference without relying on Python, and can be used for designing and\ntraining models in C++. It has also grown bindings to other languages and an inter-\nface for deploying to mobile devices. These features allow us to take advantage of\nPyTorch\u2019s flexibility and at the same time take our applications where a full Python\nruntime would be hard to get or would impose expensive overhead.\n\nOf course, claims of ease of use and high performance are trivial to make. We\nhope that by the time you are in the thick of this book, you'll agree with us that our\nclaims here are well founded.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.5,
                        "section_name": "The deep learning competitive landscape",
                        "section_path": "./screenshots-images-2/chapter_2/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_5/ce1723ad-6f66-4e1d-a6ed-cc98a9d20de7.png",
                            "./screenshots-images-2/chapter_2/section_5/53a530a3-829e-4d11-b8c4-e53f38f59c6d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The deep learning competitive landscape\n\nWhile all analogies are flawed, it seems that the release of PyTorch 0.1 in January 2017\nmarked the transition from a Cambrian-explosion-like proliferation of deep learning\nlibraries, wrappers, and data-exchange formats into an era of consolidation and\n\nunification.\n\nNOTE The deep learning landscape has been moving so quickly lately that by\nthe time you read this in print, it will likely be out of date. If you\u2019re unfamiliar\nwith some of the libraries mentioned here, that\u2019s fine.\n\nAt the time of PyTorch\u2019s first beta release:\n\n= Theano and TensorFlow were the premiere low-level libraries, working with a\nmodel that had the user define a computational graph and then execute it.\n\n= Lasagne and Keras were high-level wrappers around Theano, with Keras wrap-\nping TensorFlow and CNTK as well.\n\n= Caffe, Chainer, DyNet, Torch (the Lua-based precursor to PyTorch), MXNet,\nCNTK, DL4J, and others filled various niches in the ecosystem.\n\nIn the roughly two years that followed, the landscape changed drastically. The com-\nmunity largely consolidated behind either PyTorch or TensorFlow, with the adoption\nof other libraries dwindling, except for those filling specific niches. In a nutshell:\n\n= Theano, one of the first deep learning frameworks, has ceased active development.\n= TensorFlow:\n\u2014 Consumed Keras entirely, promoting it to a first-class API\n\u2014 Provided an immediate-execution \u201ceager mode\u201d that is somewhat similar to\nhow PyTorch approaches computation\n\u2014 Released TF 2.0 with eager mode by default\n= JAX, a library by Google that was developed independently from TensorFlow,\nhas started gaining traction as a NumPy equivalent with GPU, autograd and JIT\ncapabilities.\n= PyTorch:\n\u2014 Consumed Caffe? for its backend\n\u2014 Replaced most of the low-level code reused from the Lua-based Torch project\n\u2014 Added support for ONNX, a vendor-neutral model description and\nexchange format\n\u2014 Added a delayed-execution \u201cgraph mode\u201d runtime called TorchScript\n\u2014 Released version 1.0\n\u2014 Replaced CNTK and Chainer as the framework of choice by their respective\ncorporate sponsors\n\nTensorFlow has a robust pipeline to production, an extensive industry-wide commu-\nnity, and massive mindshare. PyTorch has made huge inroads with the research and\nteaching communities, thanks to its ease of use, and has picked up momentum since,\nas researchers and graduates train students and move to industry. It has also built up\nsteam in terms of production solutions. Interestingly, with the advent of TorchScript\nand eager mode, both PyTorch and TensorFlow have seen their feature sets start to\nconverge with the other\u2019s, though the presentation of these features and the overall\nexperience is still quite different between the two.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.6,
                        "section_name": "An overview of how PyTorch supports deep learning projects",
                        "section_path": "./screenshots-images-2/chapter_2/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_6/2f1ee18a-4de8-470c-beae-774d70e99be3.png",
                            "./screenshots-images-2/chapter_2/section_6/f9d433d9-f230-48eb-b6ec-400c138e0c15.png",
                            "./screenshots-images-2/chapter_2/section_6/9a201cc3-b191-401c-b0b4-b5823e6139c5.png",
                            "./screenshots-images-2/chapter_2/section_6/0efba23d-90db-446a-82ce-6e2ec19e8bf3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "An overview of how PyTorch supports deep learning projects\n\nWe have already hinted at a few building blocks in PyTorch. Let\u2019s now take some time\nto formalize a high-level map of the main components that form PyTorch. We can best\ndo this by looking at what a deep learning project needs from PyTorch.\n\nFirst, PyTorch has the \u201cPy\u201d as in Python, but there\u2019s a lot of non-Python code in it.\nActually, for performance reasons, most of PyTorch is written in C++ and CUDA\n(www.geforce.com/hardware/technology/cuda), a C++-like language from NVIDIA\nthat can be compiled to run with massive parallelism on GPUs. There are ways to run\nPyTorch directly from C++, and we'll look into those in chapter 15. One of the motiva-\ntions for this capability is to provide a reliable strategy for deploying models in pro-\nduction. However, most of the time we'll interact with PyTorch from Python, building\nmodels, training them, and using the trained models to solve actual problems.\n\nIndeed, the Python API is where PyTorch shines in term of usability and integra-\ntion with the wider Python ecosystem. Let\u2019s take a peek at the mental model of what\nPyTorch is.\n\nAs we already touched on, at its core, PyTorch is a library that provides multidimen-\nsional arrays, or tensors in PyTorch parlance (we'll go into details on those in chapter\n3), and an extensive library of operations on them, provided by the torch module.\nBoth tensors and the operations on them can be used on the CPU or the GPU. Mov-\ning computations from the CPU to the GPU in PyTorch doesn\u2019t require more than an\nadditional function call or two. The second core thing that PyTorch provides is the\nability of tensors to keep track of the operations performed on them and to analyti-\ncally compute derivatives of an output of a computation with respect to any of its\ninputs. This is used for numerical optimization, and it is provided natively by tensors\nby virtue of dispatching through PyTorch\u2019s autograd engine under the hood.\n\nBy having tensors and the autograd-enabled tensor standard library, PyTorch can\nbe used for physics, rendering, optimization, simulation, modeling, and more\u2014we\u2019re\nvery likely to see PyTorch used in creative ways throughout the spectrum of scientific\napplications. But PyTorch is first and foremost a deep learning library, and as such it\nprovides all the building blocks needed to build neural networks and train them. Fig-\nure 1.2 shows a standard setup that loads data, trains a model, and then deploys that\nmodel to production.\n\nThe core PyTorch modules for building neural networks are located in torch.nn,\nwhich provides common neural network layers and other architectural components.\nFully connected layers, convolutional layers, activation functions, and loss functions\ncan all be found here (we'll go into more detail about what all that means as we go\nthrough the rest of this book). These components can be used to build and initialize\nthe untrained model we see in the center of figure 1.2. In order to train our model, we\nneed a few additional things: a source of training data, an optimizer to adapt the\nmodel to the training data, and a way to get the model and data to the hardware that\nwill actually be performing the calculations needed for training the model.\n\nMULTIPROCESS, MODEL\nDATA LOADING F)\nae TRAINED\nbasta 1 MODEL\n\n\u201cp-o aie es\n\nNg\noat\n\nDISTRIBUTED TRAINING PRODUCTION\nON MULTLOLE SERVERS/GPUs SERVER\n\nFigure 1.2 Basic, high-level structure of a PyTorch project, with data loading, training, and\ndeployment to production\n\nAt left in figure 1.2, we see that quite a bit of data processing is needed before the\ntraining data even reaches our model.\u2019 First we need to physically get the data, most\noften from some sort of storage as the data source. Then we need to convert each sam-\nple from our data into a something PyTorch can actually handle: tensors. This bridge\nbetween our custom data (in whatever format it might be) and a standardized\nPyTorch tensor is the Dataset class PyTorch provides in torch.utils.data. As this\nprocess is wildly different from one problem to the next, we will have to implement\nthis data sourcing ourselves. We will look in detail at how to represent various type of\ndata we might want to work with as tensors in chapter 4.\n\nAs data storage is often slow, in particular due to access latency, we want to paral-\nlelize data loading. But as the many things Python is well loved for do not include easy,\nefficient, parallel processing, we will need multiple processes to load our data, in order\nto assemble them into batches: tensors that encompass several samples. This is rather\nelaborate; but as it is also relatively generic, PyTorch readily provides all that magic in\nthe DataLoader class. Its instances can spawn child processes to load data from a data-\nset in the background so that it\u2019s ready and waiting for the training loop as soon as the\nloop can use it. We will meet and use Dataset and DataLoader in chapter 7.\n\nWith the mechanism for getting batches of samples in place, we can turn to the\ntraining loop itself at the center of figure 1.2. Typically, the taining loop is imple-\nmented as a standard Python for loop. In the simplest case, the model runs the\nrequired calculations on the local CPU or a single GPU, and once the training loop\nhas the data, computation can start immediately. Chances are this will be your basic\nsetup, too, and it\u2019s the one we'll assume in this book.\n\nAt each step in the training loop, we evaluate our model on the samples we got\nfrom the data loader. We then compare the outputs of our model to the desired out-\nput (the targets) using some criterion or loss function. Just as it offers the components\nfrom which to build our model, PyTorch also has a variety of loss functions at our dis-\nposal. They, too, are provided in torch.nn. After we have compared our actual out-\nputs to the ideal with the loss functions, we need to push the model a little to move its\noutputs to better resemble the target. As mentioned earlier, this is where the PyTorch\nautograd engine comes in; but we also need an optimizer doing the updates, and that is\nwhat PyTorch offers us in torch. optim. We will start looking at training loops with loss\nfunctions and optimizers in chapter 5 and then hone our skills in chapters 6 through\n8 before embarking on our big project in part 2.\n\nIt\u2019s increasingly common to use more elaborate hardware like multiple GPUs or\nmultiple machines that contribute their resources to training a large model, as seen in\nthe bottom center of figure 1.2. In those cases, torch.nn.parallel.Distributed-\nDataParallel and the torch.distributed submodule can be employed to use the\nadditional hardware.\n\nThe training loop might be the most unexciting yet most time-consuming part of a\ndeep learning project. At the end of it, we are rewarded with a model whose parame-\nters have been optimized on our task: the trained model depicted to the right of the\ntraining loop in the figure. Having a model to solve a task is great, but in order for it\nto be useful, we must put it where the work is needed. This deployment part of the pro-\ncess, depicted on the right in figure 1.2, may involve putting the model on a server or\nexporting it to load it to a cloud engine, as shown in the figure. Or we might integrate\nit with a larger application, or run it on a phone.\n\nOne particular step of the deployment exercise can be to export the model. As\nmentioned earlier, PyTorch defaults to an immediate execution model (eager mode).\nWhenever an instruction involving PyTorch is executed by the Python interpreter, the\ncorresponding operation is immediately carried out by the underlying C++ or CUDA\nimplementation. As more instructions operate on tensors, more operations are exe-\ncuted by the backend implementation.\n\nPyTorch also provides a way to compile models ahead of time through TorchScript.\nUsing TorchScript, PyTorch can serialize a model into a set of instructions that can be\ninvoked independently from Python: say, from C++ programs or on mobile devices. We\ncan think about it as a virtual machine with a limited instruction set, specific to tensor\noperations. This allows us to export our model, either as TorchScript to be used with\nthe PyTorch runtime, or in a standardized format called ONNX. These features are at\n\nthe basis of the production deployment capabilities of PyTorch. We'll cover this in\nchapter 15.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.7,
                        "section_name": "Hardware and software requirements",
                        "section_path": "./screenshots-images-2/chapter_2/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_7/de6a3196-f0c6-49d8-932a-8f08605ac251.png",
                            "./screenshots-images-2/chapter_2/section_7/ce04e65f-c1db-4076-a604-9d1e3c3a3534.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Hardware and software requirements\n\nThis book will require coding and running tasks that involve heavy numerical comput-\ning, such as multiplication of large numbers of matrices. As it turns out, running a\npretrained network on new data is within the capabilities of any recent laptop or per-\nsonal computer. Even taking a pretrained network and retraining a small portion of it\nto specialize it on a new dataset doesn\u2019t necessarily require specialized hardware. You\ncan follow along with everything we do in part 1 of this book using a standard per-\nsonal computer or laptop.\n\nHowever, we anticipate that completing a full training run for the more advanced\nexamples in part 2 will require a CUDA-capable GPU. The default parameters used in\npart 2 assume a GPU with 8 GB of RAM (we suggest an NVIDIA GTX 1070 or better),\nbut those can be adjusted if your hardware has less RAM available. To be clear: such\nhardware is not mandatory if you\u2019re willing to wait, but ranning on a GPU cuts train-\ning time by at least an order of magnitude (and usually it\u2019s 40\u201450x faster). Taken indi-\nvidually, the operations required to compute parameter updates are fast (from\nfractions of a second to a few seconds) on modern hardware like a typical laptop CPU.\nThe issue is that training involves running these operations over and over, many, many\ntimes, incrementally updating the network parameters to minimize the training error.\n\nModerately large networks can take hours to days to train from scratch on large,\nreal-world datasets on workstations equipped with a good GPU. That time can be\nreduced by using multiple GPUs on the same machine, and even further on clusters\nof machines equipped with multiple GPUs. These setups are less prohibitive to access\nthan it sounds, thanks to the offerings of cloud computing providers. DAWNBench\n(https://dawn.cs.stanford.edu/benchmark/index.html) is an interesting initiative\nfrom Stanford University aimed at providing benchmarks on training time and cloud\ncomputing costs related to common deep learning tasks on publicly available datasets.\n\nSo, if there\u2019s a GPU around by the time you reach part 2, then great. Otherwise, we\nsuggest checking out the offerings from the various cloud platforms, many of which offer\nGPU-enabled Jupyter Notebooks with PyTorch preinstalled, often with a free quota. Goo-\ngle Colaboratory (https://colab.research.google.com) is a great place to start.\n\nThe last consideration is the operating system (OS). PyTorch has supported Linux\nand macOS from its first release, and it gained Windows support in 2018. Since cur-\nrent Apple laptops do not include GPUs that support CUDA, the precompiled macOS\npackages for PyTorch are CPU-only. Throughout the book, we will try to avoid assum-\ning you are running a particular OS, although some of the scripts in part 2 are shown\nas if running from a Bash prompt under Linux. Those scripts\u2019 command lines should\nconvert to a Windows-compatible form readily. For convenience, code will be listed as\nif running from a Jupyter Notebook when possible.\n\nFor installation information, please see the Get Started guide on the official\nPyTorch website (https://pytorch.org/get-started/locally). We suggest that Windows\nusers install with Anaconda or Miniconda (https://www.anaconda.com/ distribution\nor https://docs.conda.io/en/latest/miniconda.html). Other operating systems like\nLinux typically have a wider variety of workable options, with Pip being the most com-\nmon package manager for Python. We provide a requirements.txt file that pip can use\nto install dependencies. Of course, experienced users are free to install packages in\nthe way that is most compatible with your preferred development environment.\n\nPart 2 has some nontrivial download bandwidth and disk space requirements as\nwell. The raw data needed for the cancer-detection project in part 2 is about 60 GB to\ndownload, and when uncompressed it requires about 120 GB of space. The com-\npressed data can be removed after decompressing it. In addition, due to caching some\nof the data for performance reasons, another 80 GB will be needed while training.\nYou will need a total of 200 GB (at minimum) of free disk space on the system that will\nbe used for training. While it is possible to use network storage for this, there might be\ntraining speed penalties if the network access is slower than local disk. Preferably you\nwill have space on a local SSD to store the data for fast retrieval.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.8,
                        "section_name": "Using Jupyter Notebooks",
                        "section_path": "./screenshots-images-2/chapter_2/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_8/000437e4-0b34-47a8-a074-c28ed37e7622.png",
                            "./screenshots-images-2/chapter_2/section_8/fe4f8795-2f9f-4bdf-90f6-9152a66d58e7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using Jupyter Notebooks\n\nWe're going to assume you've installed PyTorch and the other dependencies and have\nverified that things are working. Earlier we touched on the possibilities for following\nalong with the code in the book. We are going to be making heavy use of Jupyter Note-\nbooks for our example code. A Jupyter Notebook shows itself as a page in the browser\nthrough which we can run code interactively. The code is evaluated by a kernel, a process\nrunning on a server that is ready to receive code to execute and send back the results,\nwhich are then rendered inline on the page. A notebook maintains the state of the ker-\nnel, like variables defined during the evaluation of code, in memory until it is termi-\nnated or restarted. The fundamental unit with which we interact with a notebook is a\ncell: a box on the page where we can type code and have the kernel evaluate it (through\nthe menu item or by pressing Shift-Enter). We can add multiple cells in a notebook, and\nthe new cells will see the variables we created in the earlier cells. The value returned by\nthe last line of a cell will be printed right below the cell after execution, and the same\ngoes for plots. By mixing source code, results of evaluations, and Markdown-formatted\ntext cells, we can generate beautiful interactive documents. You can read everything\nabout Jupyter Notebooks on the project website (https://jupyter.org).\n\nAt this point, you need to start the notebook server from the root directory of the\ncode checkout from GitHub. How exactly starting the server looks depends on the\ndetails of your OS and how and where you installed Jupyter. If you have questions, feel\nfree to ask on the book\u2019s forum.\u2019 Once started, your default browser will pop up,\nshowing a list of local notebook files.\n\nNOTE Jupyter Notebooks are a powerful tool for expressing and investigating\nideas through code. While we think that they make for a good fit for our use\ncase with this book, they\u2019re not for everyone. We would argue that it\u2019s import-\nant to focus on removing friction and minimizing cognitive overhead, and\nthat\u2019s going to be different for everyone. Use what you like during your exper-\nimentation with PyTorch.\n\nFull working code for all listings from the book can be found at the book's website\n(www.manning.com/books/deep-learning-with-pytorch) and in our repository on\nGitHub (https://github.com/deep-learning-with-pytorch/dlwpt-code).\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 3,
                "chapter_name": "Pretrained networks",
                "chapter_path": "./screenshots-images-2/chapter_3",
                "sections": [
                    {
                        "section_id": 3.1,
                        "section_name": "Pretrained networks",
                        "section_path": "./screenshots-images-2/chapter_3/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_1/58776282-bf59-4a49-9369-5a791d31c9c3.png",
                            "./screenshots-images-2/chapter_3/section_1/b653b872-d4f6-43ad-b6aa-5c80f878e3a7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "We closed our first chapter promising to unveil amazing things in this chapter, and\nnow it\u2019s time to deliver. Computer vision is certainly one of the fields that have\nbeen most impacted by the advent of deep learning, for a variety of reasons. The\nneed to classify or interpret the content of natural images existed, very large data-\nsets became available, and new constructs such as convolutional layers were\ninvented and could be run quickly on GPUs with unprecedented accuracy. All of\nthese factors combined with the internet giants\u2019 desire to understand pictures\ntaken by millions of users with their mobile devices and managed on said giants\u2019\nplatforms. Quite the perfect storm.\n\nWe are going to learn how to use the work of the best researchers in the field by\ndownloading and running very interesting models that have already been trained on\nopen, large-scale datasets. We can think of a pretrained neural network as similar to\n\na program that takes inputs and generates outputs. The behavior of such a program is\ndictated by the architecture of the neural network and by the examples it saw during\ntraining, in terms of desired input-output pairs, or desired properties that the output\nshould satisfy. Using an off-the-shelf model can be a quick way to jump-start a deep\nlearning project, since it draws on expertise from the researchers who designed the\nmodel, as well as the computation time that went into training the weights.\n\nIn this chapter, we will explore three popular pretrained models: a model that can\nlabel an image according to its content, another that can fabricate a new image from a\nreal image, and a model that can describe the content of an image using proper\nEnglish sentences. We will learn how to load and run these pretrained models in\nPyTorch, and we will inwoduce PyTorch Hub, a set of tools through which PyTorch\nmodels like the pretrained ones we'll discuss can be easily made available through a\nuniform interface. Along the way, we'll discuss data sources, define terminology like\nlabel, and attend a zebra rodeo.\n\nIf you\u2019re coming to PyTorch from another deep learning framework, and you'd\nrather jump right into learning the nuts and bolts of PyTorch, you can get away with\nskipping to the next chapter. The things we'll cover in this chapter are more fun than\nfoundational and are somewhat independent of any given deep learning tool. That\u2019s\nnot to say they\u2019re not important! But if you've worked with pretrained models in other\ndeep learning frameworks, then you already know how powerful a tool they can be.\nAnd if you\u2019re already familiar with the generative adversarial network (GAN) game,\nyou don\u2019t need us to explain it to you.\n\nWe hope you keep reading, though, since this chapter hides some important skills\nunder the fun. Learning how to run a pretrained model using PyTorch is a useful\nskill\u2014full stop. It\u2019s especially useful if the model has been trained on a large dataset.\nWe will need to get accustomed to the mechanics of obtaining and running a neural\nnetwork on real-world data, and then visualizing and evaluating its outputs, whether\nwe trained it or not.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.2,
                        "section_name": "A pretrained network that recognizes the subject of an image",
                        "section_path": "./screenshots-images-2/chapter_3/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_2/47dc4623-23f6-4ca1-8de7-13ebd32547b6.png",
                            "./screenshots-images-2/chapter_3/section_2/28432338-0270-4a91-9102-d8c4a95eefdf.png",
                            "./screenshots-images-2/chapter_3/section_2/ea2466c6-a124-40ed-b119-5297e4279276.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A pretrained network that recognizes the subject of an image\n\nAs our first foray into deep learning, we'll run a state-of-the-art deep neural network\nthat was pretrained on an object-recognition task. There are many pretrained net-\nworks that can be accessed through source code repositories. It is common for\nresearchers to publish their source code along with their papers, and often the code\ncomes with weights that were obtained by training a model on a reference dataset.\nUsing one of these models could enable us to, for example, equip our next web ser-\nvice with image-recognition capabilities with very little effort.\n\nThe pretrained network we'll explore here was trained on a subset of the ImageNet\ndataset (http://imagenet.stanford.edu). ImageNet is a very large dataset of over 14 mil-\nlion images maintained by Stanford University. All of the images are labeled with a hier-\narchy of nouns that come from the WordNet dataset (http://wordnet.princeton.edu),\nwhich is in turn a large lexical database of the English language.\n\nThe ImageNet dataset, like several other public datasets, has its origin in academic\ncompetitions. Competitions have traditionally been some of the main playing fields\nwhere researchers at institutions and companies regularly challenge each other.\nAmong others, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has\ngained popularity since its inception in 2010. This particular competition is based on\na few tasks, which can vary each year, such as image classification (telling what object\ncategories the image contains), object localization (identifying objects\u2019 position in\nimages), object detection (identifying and labeling objects in images), scene classifica-\ntion (classifying a situation in an image), and scene parsing (segmenting an image\ninto regions associated with semantic categories, such as cow, house, cheese, hat). In\nparticular, the image-classification task consists of taking an input image and produc-\ning a list of 5 labels out of 1,000 total categories, ranked by confidence, describing the\ncontent of the image.\n\nThe training set for ILSVRC consists of 1.2 million images labeled with one of\n1,000 nouns (for example, \u201cdog\u201d), referred to as the class of the image. In this sense,\nwe will use the terms /abel and class interchangeably. We can take a peek at images\nfrom ImageNet in figure 2.1.\n\nFigure 2.1 A small sample of ImageNet images\n\nPRETRALNED\n\nWEIGHTS\nclasses bar\n\u00a9 .S- tne\n(me roRMNRD\nDOG CENTER, AND\nIMAGE \u2014 NORMALTZE shonns LABELS\n\nFigure 2.2 The inference process\n\nWe are going to end up being able to take our own images and feed them into our\npretrained model, as pictured in figure 2.2. This will result in a list of predicted labels\nfor that image, which we can then examine to see what the model thinks our image is.\nSome images will have predictions that are accurate, and others will not!\n\nThe input image will first be preprocessed into an instance of the multidimen-\nsional array class torch. Tensor. It is an RGB image with height and width, so this ten-\nsor will have three dimensions: the three color channels, and two spatial image\ndimensions of a specific size. (We'll get into the details of what a tensor is in chapter 3,\nbut for now, think of it as being like a vector or matrix of floating-point numbers.)\nOur model will take that processed input image and pass it into the pretrained net-\nwork to obtain scores for each class. The highest score corresponds to the most likely\nclass according to the weights. Each class is then mapped one-to-one onto a class label.\nThat output is contained in a torch.Tensor with 1,000 elements, each representing\nthe score associated with that class.\n\nBefore we can do all that, we'll need to get the network itself, take a peek under\nthe hood to see how it\u2019s structured, and learn about how to prepare our data before\nthe model can use it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.3,
                        "section_name": "Obtaining a pretrained network for image recognition",
                        "section_path": "./screenshots-images-2/chapter_3/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_3/33367b0e-cef2-44a3-9660-d042ea0d7a47.png",
                            "./screenshots-images-2/chapter_3/section_3/b5ef0082-5055-495f-b76f-ca36acb9fa14.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Obtaining a pretrained network for image recognition\n\nAs discussed, we will now equip ourselves with a network trained on ImageNet. To do\nso, we'll take a look at the TorchVision project (https://github.com/pytorch/vision),\nwhich contains a few of the best-performing neural network architectures for com-\nputer vision, such as AlexNet (http://mng.bz/lo6z), ResNet (https://arxiv.org/pdf/\n1512.03385.pdf), and Inception v3 (https://arxiv.org/pdf/1512.00567.pdf). It also\nhas easy access to datasets like ImageNet and other utilities for getting up to speed\nwith computer vision applications in PyTorch. We'll dive into some of these further\nalong in the book. For now, let\u2019s load up and run two networks: first AlexNet, one of\nthe early breakthrough networks for image recognition; and then a residual network,\nResNet for short, which won the ImageNet classification, detection, and localization\n\ncompetitions, among others, in 2015. If you didn\u2019t get PyTorch up and running in\nchapter 1, now is a good time to do that.\n\nThe predefined models can be found in torchvision.models (code/plch2/2\n_pre_trained_networks.ipynb):\n\n# In(1]:\nfrom torchvision import models\n\nWe can take a look at the actual models:\n\n# In(2]:\ndir (models)\n\n# Out[2]:\n['AlexNet\",\n'DenseNet',\n\u2018Inception3',\n\u2018'ResNet',\n\u2018SaueezeNet',\n'VGG',\n\n\u2018alexnet',\n\n'densenet',\n\u2018densenet121',\n\n\u2018resnet',\n'resnet101',\n\u2018resnet152',\n\n]\nThe capitalized names refer to Python classes that implement a number of popular\nmodels. They differ in their architecture\u2014that is, in the arrangement of the operations\noccurring between the input and the output. The lowercase names are convenience\nfunctions that return models instantiated from those classes, sometimes with different\n\nparameter sets. For instance, resnet101 returns an instance of ResNet with 101 layers,\nresnet18 has 18 layers, and so on. We'll now turn our attention to AlexNet.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.4,
                        "section_name": "AlexNet",
                        "section_path": "./screenshots-images-2/chapter_3/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_4/b17b9ca7-a4e4-4b92-b14f-1f28a33eab62.png",
                            "./screenshots-images-2/chapter_3/section_4/a05e44e0-291c-4aef-b661-59e9969a593a.png",
                            "./screenshots-images-2/chapter_3/section_4/27009c52-fe32-46db-9e13-da5617ca7257.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "AlexNet\n\nThe AlexNet architecture won the 2012 ILSVRC by a large margin, with a top-5 test\nerror rate (that is, the correct label must be in the top 5 predictions) of 15.4%. By\ncomparison, the second-best submission, which wasn\u2019t based on a deep network,\ntrailed at 26.2%. This was a defining moment in the history of computer vision: the\nmoment when the community started to realize the potential of deep learning for\nvision tasks. That leap was followed by constant improvement, with more modern\narchitectures and training methods getting top-5 error rates as low as 3%.\n\nBy today\u2019s standards, AlexNet is a rather small network, compared to state-of-the-\nart models. But in our case, it\u2019s perfect for taking a first peek at a neural network that\ndoes something and learning how to run a pretrained version of it on a new image.\n\nWe can see the structure of AlexNet in figure 2.3. Not that we have all the elements\nfor understanding it now, but we can anticipate a few aspects. First, each block consists\nof a bunch of multiplications and additions, plus a sprinkle of other functions in the\noutput that we'll discover in chapter 5. We can think of it as a filter\u2014a function that\ntakes one or more images as input and produces other images as output. The way it\ndoes so is determined during training, based on the examples it has seen and on the\ndesired outputs for those.\n\nALEXNET\nlo\n\u2018 \\\n6\nNIN YRA _ 6\n\u201c|= - = }\n\u00b0 7\\'\n386 384 256 \u00b0\n256 \\,000\n4,040 4,046\n\na6\n\nFigure 2.3 The AlexNet architecture\n\nIn figure 2.3, input images come in from the left and go through five stacks of filters,\neach producing a number of output images. After each filter, the images are reduced\nin size, as annotated. The images produced by the last stack of filters are laid out as a\n4,096-element 1D vector and classified to produce 1,000 output probabilities, one for\neach output class.\n\nIn order to run the AlexNet architecture on an input image, we can create an\ninstance of the AlexNet class. This is how it\u2019s done:\n\n# In[3]):\nalexnet = models.AlexNet ()\n\nAt this point, alexnet is an object that can run the AlexNet architecture. It\u2019s not\nessential for us to understand the details of this architecture for now. For the time\nbeing, AlexNet is just an opaque object that can be called like a function. By providing\n\nalexnet with some precisely sized input data (we'll see shortly what this input data\nshould be), we will run a forward pass through the network. That is, the input will run\nthrough the first set of neurons, whose outputs will be fed to the next set of neurons,\nall the way to the final output. Practically speaking, assuming we have an input object\nof the right type, we can run the forward pass with output = alexnet (input).\n\nBut if we did that, we would be feeding data through the whole network to pro-\nduce ... garbage! That\u2019s because the network is uninitialized: its weights, the numbers\nby which inputs are added and multiplied, have not been trained on anything\u2014the\nnetwork itself is a blank (or rather, random) slate. We'd need to either train it from\nscratch or load weights from prior training, which we'll do now.\n\nTo this end, let\u2019s go back to the models module. We learned that the uppercase\nnames correspond to classes that implement popular architectures for computer\nvision. The lowercase names, on the other hand, are functions that instantiate models\nwith predefined numbers of layers and units and optionally download and load pre-\ntrained weights into them. Note that there\u2019s nothing essential about using one of\nthese functions: they just make it convenient to instantiate the model with a number\nof layers and units that matches how the pretrained networks were built.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.5,
                        "section_name": "ResNet",
                        "section_path": "./screenshots-images-2/chapter_3/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_5/51e28dbc-9fff-41fc-b9df-38c871dbc447.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "ResNet\n\nUsing the resnet101 function, we'll now instantiate a 101-layer convolutional neural\nnetwork. Just to put things in perspective, before the advent of residual networks in\n2015, achieving stable training at such depths was considered extremely hard. Resid-\nual networks pulled a trick that made it possible, and by doing so, beat several bench-\nmarks in one sweep that year.\n\nLet\u2019s create an instance of the network now. We'll pass an argument that will\ninstruct the function to download the weights of resnet101 trained on the ImageNet\ndataset, with 1.2 million images and 1,000 categories:\n\n# In[4]:\nresnet = models.resnet101(pretrained=True)\n\nWhile we\u2019re staring at the download progress, we can take a minute to appreciate that\nresnet101 sports 44.5 million parameters\u2014that\u2019s a lot of parameters to optimize\nautomatically!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.6,
                        "section_name": "Ready, set, almost run",
                        "section_path": "./screenshots-images-2/chapter_3/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_6/9d3bd7f3-19be-444d-b170-83e9718cc983.png",
                            "./screenshots-images-2/chapter_3/section_6/bb985e94-7a6a-4d0c-ab45-46efc239f036.png",
                            "./screenshots-images-2/chapter_3/section_6/193b5256-ce7b-478d-b62d-ef8c94a50923.png",
                            "./screenshots-images-2/chapter_3/section_6/17c504ec-4a68-4f5b-a00a-a62922f52ae2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Ready, set, almost run\n\nOK, what did we just get? Since we're curious, we'll take a peek at what a resnet101\nlooks like. We can do so by printing the value of the returned model. This gives us a\ntextual representation of the same kind of information we saw in 2.3, providing details\nabout the structure of the network. For now, this will be information overload, but as\nwe progress through the book, we'll increase our ability to understand what this code\nis telling us:\n\n# In[5):\nresnet\n\n# Out(5):\nResNet (\n{convi): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),\nbias=False)\n{(bnl): BatchNorm2d(64, eps=le-05, momentum=0.1, affine=True,\ntrack_running_stats=True)\n{relu): ReLU(inplace)\n{maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1,\nceil_mode=False)\n(layer1): Sequential (\n(0): Bottleneck (\n\n)\n)\n{avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n(fc): Linear(in_features=2048, out_features=1000, bias=True)\n\nWhat we are seeing here is modules, one per line. Note that they have nothing in com-\nmon with Python modules: they are individual operations, the building blocks of a\nneural network. They are also called /ayers in other deep learning frameworks.\n\nIf we scroll down, we'll see a lot of Bottleneck modules repeating one after the\nother (101 of them!), containing convolutions and other modules. That's the anat-\nomy of a typical deep neural network for computer vision: a more or less sequential\ncascade of filters and nonlinear functions, ending with a layer (fc) producing scores\nfor each of the 1,000 output classes (out_features).\n\nThe resnet variable can be called like a function, taking as input one or more\nimages and producing an equal number of scores for each of the 1,000 ImageNet\nclasses. Before we can do that, however, we have to preprocess the input images so\nthey are the right size and so that their values (colors) sit roughly in the same numeri-\ncal range. In order to do that, the torchvision module provides transforms, which\nallow us to quickly define pipelines of basic preprocessing functions:\n\n# In[6):\nfrom torchvision import transforms\npreprocess = transforms.Compose( [\ntransforms.Resize(256),\ntransforms.CenterCrop(224),\ntransforms .ToTensor(),\ntransforms .Normalize(\nmean=(0.485, 0.456, 0.406),\nstd=(0.229, 0.224, 0.225)\n))\n\nIn this case, we defined a preprocess function that will scale the input image to 256 x\n256, crop the image to 224 x 224 around the center, transform it to a tensor (a\nPyTorch multidimensional array: in this case, a 3D array with color, height, and\n\nwidth), and normalize its RGB (red, green, blue) components so that they have\ndefined means and standard deviations. These need to match what was presented to\nthe network during training, if we want the network to produce meaningful answers.\nWe'll go into more depth about transforms when we dive into making our own image-\nrecognition models in section 7.1.3.\n\nWe can now grab a picture of our favorite dog (say, bobby.jpg from the GitHub repo),\npreprocess it, and then see what ResNet thinks of it. We can start by loading an image\nfrom the local filesystem using Pillow (https://pillow.readthedocs.io/en/stable), an\nimage-manipulation module for Python:\n\n# In[7]:\nfrom PIL import Image\nimg = Image.open(\"../data/pich2/bobby. jpg\")\n\nIf we were following along from a Jupyter Notebook, we would do the following to see\nthe picture inline (it would be shown where the <PIL.JpegImagePlugin... is in the\nfollowing):\n\n# In(8]:\n\nimg\n\n# Out [8]:\n\n<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x720 at\n0x1B1601360B8>\n\nOtherwise, we can invoke the show method, which will pop up a window with a viewer,\nto see the image shown in figure 2.4:\n\n>>> img.show()\n\nFigure 2.4 Bobby, our very special input image\n\nNext, we can pass the image through our preprocessing pipeline:\n\n# In[9]:\nimg_t = preprocess (img)\n\nThen we can reshape, crop, and normalize the input tensor in a way that the network\nexpects. We'll understand more of this in the next two chapters; hold tight for now:\n\n# In[10]:\n\nimport torch\n\nbatch_t = torch.unsqueeze(img_t, 0)\n\nWe\u2019re now ready to run our model.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.7,
                        "section_name": "Run!",
                        "section_path": "./screenshots-images-2/chapter_3/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_7/b1565564-62c3-49c2-92f1-98b867dbe407.png",
                            "./screenshots-images-2/chapter_3/section_7/d1b208ce-f4aa-4cae-b318-21b356538769.png",
                            "./screenshots-images-2/chapter_3/section_7/41f29dfb-f989-4c60-8b4a-4def8f51dd9a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Run!\n\nThe process of running a trained model on new data is called inference in deep learn-\ning circles. In order to do inference, we need to put the network in eval mode:\n\n# In[11):\nresnet .eval()\n\n# Out[ii)}:\nResNet (\n{convi): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),\nbias=False)\n{bn1): BatchNorm2d(64, eps=le-05, momentum=0.1, affine=True,\ntrack_running_stats=True)\n{relu): ReLU(inplace)\n({maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1,\nceil_mode=False)\n({layer1): Sequential (\n(0): Bottleneck (\n\n)\n)\n{avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n(fc): Linear (in_features=2048, out_features=1000, bias=True)\n)\n\nIf we forget to do that, some pretrained models, like batch normalization and dropout,\nwill not produce meaningful answers, just because of the way they work internally.\nNow that eval has been set, we're ready for inference:\n\n# In[12):\nout = resnet (batch_t)\nout\n\n# Out[1i2]):\ntensor([{[ -3.4803, -1.6618, -2.4515, -3.2662, -3.2466, -1.3611\n~2.0465, -2.5112, -1.3043, -2.8900, -1.6862, -1.3055\n\n2.8674, -3.7442, 1.5085, -3.2500, -2.4894, -0.3354,\n0.1286, -1.1355, 3.3969, 4.4584)])\n\nAstaggering set of operations involving 44.5 million parameters has just happened, pro-\nducing a vector of 1,000 scores, one per ImageNet class. That didn\u2019t take long, did it?\n\nWe now need to find out the label of the class that received the highest score. This\nwill tell us what the model saw in the image. If the label matches how a human would\ndescribe the image, that\u2019s great! It means everything is working. If not, then either some-\nthing went wrong during training, or the image is so different from what the model\nexpects that the model can\u2019t process it properly, or there\u2019s some other similar issue.\n\nTo see the list of predicted labels, we will load a text file listing the labels in the\nsame order they were presented to the network during training, and then we will pick\nout the label at the index that produced the highest score from the network. Almost\nall models meant for image recognition have output in a form similar to what we're\nabout to work with.\n\nLet\u2019s load the file containing the 1,000 labels for the ImageNet dataset classes:\n\n# In[(13]:\nwith open('../data/plch2/imagenet_classes.txt') as \u00a3:\nlabels = [line.strip() for line in f.readlines())\n\nAt this point, we need to determine the index corresponding to the maximum score\nin the out tensor we obtained previously. We can do that using the max function in\nPyTorch, which outputs the maximum value in a tensor as well as the indices where\nthat maximum value occurred:\n\n# In{14]:\n_, index = torch.max(out, 1)\n\nWe can now use the index to access the label. Here, index is not a plain Python num-\nber, but a one-element, one-dimensional tensor (specifically, tensor ([207])), so we\nneed to get the actual numerical value to use as an index into our labels list using\nindex[0]. We also use torch.nn. functional.softmax (http://mng.bz/BYnq) to nor-\nmalize our outputs to the range [0, 1], and divide by the sum. That gives us something\nroughly akin to the confidence that the model has in its prediction. In this case, the\nmodel is 96% certain that it knows what it\u2019s looking at is a golden retriever:\n\n# In(15]:\n\npercentage = torch.nn.functional.softmax(out, dim=1)[0) * 100\nlabels [index[0]], percentage[index[0]].item()}\n\n# Out[15]:\n\n(\"golden retriever', 96.29334259033203)\n\nUh oh, who's a good boy?\n\nSince the model produced scores, we can also find out what the second best, third\nbest, and so on were. To do this, we can use the sort function, which sorts the values\nin ascending or descending order and also provides the indices of the sorted values in\nthe original array:\n\n# In[16):\n.. indices = torch.sort(out, descending=True)\n((labels[idx], percentage[idx].item()} for idx in indices[0][:5]]\n\n# Out[16]:\n((\u2018golden retriever', 96.29334259033203),\n(\"Labrador retriever\u2019, 2.80812406539917),\n{*cocker spaniel, English cocker spaniel, cocker', 0.28267428278923035),\n{\u2018redbone', 0.2086310237646103),\n(\u2018tennis ball', 0.11621569097042084))\n\nWe see that the first four are dogs (redbone is a breed; who knew?), after which things\nstart to get funny. The fifth answer, \u201ctennis ball,\u201d is probably because there are enough\npictures of tennis balls with dogs nearby that the model is essentially saying, \u201cThere\u2019s a\n0.1% chance that I\u2019ve completely misunderstood what a tennis ball is.\u201d This is a great\nexample of the fundamental differences in how humans and neural networks view the\nworld, as well as how easy it is for strange, subtle biases to sneak into our data.\n\nTime to play! We can go ahead and interrogate our network with random images\nand see what it comes up with. How successful the network will be will largely depend\non whether the subjects were well represented in the training set. If we present an\nimage containing a subject outside the training set, it\u2019s quite possible that the network\nwill come up with a wrong answer with pretty high confidence. It\u2019s useful to experi-\nment and get a feel for how a model reacts to unseen data.\n\nWe've just run a network that won an image-classification competition in 2015. It\nlearned to recognize our dog from examples of dogs, together with a ton of other\nreal-world subjects. We'll now see how different architectures can achieve other kinds\nof tasks, starting with image generation.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.8,
                        "section_name": "A pretrained model that fakes it until it makes it",
                        "section_path": "./screenshots-images-2/chapter_3/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_8/ee5bc59d-71f6-4359-af59-1fdbaf712c15.png",
                            "./screenshots-images-2/chapter_3/section_8/180becbe-1f5f-4730-998b-43381cec8ae2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A pretrained model that fakes it until it makes it\n\nLet\u2019s suppose, for a moment, that we're career criminals who want to move into sell-\ning forgeries of \u201clost\u201d paintings by famous artists. We're criminals, not painters, so as\nwe paint our fake Rembrandts and Picassos, it quickly becomes apparent that they're\namateur imitations rather than the real deal. Even if we spend a bunch of time practic-\ning until we get a canvas that we can\u2019t tell is fake, trying to pass it off at the local art\nauction house is going to get us kicked out instantly. Even worse, being told \u201cThis is\nclearly fake; get out,\u201d doesn\u2019t help us improve! We'd have to randomly try a bunch of\nthings, gauge which ones took slightly longer to recognize as forgeries, and emphasize\nthose traits on our future attempts, which would take far too long.\n\nInstead, we need to find an art historian of questionable moral standing to inspect\nour work and tell us exactly what it was that tipped them off that the painting wasn\u2019t\nlegit. With that feedback, we can improve our output in clear, directed ways, until our\nsketchy scholar can no longer tell our paintings from the real thing.\n\nSoon, we'll have our \u201cBotticelli\u201d in the Louvre, and their Benjamins in our pockets.\nWe'll be rich!\n\nWhile this scenario is a bit farcical, the underlying technology is sound and will\nlikely have a profound impact on the perceived veracity of digital data in the years to\ncome. The entire concept of \u201cphotographic evidence\u201d is likely to become entirely sus-\npect, given how easy it will be to automate the production of convincing, yet fake,\nimages and video. The only key ingredient is data. Let\u2019s see how this process works.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.9,
                        "section_name": "The GAN game",
                        "section_path": "./screenshots-images-2/chapter_3/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_9/33719461-9683-49bb-bbe1-2c497fa524b9.png",
                            "./screenshots-images-2/chapter_3/section_9/cfce21a7-ef70-4f1f-807e-6b5b72937c1d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The GAN game\n\nIn the context of deep learning, what we've just described is known as the GAN game,\nwhere two networks, one acting as the painter and the other as the art historian, com-\npete to outsmart each other at creating and detecting forgeries. GAN stands for gener-\native adversarial network, where generative means something is being created (in this\ncase, fake masterpieces), adversarial means the two networks are competing to out-\nsmart the other, and well, network is pretty obvious. These networks are one of the\nmost original outcomes of recent deep learning research.\n\nRemember that our overarching goal is to produce synthetic examples of a class of\nimages that cannot be recognized as fake. When mixed in with legitimate examples, a\nskilled examiner would have trouble determining which ones are real and which are\nour forgeries.\n\nThe generator network takes the role of the painter in our scenario, tasked with pro-\nducing realisticlooking images, starting from an arbitrary input. The discriminator net-\nwork is the amoral art inspector, needing to tell whether a given image was fabricated\nby the generator or belongs in a set of real images. This two-network design is atypical\nfor most deep learning architectures but, when used to implement a GAN game, can\nlead to incredible results.\n\nFigure 2.5 shows a rough picture of what's going on. The end goal for the generator\nis to fool the discriminator into mixing up real and fake images. The end goal for the\ndiscriminator is to find out when it\u2019s being tricked, but it also helps inform the gener-\nator about the identifiable mistakes in the generated images. At the start, the generator\nproduces confused, three-eyed monsters that look nothing like a Rembrandt portrait.\nThe discriminator is easily able to distinguish the muddled messes from the real paint-\nings. As training progresses, information flows back from the discriminator, and the\ngenerator uses it to improve. By the end of training, the generator is able to produce\nconvincing fakes, and the discriminator no longer is able to tell which is which.\n\nNote that \u201cDiscriminator wins\u201d or \u201cGenerator wins\u201d shouldn't be taken literally\u2014\nthere\u2019s no explicit tournament between the two. However, both networks are trained\nbased on the outcome of the other network, which drives the optimization of the\nparameters of each network.\n\nThis technique has proven itself able to lead to generators that produce realistic\nimages from nothing but noise and a conditioning signal, like an attribute (for exam-\nple, for faces: young, female, glasses on) or another image. In other words, a well-\ntrained generator learns a plausible model for generating images that look real even\nwhen examined by humans.\n\nBE\n\nrhAans ales)\nGENERATOR\n\neee\n\nos GENeRATED DISCRIMINATOR WINS\n\n6 i WINS\n\nGETS BETTER\nAT MAKING\nSTUFF UP\nGETS BETTER\nAT NOT BEING\nFOOLED\n\nFigure 2.56 Concept of a GAN game\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.1,
                        "section_name": "CycleGAN",
                        "section_path": "./screenshots-images-2/chapter_3/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_10/2762ba2b-642e-4278-b589-62167207163d.png",
                            "./screenshots-images-2/chapter_3/section_10/3b8390f5-a69b-4b12-941a-5df1eb16b35e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "CycleGAN\n\nAn interesting evolution of this concept is the CycleGAN. A CycleGAN can turn\nimages of one domain into images of another domain (and back), without the need\nfor us to explicitly provide matching pairs in the training set.\n\nIn figure 2.6, we have a CycleGAN workflow for the task of turning a photo of a\nhorse into a zebra, and vice versa. Note that there are two separate generator net-\nworks, as well as two distinct discriminators.\n\nZEBRA!\n\n\u00ab+ SAME PROCESS STARTING\nFROM ZEBRA...\n\nFigure 2.6 A CycleGAN trained to the point that it can fool both discriminator networks\n\nAs the figure shows, the first generator learns to produce an image conforming to a tar-\nget distribution (zebras, in this case) starting from an image belonging to a different\ndistribution (horses), so that the discriminator can\u2019t tell if the image produced from a\nhorse photo is actually a genuine picture of a zebra or not. At the same time\u2014and\nhere\u2019s where the Cycle prefix in the acronym comes in\u2014the resulting fake zebra is sent\nthrough a different generator going the other way (zebra to horse, in our case), to be\njudged by another discriminator on the other side. Creating such a cycle stabilizes the\ntraining process considerably, which addresses one of the original issues with GANs.\n\nThe fun part is that at this point, we don\u2019t need matched horse/zebra pairs as\nground truths (good luck getting them to match poses!). It\u2019s enough to start from a\ncollection of unrelated horse images and zebra photos for the generators to learn\ntheir task, going beyond a purely supervised setting. The implications of this model go\neven further than this: the generator learns how to selectively change the appearance\nof objects in the scene without supervision about what's what. There\u2019s no signal indi-\ncating that manes are manes and legs are legs, but they get translated to something\nthat lines up with the anatomy of the other animal.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.11,
                        "section_name": "A network that turns horses into zebras",
                        "section_path": "./screenshots-images-2/chapter_3/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_11/3f94a810-4b9f-4589-bf49-9cfd059823d7.png",
                            "./screenshots-images-2/chapter_3/section_11/83076006-3612-42d8-a17b-bb6f8e79d141.png",
                            "./screenshots-images-2/chapter_3/section_11/50dcae32-d31c-4424-bb6c-2e1b378a5a4c.png",
                            "./screenshots-images-2/chapter_3/section_11/b6432a84-e10f-4dc3-a93d-1372f9a53365.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A network that turns horses into zebras\n\nWe can play with this model right now. The CycleGAN network has been trained on a\ndataset of (unrelated) horse images and zebra images extracted from the ImageNet\ndataset. The network learns to take an image of one or more horses and turn them all\ninto zebras, leaving the rest of the image as unmodified as possible. While humankind\nhasn't held its breath over the last few thousand years for a tool that turn horses into\nzebras, this task showcases the ability of these architectures to model complex real-\nworld processes with distant supervision. While they have their limits, there are hints\nthat in the near future we won't be able to tell real from fake in a live video feed,\nwhich opens a can of worms that we'll duly close right now.\n\nPlaying with a pretrained CycleGAN will give us the opportunity to take a step\ncloser and look at how a network\u2014a generator, in this case\u2014is implemented. We'll\nuse our old friend ResNet. We'll define a ResNetGenerator class offscreen. The code\nis in the first cell of the 3_cyclegan.ipynb file, but the implementation isn\u2019t relevant\nright now, and it\u2019s too complex to follow until we\u2019ve gotten a lot more PyTorch experi-\nence. Right now, we\u2019re focused on what it can do, rather than how it does it. Let\u2019s\ninstantiate the class with default parameters (code/p1lch2/3_cyclegan.ipynb):\n\n# In(2]:\nnetG = ResNetGenerator()\n\nThe netG model has been created, but it contains random weights. We mentioned ear-\nlier that we would run a generator model that had been pretrained on the horse2zebra\ndataset, whose training set contains two sets of 1068 and 1335 images of horses and\nzebras, respectively. The dataset be found at http://mng.bz/8pKP. The weights of the\nmodel have been saved in a .pth file, which is nothing but a pickle file of the model\u2019s\n\ntensor parameters. We can load those into ResNetGenerator using the model\u2019s load\n_state_dict method:\n\n# In[3):\n\nmodel_path = '../data/plch2/horse2zebra_0.4.0.pth'\nmodel_data = torch. load (model_path)\n\nnetG. load_state_dict (model_data)\n\nAt this point, netG has acquired all the knowledge it achieved during training. Note\nthat this is fully equivalent to what happened when we loaded resnet101 from torch-\nvision in section 2.1.3; but the torchvision.resnet101 function hid the loading\nfrom us.\n\nLet\u2019s put the network in eval mode, as we did for resnet101:\n\n# In[4):\nnetG.eval()\n\n# Out[4):\nResNetGenerator (\n(model): Sequential (\n\n)\n\nPrinting out the model as we did earlier, we can appreciate that it\u2019s actually pretty con-\ndensed, considering what it does. It takes an image, recognizes one or more horses in\nit by looking at pixels, and individually modifies the values of those pixels so that what\ncomes out looks like a credible zebra. We won't recognize anything zebra-like in the\nprintout (or in the source code, for that matter): that\u2019s because there\u2019s nothing zebra-\nlike in there. The network is a scaffold\u2014the juice is in the weights.\n\nWe're ready to load a random image of a horse and see what our generator pro-\nduces. First, we need to import PIL and torchvision:\n\n# In[5):\nfrom PIL import Image\nfrom torchvision import transforms\n\nThen we define a few input transformations to make sure data enters the network with\nthe right shape and size:\n\n# In[6):\npreprocess = transforms.Compose(([transforms.Resize(256),\ntransforms .ToTensor(}])\n\nLet\u2019s open a horse file (see figure 2.7):\n\n# In[7):\nimg = Image.open(*../data/plch2/horse. jpg\")\nimg\n\nFigure 2.7 A man riding a\nhorse. The horse is not having it.\n\nOK, there\u2019s a dude on the horse. (Not for long, judging by the picture.) Anyhow, let's\npass it through preprocessing and turn it into a properly shaped variable:\n\n# In(8]:\nimg_t = preprocess (img)\nbatch_t = torch.unsqueeze(img_t, 0)\n\nWe shouldn't worry about the details right now. The important thing is that we follow\nfrom a distance. At this point, batch_t can be sent to our model:\n\n# In(9]:\nbatch_out = netG(batch_t)\n\nbatch_out is now the output of the generator, which we can convert back to an image:\n\n# In(10]:\n\nout_t = (batch_out.data.squeeze() + 1.0) / 2.0\nout_img = transforms.ToPILImage() (out_t)\n\n# out_img.save('../data/plch2/zebra.jpg')\nout_img\n\n# Out[10]:\n<PIL.Image.Image image mode=RGB size=316x256 at 0x23B24634F98>\n\nOh, man. Who rides a zebra that way? The resulting image (figure 2.8) is not perfect,\nbut consider that it is a bit unusual for the network to find someone (sort of) riding on\ntop of a horse. It bears repeating that the learning process has not passed through\ndirect supervision, where humans have delineated tens of thousands of horses or man-\nually Photoshopped thousands of zebra stripes. The generator has learned to produce\nan image that would fool the discriminator into thinking that was a zebra, and there was\nnothing fishy about the image (clearly the discriminator has never been to a rodeo).\n\nFigure 2.8 A man riding a\nzebra. The zebra is not having it.\n\nMany other fun generators have been developed using adversarial training or other\napproaches. Some of them are capable of creating credible human faces of nonexis-\ntent individuals; others can translate sketches into real-looking pictures of imaginary\nlandscapes. Generative models are also being explored for producing real-sounding\naudio, credible text, and enjoyable music. It is likely that these models will be the basis\nof future tools that support the creative process.\n\nOn a serious note, it\u2019s hard to overstate the implications of this kind of work. Tools\nlike the one we just downloaded are only going to become higher quality and more\nubiquitous. Face-swapping technology, in particular, has gotten considerable media\nattention. Searching for \u201cdeep fakes\u201d will turn up a plethora of example content\u2019\n(though we must note that there is a nontrivial amount of not-safe-for-work content\nlabeled as such; as with everything on the internet, click carefully).\n\nSo far, we've had a chance to play with a model that sees into images and a model\nthat generates new images. We'll end our tour with a model that involves one more,\nfundamental ingredient: natural language.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.12,
                        "section_name": "A pretrained network that describes scenes",
                        "section_path": "./screenshots-images-2/chapter_3/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_12/f3197d23-862c-4410-9ce4-65221e47f14f.png",
                            "./screenshots-images-2/chapter_3/section_12/1215454f-29b2-45ff-9241-5a9b5af1655a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A pretrained network that describes scenes\n\nIn order to get firsthand experience with a model involving natural language, we will\nuse a pretrained image-captioning model, generously provided by Ruotian Luo.? It is\nan implementation of the NeuralTalk2 model by Andrej Karpathy. When presented\nwith a natural image, this kind of model generates a caption in English that describes\nthe scene, as shown in figure 2.9. The model is trained on a large dataset of images\n\nmee oe ee\n\n\u2018\n_, 8 (\n\nCS \u201cAN OPD-LOOKING\n, fore) (2, 7 > FELLOW HOLDING\n\\ A PINK BALLOON\u201d\n\n(25 | \u2014\n\nOw gee\u201d mons ged\nCONVOLUTIONAL = RECURRENT\n(IMAGE RECOGNITION) (TEXT GENERATION)\n\nTRALNED END-TO-END ON\nTMAGE-CAPTION PALRS\n\nFigure 2.9 Concept of a captioning model\n\nalong with a paired sentence description: for example, \u201cA Tabby cat is leaning on a\nwooden table, with one paw on a laser mouse and the other on a black laptop.\u201d*\n\nThis captioning model has two connected halves. The first half of the model is a\nnetwork that learns to generate \u201cdescriptive\u201d numerical representations of the scene\n(Tabby cat, laser mouse, paw), which are then taken as input to the second half. That\nsecond half is a recurrent neural network that generates a coherent sentence by putting\nthose numerical descriptions together. The two halves of the model are trained\ntogether on image-caption pairs.\n\nThe second half of the model is called recurrent because it generates its outputs\n(individual words) in subsequent forward passes, where the input to each forward pass\nincludes the outputs of the previous forward pass. This generates a dependency of the\nnext word on words that were generated earlier, as we would expect when dealing with\nsentences or, in general, with sequences.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.13,
                        "section_name": "NeuralTalk2",
                        "section_path": "./screenshots-images-2/chapter_3/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_13/be0cdb50-aee8-425b-832c-206acbd790aa.png",
                            "./screenshots-images-2/chapter_3/section_13/998947ea-81c8-4674-9cb3-2fd6c1c8663f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "NeuralTalk2\n\nThe NeuralTalk2 model can be found at https://github.com/deep-learning-with-\npytorch/ImageCaptioning.pytorch. We can place a set of images in the data directory\nand run the following script:\n\npython eval.py --model ./data/FC/fc-model.pth\nwe --infos_path ./data/FC/fc-infos.pkl --image_folder ./data\n\nLet's ty it with our horse.jpg image. It says, \u201cA person riding a horse on a beach.\u201d\nQuite appropriate.\n\nNow, just for fun, let\u2019s see if our CycleGAN can also fool this NeuralTalk2 model.\nLet\u2019s add the zebra.jpg image in the data folder and rerun the model: \u201cA group of\nzebras are standing in a field.\u201d Well, it got the animal right, but it saw more than one\nzebra in the image. Certainly this is not a pose that the network has ever seen a zebra\nin, nor has it ever seen a rider on a zebra (with some spurious zebra patterns). In addi-\ntion, it is very likely that zebras are depicted in groups in the training dataset, so there\nmight be some bias that we could investigate. The captioning network hasn't\ndescribed the rider, either. Again, it\u2019s probably for the same reason: the network\nwasn't shown a rider on a zebra in the training dataset. In any case, this is an impres-\nsive feat: we generated a fake image with an impossible situation, and the captioning\nnetwork was flexible enough to get the subject right.\n\nWe'd like to stress that something like this, which would have been extremely hard\nto achieve before the advent of deep learning, can be obtained with under a thousand\nlines of code, with a general-purpose architecture that knows nothing about horses or\nzebras, and a corpus of images and their descriptions (the MS COCO dataset, in this\ncase). No hardcoded criterion or grammar\u2014everything, including the sentence,\nemerges from patterns in the data.\n\nThe network architecture in this last case was, in a way, more complex than the\nones we saw earlier, as it includes two networks. One is recurrent, but it was built out\nof the same building blocks, all of which are provided by PyTorch.\n\nAt the time of this writing, models such as these exist more as applied research or\nnovelty projects, rather than something that has a well-defined, concrete use. The\nresults, while promising, just aren\u2019t good enough to use ... yet. With time (and addi-\ntional training data), we should expect this class of models to be able to describe the\nworld to people with vision impairment, transcribe scenes from video, and perform\nother similar tasks.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.14,
                        "section_name": "Torch Hub",
                        "section_path": "./screenshots-images-2/chapter_3/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_14/9b6bd964-bb69-4827-b0b6-6cecb8158945.png",
                            "./screenshots-images-2/chapter_3/section_14/4818d725-a688-4bcf-ac07-06db37c1ee60.png",
                            "./screenshots-images-2/chapter_3/section_14/774625e4-a623-4a4a-a4eb-7c7b430ce1cd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Torch Hub\n\nPretrained models have been published since the early days of deep learning, but\nuntil PyTorch 1.0, there was no way to ensure that users would have a uniform inter-\nface to get them. TorchVision was a good example of a clean interface, as we saw ear-\nlier in this chapter; but other authors, as we have seen for CycleGAN and NeuralTalk2,\nchose different designs.\n\nPyTorch 1.0 saw the introduction of Torch Hub, which is a mechanism through\nwhich authors can publish a model on GitHub, with or without pretrained weights,\nand expose it through an interface that PyTorch understands. This makes loading a\npretrained model from a third party as easy as loading a TorchVision model.\n\nAll it takes for an author to publish a model through the Torch Hub mechanism is\nto place a file named hubconf.py in the root directory of the GitHub repository. The\nfile has a very simple structure:\n\nOptional list of modules the code depends on\n\nLs dependencies = ['torch', 'math']\ndef some_entry_fn(*args, **kwargs): 3 One or more functions to be\nmodel = build _some_model(*args, **kwargs) exposed to users as entry points\nreturn model for the repository. These functions\nshould initialize models according\ndef another_entry_fn(*args, **kwargs) : to the arguments and return them.\n\nmodel = build_another_model(*args, **kwargs)\nreturn model\n\nIn our quest for interesting pretrained models, we can now search for GitHub reposi-\ntories that include hubconf.py, and we'll know right away that we can load them using\nthe torch.hub module. Let\u2019s see how this is done in practice. To do that, we'll go back\nto TorchVision, because it provides a clean example of how to interact with Torch Hub.\n\nLet\u2019s visit https://github.com/pytorch/vision and notice that it contains a hub-\nconf.py file. Great, that checks. The first thing to do is to look in that file to see the entry\npoints for the repo\u2014we\u2019ll need to specify them later. In the case of TorchVision, there\nare two: resnet18 and resnet50. We already know what these do: they return an 18-\nlayer and a 50-layer ResNet model, respectively. We also see that the entry-point func-\ntions include a pretrained keyword argument. If True, the returned models will be ini-\ntialized with weights learned from ImageNet, as we saw earlier in the chapter.\n\nNow we know the repo, the entry points, and one interesting keyword argument.\nThat\u2019s about all we need to load the model using torch.hub, without even cloning the\nrepo. That\u2019s right, PyTorch will handle that for us:\n\nimport torch\n\n. Name and branch\nfrom torch import hub of the GitHub repo\nName of the entry-\nresnet18_model = hub.load('pytorch/vision:master', << point function\n\n\u2018resnet18',\n\npretrained=True) \u00ab\nKeyword argument\n\nThis manages to download a snapshot of the master branch of the pytorch/vision\nrepo, along with the weights, to a local directory (defaults to .torch/hub in our home\ndirectory) and run the resnet18 entry-point function, which returns the instantiated\nmodel. Depending on the environment, Python may complain that there\u2019s a module\nmissing, like PIL. Torch Hub won't install missing dependencies, but it will report\nthem to us so that we can take action.\n\nAt this point, we can invoke the returned model with proper arguments to run a\nforward pass on it, the same way we did earlier. The nice part is that now every model\npublished through this mechanism will be accessible to us using the same modalities,\nwell beyond vision.\n\nNote that entry points are supposed to return models; but, strictly speaking, they\nare not forced to. For instance, we could have an entry point for transforming inputs\nand another one for turning the output probabilities into a text label. Or we could\nhave an entry point for just the model, and another that includes the model along\nwith the pre- and postprocessing steps. By leaving these options open, the PyTorch\ndevelopers have provided the community with just enough standardization and a lot\nof flexibility. We'll see what patterns will emerge from this opportunity.\n\nTorch Hub is quite new at the time of writing, and there are only a few models pub-\nlished this way. We can get at them by Googling \u201cgithub.com hubconf.py.\u201d Hopefully\nthe list will grow in the future, as more authors share their models through this channel.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.15,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_3/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_15/d53c6334-359d-4665-a404-2b2b9ae34df3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\n\nWe hope this was a fun chapter. We took some time to play with models created with\nPyTorch, which were optimized to carry out specific tasks. In fact, the more enterpris-\ning of us could already put one of these models behind a web server and start a busi-\nness, sharing the profits with the original authors!* Once we learn how these models\nare built, we will also be able to use the knowledge we gained here to download a pre-\ntrained model and quickly fine-tune it on a slightly different task.\n\nWe will also see how building models that deal with different problems on differ-\nent kinds of data can be done using the same building blocks. One thing that PyTorch\ndoes particularly right is providing those building blocks in the form of an essential\ntoolset\u2014PyTorch is not a very large library from an API perspective, especially when\ncompared with other deep learning frameworks.\n\nThis book does not focus on going through the complete PyTorch API or review-\ning deep learning architectures; rather, we will build hands-on knowledge of these\nbuilding blocks. This way, you will be able to consume the excellent online documen-\ntation and repositories on top of a solid foundation.\n\nStarting with the next chapter, we'll embark on a journey that will enable us to\nteach our computer skills like those described in this chapter from scratch, using\nPyTorch. We'll also learn that starting from a pretrained network and fine-tuning it on\nnew data, without starting from scratch, is an effective way to solve problems when the\ndata points we have are not particularly numerous. This is one further reason pre-\ntrained networks are an important tool for deep learning practitioners to have. Time\nto learn about the first fundamental building block: tensors.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 4,
                "chapter_name": "It starts with a tensor",
                "chapter_path": "./screenshots-images-2/chapter_4",
                "sections": [
                    {
                        "section_id": 4.1,
                        "section_name": "It starts with a tensor",
                        "section_path": "./screenshots-images-2/chapter_4/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_1/83748287-6627-4499-ac5e-cbe1b622585c.png",
                            "./screenshots-images-2/chapter_4/section_1/6fdd90b5-32d0-4918-ada6-c557309927f5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the previous chapter, we took a tour of some of the many applications that deep\nlearning enables. They invariably consisted of taking data in some form, like images\nor text, and producing data in another form, like labels, numbers, or more images\nor text. Viewed from this angle, deep learning really consists of building a system\nthat can transform data from one representation to another. This transformation is\ndriven by extracting commonalities from a series of examples that demonstrate the\ndesired mapping. For example, the system might note the general shape of a dog\nand the typical colors of a golden retriever. By combining the two image properties,\nthe system can correctly map images with a given shape and color to the golden\nretriever label, instead of a black lab (or a tawny tomcat, for that matter). The\nresulting system can consume broad swaths of similar inputs and produce meaning-\nful output for those inputs.\n\nThe process begins by converting our input into floating-point numbers. We will\ncover converting image pixels to numbers, as we see in the first step of figure 3.1, in chap-\nter 4 (along with many other types of data). But before we can get to that, in this chapter,\nwe learn how to deal with all the floating-point numbers in PyTorch by using tensors.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.2,
                        "section_name": "The world as floating-point numbers",
                        "section_path": "./screenshots-images-2/chapter_4/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_2/89a4956b-8cdf-4e90-a171-1f334b61af04.png",
                            "./screenshots-images-2/chapter_4/section_2/d0753e80-fd5c-4113-b8ce-a60f87c149fe.png",
                            "./screenshots-images-2/chapter_4/section_2/a57cfe4e-4298-4c7e-b6c7-ab9ae213775a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The world as floating-point numbers\n\nSince floating-point numbers are the way a network deals with information, we need a\nway to encode real-world data of the kind we want to process into something digestible\nby a network and then decode the output back to something we can understand and\nuse for our purpose.\n\n6. Sa \u201cSUN\u201d\n\u2014? \u201cSEASIDE\u201d\n\u201cSCENERY\u201d\n\n\\ecs|\n\nff\n\noa | joa 0.01\n0.23| \\0.74 \u00e9f\nO4G| [0.4 O4l\n0.17 we 0.0\nTINeUT INTERMEDIATE OUTPUT\nREPRESENTATION REPRESENTATIONS REPRESENTATION\n(VALUES OF PIXELS) | (PROBABILITY OF CLASSES)\nSIMILAR INPUTS\nSHOULD LEAD To\nCLOSE REPRESENTATIONS\n\n(ESPECTALLY AT DEEPER LEVELS)\n\nFigure 3.1 A deep neural network learns how to transform an input representation to an output\nrepresentation. (Note: The numbers of neurons and outputs are not to scale.)\n\nA deep neural network typically learns the transformation from one form of data to\nanother in stages, which means the partially transformed data between each stage can\nbe thought of as a sequence of intermediate representations. For image recognition,\nearly representations can be things such as edge detection or certain textures like fur.\nDeeper representations can capture more complex structures like ears, noses, or eyes.\n\nIn general, such intermediate representations are collections of floating-point\nnumbers that characterize the input and capture the data\u2019s structure in a way that is\ninstrumental for describing how inputs are mapped to the outputs of the neural net-\n\nam ae 7 a eo eee en a\n\nexamples. These collections of floating-point numbers and their manipulation are at\nthe heart of modern Al\u2014we will see several examples of this throughout the book.\n\nIt\u2019s important to keep in mind that these intermediate representations (like those\nshown in the second step of figure 3.1) are the results of combining the input with the\nweights of the previous layer of neurons. Each intermediate representation is unique\nto the inputs that preceeded it.\n\nBefore we can begin the process of converting our data to floating-point input, we\nmust first have a solid understanding of how PyTorch handles and stores data\u2014as\ninput, as intermediate representations, and as output. This chapter will be devoted to\nprecisely that.\n\nTo this end, PyTorch introduces a fundamental data structure: the tensor. We\nalready bumped into tensors in chapter 2, when we ran inference on pretrained net-\nworks. For those who come from mathematics, physics, or engineering, the term fensor\ncomes bundled with the notion of spaces, reference systems, and transformations\nbetween them. For better or worse, those notions do not apply here. In the context of\ndeep learning, tensors refer to the generalization of vectors and matrices to an arbi-\ntrary number of dimensions, as we can see in figure 3.2. Another name for the same\nconcept is multidimensional array. The dimensionality of a tensor coincides with the\nnumber of indexes used to refer to scalar values within the tensor.\n\na\n4 467 571\n3 \\ 134 Q42\n5 (25 352\nSCALAR VECTOR MATRIX TENSOR TENSOR\nx[2]=5 X[L o] =7 Xo, 2 \\]=5 XU 3, 2] = 4\nop i) 2D 3D |\n\nN-D DATA -> N INDICES\n\nFigure 3.2 Tensors are the building blocks for representing data in PyTorch.\n\nPyTorch is not the only library that deals with multidimensional arrays. NumPy is by\nfar the most popular multidimensional array library, to the point that it has now argu-\nably become the lingua franca of data science. PyTorch features seamless interoperabil-\nity with NumPy, which brings with it first-class integration with the rest of the scientific\nlibraries in Python, such as SciPy (www.scipy.org), Scikit-learn (https://scikit-learn\norg), and Pandas (https: / /pandas.pydata.org).\n\nCompared to NumPy arrays, PyTorch tensors have a few superpowers, such as the\nability to perform very fast operations on graphical processing units (GPUs),\ndistribute operations on multiple devices or machines, and keep track of the graph of\n\ncomputations that created them. These are all important features when implementing\na modern deep learning library.\n\nWe'll start this chapter by introducing PyTorch tensors, covering the basics in\norder to set things in motion for our work in the rest of the book. First and foremost,\nwe'll learn how to manipulate tensors using the PyTorch tensor library. This includes\nthings like how the data is stored in memory, how certain operations can be per-\nformed on arbitrarily large tensors in constant time, and the aforementioned NumPy\ninteroperability and GPU acceleration. Understanding the capabilities and API of ten-\nsors is important if they\u2019re to become go-to tools in our programming toolbox. In the\nnext chapter, we'll put this knowledge to good use and learn how to represent several\ndifferent kinds of data in a way that enables learning with neural networks.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.3,
                        "section_name": "Tensors: Multidimensional arrays",
                        "section_path": "./screenshots-images-2/chapter_4/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_3/b792d792-2258-46bf-b714-4298874dfcf0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tensors: Multidimensional arrays\n\nWe have already learned that tensors are the fundamental data structure in PyTorch. A\ntensor is an array: that is, a data structure that stores a collection of numbers that are\naccessible individually using an index, and that can be indexed with multiple indices.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.4,
                        "section_name": "From Python lists to PyTorch tensors",
                        "section_path": "./screenshots-images-2/chapter_4/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_4/5300f08f-c42a-4e57-8e49-41c128146b04.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "From Python lists to PyTorch tensors\n\nLet's see List indexing in action so we can compare it to tensor indexing. Take a list\nof three numbers in Python (.code/p1ch3/1_tensors.ipynb):\n\n# In{1]:\na= (1.0, 2.0, 1.0)\n\nWe can access the first element of the list using the corresponding zero-based index:\n\n# In[2]:\na[0)\n\n# Out[2]:\n1.0\n\n# In{3]:\na(2) = 3.0\na\n\n# Out[3]:\n[1.0, 2.0, 3.0)\n\nIt is not unusual for simple Python programs dealing with vectors of numbers, such as\nthe coordinates of a 2D line, to use Python lists to store the vectors. As we will see in\nthe following chapter, using the more efficient tensor data structure, many types of\ndata\u2014from images to time series, and even sentences\u2014can be represented. By defin-\ning operations over tensors, some of which we'll explore in this chapter, we can slice\nand manipulate data expressively and efficiently at the same time, even from a high-\nlevel (and not particularly fast) language such as Python.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.5,
                        "section_name": "Constructing our first tensors",
                        "section_path": "./screenshots-images-2/chapter_4/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_5/ffea1bad-eec3-4743-a99a-54b6b898f419.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Constructing our first tensors\n\nLet\u2019s construct our first PyTorch tensor and see what it looks like. It won't be a partic-\nularly meaningful tensor for now, just three ones in a column:\n\n# In[4): | Imports the torch module\nimport torch css\na = torch.ones (3)\n\n\u2014\na Creates a one-dimensional\n\ntensor of size 3 filled with 1s\n\n# Out[4):\ntensor({1., 1., 1.])\n\n# In[5):\na{i)\n\n# Out[5):\ntensor (1i.)\n\n# In[6]:\nfloat (a(1))\n\n# Out[6):\n1.0\n\n# In[7)}:\na(2) = 2.0\na\n\n# Out(7):\ntensor({1., 1., 2.])\n\nAfter importing the torch module, we call a function that creates a (one-dimensional)\ntensor of size 3 filled with the value 1.0. We can access an element using its zero-based\nindex or assign a new value to it. Although on the surface this example doesn\u2019t differ\nmuch from a list of number objects, under the hood things are completely different.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.6,
                        "section_name": "The essence of tensors",
                        "section_path": "./screenshots-images-2/chapter_4/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_6/d96f558f-1218-4f12-b301-9a89668ccb69.png",
                            "./screenshots-images-2/chapter_4/section_6/eee52d7f-cf5d-4476-9627-1beaa7b86adb.png",
                            "./screenshots-images-2/chapter_4/section_6/0fdf860b-e6a3-4f62-ba94-9b3e01c78ebd.png",
                            "./screenshots-images-2/chapter_4/section_6/2050daf2-145b-4127-85af-8d620f9f72b4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The essence of tensors\n\nPython lists or tuples of numbers are collections of Python objects that are individually\nallocated in memory, as shown on the left in figure 3.3. PyTorch tensors or NumPy\narrays, on the other hand, are views over (typically) contiguous memory blocks contain-\ning unboxedC numeric types rather than Python objects. Each elementis a 32-bit (4-byte)\nfloat in this case, as we can see on the right side of figure 3.3. This means storing a 1D\ntensor of 1,000,000 float numbers will require exactly 4,000,000 contiguous bytes, plus\na small overhead for the metadata (such as dimensions and numeric type).\n\nSay we have a list of coordinates we'd like to use to represent a geometrical object:\nperhaps a 2D triangle with vertices at coordinates (4, 1), (5, 3), and (2, 1). The\nexample is not particularly pertinent to deep learning, but it\u2019s easy to follow. Instead\nof having coordinates as numbers in a Python list, as we did earlier, we can use a\n\nMEMORY\n\na\n\n[L0, 2.2, 0.3, 1.6, ... (ILO, 2.2, 0.3, 7.6, ...1)\nPYTHON LIST TENSOR OR ARRAY\n\nFigure 3.3 Python object (boxed) numeric values versus tensor (unboxed array)\nnumeric values\n\none-dimensional tensor by storing Xs in the even indices and Ys in the odd indices,\nlike this:\nUsing .zeros is just a way to get\n\n# In[8]: an appropriately sized array.\npoints = torch.zeros(6) <\u2014~\n\npoints[0] = 4.0\n\npoints[1] = 1.0 We overwrite those zeros with\npoints[2] = 5.0 the values we actually want.\npoints[3] = 3.0\n\npoints[4] = 2.0\n\npoints[5] = 1.0\n\nWe can also pass a Python list to the constructor, to the same effect:\n\n# In(9]:\npoints = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0))\npoints\n\n# Out[9]:\ntensor([4., 1., 5., 3., 2., 1.])\n\nTo get the coordinates of the first point, we do the following:\n\n# In(10):\nfloat (points[0]), float(points[1))\n\n# Out[10):\n(4.0, 1.0)\n\nThis is OK, although it would be practical to have the first index refer to individual 2D\npoints rather than point coordinates. For this, we can use a 2D tensor:\n\n# In{1l1]:\npoints = torch.tensor([[4.0, 1.0), [5.0, 3.0], (2.0, 1.0]]\npoints\n\n# Out[ii]:\n\ntensor([{[4., 1.]\n[5., 3.]\n[2., 1.)\n\nHere, we pass a list of lists to the constructor. We can ask the tensor about its shape:\n\n# In[12):\npoints.shape\n\n# Out[i2]:\ntorch.Size([3, 2))\n\nThis informs us about the size of the tensor along each dimension. We could also use\nzeros or ones to initialize the tensor, providing the size as a tuple:\n\n# In[13]:\npoints = torch.zeros(3, 2)\npoints\n# Out[i3):\ntensor([{[0., 0.],\n{0., 0.),\n[0., 0.)))\n\nNow we can access an individual element in the tensor using two indices:\n\n# In[14):\npoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]})\npoints\n# Out[i4]:\ntensor(([4., 1.],\n(S., 3.),\n[2., 1.}))\n# In[15):\n\npoints[0, 1)\n\n# Out[i5]:\ntensor (1.)\n\nThis returns the \u00a5-coordinate of the zeroth point in our dataset. We can also access\nthe first element in the tensor as we did before to get the 2D coordinates of the first\n\npoint:\n\n# In[16]:\npoints[0]\n\n# Out[i6]:\ntensor((4., 1.])\n\nThe output is another tensor that presents a different view of the same underlying data.\nThe new tensor is a 1D tensor of size 2, referencing the values of the first row in the\npoints tensor. Does this mean a new chunk of memory was allocated, values were copied\ninto it, and the new memory was returned wrapped in a new tensor object? No, because\nthat would be very inefficient, especially if we had millions of points. We'll revisit how\ntensors are stored later in this chapter when we cover views of tensors in section 3.7.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.7,
                        "section_name": "Indexing tensors",
                        "section_path": "./screenshots-images-2/chapter_4/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_7/40b9a892-7063-4589-984c-10d354cbba0c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Indexing tensors\n\nWhat if we need to obtain a tensor containing all points but the first? That's easy using\nrange indexing notation, which also applies to standard Python lists. Here's a\nreminder:\n\nAll elements in the list From element 1 inclusive\n# In(53]: to element 4 exclusive\nsome_list = list(range(6))\n\n\u2014 some_list[:] From element 1 inclusive\nsome_list({1:4] to the end of the list\nsome_list[1:]\nsome_list[:4] ss )\nsome_list[{:-1] 4 From the start of the lst\n\n> some_list(1:4:2] to element 4 exclus\n\nFrom element 1 inclusive to From the start of the list to\n\nelement 4 exclusive, in steps of 2 one before the last element\n\nTo achieve our goal, we can use the same notation for PyTorch tensors, with the added\nbenefit that, just as in NumPy and other Python scientific libraries, we can use range\nindexing for each of the tensor\u2019s dimensions:\n\nAll rows after the first; All rows after the\nimplicitly all columns first; all columns\n# In(54]:\n\u00a9 points[1:] All rows after the\npoints[1:, :] =< first; first column\npoints[1:, 0) 2 4\nints [Ni\npoints [None] Adds a dimension of size 1,\n\njust like unsqueeze\n\nIn addition to using ranges, PyTorch features a powerful form of indexing, called\nadvanced indexing, which we will look at in the next chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.8,
                        "section_name": "Named tensors",
                        "section_path": "./screenshots-images-2/chapter_4/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_8/b95ccde7-e9e7-4b8c-be63-f366d67a0083.png",
                            "./screenshots-images-2/chapter_4/section_8/0ca7c9d2-bc44-451e-afad-301bdbcaae58.png",
                            "./screenshots-images-2/chapter_4/section_8/8d4b5eb5-4841-4109-90c9-a4ea8f22ba7e.png",
                            "./screenshots-images-2/chapter_4/section_8/6a19f743-beb0-4b87-b571-09680d9ba81f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Named tensors\n\nThe dimensions (or axes) of our tensors usually index something like pixel locations\nor color channels. This means when we want to index into a tensor, we need to\nremember the ordering of the dimensions and write our indexing accordingly. As\ndata is transformed through multiple tensors, keeping track of which dimension con-\ntains what data can be error-prone.\n\nTo make things concrete, imagine that we have a 3D tensor like img_t from section\n2.1.4 (we will use dummy data for simplicity here), and we want to convert it to gray-\nscale. We looked up typical weights for the colors to derive a single brightness value:!\n\n# In[2):\nimg_t = torch.randn(3, 5, 5) # shape (channels, rows, columns)\nweights = torch.tensor([0.2126, 0.7152, 0.0722))\n\nWe also often want our code to generalize\u2014for example, from grayscale images repre-\nsented as 2D tensors with height and width dimensions to color images adding a third\nchannel dimension (as in RGB), or from a single image to a batch of images. In sec-\ntion 2.1.4, we introduced an additional batch dimension in batch_t; here we pretend\nto have a batch of 2:\n\n# In[3):\nbatch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n\nSo sometimes the RGB channels are in dimension 0, and sometimes they are in dimen-\nsion 1. But we can generalize by counting from the end: they are always in dimension\n-3, the third from the end. The lazy, unweighted mean can thus be written as follows:\n\n# In[4):\n\nimg_gray_naive = img_t.mean(-3)\nbatch_gray_naive = batch_t.mean(-3)\nimg_gray_naive.shape, batch_gray_naive.shape\n\n# Out[4):\n{torch.Size((5, 5])), torch.Size([2, 5, 5}))\n\nBut now we have the weight, too. PyTorch will allow us to multiply things that are the\nsame shape, as well as shapes where one operand is of size 1 in a given dimension. It\nalso appends leading dimensions of size 1 automatically. This is a feature called broad-\ncasting. batch_t of shape (2, 3, 5, 5) is multiplied by unsqueezed_weights of shape (3,\n1, 1), resulting in a tensor of shape (2, 3, 5, 5), from which we can then sum the third\ndimension from the end (the three channels):\n\n# In[5):\n\nunsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\nimg_weights = (img_t * unsqueezed_weights)\n\nbatch_weights = (batch_t * unsqueezed_weights)\nimg_gray_weighted = img_weights.sum(-3)\n\nbatch_gray_weighted = batch_weights.sum(-3)\nbatch_weights.shape, batch_t.shape, unsqueezed_weights.shape\n\n# Out[5):\n{torch.Size((2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))\n\nBecause this gets messy quickly\u2014and for the sake of efficiency\u2014the PyTorch function\neinsum (adapted from NumPy) specifies an indexing mini-language\u201d giving index\nnames to dimensions for sums of such products. As often in Python, broadcasting\u2014a\nform of summarizing unnamed things\u2014is done using three dots '...'; but don\u2019t worry\ntoo much about einsum, because we will not use it in the following:\n\n# In[6]:\nimg_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\nbatch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n\nbatch_gray_weighted_fancy.shape\n\n# Out [6]:\ntorch.Size([2, 5, 5])\n\nAs we can see, there is quite a lot of bookkeeping involved. This is error-prone, espe-\ncially when the locations where tensors are created and used are far apart in our code.\nThis has caught the eye of practitioners, and so it has been suggested\u2019 that the dimen-\nsion be given a name instead.\n\nPyTorch 1.3 added named tensors as an experimental feature (see https://pytorch\n.org/tutorials/intermediate/named_tensor_tutorial.html and_https://pytorch.org/\ndocs/stable/named_tensor.html). Tensor factory functions such as tensor and rand\ntake a names argument. The names should be a sequence of strings:\n\n# In[7]:\nweights_named = torch.tensor([(0.2126, 0.7152, 0.0722), names=['channels'))\nweights_named\n\n# Out[7]:\ntensor([0.2126, 0.7152, 0.0722], names=('channels',))\n\nWhen we already have a tensor and want to add names (but not change existing\nones), we can call the method refine_names on it. Similar to indexing, the ellipsis (...)\nallows you to leave out any number of dimensions. With the rename sibling method,\nyou can also overwrite or drop (by passing in None) existing names:\n\n# In[(8]:\n\nimg_named = img_t.refine_names(..., \u2018'channels', \u2018rows', \u2018columns')\nbatch_named = batch_t.refine_names(..., \u2018channels\u2019, \u2018rows', '\u2018columns')\nprint (\"img named:\", img_named.shape, img_named.names)\n\nprint (\"batch named:*, batch_named.shape, batch_named.names)\n\n# Out[8]:\n\nimg named: torch.Size([3, 5, 5]) (\u2018channels', 'rows', \u2018columns')\n\nbatch named: torch.Size((2, 3, 5, 5]) (None, 'channels', \u2018rows', 'columns')\n\nFor operations with two inputs, in addition to the usual dimension checks\u2014whether\nsizes are the same, or if one is 1 and can be broadcast to the other\u2014PyTorch will now\ncheck the names for us. So far, it does not automatically align dimensions, so we need\nto do this explicitly. The method align_as returns a tensor with missing dimensions\nadded and existing ones permuted to the right order:\n\n# In[9]:\nweights_aligned = weights_named.align_as (img_named)\nweights_aligned.shape, weights_aligned.names\n\n# Out[9):\n(torch.Size((3, 1, 1)), ('channels', \u2018rows', \u2018columns')\n\nFunctions accepting dimension arguments, like sum, also take named dimensions:\n\n# In[10):\ngray_named = (img_named * weights_aligned) .sum('channels\")\ngray_named.shape, gray_named.names\n\n# Out[i0):\n({torch.Size((5, 5])), (\u2018rows', \u2018columns')\n\nIf we try to combine dimensions with different names, we get an error:\n\ngray_named = (img_named[..., :3] * weights_named).sum('channels'\n\nRuntimeError: Error when\nattempting to broadcast dims [\u2018channels', 'rows',\n\u2018columns') and dims ['channels']: dim 'columns' and dim 'channels'\nare at the same position from the right but do not match.\n\nIf we want to use tensors outside functions that operate on named tensors, we need to\ndrop the names by renaming them to None. The following gets us back into the world\nof unnamed dimensions:\n\n# In[12):\ngray_plain = gray_named. rename (None)\ngray_plain.shape, gray_plain.names\n\n# Out[i2]:\n(torch.Size([{5, 5)), (None, None))\n\nGiven the experimental nature of this feature at the time of writing, and to avoid\nmucking around with indexing and alignment, we will stick to unnamed in the\nremainder of the book. Named tensors have the potential to eliminate many sources\nof alignment errors, which\u2014if the PyTorch forum is any indication\u2014can be a source\nof headaches. It will be interesting to see how widely they will be adopted.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.9,
                        "section_name": "Tensor element types",
                        "section_path": "./screenshots-images-2/chapter_4/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_9/501b85c5-b2f9-4126-861f-8335bfa91af9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tensor element types\n\nSo far, we have covered the basics of how tensors work, but we have not yet touched on\nwhat kinds of numeric types we can store in a Tensor. As we hinted at in section 3.2,\nusing the standard Python numeric types can be suboptimal for several reasons:\n\n= Numbers in Python are objects. Whereas a floating-point number might require\nonly, for instance, 32 bits to be represented on a computer, Python will convert\nit into a full-fledged Python object with reference counting, and so on. This\noperation, called boxing, is not a problem if we need to store a small number of\nnumbers, but allocating millions gets very inefficient.\n\n= Lists in Python are meant for sequential collections of objects. There are no operations\ndefined for, say, efficiently taking the dot product of two vectors, or summing vec-\ntors together. Also, Python lists have no way of optimizing the layout of their con-\ntents in memory, as they are indexable collections of pointers to Python objects\n(of any kind, not just numbers). Finally, Python lists are one-dimensional, and\nalthough we can create lists of lists, this is again very inefficient.\n\n= The Python interpreter is slow compared to optimized, compiled code. Performing math-\nematical operations on large collections of numerical data can be much faster\nusing optimized code written in a compiled, low-level language like C.\n\nFor these reasons, data science libraries rely on NumPy or introduce dedicated data\nstructures like PyTorch tensors, which provide efficient low-level implementations of\nnumerical data structures and related operations on them, wrapped in a convenient\nhigh-level API. To enable this, the objects within a tensor must all be numbers of the\nsame type, and PyTorch must keep track of this numeric type.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.1,
                        "section_name": "Specifying the numeric type with dtype",
                        "section_path": "./screenshots-images-2/chapter_4/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_10/d2ab2539-e3bd-4adb-8922-3837c79fd53e.png",
                            "./screenshots-images-2/chapter_4/section_10/36044322-c34c-4fe7-af10-a23539d9adf0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Specifying the numeric type with dtype\n\nThe dtype argument to tensor constructors (that is, functions like tensor, zeros, and\nones) specifies the numerical data (d) type that will be contained in the tensor. The\ndata type specifies the possible values the tensor can hold (integers versus floating-\npoint numbers) and the number of bytes per value.* The dtype argument is deliber-\nately similar to the standard NumPy argument of the same name. Here\u2019s a list of the\npossible values for the dtype argument:\n\ntorch\n\ntorch.\ntorch.\n\ntorch\n\ntorch.\n\ntorch\n\ntorch.\n\ntorch\n\ntorch.\n\n. float32 or torch. float: 32-bit floating-point\n\nfloat64 or torch.double: 64-bit, double-precision floating-point\nfloat16 or torch.half: 16-bit, half-precision floating-point\n\n.int8: signed 8-bit integers\n\nuint8: unsigned 8-bit integers\n\n.inti6 or torch. short: signed 16-bit integers\n\nint32 or torch. int: signed 32-bit integers\n\n.int64 or torch. long: signed 64-bit integers\n\nbool: Boolean\n\nThe default data type for tensors is 32-bit floating-point.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.11,
                        "section_name": "A dtype for every occasion",
                        "section_path": "./screenshots-images-2/chapter_4/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_11/7128b5ee-d994-4525-8d2b-8007509abb9a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A dtype for every occasion\n\nAs we will see in future chapters, computations happening in neural networks are typ-\nically executed with 32-bit floating-point precision. Higher precision, like 64-bit, will\nnot buy improvements in the accuracy of a model and will require more memory and\ncomputing time. The 16-bit floating-point, half-precision data type is not present\nnatively in standard CPUs, but it is offered on modern GPUs. It is possible to switch to\nhalf-precision to decrease the footprint of a neural network model if needed, with a\nminor impact on accuracy.\n\nTensors can be used as indexes in other tensors. In this case, PyTorch expects\nindexing tensors to have a 64-bit integer data type. Creating a tensor with integers as\narguments, such as using torch.tensor([2, 2)]), will create a 64-bit integer tensor by\ndefault. As such, we'll spend most of our time dealing with float32 and int\u00e94.\n\nFinally, predicates on tensors, such as points > 1.0, produce bool tensors indicat-\ning whether each individual element satisfies the condition. These are the numeric\ntypes in a nutshell.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.12,
                        "section_name": "Managing a tensor's dtype attribute",
                        "section_path": "./screenshots-images-2/chapter_4/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_12/c2411d8d-5916-4fe2-b657-06261279b454.png",
                            "./screenshots-images-2/chapter_4/section_12/59826582-e1b9-4972-a7e7-9e5248dc4fb3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Managing a tensor's dtype attribute\n\nIn order to allocate a tensor of the right numeric type, we can specify the proper\ndtype as an argument to the constructor. For example:\n\n# In[47):\ndouble points = torch.ones(10, 2, dtype=torch.double)\nshort_points = torch.tensor([[1, 2], (3, 4]], dtype=torch.short)\n\nWe can find out about the dtype for a tensor by accessing the corresponding attribute:\n\n# In[48):\nshort_points.dtype\n\n# Out[48):\ntorch. int16\n\nWe can also cast the output of a tensor creation function to the right type using the\ncorresponding casting method, such as\n\n# In[49):\ndouble_points = torch.zeros(10, 2).double()\nshort_points = torch.ones(10, 2).short()\n\nor the more convenient to method:\n# In[50):\n\ndouble_points = torch.zeros(10, 2).to(torch.double)\nshort_points = torch.ones(10, 2).to(dtype=torch.short)\n\nUnder the hood, to checks whether the conversion is necessary and, if so, does it. The\ndtype-named casting methods like float are shorthands for to, but the to method\ncan take additional arguments that we'll discuss in section 3.9.\n\nWhen mixing input types in operations, the inputs are converted to the larger type\nautomatically. Thus, if we want 32-bit computation, we need to make sure all our\ninputs are (at most) 32-bit:\n\n# In[51]: rand initializes the tensor elements to\npoints 64 = torch.rand(5, dtype=torch.double) <\u2014! random numbers between 0 and 1.\npoints_short = points_64.to(torch.short)\n\npoints_64 * points_short # works from PyTorch 1.3 onwards\n\n# Out[51):\ntensor([0., 0., 0., 0., 0.], dtype=torch. float\u00e964)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.13,
                        "section_name": "The tensor API",
                        "section_path": "./screenshots-images-2/chapter_4/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_13/1e8e86ac-0c28-4c64-9f5f-2788fcde4b8a.png",
                            "./screenshots-images-2/chapter_4/section_13/33700bde-e217-4fb6-ad55-c1e6c3ca5168.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The tensor API\n\nAt this point, we know what PyTorch tensors are and how they work under the hood.\nBefore we wrap up, it is worth taking a look at the tensor operations that PyTorch\noffers. It would be of little use to list them all here. Instead, we're going to get a gen-\neral feel for the API and establish a few directions on where to find things in the\nonline documentation at http://pytorch.org/docs.\n\nFirst, the vast majority of operations on and between tensors are available in the\ntorch module and can also be called as methods of a tensor object. For instance, the\ntranspose function we encountered earlier can be used from the torch module\n\n# In(71):\na = torch.ones(3, 2)\na_t = torch.transpose(a, 0, 1)\n\na.shape, a_t.shape\n\n# Out[71):\n(torch.Size([3, 2]), torch.Size([2, 3]))\n\nor as a method of the a tensor:\n\n# In(72]:\na = torch.ones(3, 2)\na_t = a.transpose(Q, 1)\n\na.shape, a_t.shape\n\n# Out[72):\n(torch.Size([3, 2]), torch.Size([2, 3]))\n\nThere is no difference between the two forms; they can be used interchangeably.\nWe mentioned the online docs earlier (http://pytorch.org/docs). They are\nexhaustive and well organized, with the tensor operations divided into groups:\n\n= Creation ops\u2014Functions for constructing a tensor, like ones and from_numpy\n= Indexing, slicing, joining, mutating ops\u2014Functions for changing the shape, stride,\nor content of a tensor, like transpose\n= Math ops\u2014Functions for manipulating the content of the tensor through\ncomputations\n\u2014 Pointwise ops\u2014Functions for obtaining a new tensor by applying a function to\neach element independently, like abs and cos\n\u2014 Reduction ops\u2014Functions for computing aggregate values by iterating\nthrough tensors, like mean, std, and norm\n\u2014 Comparison ops\u2014Functions for evaluating numerical predicates over tensors,\nlike equal and max\n\u2014 Spectral ops\u2014Functions for transforming in and operating in the frequency\ndomain, like st\u00a3t and hamming_window\n\u2014 Other operations\u2014Special functions operating on vectors, like cross, or matri-\nces, like trace\n\u2014 BLAS and LAPACK operations\u2014Functions following the Basic Linear Algebra\nSubprograms (BLAS) specification for scalar, vector-vector, matrix-vector,\nand matrix-matrix operations\n= Random sampling\u2014Functions for generating values by drawing randomly from\nprobability distributions, like randn and normal\n\u00ab Serialization\u2014Functions for saving and loading tensors, like load and save\n* Parallelism\u2014Functions for controlling the number of threads for parallel CPU\nexecution, like set_num_threads\n\nTake some time to play with the general tensor API. This chapter has provided all the\nprerequisites to enable this kind of interactive exploration. We will also encounter sev-\neral of the tensor operations as we proceed with the book, starting in the next chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.14,
                        "section_name": "Tensors: Scenic views of storage",
                        "section_path": "./screenshots-images-2/chapter_4/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_14/9298c6b6-a0f1-4b8d-a643-a9bbce5a7d62.png",
                            "./screenshots-images-2/chapter_4/section_14/481e2651-fe2b-4c33-bb77-1407deaf9e26.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tensors: Scenic views of storage\n\nIt is time for us to look a bit closer at the implementation under the hood. Values in\ntensors are allocated in contiguous chunks of memory managed by torch. Storage\ninstances. A storage is a one-dimensional array of numerical data: that is, a contiguous\nblock of memory containing numbers of a given type, such as float (32 bits repre-\nsenting a floating-point number) or int6\u00e94 (64 bits representing an integer). A\nPyTorch Tensor instance is a view of such a Storage instance that is capable of index-\ning into that storage using an offset and per-dimension strides.\u00ae\n\nMultiple tensors can index the same storage even if they index into the data differ-\nently. We can see an example of this in figure 3.4. In fact, when we requested\npoints[0] in section 3.2, what we got back is another tensor that indexes the same\n\n\"START AT O\n2 ROWS\n\n3 Cols\u201d \u2018START AT O\n\n3 ROWS\n2 CoLs\u201d WHERE THE\n\nFigure 3.4 Tensors are views of a Storage instance.\n\nstorage as the points tensor\u2014just not all of it, and with different dimensionality (1D\nversus 2D). The underlying memory is allocated only once, however, so creating alter-\nnate tensor-views of the data can be done quickly regardless of the size of the data\nmanaged by the Storage instance.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.15,
                        "section_name": "Indexing into storage",
                        "section_path": "./screenshots-images-2/chapter_4/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_15/997d4ca1-d11b-4fee-a6ce-4fbfff5c7b68.png",
                            "./screenshots-images-2/chapter_4/section_15/617a7f85-c6e4-40f2-81d3-845b7ed4f7ab.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Indexing into storage\n\nLet\u2019s see how indexing into the storage works in practice with our 2D points. The stor-\nage for a given tensor is accessible using the . storage property:\n\n# In{17]:\npoints = torch.tensor([(4.0, 1.0], [5.0, 3.0], (2.0, 1.0)]\npoints.storage()\n\n# Out[17):\n0\n\neceooo\n\n4\n1\n5\n3.\n2\n1.0\n[torch.FloatStorage of size 6]\nEven though the tensor reports itself as having three rows and two columns, the stor-\nage under the hood is a contiguous array of size 6. In this sense, the tensor just knows\nhow to translate a pair of indices into a location in the storage.\n\nWe can also index into a storage manually. For instance:\n\n# In(18]:\npoints_storage = points.storage()\npoints_storage[0]\n\n# Out[18):\n4.0\n\n# In[19]:\npoints. storage () [1]\n\n# Out[19]:\n1.0\n\nWe can\u2019t index a storage of a 2D tensor using two indices. The layout of a storage is\nalways one-dimensional, regardless of the dimensionality of any and all tensors that\nmight refer to it.\n\nAt this point, it shouldn\u2019t come as a surprise that changing the value of a storage\nleads to changing the content of its referring tensor:\n\n# In[20):\n\npoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]})\npoints_storage = points.storage()\n\npoints_storage[0] = 2.0\n\npoints\n\n# Out[20]:\n\ntensor([{[2., 1.]\n(5., 3.]\n(2., 1.)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.16,
                        "section_name": "Modifying stored values: In-place operations",
                        "section_path": "./screenshots-images-2/chapter_4/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_16/36f65bd3-305e-48b8-9495-068e27801dcd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Modifying stored values: In-place operations\n\nIn addition to the operations on tensors introduced in the previous section, a small\nnumber of operations exist only as methods of the Tensor object. They are recogniz-\nable from a trailing underscore in their name, like zero_, which indicates that the\nmethod operates in place by modifying the input instead of creating a new output tensor\nand returning it. For instance, the zero_ method zeros out all the elements of the input.\nAny method without the trailing underscore leaves the source tensor unchanged and\ninstead returns a new tensor:\n\n# In[73]:\na = torch.ones(3, 2)\n\n# In[74):\n\na.zero_()\n\na\n\n# Out[74]:\n\ntensor([([0., 0.],\n(O., 0.),\n[0., 0.}))\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.17,
                        "section_name": "Tensor metadata: Size, offset, and stride",
                        "section_path": "./screenshots-images-2/chapter_4/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_17/0851a52d-abb8-472c-8d12-b1829e238267.png",
                            "./screenshots-images-2/chapter_4/section_17/642f8e90-8e53-4eb3-a133-f3fe3cf3eb37.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tensor metadata: Size, offset, and stride\n\nIn order to index into a storage, tensors rely on a few pieces of information that,\ntogether with their storage, unequivocally define them: size, offset, and stride. How\nthese interact is shown in figure 3.5. The size (or shape, in NumPy parlance) is a tuple\n\nSHAPE = (3, 3)\n\ntN\nROWS COLS\n(FIRST INDEX) (SECOND INDEX)\n\nOFFSET =| STRIDE = (3, 1)\n\n+13 NEXT COL (sTRIDE[I]=1)\n\nsit la ]}ils 2|1|s/s\n\n+3\u2014 NEXT ROW (STRIDE[O]=3)\n\nFigure 3.5 Relationship between a tensor's offset, size, and stride. Here the tensor is a view\nof a larger storage, like one that might have been allocated when creating a larger tensor.\n\nindicating how many elements across each dimension the tensor represents. The stor-\nage offset is the index in the storage corresponding to the first element in the tensor.\nThe stride is the number of elements in the storage that need to be skipped over to\nobtain the next element along each dimension.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.18,
                        "section_name": "Views of another tensor\u2019s storage",
                        "section_path": "./screenshots-images-2/chapter_4/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_18/2a0fcdc1-c2bb-4b12-973b-450ddfa9500e.png",
                            "./screenshots-images-2/chapter_4/section_18/abbcdabf-6304-4c99-9e89-9afbbc5435ed.png",
                            "./screenshots-images-2/chapter_4/section_18/efa17771-3389-4ae1-9ade-934f6ec54fe4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Views of another tensor\u2019s storage\n\nWe can get the second point in the tensor by providing the corresponding index:\n\n# In[21):\n\npoints = torch.tensor([(4.0, 1.0), [5.0, 3.0], (2.0, 1.0]])\nsecond_point = points[1)]\n\nsecond_point.storage_offset ()\n\n# Out[21):\n2\n\n# In[22]:\nsecond_point.size()\n\n# Out [22]:\ntorch. Size([2])\n\nThe resulting tensor has offset 2 in the storage (since we need to skip the first point,\nwhich has two items), and the size is an instance of the Size class containing one\n\nelement, since the tensor is one-dimensional. It\u2019s important to note that this is the\nsame information contained in the shape property of tensor objects:\n\n# In[23):\nsecond_point.shape\n\n# Out[(23):\ntorch.Size([2])\n\nThe stride is a tuple indicating the number of elements in the storage that have to be\nskipped when the index is increased by 1 in each dimension. For instance, our points\ntensor has a stride of (2, 1):\n\n# In[24):\npoints.stride()\n\n# Out(24):\n(2, 1)\n\nAccessing an element i, j ina 2D tensor results in accessing the storage_offset +\nstride[0] * i + stride[1] * j element in the storage. The offset will usually be\nzero; if this tensor is a view of a storage created to hold a larger tensor, the offset might\nbe a positive value.\n\nThis indirection between Tensor and Storage makes some operations inexpen-\nsive, like transposing a tensor or extracting a subtensor, because they do not lead to\nmemory reallocations. Instead, they consist of allocating a new Tensor object with a\ndifferent value for size, storage offset, or stride.\n\nWe already extracted a subtensor when we indexed a specific point and saw the\nstorage offset increasing. Let\u2019s see what happens to the size and stride as well:\n\n# In[25]:\nsecond_point = points[1]\nsecond_point.size()\n\n# Out (25):\ntorch.Size([2])\n\n# In[26):\nsecond_point.storage_offset ()\n\n# Out (26):\n2\n\n# In[27):\nsecond_point.stride()\n\n# Out (27):\n{1,)\n\nThe bottom line is that the subtensor has one less dimension, as we would expect,\nwhile still indexing the same storage as the original points tensor. This also means\nchanging the subtensor will have a side effect on the original tensor:\n\n# In(28]:\n\npoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]\nsecond_point = points[1)\n\nsecond_point[0] = 10.0\n\npoints\n\n# Out [28):\n\ntensor([{ 4., 1.]\n(10., 3.)\n(2., 1.)\n\n)\nThis might not always be desirable, so we can eventually clone the subtensor into a\nnew tensor:\n\n# In(29]:\n\npoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]\nsecond_point = points[1].clone()\n\nsecond_point[0] = 10.0\n\npoints\n\n# Out[29):\n\ntensor([[{4., 1.],\n(5., 3.],\n(2., 1.11)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.19,
                        "section_name": "Transposing without copying",
                        "section_path": "./screenshots-images-2/chapter_4/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_19/59bd2d0e-1c97-49e5-a17e-7d7b82c39fad.png",
                            "./screenshots-images-2/chapter_4/section_19/67b6f302-6255-4fa0-a724-e120840f6e2e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Transposing without copying\n\nLet's try transposing now. Let\u2019s take our points tensor, which has individual points in\nthe rows and Xand Y coordinates in the columns, and turn it around so that individ-\nual points are in the columns. We take this opportunity to introduce the t function, a\nshorthand alternative to transpose for two-dimensional tensors:\n\n# In(30):\npoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], (2.0, 1.0]]\npoints\n\n# Out [30]:\n\ntensor([[4., 1.],\n(5., 3.1],\n(2., 1.11)\n\n# In(31):\npoints_t = points.t()\npoints_t\n\n# Out[31):\ntensor([[4., 5., 2.],\n(1., 3., 1.)))\n\nTIP To help build a solid understanding of the mechanics of tensors, it may\nbe a good idea to grab a pencil and a piece of paper and scribble diagrams\nlike the one in figure 3.5 as we step through the code in this section.\n\nWe can easily verify that the two tensors share the same storage\n\n# In[32):\nid(points.storage()) == id(points_t.storage()\n\n# Out[32]:\nTrue\n\nand that they differ only in shape and stride:\n\n# In[33):\npoints.stride()\n\n# Out[33]:\n\n(2, 1)\n\n# In[34):\npoints_t.stride()\n\n# Out[34]:\n(i, 2)\n\nThis tells us that increasing the first index by one in points\u2014for example, going from\npoints [0,0] to points [1,0]\u2014will skip along the storage by two elements, while increas-\ning the second index\u2014from points [0,0] topoints[0,1]\u2014will skip along the storage by\none. In other words, the storage holds the elements in the tensor sequentially row by row.\n\nWe can transpose points into points_t, as shown in figure 3.6. We change the order\nof the elements in the stride. After that, increasing the row (the first index of the ten-\nsor) will skip along the storage by one, just like when we were moving along columns in\npoints. This is the very definition of transposing. No new memory is allocated: trans-\nposing is obtained only by creating a new Tensor instance with different stride ordering\nthan the original.\n\nTRANSPOSE\n\u2014_\u2014\u2014\nSTRIDE = (3, |) STRIDE = (\\, 3)\nNEXT COL\n+\\ NEXT ROW\nI\nali j2i4 fifa\n+3 NEXT Row gure 3.6 Transpose\n\nNEXT COL \u2018eration applied to a tensor\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.2,
                        "section_name": "Transposing in higher dimensions",
                        "section_path": "./screenshots-images-2/chapter_4/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_20/946c1813-630a-444e-86d1-673590fd2270.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Transposing in higher dimensions\n\nTransposing in PyTorch is not limited to matrices. We can transpose a multidimen-\nsional array by specifying the two dimensions along which transposing (flipping shape\nand stride) should occur:\n\n# In(35):\n\nsome_t = torch.ones(3, 4, 5)\ntranspose_t = some_t.transpose(0, 2)\nsome_t.shape\n\n# Out [35]:\ntorch.Size([3, 4, 5])\n\n# In(36]:\ntranspose_t.shape\n\n# Out [36]:\ntorch.Size([5, 4, 3])\n\n# In(37):\nsome_t.stride()\n\n# Out (37):\n(20, 5, 1)\n\n# In(38):\ntranspose_t.stride()\n\n# Out [38):\n(1, 5, 20)\n\nA tensor whose values are laid out in the storage starting from the rightmost dimen-\nsion onward (that is, moving along rows for a 2D tensor) is defined as contiguous.\nContiguous tensors are convenient because we can visit them efficiently in order with-\nout jumping around in the storage (improving data locality improves performance\nbecause of the way memory access works on modern CPUs). This advantage of course\ndepends on the way algorithms visit.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.21,
                        "section_name": "Contiguous tensors",
                        "section_path": "./screenshots-images-2/chapter_4/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_21/f07c5aaa-d521-4a01-b23d-9384a57225de.png",
                            "./screenshots-images-2/chapter_4/section_21/d4bc4c7d-4b90-4b2c-9807-19807d254ff2.png",
                            "./screenshots-images-2/chapter_4/section_21/4cc11d87-4671-4156-9d1c-f724d96e953b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Contiguous tensors\n\nSome tensor operations in PyTorch only work on contiguous tensors, such as view,\nwhich we'll encounter in the next chapter. In that case, PyTorch will throw an infor-\nmative exception and require us to call contiguous explicitly. It\u2019s worth noting that\ncalling contiguous will do nothing (and will not hurt performance) if the tensor is\nalready contiguous.\n\nIn our case, points is contiguous, while its transpose is not:\n\n# In(39):\npoints.is_contiguous()\n\n# Out[39]:\nTrue\n\n# In[40):\npoints_t.is_contiguous()\n\n# Out[40]:\nFalse\n\nWe can obtain a new contiguous tensor from a non-contiguous one using the contigu-\nous method. The content of the tensor will be the same, but the stride will change, as\nwill the storage:\n\n# In[41i):\npoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]})\npoints_t = points.t()\n\npoints_t\n\n# Out[41]:\n\ntensor([([4., 5., 2.],\n(1., 3., 1.1))\n\n# In[42]}:\n\npoints_t.storage()\n\n# Out[42]:\n\n4.0\n\n1.0\n\n5.0\n\n3.0\n\n2.0\n\n1.0\n\n(torch.PloatStorage of size 6]\n\n# In[43]:\npoints_t.stride()\n\n# Out[43]:\n(i, 2)\n\n# In[44):\npoints_t_cont = points_t.contiguous()\npoints_t_cont\n\n# Out[44]:\ntensor([[4., 5., 2.],\n{1., 3., 1.]))\n\n# In[45):\npoints_t_cont.stride()\n\n# Out[45]:\n(3, 1)\n\n# In[46]:\npoints_t_cont.storage()\n\n# Out [46):\n\nececce\n\n4\n5\n2\n1.\n3\n1.0\n[torch.FloatStorage of size 6)\nNotice that the storage has been reshuffled in order for elements to be laid out row-\nby-row in the new storage. The stride has been changed to reflect the new layout.\n\nAs a refresher, figure 3.7 shows our diagram again. Hopefully it will all make sense\nnow that we've taken a good look at how tensors are built.\n\nf Ul\n\nSHAPE = (3, 3)\n\nt\nROWS  COLS\n(FIRST INDEX) (SECOND INDEX)\n\nOFFSET = | STRIDE = (3, |)\nH -4SNEXT COL (STRIDE[I]=1)\n6 5|7 [4 ie 2|1\\|3/e\na\n+3 NEXTROW (gTRIDE[O]=3)\n\nFigure 3.7 Relationship between a tensor\u2019s offset, size, and stride. Here the tensor is a view\nof a larger storage, like one that might have been allocated when creating a larger tensor.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.22,
                        "section_name": "Moving tensors to the GPU",
                        "section_path": "./screenshots-images-2/chapter_4/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_22/2deb12bb-710b-4851-8d18-ed94b26a523c.png",
                            "./screenshots-images-2/chapter_4/section_22/0f5bea23-a9c6-469f-8dd5-d0540a8a59ea.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Moving tensors to the GPU\n\nSo far in this chapter, when we've talked about storage, we\u2019ve meant memory on the\nCPU. PyTorch tensors also can be stored on a different kind of processor: a graphics\nprocessing unit (GPU). Every PyTorch tensor can be transferred to (one of) the\nGPU(s) in order to perform massively parallel, fast computations. All operations that\nwill be performed on the tensor will be carried out using GPU-specific routines that\ncome with PyTorch.\n\nPyTorch support for various GPUs\n\nAs of mid-2019, the main PyTorch releases only have acceleration on GPUs that have\nsupport for CUDA. PyTorch can run on AMD's ROCm (https://rocm.github.io), and the\nmaster repository provides support, but so far, you need to compile it yourself.\n(Before the regular build process, you need to run tools/amd_build/build_amd.py\nto translate the GPU code.) Support for Google's tensor processing units (TPUs) is a\nwork in progress (https://github.com/pytorch/xla), with the current proof of concept\navailable to the public in Google Colab: https://colab.research.google.com. Imple-\nmentation of data structures and kernels on other GPU technologies, such as\nOpenCL, are not planned at the time of this writing.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.23,
                        "section_name": "Managing a tensor\u2019s device attribute",
                        "section_path": "./screenshots-images-2/chapter_4/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_23/39e8f166-5000-40da-8a16-0b4c7143b3ef.png",
                            "./screenshots-images-2/chapter_4/section_23/4e803818-e024-4b36-bc65-ffb586496b4a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Managing a tensor\u2019s device attribute\n\nIn addition to dtype, a PyTorch Tensor also has the notion of device, which is where\non the computer the tensor data is placed. Here is how we can create a tensor on the\nGPU by specifying the corresponding argument to the constructor:\n\n# In[64):\npoints_gpu = torch.tensor([[4.0, 1.0], (5.0, 3.0], [2.0, 1.0}], device='cuda\")\n\nWe could instead copy a tensor created on the CPU onto the GPU using the to\nmethod:\n\n# In[65):\npoints_gpu = points.to(device='cuda')\n\nDoing so returns a new tensor that has the same numerical data, but stored in the\nRAM of the GPU, rather than in regular system RAM. Now that the data is stored\nlocally on the GPU, we'll start to see the speedups mentioned earlier when perform-\ning mathematical operations on the tensor. In almost all cases, CPU- and GPU-based\ntensors expose the same user-facing API, making it much easier to write code that is\nagnostic to where, exactly, the heavy number crunching is running.\n\nIf our machine has more than one GPU, we can also decide on which GPU we allo-\ncate the tensor by passing a zero-based integer identifying the GPU on the machine,\nsuch as\n\n# In[66):\npoints_gpu = points.to(device='cuda:0\")\n\nAt this point, any operation performed on the tensor, such as multiplying all elements\nby a constant, is carried out on the GPU:\n\nMultiplication performed on the CPU\n# In[67):\npoints = 2 * points \u00a2 Multiplication performed\npoints_gpu = 2 * points.to(device='cuda') \u00ab| on the GPU\n\n\nNote that the points_gpu tensor is not brought back to the CPU once the result has\nbeen computed. Here\u2019s what happened in this line:\n\na The points tensor is copied to the GPU.\n\n2 Anew tensor is allocated on the GPU and used to store the result of the multi-\nplication.\n\n3 Ahandle to that GPU tensor is returned.\n\nTherefore, if we also add a constant to the result\n\n# In(68]:\npoints_gpu = points_gpu + 4\n\nthe addition is still performed on the GPU, and no information flows to the CPU\n(unless we print or access the resulting tensor). In order to move the tensor back to\nthe CPU, we need to provide a cpu argument to the to method, such as\n\n# In[69):\npoints_cpu = points_gpu.to(device='cpu')\n\nWe can also use the shorthand methods cpu and cuda instead of the to method to\nachieve the same goal:\n\n# In[(70):\n\npoints_gpu = points.cuda() <+\u2014 Defaults to GPU index 0\npoints_gpu = points.cuda(0)\n\npoints_cpu = points_gpu.cpu()\n\nIt\u2019s also worth mentioning that by using the to method, we can change the placement\nand the data type simultaneously by providing both device and dtype as arguments.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.24,
                        "section_name": "NumPy interoperability",
                        "section_path": "./screenshots-images-2/chapter_4/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_24/c01a0169-fa93-43fb-a1b1-08c011df84a2.png",
                            "./screenshots-images-2/chapter_4/section_24/e879b1d8-4edb-4c6c-a875-50674d175fae.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "NumPy interoperability\nWe\u2019ve mentioned NumPy here and there. While we do not consider NumPy a prereq-\nuisite for reading this book, we strongly encourage you to become familiar with\nNumPy due to its ubiquity in the Python data science ecosystem. PyTorch tensors can\nbe converted to NumPy arrays and vice versa very efficiently. By doing so, we can take\nadvantage of the huge swath of functionality in the wider Python ecosystem that has\nbuilt up around the NumPy array type. This zero-copy interoperability with NumPy\narrays is due to the storage system working with the Python buffer protocol\n(https: / /docs.python.org/3/c-api/buffer.huml).\n\nTo get a NumPy array out of our points tensor, we just call\n# In[(55]:\npoints = torch.ones(3, 4)\n\npoints_np = points.numpy()\npoints_np\n\n# Out[55]:\n\narray(((1., 1., 1., 1.],\n(1., 1., 1., 1.1,\n(1., 1., 1., 1.]], dtype=float32)\n\nwhich will return a NumPy multidimensional array of the right size, shape, and\n\nnumerical type. Interestingly, the returned array shares the same underlying buffer\n\nwith the tensor storage. This means the numpy method can be effectively executed at\n\nbasically no cost, as long as the data sits in CPU RAM. It also means modifying the\n\nNumPy array will lead to a change in the originating tensor. If the tensor is allocated\n\non the GPU, PyTorch will make a copy of the content of the tensor into a NumPy array\n\nallocated on the CPU.\n\nConversely, we can obtain a PyTorch tensor from a NumPy array this way\n\n# In[56):\npoints = torch. from_numpy (points_np)\n\nwhich will use the same buffer-sharing strategy we just described.\n\nNOTE While the default numeric type in PyTorch is 32-bit floating-point, for\nNumPy it is 64-bit. As discussed in section 3.5.2, we usually want to use 32-bit\nfloating-points, so we need to make sure we have tensors of dtype torch\n-float after converting.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.25,
                        "section_name": "Generalized tensors are tensors, too",
                        "section_path": "./screenshots-images-2/chapter_4/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_25/06f0503d-9dbc-485c-a35e-f7b9d1431054.png",
                            "./screenshots-images-2/chapter_4/section_25/2ff30735-f93a-408f-abb7-559d6db88c15.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generalized tensors are tensors, too\n\nFor the purposes of this book, and for the vast majority of applications in general, ten-\nsors are multidimensional arrays, just as we've seen in this chapter. If we risk a peek\nunder the hood of PyTorch, there is a twist: how the data is stored under the hood is\nseparate from the tensor API we discussed in section 3.6. Any implementation that\nmeets the contract of that API can be considered a tensor!\n\nPyTorch will cause the right computation functions to be called regardless of\nwhether our tensor is on the CPU or the GPU. This is accomplished through a dis-\npatching mechanism, and that mechanism can cater to other tensor types by hooking\nup the user-facing API to the right backend functions. Sure enough, there are other\nkinds of tensors: some are specific to certain classes of hardware devices (like Google\nTPUs), and others have data-representation strategies that differ from the dense array\nstyle we've seen so far. For example, sparse tensors store only nonzero entries, along\nwith index information. The PyTorch dispatcher on the left in figure 3.8 is designed\nto be extensible; the subsequent switching done to accommodate the various numeric\ntypes of figure 3.8 shown on the right is a fixed aspect of the implementation coded\ninto each backend.\n\nWe will meet quantized tensors in chapter 15, which are implemented as another\ntype of tensor with a specialized computational backend. Sometimes the usual tensors\nwe use are called dense or strided to differentiate them from tensors using other mem-\nory layouts.\n\n= \u00a9\nDispatchec P add_epu\n\u201csindoacye\u201d mid oatyre \\ Mm Kemeleid@esy\nais tors long > 2 \"fe Ciel\nha \\ Eee ds\n\nFigure 3.8 The dispatcher\nin PyTorch is one of its key\ninfrastructure bits.\n\nAs with many things, the number of kinds of tensors has grown as PyTorch supports a\nbroader range of hardware and applications. We can expect new kinds to continue to\narise as people explore new ways to express and perform computations with PyTorch.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.26,
                        "section_name": "Serializing tensors",
                        "section_path": "./screenshots-images-2/chapter_4/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_26/a6758b84-1e13-41cc-8368-cf30a5c11421.png",
                            "./screenshots-images-2/chapter_4/section_26/61255eed-41e0-4026-afd1-2c7fd6e5b1d6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Serializing tensors\n\nCreating a tensor on the fly is all well and good, but if the data inside is valuable, we will\nwant to save it to a file and load it back at some point. After all, we don\u2019t want to have\nto retrain a model from scratch every time we start running our program! PyTorch uses\npickle under the hood to serialize the tensor object, plus dedicated serialization code\nfor the storage. Here\u2019s how we can save our points tensor to an ourpoints.t file:\n\n# In(57):\ntorch.save(points, *../data/plch3/ourpoints.t')\n\nAs an alternative, we can pass a file descriptor in lieu of the filename:\n# In(58]:\n\nwith open('../data/plch3/ourpoints.t','wb') as \u00a3:\ntorch.save(points, \u00a3)\n\nLoading our points back is similarly a one-liner\n\n# In(59):\npoints = torch. load('../data/plch3/ourpoints.t')\n\nor, equivalently,\n\n# In(60):\nwith open('../data/plch3/ourpoints.t','rb') as \u00a3:\npoints = torch. load(f)\n\nWhile we can quickly save tensors this way if we only want to load them with PyTorch,\nthe file format itself is not interoperable: we can\u2019t read the tensor with software other\nthan PyTorch. Depending on the use case, this may or may not be a limitation, but we\n\nshould learn how to save tensors interoperably for those times when it is. We'll look\nnext at how to do so.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.27,
                        "section_name": "Serializing to HDF5 with hSpy",
                        "section_path": "./screenshots-images-2/chapter_4/section_27",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_27/05ab4f51-a439-4994-b145-288b8c15aabe.png",
                            "./screenshots-images-2/chapter_4/section_27/0b0a6a9b-5885-4b2f-b928-4ff543c00659.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": ", Serializing to HDF5 with hSpy\n\nEvery use case is unique, but we suspect needing to save tensors interoperably will be\nmore common when introducing PyTorch into existing systems that already rely on\ndifferent libraries. New projects probably won't need to do this as often.\n\nFor those cases when you need to, however, you can use the HDF5 format and\nlibrary (www.hdfgroup.org/solutions/hd{5). HDF5 is a portable, widely supported\nformat for representing serialized multidimensional arrays, organized in a nested key-\nvalue dictionary. Python supports HDF5 through the h5py library (www.h5py.org),\nwhich accepts and returns data in the form of NumPy arrays.\n\nWe can install hSpy using\n\n$ conda install h5py\n\nAt this point, we can save our points tensor by converting it to a NumPy array (at no\ncost, as we noted earlier) and passing it to the create_dataset function:\n\n# In[61):\nimport h5py\n\n\u00a3 = hS5py.File('../data/plch3/ourpoints.hdf5', 'w')\ndset = f.create_dataset('coords', data=points.numpy())\n\u00a3.close()\n\nHere \u2018coords\u2019 is a key into the HDF5 file. We can have other keys\u2014even nested ones.\nOne of the interesting things in HDF is that we can index the dataset while on disk\nand access only the elements we\u2019re interested in. Let\u2019s suppose we want to load just\nthe last two points in our dataset:\n\n# In[62):\n\n\u00a3 = hSpy.File('../data/plch3/ourpoints.hdf5', \u2018r')\ndset = f['coords')\n\nlast_points = dset[-2:]\n\nThe data is not loaded when the file is opened or the dataset is required. Rather, the\ndata stays on disk until we request the second and last rows in the dataset. At that\npoint, h5py accesses those two columns and returns a NumPy array-like object\nencapsulating that region in that dataset that behaves like a NumPy array and has the\nsame API.\n\nOwing to this fact, we can pass the returned object to the torch. from_numpy func-\ntion to obtain a tensor directly. Note that in this case, the data is copied over to the\ntensor\u2019s storage:\n\n# In[63):\nlast_points = torch. from_numpy (dset[-2:])\nf\u00a3.close()\n\nOnce we're finished loading data, we close the file. Closing the HDFS file invalidates\nthe datasets, and trying to access dset afterward will give an exception. As long as we\nstick to the order shown here, we are fine and can now work with the last_points\ntensor.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.28,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_4/section_28",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_28/b165a754-e5fa-4834-b1a8-a66635596dd6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\nNow we have covered everything we need to get started with representing everything in\nfloats. We'll cover other aspects of tensors\u2014such as creating views of tensors; indexing\ntensors with other tensors; and broadcasting, which simplifies performing element-wise\noperations between tensors of different sizes or shapes\u2014as needed along the way.\n\nIn chapter 4, we will learn how to represent real-world data in PyTorch. We will\nstart with simple tabular data and move on to something more elaborate. In the pro-\ncess, we will get to know more about tensors.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 5,
                "chapter_name": "Real-world data\nrepresentation\nusing tensors",
                "chapter_path": "./screenshots-images-2/chapter_5",
                "sections": [
                    {
                        "section_id": 5.1,
                        "section_name": "Real-world data\nrepresentation\nusing tensors",
                        "section_path": "./screenshots-images-2/chapter_5/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_1/70c835ac-f5b0-4005-a0f7-5120e049b326.png",
                            "./screenshots-images-2/chapter_5/section_1/eb24d8ea-b623-40d7-be53-1db7193174f7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the previous chapter, we learned that tensors are the building blocks for data in\nPyTorch. Neural networks take tensors as input and produce tensors as outputs. In\nfact, all operations within a neural network and during optimization are operations\nbetween tensors, and all parameters (for example, weights and biases) in a neural\nnetwork are tensors. Having a good sense of how to perform operations on tensors\nand index them effectively is central to using tools like PyTorch successfully. Now\n\nthat you know the basics of tensors, your dexterity with them will grow as you make\nyour way through the book.\n\nHere's a question that we can already address: how do we take a piece of data, a\nvideo, or a line of text, and represent it with a tensor in a way that is appropriate for\ntraining a deep learning model? This is what we'll learn in this chapter. We'll cover\ndifferent types of data with a focus on the types relevant to this book and show how to\nrepresent that data as tensors. Then we'll learn how to load the data from the most\ncommon on-disk formats and get a feel for those data types\u2019 structure so we can see\nhow to prepare them for training a neural network. Often, our raw data won't be per-\nfectly formed for the problem we'd like to solve, so we'll have a chance to practice our\ntensor-manipulation skills with a few more interesting tensor operations.\n\nEach section in this chapter will describe a data type, and each will come with its\nown dataset. While we've structured the chapter so that each data type builds on the\nprevious one, feel free to skip around a bit if you\u2019re so inclined.\n\nWe'll be using a lot of image and volumetric data through the rest of the book,\nsince those are common data types and they reproduce well in book format. We'll also\ncover tabular data, time series, and text, as those will also be of interest to a number of\nour readers. Since a picture is worth a thousand words, we'll start with image data.\nWe'll then demonstrate working with a three-dimensional array using medical data\nthat represents patient anatomy as a volume. Next, we'll work with tabular data about\nwines, just like what we'd find in a spreadsheet. After that, we'll move to ordered tabular\ndata, with a time-series dataset from a bike-sharing program. Finally, we'll dip our toes\ninto text data from Jane Austen. Text data retains its ordered aspect but introduces\nthe problem of representing words as arrays of numbers.\n\nIn every section, we will stop where a deep learning researcher would start: right\nbefore feeding the data to a model. We encourage you to keep these datasets; they will\nconstitute excellent material for when we start learning how to train neural network\nmodels in the next chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.2,
                        "section_name": "Working with images",
                        "section_path": "./screenshots-images-2/chapter_5/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_2/0415c22f-88de-4549-8250-33afb4c46de7.png",
                            "./screenshots-images-2/chapter_5/section_2/6a7dcebb-389f-4db4-a2d1-82f05bc6704d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Working with images\n\nThe introduction of convolutional neural networks revolutionized computer vision\n(see http://mng.bz/zjMa), and image-based systems have since acquired a whole new\nset of capabilities. Problems that required complex pipelines of highly tuned algorith-\nmic building blocks are now solvable at unprecedented levels of performance by train-\ning end-to-end networks using paired input-and-desired-output examples. In order to\nparticipate in this revolution, we need to be able to load an image from common\nimage formats and then transform the data into a tensor representation that has the\nvarious parts of the image arranged in the way PyTorch expects.\n\nAn image is represented as a collection of scalars arranged in a regular grid with a\nheight and a width (in pixels). We might have a single scalar per grid point (the\npixel), which would be represented as a grayscale image; or multiple scalars per grid\npoint, which would typically represent different colors, as we saw in the previous chap-\nter, or different features like depth from a depth camera.\n\nScalars representing values at individual pixels are often encoded using 8-bit inte-\ngers, as in consumer cameras. In medical, scientific, and industrial applications, it is\nnot unusual to find higher numerical precision, such as 12-bit or 16-bit. This allows a\nwider range or increased sensitivity in cases where the pixel encodes information\nabout a physical property, like bone density, temperature, or depth.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.3,
                        "section_name": "Adding color channels",
                        "section_path": "./screenshots-images-2/chapter_5/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_3/ac41f41b-fc97-4119-adc9-323b1a3646ec.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adding color channels\n\nWe mentioned colors earlier. There are several ways to encode colors into numbers.!\nThe most common is RGB, where a color is defined by three numbers representing\nthe intensity of red, green, and blue. We can think of a color channel as a grayscale\nintensity map of only the color in question, similar to what you'd see if you looked at\nthe scene in question using a pair of pure red sunglasses. Figure 4.1 shows a rainbow,\nwhere each of the RGB channels captures a certain portion of the spectrum (the fig-\nure is simplified, in that it elides things like the orange and yellow bands being repre-\nsented as a combination of red and green).\n\nRED GREEN BLUE\n\nFigure 4.1 A rainbow, broken into red, green, and blue channels\n\nThe red band of the rainbow is brightest in the red channel of the image, while the\nblue channel has both the blue band of the rainbow and the sky as high-intensity.\nNote also that the white clouds are high-intensity in all three channels.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.4,
                        "section_name": "Loading an image file",
                        "section_path": "./screenshots-images-2/chapter_5/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_4/1d6aba4d-01f8-4301-bfa5-355100c6c596.png",
                            "./screenshots-images-2/chapter_5/section_4/9e8b69c7-fdaf-416e-8822-94bc5e45f1a4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Loading an image file\n\nImages come in several different file formats, but luckily there are plenty of ways to\n\nload images in Python. Let's start by loading a PNG image using the imageio module\n(code/plch4/1_image_dog.ipynb).\n\nListing 4.1 code/pich4/1_image_dog.ipynb\n\n# In[2]:\nimport imageio\n\nimg_arr = imageio.imread('../data/plch4/image-dog/bobby.jpg')\nimg_arr.shape\n\n# Out[2):\n(720, 1280, 3)\n\nNOTE We'll use imageio throughout the chapter because it handles different\ndata types with a uniform API. For many purposes, using TorchVision is a\ngreat default choice to deal with image and video data. We go with imageio\nhere for somewhat lighter exploration.\n\nAt this point, img is a NumPy array-like object with three dimensions: two spatial\ndimensions, width and height; and a third dimension corresponding to the red,\ngreen, and blue channels. Any library that outputs a NumPy array will suffice to obtain\na PyTorch tensor. The only thing to watch out for is the layout of the dimensions.\nPyTorch modules dealing with image data require tensors to be laid out as Cx Hx W:\nchannels, height, and width, respectively.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.5,
                        "section_name": "Changing the layout",
                        "section_path": "./screenshots-images-2/chapter_5/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_5/8853ead0-5f1f-43a1-84a8-f026a63dd758.png",
                            "./screenshots-images-2/chapter_5/section_5/3b10ed69-503a-4e3b-9cb0-3e3670f046dc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Changing the layout\n\nWe can use the tensor\u2019s permute method with the old dimensions for each new dimen-\nsion to get to an appropriate layout. Given an input tensor H x Wx Cas obtained pre-\nviously, we get a proper layout by having channel 2 first and then channels 0 and 1:\n\n# In[3):\nimg = torch. from_numpy(img_arr)\nout = img.permute(2, 0, 1)\n\nWe've seen this previously, but note that this operation does not make a copy of the\ntensor data. Instead, out uses the same underlying storage as img and only plays with\nthe size and stride information at the tensor level. This is convenient because the\noperation is very cheap; but just as a heads-up: changing a pixel in img will lead to a\nchange in out.\n\nNote also that other deep learning frameworks use different layouts. For instance,\noriginally TensorFlow kept the channel dimension last, resulting in an H x Wx C lay-\nout (it now supports multiple layouts). This strategy has pros and cons from a low-level\nperformance standpoint, but for our concerns, it doesn\u2019t make a difference as long as\nwe reshape our tensors properly.\n\nSo far, we have described a single image. Following the same strategy we've used\nfor earlier data types, to create a dataset of multiple images to use as an input for our\nneural networks, we store the images in a batch along the first dimension to obtain an\nNx Cx Hx Wtensor.\n\nAsaslightly more efficient alternative to using stack to build up the tensor, we can pre-\nallocate a tensor of appropriate size and fill it with images loaded from a directory, like so:\n# In[4]:\n\nbatch_size = 3\nbatch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)\n\nThis indicates that our batch will consist of three RGB images 256 pixels in height and\n256 pixels in width. Notice the type of the tensor: we're expecting each color to be rep-\nresented as an 8-bit integer, as in most photographic formats from standard consumer\ncameras. We can now load all PNG images from an input directory and store them in\nthe tensor:\n\n# In[5]:\n\nimport os\n\ndata_dir '../data/plch4/image-cats/'\n\nfilenames = [name for name in os.listdir(data_dir)\n\nif os.path.splitext (name) [-1] == '.png']\nfor i, filename in enumerate(filenames) :\nimg_arr = imageio.imread(os.path.join(data_dir, filename) )\nimg_t = torch. from_numpy (img_arr)\nimg_t = img_t.permute(2, 0, 1) Here we keep only the first three channels.\n. Ls Sometimes images also have an alpha channel\nimg_t = img_t[:3] -\nbatch[i] = img_t indicating transparency, but our network only\n- wants RGB input.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.6,
                        "section_name": "Normalizing the data",
                        "section_path": "./screenshots-images-2/chapter_5/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_6/e934536d-f12b-42e6-b6a0-abd613264adc.png",
                            "./screenshots-images-2/chapter_5/section_6/cd6d74f1-75a0-499c-8bca-4c9dc9bd9872.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Normalizing the data\n\nWe mentioned earlier that neural networks usually work with floating-point tensors as\ntheir input. Neural networks exhibit the best training performance when the input\ndata ranges roughly from 0 to 1, or from -1 to 1 (this is an effect of how their building\nblocks are defined).\n\nSo a typical thing we'll want to do is cast a tensor to floating-point and normalize\nthe values of the pixels. Casting to floating-point is easy, but normalization is trickier,\nas it depends on what range of the input we decide should lie between 0 and 1 (or -1\nand 1). One possibility is to just divide the values of the pixels by 255 (the maximum\nrepresentable number in 8-bit unsigned):\n\n# In(6]:\nbatch = batch. float ()\nbatch /= 255.0\n\nAnother possibility is to compute the mean and standard deviation of the input data\nand scale it so that the output has zero mean and unit standard deviation across each\nchannel:\n\n# In[(7]:\nn_channels = batch.shape[1]\nfor \u00a2 in range(n_channels) :\nmean = torch.mean(batch[:, c)])\nstd = torch.std(batch[:, \u00a2])\nbatch[:, c]) = (batch[:, c] - mean) / std\n\nNOTE Here, we normalize just a single batch of images because we do not\nknow yet how to operate on an entire dataset. In working with images, it is good\npractice to compute the mean and standard deviation on all the training data\nin advance and then subtract and divide by these fixed, precomputed quanti-\nties. We saw this in the preprocessing for the image classifier in section 2.1.4.\n\nWe can perform several other operations on inputs, such as geometric transforma-\ntions like rotations, scaling, and cropping. These may help with training or may be\nrequired to make an arbitrary input conform to the input requirements of a network,\nlike the size of the image. We will stumble on quite a few of these strategies in section\n12.6. For now, just remember that you have image-manipulation options available.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.7,
                        "section_name": "3D images: Volumetric data",
                        "section_path": "./screenshots-images-2/chapter_5/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_7/bbab9910-73d9-4757-8fc5-58cc5419afb6.png",
                            "./screenshots-images-2/chapter_5/section_7/a0b7f001-9822-4d32-923a-6047a776a056.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "3D images: Volumetric data\n\nWe've learned how to load and represent 2D images, like the ones we take with a camera.\nIn some contexts, such as medical imaging applications involving, say, CT (computed\ntomography) scans, we typically deal with sequences of images stacked along the head-\nto-foot axis, each corresponding to aslice across the human body. In CT scans, the inten-\nsity represents the density of the different parts of the body\u2014lungs, fat, water, muscle,\nand bone, in order of increasing density\u2014mapped from dark to bright when the CT\nscan is displayed on a clinical workstation. The density at each point is computed from\nthe amount of X-rays reaching a detector after crossing through the body, with some\ncomplex math to deconvolve the raw sensor data into the full volume.\n\nCTs have only a single intensity channel, similar to a grayscale image. This means\nthat often, the channel dimension is left out in native data formats; so, similar to the\nlast section, the raw data typically has three dimensions. By stacking individual 2D\nslices into a 3D tensor, we can build volumewic data representing the 3D anatomy of a\nsubject. Unlike what we saw in figure 4.1, the extra dimension in figure 4.2 represents\nan offset in physical space, rather than a particular band of the visible spectrum.\n\nToe MIDDLE BOTTOM\n\nFigure 4.2 Slices of a CT scan, from the top of the head to the jawline\n\nPart 2 of this book will be devoted to tackling a medical imaging problem in the real\nworld, so we won\u2019t go into the details of medical-imaging data formats. For now, it suf-\nfices to say that there\u2019s no fundamental difference between a tensor storing volumet-\nric data versus image data. We just have an extra dimension, depth, after the channel\ndimension, leading to a 5D tensor of shape Nx Cx Dx Hx W.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.8,
                        "section_name": "Loading a specialized format",
                        "section_path": "./screenshots-images-2/chapter_5/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_8/bd71d5fc-dd97-4404-adf9-e0aa4502e911.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Loading a specialized format\n\nLet's load a sample CT scan using the volread function in the imageio module, which\ntakes a directory as an argument and assembles all Digital Imaging and Communi-\ncations in Medicine (DICOM) files\u201d in a series in a NumPy 3D array (code/plch4/\n2_volumetric_ct.ipynb).\n\nListing 4.2 code/pic' 2_volumetric_ct.ipynb\n\n# In[2]:\nimport imageio\n\ndir_path = \"../data/plch4/volumetric-dicom/2-LUNG 3.0 B70\u00a3-04083\"\nvol_arr = imageio.volread(dir_path, 'DICOM')\nvol_arr.shape\n\n# Out[2]:\n\nReading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%)\nFound 1 correct series.\n\nReading DICOM (loading data): 31/99 (31.392/99 (92.999/99 (100.0%)\n\n(99, 512, 512)\n\nAs was true in section 4.1.3, the layout is different from what PyTorch expects, due to\nhaving no channel information. So we'll have to make room for the channel dimen-\nsion using unsqueeze:\n\n# In(3]:\nvol = torch. from_numpy(vol_arr) .float()\nvol = torch.unsqueeze(vol, 0)\n\nvol.shape\n\n# Out[3]:\ntorch.Size([1, 99, 512, 512)})\n\nAt this point we could assemble a 5D dataset by stacking multiple volumes along the\nbatch direction, just as we did in the previous section. We'll see a lot more CT data in\n\npart 2.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.9,
                        "section_name": "Representing tabular data",
                        "section_path": "./screenshots-images-2/chapter_5/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_9/012e9a67-2636-4e5e-8338-08ea7d2fb991.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Representing tabular data\n\nThe simplest form of data we'll encounter on a machine learning job is sitting in a\nspreadsheet, CSV file, or database. Whatever the medium, it\u2019s a table containing one\nrow per sample (or record), where columns contain one piece of information about\nour sample.\n\nAt first we are going to assume there\u2019s no meaning to the order in which samples\nappear in the table: such a table is a collection of independent samples, unlike a time\nseries, for instance, in which samples are related by a time dimension.\n\nColumns may contain numerical values, like temperatures at specific locations; or\nlabels, like a string expressing an attribute of the sample, like \u201cblue.\u201d Therefore, tabu-\nlar data is typically not homogeneous: different columns don\u2019t have the same type. We\nmight have a column showing the weight of apples and another encoding their color\nin a label.\n\nPyTorch tensors, on the other hand, are homogeneous. Information in PyTorch is\ntypically encoded as a number, typically floating-point (though integer types and\nBoolean are supported as well). This numeric encoding is deliberate, since neural\nnetworks are mathematical entities that take real numbers as inputs and produce real\nnumbers as output through successive application of matrix multiplications and\nnonlinear functions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.1,
                        "section_name": "Using a real-world dataset",
                        "section_path": "./screenshots-images-2/chapter_5/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_10/613bb447-72e7-4207-9f47-b7b5e2eed6d1.png",
                            "./screenshots-images-2/chapter_5/section_10/ea499dcb-0bc0-46dd-a016-1e8998b72fbe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using a real-world dataset\n\nOur first job as deep learning practitioners is to encode heterogeneous, real-world\ndata into a tensor of floating-point numbers, ready for consumption by a neural net-\nwork. A large number of tabular datasets are freely available on the internet; see, for\ninstance, https://github.com/caesar0301/awesome-public-datasets. Let\u2019s start with\nsomething fun: wine! The Wine Quality dataset is a freely available table containing\nchemical characterizations of samples of vinho verde, a wine from north Portugal,\ntogether with a sensory quality score. The dataset for white wines can be downloaded\nhere: http://mng.bz/90Ol. For convenience, we also created a copy of the dataset on\nthe Deep Learning with PyTorch Git repository, under data/plch4/tabular-wine.\n\nThe file contains a comma-separated collection of values organized in 12 columns\npreceded by a header line containing the column names. The first 11 columns con-\ntain values of chemical variables, and the last column contains the sensory quality\nscore from 0 (very bad) to 10 (excellent). These are the column names in the order\nthey appear in the dataset:\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\n\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\n\npH\nsulphates\nalcohol\nquality\n\nA possible machine learning task on this dataset is predicting the quality score from\nchemical characterization alone. Don\u2019t worry, though; machine learning is not going\nto kill wine tasting anytime soon. We have to get the training data from somewhere! As\nwe can see in figure 4.3, we\u2019re hoping to find a relationship between one of the chem-\nical columns in our data and the quality column. Here, we're expecting to see quality\nincrease as sulfur decreases.\n\nQUALITY\n\nFigure 4.3 The (we hope) relationship between sulfur and quality in wine\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.11,
                        "section_name": "Loading a wine data tensor",
                        "section_path": "./screenshots-images-2/chapter_5/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_11/c6ccb916-1543-4f30-bdd3-26e1d15a3c95.png",
                            "./screenshots-images-2/chapter_5/section_11/14a35093-77aa-4e97-9f40-6a4ed5fb5350.png",
                            "./screenshots-images-2/chapter_5/section_11/857acac4-caac-44e6-a9cd-9ddc18b8bce9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Loading a wine data tensor\n\nBefore we can get to that, however, we need to be able to examine the data in a more\nusable way than opening the file in a text editor. Let\u2019s see how we can load the data\nusing Python and then turn it into a PyTorch tensor. Python offers several options for\nquickly loading a CSV file. Three popular options are\n\n= The csv module that ships with Python\n= NumPy\n\n= Pandas\n\nThe third option is the most time- and memory-efficient. However, we'll avoid intro-\nducing an additional library in our learning trajectory just because we need to load a\nfile. Since we already introduced NumPy in the previous section, and PyTorch has\nexcellent NumPy interoperability, we'll go with that. Let\u2019s load our file and turn the\nresulting NumPy array into a PyTorch tensor (code/plch4/3_tabular_wine.ipynb).\n\nting 4.3 c pi | tabular_wine.ipynb\n\n# In[2]):\n\nimport csv\n\nwine_path = *../data/plch4/tabular-wine/winequality-white.csv\"\n\nwineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\",\nskiprows=1)\n\nwineq_numpy\n# Out[2):\narray(({ 7. , 0.27, 0.45, 8.8, 6. J,\n(6.3, 0.3, 0.49, 9.5, 6. J,\n(8.1, 0.28, 0.44, 10.1, 6. J,\n(6.5, 0.24, 0.19, ..., 0.46, 9.4, 6. J,\n(5.5, 0.29, 0.3, . 0.38, 12.8, 7. J,\n(6. , 0.21, 0.38, ..., 0.32, 11.8, 6. J], dtype=float32)\n\nHere we just prescribe what the type of the 2D array should be (32-bit floating-point),\nthe delimiter used to separate values in each row, and the fact that the first line should\nnot be read since it contains the column names. Let\u2019s check that all the data has been\nread\n\n# In[3):\ncol_list = next(csv.reader(open(wine_path), delimiter=';'))\n\nwineq_numpy.shape, col_list\n\n# Out[3]:\n((4898, 12),\n\n(\u2018fixed acidity\u2019,\n\u2018volatile acidity',\n\u2018eitric acid',\n\u2018residual sugar',\n\u2018chlorides\u2019,\n\n\u2018free sulfur dioxide\u2019,\n\u2018total sulfur dioxide\u2019,\n\u2018density',\n\n\u2018pH',\n\n*sulphates',\n\n\u2018alcohol\u2019,\n\n\u2018quality'))\n\nand proceed to convert the NumPy array to a PyTorch tensor:\n\n# In[4]:\nwineq = torch. from_numpy (wineq_numpy)\n\nwineq.shape, wineq.dtype\n\n# Out [4]:\n(torch.Size([4898, 12]), torch. float32)\n\nAt this point, we have a floating-point torch.Tensor containing all the columns,\nincluding the last, which refers to the quality score.\n\nContinuous, ordinal, and categorical values\n\nWe should be aware of three different kinds of numerical values as we attempt to\nmake sense of our data. The first kind is continuous values. These are the most intu-\nitive when represented as numbers. They are strictly ordered, and a difference\nbetween various values has a strict meaning. Stating that package A is 2 kilograms\nheavier than package B, or that package B came from 100 miles farther away than A\nhas a fixed meaning, regardless of whether package A is 3 kilograms or 10, or if B\ncame from 200 miles away or 2,000. If you're counting or measuring something with\nunits, it\u2019s probably a continuous value. The literature actually divides continuous val-\nues further: in the previous examples, it makes sense to say something is twice as\nheavy or three times farther away, so those values are said to be on a ratio scale.\nThe time of day, on the other hand, does have the notion of difference, but it is not\nreasonable to claim that 6:00 is twice as late as 3:00; so time of day only offers an\ninterval scale.\n\nNext we have ordinal values. The strict ordering we have with continuous values\nremains, but the fixed relationship between values no longer applies. A good example\nof this is ordering a small, medium, or large drink, with small mapped to the value 1,\nmedium 2, and large 3. The large drink is bigger than the medium, in the same way\nthat 3 is bigger than 2, but it doesn't tell us anything about how much bigger. If we\nwere to convert our 1, 2, and 3 to the actual volumes (say, 8, 12, and 24 fluid\nounces), then they would switch to being interval values. It's important to remember\nthat we can't \u201cdo math\u201d on the values outside of ordering them; trying to average\nlarge = 3 and small = 1 does not result in a medium drink!\n\nFinally, categorical values have neither ordering nor numerical meaning to their values.\nThese are often just enumerations of possibilities assigned arbitrary numbers. Assign-\ning water to 1, coffee to 2, soda to 3, and milk to 4 is a good example. There's no\nreal logic to placing water first and milk last; they simply need distinct values to dif-\nferentiate them. We could assign coffee to 10 and milk to -3, and there would be no\nsignificant change (though assigning values in the range 0..N- 1 will have advantages\nfor one-hot encoding and the embeddings we'll discuss in section 4.5.4.) Because\nthe numerical values bear no meaning, they are said to be on a nominal scale.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.12,
                        "section_name": "Representing scores",
                        "section_path": "./screenshots-images-2/chapter_5/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_12/efe75c3b-1db1-49a9-bdc3-d790bc5e3a6c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Representing scores\n\nWe could treat the score as a continuous variable, keep it as a real number, and per-\nform a regression task, or treat it as a label and try to guess the label from the chemi-\ncal analysis in a classification task. In both approaches, we will typically remove the\nscore from the tensor of input data and keep it in a separate tensor, so that we can use\nthe score as the ground truth without it being input to our model:\n\n# In[5):\nGate \u00a9 wineg (9 ol \u201c) Selects all rows and all\na, Gata.shape columns except the last\n# Out[5):\n(tensor([[ 7.00, 0.27, . 0.45, 8.80),\n[ 6.30, 0.30, wee, 0.49, 9.50),\n(5.50, 0.29, ..., 0.38, 12.80),\n(6.00, 0.21, ..., 0.32, 11.80)]), torch.Size([4898, 11)))\n# In[6]:\ntarget = wineq[:, -1] \u201c$ Selects all rows and\ntarget, target.shape the last column\n# Out[6):\n(tensor([6., 6., ..., 7., 6.]), torch.Size([4898)))\n\nIf we want to transform the target tensor in a tensor of labels, we have two options,\ndepending on the strategy or what we use the categorical data for. One is simply to\ntreat labels as an integer vector of scores:\n\n# In[7):\n\ntarget = wineq[:, -1].long({)\ntarget\n\n# Out[7):\n\ntensor((6, 6, ..., 7, 6))\n\nIf targets were string labels, like wine color, assigning an integer number to each string\nwould let us follow the same approach.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.13,
                        "section_name": "One-hot encoding",
                        "section_path": "./screenshots-images-2/chapter_5/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_13/8407a49a-61fc-4674-85c8-d378f4cfc98f.png",
                            "./screenshots-images-2/chapter_5/section_13/168e9b65-ea56-4f0d-8421-31af722a1a0f.png",
                            "./screenshots-images-2/chapter_5/section_13/9201e6f8-4c95-4594-a78a-eff32a26639e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "One-hot encoding\n\nThe other approach is to build a one-hot encoding of the scores: that is, encode each of\nthe 10 scores in a vector of 10 elements, with all elements set to 0 but one, at a differ-\nent index for each score. This way, a score of 1 could be mapped onto the vector\n(1,0,0,0,0,0,0,0,0,0), ascore of 5 onto (0,0,0,0,1,0,0,0,0,0), and so on. Note\nthat the fact that the score corresponds to the index of the nonzero element is purely\nincidental: we could shuffle the assignment, and nothing would change from a classifi-\ncation standpoint.\n\nThere\u2019s a marked difference between the two approaches. Keeping wine quality\nscores in an integer vector of scores induces an ordering on the scores\u2014which might\nbe totally appropriate in this case, since a score of 1 is lower than a score of 4. It also\ninduces some sort of distance between scores: that is, the distance between 1 and 3 is the\nsame as the distance between 2 and 4. If this holds for our quantity, then great. If, on\nthe other hand, scores are purely discrete, like grape variety, one-hot encoding will be\na much better fit, as there\u2019s no implied ordering or distance. One-hot encoding is also\nappropriate for quantitative scores when fractional values in between integer scores,\nlike 2.4, make no sense for the application\u2014for when the score is either this or that.\n\nWe can achieve one-hot encoding using the scatter_ method, which fills the ten-\nsor with values from a source tensor along the indices provided as arguments:\n\n# In(8]:\ntarget_onehot = torch.zeros(target.shape[0), 10)\n\ntarget_onehot.scatter_(1, target.unsqueeze(1), 1.0)\n\n# Out[8]:\n\ntensor([[0., 0.,  ..., 0., 0.),\n(0., O., ..., O., O.),\n(0., O., ..., O., O.),\n(0., O., ..., O., 0.37)\n\nLet\u2019s see what scatter_ does. First, we notice that its name ends with an underscore.\nAs you learned in the previous chapter, this is a convention in PyTorch that indicates\nthe method will not return a new tensor, but will instead modify the tensor in place.\nThe arguments for scatter_ are as follows:\n\n= The dimension along which the following two arguments are specified\n= Acolumn tensor indicating the indices of the elements to scatter\n\n= A tensor containing the elements to scatter or a single scalar to scatter (1, in\nthis case)\n\nIn other words, the previous invocation reads, \u201cFor each row, take the index of the tar-\nget label (which coincides with the score in our case) and use it as the column index\nto set the value 1.0.\u201d The end result is a tensor encoding categorical information.\n\nThe second argument of scatter_, the index tensor, is required to have the same\nnumber of dimensions as the tensor we scatter into. Since target_onehot has two\ndimensions (4,898 x 10), we need to add an extra dummy dimension to target using\nunsqueeze:\n\n# In[9]:\ntarget_unsqueezed = target .unsqueeze(1)\ntarget_unsqueezed\n\n# Out[9]:\ntensor ([[6],\n\n(6),\n\n(7),\n\n(6)})\nThe call to unsqueeze adds a singleton dimension, from a 1D tensor of 4,898 elements\nto a 2D tensor of size (4,898 x 1), without changing its contents\u2014no extra elements\nare added; we just decided to use an extra index to access the elements. That is, we\naccess the first element of target as target[0] and the first element of its\nunsqueezed counterpart as target_unsqueezed[0,0].\n\nPyTorch allows us to use class indices directly as targets while training neural net-\n\nworks. However, if we wanted to use the score as a categorical input to the network, we\nwould have to transform it to a one-hot-encoded tensor.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.14,
                        "section_name": "When to categorize",
                        "section_path": "./screenshots-images-2/chapter_5/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_14/243bdc73-fc51-43c5-8f3f-956b4675e154.png",
                            "./screenshots-images-2/chapter_5/section_14/2e60d56c-29dd-4e6d-81e5-b4ab0dab60de.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "When to categorize\n\nNow we have seen ways to deal with both continuous and categorical data. You may\nwonder what the deal is with the ordinal case discussed in the earlier sidebar. There is\n\nno general recipe for it; most commonly, such data is either treated as categorical (los-\ning the ordering part, and hoping that maybe our model will pick it up during train-\ning if we only have a few categories) or continuous (introducing an arbitrary notion of\ndistance). We will do the latter for the weather situation in figure 4.5. We summarize\nour data mapping in a small flow chart in figure 4.4.\n\nEXAMPLE REPRESENTATION\nOF ONE VALUE\n\nBMS\n\nO0000\\0000\n\nFigure 4.4 How to treat columns with continuous, ordinal, and categorical data\n\nLet\u2019s go back to our data tensor, containing the 11 variables associated with the chemical\nanalysis. We can use the functions in the PyTorch Tensor API to manipulate our data in\ntensor form. Let's first obtain the mean and standard deviations for each column:\n\n# In(10]:\ndata_mean = torch.mean(data, dim=0)\ndata_mean\n\n# Out[10):\ntensor([6.85e+00, 2.78e-01, 3.3d4e-01, 6.39e+00, 4.58e-02, 3.53e+01,\n1.38e+02, 9.94e-01, 3.19e+00, 4.90e-01, 1.05e+01))\n\n# In{1l]:\ndata_var = torch.var(data, dim=0)\ndata_var\n\n# Out[11]:\ntensor([7.12e-01, 1.02e-02, 1.46e-02, 2.57e+01, 4.77e-04, 2.89e+02,\n1.8le+03, 8.95e-06, 2.28e-02, 1.30e-02, 1.51e+00))\n\nIn this case, dim=0 indicates that the reduction is performed along dimension 0. At\nthis point, we can normalize the data by subtracting the mean and dividing by the\nstandard deviation, which helps with the learning process (we'll discuss this in more\ndetail in chapter 5, in section 5.4.4):\n\n# In(12]:\ndata_normalized = (data - data_mean) / torch.sqrt(data_var)\ndata_normalized\n\n# Out[12]):\n\ntensor([[ 1.72e-01, -8.18e-02, ..., -3.49e-01, -1.39e+00)\n([-6.57e-01, 2.16e-01, ..., 1.35e-03, -8.24e-01),\n[-1.6le+00, 1.17e-01, ..., -9.63e-01, 1.86e+00),\n\n(-1.01le+00, -6.77e-01, ..., -1.49e+00, 1.04e+00)))\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.15,
                        "section_name": "Finding thresholds",
                        "section_path": "./screenshots-images-2/chapter_5/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_15/07c32fe4-2bed-459c-be91-31ac86c1a6c8.png",
                            "./screenshots-images-2/chapter_5/section_15/30cdfc4a-0d22-4da9-a665-b7623f768153.png",
                            "./screenshots-images-2/chapter_5/section_15/ebb6589e-004d-4b2a-9a03-0370282ff5b2.png",
                            "./screenshots-images-2/chapter_5/section_15/06368b47-9a13-490d-9e45-7dfe1910c49a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Finding thresholds\n\nNext, let\u2019s start to look at the data with an eye to seeing if there is an easy way to tell\n\ngood and bad wines apart at a glance. First, we\u2019re going to determine which rows in\ntarget correspond to a score less than or equal to 3:\n\nPyTorch also provides comparison functions,\n# n(13]: here torch.le(target, 3), but using operators\nbad_indexes = target <= 3 < seems to be ag *\n\nbad_indexes.shape, bad_indexes.dtype, bad_indexes.sum()}\n\n# Out[13):\n(torch.Size([4898]), torch.bool, tensor(20))\n\nNote that only 20 of the bad_indexes entries are set to True! By using a feature in\nPyTorch called advanced indexing, we can use a tensor with data type torch.bool to\nindex the data tensor. This will essentially filter data to be only items (or rows) corre-\nsponding to True in the indexing tensor. The bad_indexes tensor has the same shape\nas target, with values of False or True depending on the outcome of the comparison\nbetween our threshold and each element in the original target tensor:\n\n# In[14):\nbad_data = data[bad_indexes]\nbad_data.shape\n\n# Out[14]:\n\ntorch.Size((20, 11))\n\nNote that the new bad_data tensor has 20 rows, the same as the number of rows with\nTrue in the bad_indexes tensor. It retains all 11 columns. Now we can start to get\ninformation about wines grouped into good, middling, and bad categories. Let\u2019s take\nthe .mean() of each column:\n\n# In[15): For Boolean NumPy arrays and\nbad_data = data[target <= 3] PyTorch tensors, the & operator\nmid_data = data[(target > 3) & (target < 7)] \u2014 does a logical \u201cand\u201d operation.\n\ngood_data = data[target >= 7]\n\nbad_mean =\nmid_mean =\ngood_mean =\n\ntorch.mean(bad_data, dim=0)\ntorch.mean(mid_data, dim=0)\ntorch.mean(good_data, dim=0)\n\nfor i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):\n\nprint('{:2} (:20) (:6.2\u00a3} (:6.2\u00a3) {:6.2\u00a3}'.format(i, *args))\n\n# Out[i5]):\n\n0 fixed acidity 7.60 6.89 6.73\n1 volatile acidity 0.33 0.28 0.27\n2 citric acid 0.34 0.34 0.33\n3 residual sugar 6.39 6.71 5.26\n4 chlorides 0.05 0.05 0.04\n5 free sulfur dioxide 53.33 35.42 34.55\n6 total sulfur dioxide 170.60 141.83 125.25\n7 density 0.99 0.99 0.99\n8 pH 3.19 3.18 3.22\n9 sulphates 0.47 0.49 0.50\n10 alcohol 10.34 10.26 11.42\n\nIt looks like we\u2019re on to something here: at first glance, the bad wines seem to have\nhigher total sulfur dioxide, among other differences. We could use a threshold on\ntotal sulfur dioxide as a crude criterion for discriminating good wines from bad ones.\nLet\u2019s get the indexes where the total sulfur dioxide column is below the midpoint we\ncalculated earlier, like so:\n\n# In[(16]:\n\ntotal_sulfur_threshold = 141.83\n\ntotal_sulfur_data = data[:,6]\n\npredicted_indexes = torch.1t(total_sulfur_data, total_sulfur_threshold)\n\npredicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.sum/()\n\n# Out[16):\n(torch.Size([4898]), torch.bool, tensor(2727))\n\nThis means our threshold implies that just over half of all the wines are going to be\nhigh quality. Next, we'll need to get the indexes of the actually good wines:\n\n# In(17]:\nactual_indexes = target > 5\n\nactual_indexes.shape, actual_indexes.dtype, actual_indexes.sum()\n\n# Out[17):\n(torch.Size([4898]), torch.bool, tensor(3258))\n\nSince there are about 500 more actually good wines than our threshold predicted, we\nalready have hard evidence that it\u2019s not perfect. Now we need to see how well our pre-\ndictions line up with the actual rankings. We will perform a logical \u201cand\u201d between our\nprediction indexes and the actual good indexes (remember that each is just an array\nof zeros and ones) and use that intersection of wines-in-agreement to determine how\nwell we did:\n\n# In(18]:\n\nnumatches = torch.sum(actual_indexes & predicted_indexes) .item()\nn_predicted = torch.sum(predicted_indexes) .item()\n\nn_actual = torch.sum(actual_indexes) .item()\n\nn_imatches, n_matches / n_predicted, n_matches / n_actual\n\n# Out [18]:\n(2018, 0.74000733406674, 0.6193984039287906)\n\nWe got around 2,000 wines right! Since we predicted 2,700 wines, this gives us a 74%\nchance that if we predict a wine to be high quality, it actually is. Unfortunately, there\nare 3,200 good wines, and we only identified 61% of them. Well, we got what we\nsigned up for; that\u2019s barely better than random! Of course, this is all very naive: we\nknow for sure that multiple variables contribute to wine quality, and the relationships\nbetween the values of these variables and the outcome (which could be the actual\nscore, rather than a binarized version of it) is likely more complicated than a simple\nthreshold on a single value.\n\nIndeed, a simple neural network would overcome all of these limitations, as would\na lot of other basic machine learning methods. We'll have the tools to tackle this prob-\nlem after the next two chapters, once we have learned how to build our first neural\n\nnetwork from scratch. We will also revisit how to better grade our results in chapter 12.\nLet\u2019s move on to other data types for now.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.16,
                        "section_name": "Working with time series",
                        "section_path": "./screenshots-images-2/chapter_5/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_16/8984d893-6474-4bfc-b07d-e6ff0a36cb41.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "4.4\n\nWorking with time series\n\nIn the previous section, we covered how to represent data organized in a flat table. As\nwe noted, every row in the table was independent from the others; their order did not\nmatter. Or, equivalently, there was no column that encoded information about what\nrows came earlier and what came later.\n\nGoing back to the wine dataset, we could have had a \u201cyear\u201d column that allowed us\nto look at how wine quality evolved year after year. Unfortunately, we don\u2019t have such\ndata at hand, but we're working hard on manually collecting the data samples, bottle\nby bottle. (Stuff for our second edition.) In the meantime, we'll switch to another\ninteresting dataset: data from a Washington, D.C., bike-sharing system reporting the\nhourly count of rental bikes in 2011-2012 in the Capital Bikeshare system, along with\nweather and seasonal information (available here: http://mng.bz/jgOx). Our goal\nwill be to take a flat, 2D dataset and transform it into a 3D one, as shown in figure 4.5.\n\nDAY | DAY 2 DAY 3\n\n<-MIDNIGHT - NOON - MIDNIGHT-> | <-MIDNIGHT - NOON = MIDNIGHT->\n\n\\ | TIME OF DAY\nWEATHER\nTEMPERATURE\nHUMIDITY\nWIND SPEED |\nBIKE COUNT\n\nFigure 4.5 Transforming a 1D, multichannel dataset into a 2D, multichannel dataset by separating the date and\nhour of each sample into separate axes\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.17,
                        "section_name": "Adding a time dimension",
                        "section_path": "./screenshots-images-2/chapter_5/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_17/76481792-7ca3-42c0-b33c-1fb76a281f85.png",
                            "./screenshots-images-2/chapter_5/section_17/e0e6c7ac-4dfb-4fac-ae63-5fb634e67d5e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adding a time dimension\n\nIn the source data, each row is a separate hour of data (figure 4.5 shows a transposed\n\nversion of this to better fit on the printed page). We want to change the row-per-hour\n\norganization so that we have one axis that increases at a rate of one day per index incre-\n\nment, and another axis that represents the hour of the day (independent of the date).\n\nThe third axis will be our different columns of data (weather, temperature, and so on).\nLet's load the data (code/plch4/4_time_series_bikes.ipynb).\n\nListing 4.4 code/pich4/4_time_series_bikes.ipynb\n\n# In[2]:\nbikes_numpy = np. loadtxt (\n*../data/plch4/bike-sharing-dataset/hour-fixed.csv\",\ndtype=np. float32,\ndelimiter=\",\",\nskiprows=1,\nconverters=(1: lambda x: float(x[8:10])}) <-\u2014~ Converts date strings to\nbikes = torch. from_numpy(bikes_numpy)\n\nnumbers corresponding to the\nbikes day of the month in column 1\n# Out[2]:\ntensor([[1.0000e+00, 1.0000e+00, ..., 1.3000e+01, 1.6000e+01),\n[2.0000e+00, 1.0000e+00, ..., 3.2000e+01, 4.0000e+01),\n(1.7378e+04, 3.1000e+01, ..., 4.8000e+01, 6.1000e+01),\n(1.7379e+04, 3.1000e+01, ..., 3.7000e+01, 4.9000e+01)))\n\nFor every hour, the dataset reports the following variables:\n\n= Index of record: instant\n\n= Day of month: day\n\n= Season: season (1: spring, 2: summer, 3: fall, 4: winter)\n\n= Year: yr (0: 2011, 1: 2012)\n\n= Month: mnth (1 to 12)\n\n= Hour: hr (0 to 23)\n\n= Holiday status: holiday\n\n= Day of the week: weekday\n\n= Working day status: workingday\n\n= Weather situation: weathersit (1: clear, 2:mist, 3: light rain/snow, 4: heavy\nrain/snow)\n\n= Temperature in \u00b0C: temp\n\n= Perceived temperature in \u00b0C: atemp\n\n= Humidity: hum\n\n= Wind speed: windspeed\n\n= Number of casual users: casual\n\n= Number of registered users: registered\n\n= Count of rental bikes: cnt\n\nIn a time series dataset such as this one, rows represent successive time-points: there is\na dimension along which they are ordered. Sure, we could treat each row as indepen-\ndent and try to predict the number of circulating bikes based on, say, a particular time\nof day regardless of what happened earlier. However, the existence of an ordering\ngives us the opportunity to exploit causal relationships across time. For instance, it\nallows us to predict bike rides at one time based on the fact that it was raining at an\nearlier time. For the time being, we\u2019re going to focus on learning how to turn our\nbike-sharing dataset into something that our neural network will be able to ingest in\nfixed-size chunks.\n\nThis neural network model will need to see a number of sequences of values for\neach different quantity, such as ride count, time of day, temperature, and weather con-\nditions: N parallel sequences of size C. C stands for channel, in neural network par-\nlance, and is the same as column for 1D data like we have here. The N dimension\nrepresents the time axis, here one entry per hour.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.18,
                        "section_name": "Shaping the data by time period",
                        "section_path": "./screenshots-images-2/chapter_5/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_18/9fa684bb-7a36-49a0-aabe-aaf648f59bc4.png",
                            "./screenshots-images-2/chapter_5/section_18/adbb1809-c577-44d1-874b-b1bbe60cca17.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Shaping the data by time period\n\nWe might want to break up the two-year dataset into wider observation periods, like\ndays. This way we'll have N (for number of samples) collections of C sequences of length\nL. In other words, our time series dataset would be a tensor of dimension 3 and shape\nNx Cx L. The Cwould remain our 17 channels, while L would be 24: 1 per hour of\nthe day. There\u2019s no particular reason why we must use chunks of 24 hours, though the\ngeneral daily rhythm is likely to give us patterns we can exploit for predictions. We\ncould also use 7 x 24 = 168 hour blocks to chunk by week instead, if we desired. All of\nthis depends, naturally, on our dataset having the right size\u2014the number of rows must\nbe a multiple of 24 or 168. Also, for this to make sense, we cannot have gaps in the\ntime series.\n\nLet\u2019s go back to our bike-sharing dataset. The first column is the index (the global\nordering of the data), the second is the date, and the sixth is the time of day. We have\neverything we need to create a dataset of daily sequences of ride counts and other\nexogenous variables. Our dataset is already sorted, but if it were not, we could use\ntorch.sort on it to order it appropriately.\n\nNOTE The version of the file we\u2019re using, hour-fixed.csv, has had some pro-\ncessing done to include rows missing from the original dataset. We presume\nthat the missing hours had zero bike active (they were typically in the early\nmorning hours).\n\nAll we have to do to obtain our daily hours dataset is view the same tensor in batches\nof 24 hours. Let\u2019s take a look at the shape and strides of our bikes tensor:\n\n# In[3):\nbikes.shape, bikes.stride()\n\n# Out(3):\n(torch.Size((17520, 17)), (17, 1))\n\nThat\u2019s 17,520 hours, 17 columns. Now let\u2019s reshape the data to have 3 axes\u2014day, hour,\nand then our 17 columns:\n\n# In[{4]:\ndaily_bikes = bikes.view(-1, 24, bikes.shape[1])\ndaily_bikes.shape, daily_bikes.stride()\n\n# Out[4]:\n(torch.Size([730, 24, 17]), (408, 17, 1))\n\nWhat happened here? First, bikes.shape[1] is 17, the number of columns in the\nbikes tensor. But the real crux of this code is the call to view, which is really import-\nant: it changes the way the tensor looks at the same data as contained in storage.\n\nAs you learned in the previous chapter, calling view on a tensor returns a new ten-\nsor that changes the number of dimensions and the striding information, without\nchanging the storage. This means we can rearrange our tensor at basically zero cost,\nbecause no data will be copied. Our call to view requires us to provide the new shape\nfor the returned tensor. We use -1 as a placeholder for \u201chowever many indexes are\nleft, given the other dimensions and the original number of elements.\u201d\n\nRemember also from the previous chapter that storage is a contiguous, linear con-\ntainer for numbers (floating-point, in this case). Our bikes tensor will have each row\nstored one after the other in its corresponding storage. This is confirmed by the out-\nput from the call to bikes.stride() earlier.\n\nFor daily_bikes, the stride is telling us that advancing by 1 along the hour dimen-\nsion (the second dimension) requires us to advance by 17 places in the storage (or\none set of columns); whereas advancing along the day dimension (the first dimen-\nsion) requires us to advance by a number of elements equal to the length of a row in\nthe storage times 24 (here, 408, which is 17 x 24).\n\nWe see that the rightmost dimension is the number of columns in the original\ndataset. Then, in the middle dimension, we have time, split into chunks of 24 sequen-\ntial hours. In other words, we now have N sequences of L hours in a day, for C chan-\nnels. To get to our desired Nx Cx L ordering, we need to transpose the tensor:\n\n# In(5]:\ndaily_bikes = daily _bikes.transpose(1, 2)\ndaily_bikes.shape, daily_bikes.stride()\n\n# Out[5]:\n(torch.Size([730, 17, 24)), (408, 1, 17))\n\nNow let\u2019s apply some of the techniques we learned earlier to this dataset.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.19,
                        "section_name": "Ready for training",
                        "section_path": "./screenshots-images-2/chapter_5/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_19/384d5542-9322-4236-8ce7-e301192969d7.png",
                            "./screenshots-images-2/chapter_5/section_19/765c1fba-c178-4b55-8268-cef864f3e28f.png",
                            "./screenshots-images-2/chapter_5/section_19/1726a180-8a75-4f83-8ac3-a0921372ea96.png",
                            "./screenshots-images-2/chapter_5/section_19/2e5c2280-f5e7-4102-91d8-7b96a8aea2c1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Ready for training\n\nThe \u201cweather situation\u201d variable is ordinal. It has four levels: 1 for good weather, and 4\nfor, er, really bad. We could treat this variable as categorical, with levels interpreted as\nlabels, or as a continuous variable. If we decided to go with categorical, we would turn\n\nthe variable into a one-hot-encoded vector and concatenate the columns with the\ndataset.*\n\nIn order to make it easier to render our data, we're going to limit ourselves to the\nfirst day for a moment. We initialize a zero-filled matrix with a number of rows equal\nto the number of hours in the day and number of columns equal to the number of\nweather levels:\n\n# In[6):\n\nfirst_day = bikes[:24].long()\n\nweather_onehot = torch.zeros(first_day.shape[0], 4)\nfirst_day[:,9]\n\n# Out[6):\ntensor({1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2,\n2, 2))\n\nThen we scatter ones into our matrix according to the corresponding level at each\nrow. Remember the use of unsqueeze to add a singleton dimension as we did in the\nprevious sections:\n\n# In(7): Decreases the values by 1\nweather_onehot.scatter_( because weather situation\ndim=1, ranges from 1 to 4, while\nindex=first_day[:,9].unsqueeze(1).long() - 1, < indices are 0-based\nvalue=1.0)\n# Out[7):\ntensor([{[1., 0., 0., 0.],\n(1., 0., 0., 0.),\n(O., 1., 0., 0.),\n[O., 1., 0., 0.]])\n\nOur day started with weather \u201c1\u201d and ended with \u201c2,\u201d so that seems right.\nLast, we concatenate our matrix to our original dataset using the cat function.\nLet\u2019s look at the first of our results:\n\n# In[8]:\ntorch.cat((bikes[:24], weather_onehot), 1) [:1)\n\n# Out[8):\n\ntensor([{[ 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n6.0000, 0.0000, 1.0000, 0.2400, 0.2879, 0.8100, 0.0000,\n3.0000, 13.0000, 16.0000, 1.0000, 0.0000, 0.0000, 0.0000)})\n\nHere we prescribed our original bikes dataset and our one-hot-encoded \u201cweather sit-\nuation\u201d matrix to be concatenated along the column dimension (that is, 1). In other\nwords, the columns of the two datasets are stacked together; or, equivalently, the new\none-hot-encoded columns are appended to the original dataset. For cat to succeed, it\nis required that the tensors have the same size along the other dimensions\u2014the row\ndimension, in this case. Note that our new last four columns are i, 0, 0, 0, exactly\nas we would expect with a weather value of 1.\n\nWe could have done the same with the reshaped daily_bikes tensor. Remember\nthat it is shaped (B, C, L), where L = 24. We first create the zero tensor, with the same\nBand L, but with the number of additional columns as C:\n\n# In[9]:\n\ndaily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4,\ndaily_bikes.shape[2]}\n\ndaily_weather_onehot.shape\n\n# Out[9]:\ntorch.Size([730, 4, 24))\n\nThen we scatter the one-hot encoding into the tensor in the C dimension. Since this\noperation is performed in place, only the content of the tensor will change:\n\n# In(10]:\ndaily_weather_onehot.scatter_(\n\n1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0)\ndaily_weather_onehot.shape\n\n# Out[10):\ntorch.Size([730, 4, 24])\n\nAnd we concatenate along the C dimension:\n\n# In{11]:\ndaily_bikes = torch.cat((daily_ bikes, daily _weather_onehot), dim=1)\n\nWe mentioned earlier that this is not the only way to treat our \u201cweather situation\u201d vari-\nable. Indeed, its labels have an ordinal relationship, so we could pretend they are spe-\ncial values of a continuous variable. We could just transform the variable so that it runs\nfrom 0.0 to 1.0:\n\n# In(12]:\ndaily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1.0) / 3.0\n\nAs we mentioned in the previous section, rescaling variables to the [0.0, 1.0] interval\nor the [-1.0, 1.0] interval is something we'll want to do for all quantitative variables,\nlike temperature (column 10 in our dataset). We'll see why later; for now, let\u2019s just say\nthat this is beneficial to the training process.\n\nThere are multiple possibilities for rescaling variables. We can either map their\nrange to [0.0, 1.0]\n\n# In[13]:\n\ntemp = daily _bikes[:, 10, :]\n\ntemp_min = torch.min(temp)\n\ntemp_max = torch.max(temp)\n\ndaily _bikes[:, 10, :] = ((daily_bikes[:, 10, :] - temp_min)\n/ (temp_max - temp_min))\n\nor subtract the mean and divide by the standard deviation:\n\n# In[14):\ntemp = daily_bikes[:, 10, :]\ndaily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - torch.mean(temp) }\n\n/ torch.std(temp) )\n\nIn the latter case, our variable will have 0 mean and unitary standard deviation. If our\nvariable were drawn from a Gaussian distribution, 68% of the samples would sit in the\n[-1.0, 1.0} interval.\n\nGreat: we've built another nice dataset, and we've seen how to deal with time series\ndata. For this tour d\u2019horizon, it\u2019s important only that we got an idea of how a time\nseries is laid out and how we can wrangle the data in a form that a network will digest.\n\nOther kinds of data look like a time series, in that there is a strict ordering. Top\ntwo on the list? Text and audio. We'll take a look at text next, and the \u201cConclusion\u201d\nsection has links to additional examples for audio.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.2,
                        "section_name": "Representing text",
                        "section_path": "./screenshots-images-2/chapter_5/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_20/b73ee625-d4fb-45b7-ae07-16970d2e4ffe.png",
                            "./screenshots-images-2/chapter_5/section_20/8de37b49-fd55-4137-aff5-e2028eb718e9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Representing text\n\nDeep learning has taken the field of natural language processing (NLP) by storm, par-\nticularly using models that repeatedly consume a combination of new input and previ-\nous model output. These models are called recurrent neural networks (RNNs), and they\nhave been applied with great success to text categorization, text generation, and auto-\nmated translation systems. More recently, a class of networks called transformers with a\nmore flexible way to incorporate past information has made a big splash. Previous\nNLP workloads were characterized by sophisticated multistage pipelines that included\nrules encoding the grammar of a language.\u201d Now, state-of-the-art work trains networks\nend to end on large corpora starting from scratch, letting those rules emerge from the\ndata. For the last several years, the most-used automated translation systems available\nas services on the internet have been based on deep learning.\n\nOur goal in this section is to turn text into something a neural network can pro-\ncess: a tensor of numbers, just like our previous cases. If we can do that and later\nchoose the right architecture for our text-processing job, we'll be in the position of\ndoing NLP with PyTorch. We see right away how powerful this all is: we can achieve\n\nstate-of-the-art performance on a number of tasks in different domains with the same\nPyTorch tools; we just need to cast our problem in the right form. The first part of this\njob is reshaping the data.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.21,
                        "section_name": "Converting text to numbers",
                        "section_path": "./screenshots-images-2/chapter_5/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_21/8468d512-282f-4391-a51c-9f0440fcf3e4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Converting text to numbers\n\nThere are two particularly intuitive levels at which networks operate on text: at the\ncharacter level, by processing one character at a time, and at the word level, where\nindividual words are the finest-grained entities to be seen by the network. The tech-\nnique with which we encode text information into tensor form is the same whether we\noperate at the character level or the word level. And it\u2019s not magic, either. We stum-\nbled upon it earlier: one-hot encoding.\n\nLet\u2019s start with a character-level example. First, let\u2019s get some text to process. An\namazing resource here is Project Gutenberg (www.gutenberg.org), a volunteer effort\nto digitize and archive cultural work and make it available for free in open formats,\nincluding plain text files. If we're aiming at larger-scale corpora, the Wikipedia corpus\nstands out: it\u2019s the complete collection of Wikipedia articles, containing 1.9 billion\nwords and more than 4.4 million articles. Several other corpora can be found at the\nEnglish Corpora website (www.english-corpora.org) .\n\nLet\u2019s load Jane Austen's Pride and Prejudice from the Project Gutenberg website:\nwww.gutenberg.org/files/1342/1342-0.txt. We'll just save the file and read it in\n(code/plch4/5_text_jane_austen.ipynb).\n\nListing 4.5 code/pich4/5_text_jane_austen.ipynb\n\n# In[2]:\nwith open('../data/plch4/jane-austen/1342-0.txt', encoding=\"utf8') as f:\ntext = f.read()\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.22,
                        "section_name": "One-hot-encoding characters",
                        "section_path": "./screenshots-images-2/chapter_5/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_22/7c8a0737-3ff4-401c-b784-2905e7b07616.png",
                            "./screenshots-images-2/chapter_5/section_22/3f8dcd76-9a4a-45e0-8185-8be5f0d04b4e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "One-hot-encoding characters\n\nThere\u2019s one more detail we need to take care of before we proceed: encoding. This is\na pretty vast subject, and we will just touch on it. Every written character is represented\nby a code: a sequence of bits of appropriate length so that each character can be\nuniquely identified. The simplest such encoding is ASCII (American Standard Code\nfor Information Interchange), which dates back to the 1960s. ASCII encodes 128 char-\nacters using 128 integers. For instance, the letter a corresponds to binary 1100001 or\ndecimal 97, the letter } to binary 1100010 or decimal 98, and so on. The encoding fits\n8 bits, which was a big bonus in 1965.\n\nNOTE 128 characters are clearly not enough to account for all the glyphs,\naccents, ligatures, and so on that are needed to properly represent written\ntext in languages other than English. To this end, a number of encodings\nhave been developed that use a larger number of bits as code for a wider\nrange of characters. That wider range of characters was standardized as Uni-\ncode, which maps all known characters to numbers, with the representation\n\nin bits of those numbers provided by a specific encoding. Popular encodings\nare UTF-8, UTF-16, and UTF-32, in which the numbers are a sequence of 8-,\n16-, or 32-bit integers, respectively. Strings in Python 3.x are Unicode strings.\n\nWe are going to one-hot encode our characters. It is instrumental to limit the one-hot\nencoding to a character set that is useful for the text being analyzed. In our case, since\nwe loaded text in English, it is safe to use ASCII and deal with a small encoding. We\ncould also make all of the characters lowercase, to reduce the number of different\ncharacters in our encoding. Similarly, we could screen out punctuation, numbers, or\nother characters that aren\u2019t relevant to our expected kinds of text. This may or may\nnot make a practical difference to a neural network, depending on the task at hand.\n\nAt this point, we need to parse through the characters in the text and provide a\none-hot encoding for each of them. Each character will be represented by a vector of\nlength equal to the number of different characters in the encoding. This vector will\ncontain all zeros except a one at the index corresponding to the location of the char-\nacter in the encoding.\n\nWe first split our text into a list of lines and pick an arbitrary line to focus on:\n\n# In[3}:\n\nlines = text.split('\\n')\nline = lines[200)\n\nline\n\n# Out(3):\n\"Impossible, Mr. Bennet, impossible, when I am not acquainted with him\u2019\n\nLet\u2019s create a tensor that can hold the total number of one-hot-encoded characters for\nthe whole line:\n\n# In[4): 128 hardcoded due to\nletter_t = torch.zeros(len(line}), 128) a the limits of ASCII\nletter_t.shape\n\n# Out[4):\ntorch.Size((70, 128])\n\nNote that letter_t holds a one-hot-encoded character per row. Now we just have to\nset a one on each row in the correct position so that each row represents the correct\ncharacter. The index where the one has to be set corresponds to the index of the char-\nacter in the encoding:\n\n# In[5):\nfor i, letter in enumerate(line.lower().strip()):\nletter_index = ord(letter) if ord(letter) < 128 else 0 a\nletter_t[(i] [letter_index] = 1\nThe text uses directional double\nquotes, which are not valid ASCII,\n\nso we screen them out here.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.23,
                        "section_name": "One-hot encoding whole words",
                        "section_path": "./screenshots-images-2/chapter_5/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_23/9606a7e4-6273-4588-b754-d0f326e065eb.png",
                            "./screenshots-images-2/chapter_5/section_23/6501f227-21e5-496e-b75e-40699f3bebae.png",
                            "./screenshots-images-2/chapter_5/section_23/be8f1f4c-b224-468d-aabc-bdb7aa12471c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "One-hot encoding whole words\n\nWe have one-hot encoded our sentence into a representation that a neural network\ncould digest. Word-level encoding can be done the same way by establishing a vocabu-\nlary and one-hot encoding sentences\u2014sequences of words\u2014along the rows of our\ntensor. Since a vocabulary has many words, this will produce very wide encoded vec-\ntors, which may not be practical. We will see in the next section that there is a more\nefficient way to represent text at the word level, using embeddings. For now, let\u2019s stick\nwith one-hot encodings and see what happens.\n\nWe'll define clean_words, which takes text and returns it in lowercase and\nstripped of punctuation. When we call it on our \u201cImpossible, Mr. Bennet\u201d line, we get\nthe following:\n\n# In(6]:\n\ndef clean_words(input_str\npunctuation = '.,;:\"!?\"\"%_-'\nword_list = input_str.lower().replace('\\n',' *).split()\nword_list = [word.strip(punctuation) for word in word_list]\n\nreturn word_list\n\nwords_in_line = clean_words (line)\nline, words_in_line\n\n# Out [6]:\n('\u201cImpossible, Mr. Bennet, impossible, when I am not acquainted with him',\n['impossible',\n\u2018mrt,\n'bennet',\n\u2018impossible\u2019,\n\n\u2018acquainted\u2019,\n\u2018with',\n\u2018him'))\n\nNext, let\u2019s build a mapping of words to indexes in our encoding:\n\n# In[7]:\nword_list = sorted(set(clean_words (text) ))\nword2index_dict = (word: i for (i, word) in enumerate(word_list))\n\nlen(word2index_dict), word2index_dict['impossible']\n\n# Out[7]:\n(7261, 3394)\n\nNote that word2index_dict is now a dictionary with words as keys and an integer as a\nvalue. We will use it to efficiently find the index of a word as we one-hot encode it.\nLet\u2019s now focus on our sentence: we break it up into words and one-hot encode it\u2014\n\nthat is, we populate a tensor with one one-hot-encoded vector per word. We create an\nempty vector and assign the one-hot-encoded values of the word in the sentence:\n\n# In[8):\nword_t = torch.zeros(len(words_in_line), len(word2index_dict) }\nfor i, word in enumerate (words_in_line):\n\nword_index = word2index_dict [word]\n\nword_t[i) [word_index] = 1\n\nprint('({:2} (:4} ()}'.format(i, word_index, word)\n\nprint (word_t.shape)\n\n# Out[8]:\n\n3394 impossible\n4305 mr\n\n813 bennet\n3394 impossible\n7078 when\n\n3315 i\n\n415 am\n4436 not\n\n239 acquainted\n7148 with\n10 3215 him\ntorch.Size([{1l, 7261))\n\nBIN eRwWNH OS\n\n\u00a9\n\nAt this point, tensor represents one sentence of length 11 in an encoding space of size\n7,261, the number of words in our dictionary. Figure 4.6 compares the gist of our two\noptions for splitting text (and using the embeddings we'll look at in the next section).\n\nThe choice between character-level and word-level encoding leaves us to make a\ntrade-off. In many languages, there are significantly fewer characters than words: rep-\nresenting characters has us representing just a few classes, while representing words\nrequires us to represent a very large number of classes and, in any practical applica-\ntion, deal with words that are not in the dictionary. On the other hand, words convey\nmuch more meaning than individual characters, so a representation of words is con-\nsiderably more informative by itself. Given the stark contrast between these two\noptions, it is perhaps unsurprising that intermediate ways have been sought, found,\nand applied with great success: for example, the byte pair encoding method\u2019 starts with a\ndictionary of individual letters but then iteratively adds the most frequently observed\npairs to the dictionary until it reaches a prescribed dictionary size. Our example sen-\ntence might then be split into tokens like this:\u2019\n\n?Im|pos|s|ible|,|?Mr|.|?B|en|net|,|?impossible], | ?when|?I|?am|?not |\"\n?Pacquainted| ?with| ?him\n\nCHARACTER ONE-HOT\n\n' 10s\nAPE IC x12\n\u2018 104 SAAPES Ce\n\u00b0 2 0..0010..0\n\u00b0 uw \u2014\u2014\u2014\u2014\u2014 > 0...0100...0\ns us \u00ab\n8 us\n\\ oS\n8 a8\nu \\o8\ne (ol\nWORD LooKuP\nEMBEDDING MATRIX 126 x 300 SHAPE: |x 300\nWORD EMBEDDING\n3344 \u2014\u2014> 20 120 UA Om -.74\n\nONS OMT -0.42 -01% 1.21\nROW 3344-170 -084-020 (18 ig 7 *10 ~0.89 -0.20 18 ~u8\n\n0495 0.01 O04 124 -0.23 A\n\n126 O19 0.84 0.86 O14 \u00b0\n\nMULTIPLICATION WITH EMBEDDING MATRIX\n\nVARIOUS POSSIBILITIES FOR REPRESENTING\nTHE WORD \u201cIMPOSSIBLE\u201d\n\nFigure 4.6 Three ways to encode a word\n\nFor most things, our mapping is just splitting by words. But the rarer parts\u2014the capi-\ntalized Impossible and the name Bennet\u2014are composed of subunits.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.24,
                        "section_name": "Text embeddings",
                        "section_path": "./screenshots-images-2/chapter_5/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_24/7d053521-4161-47ac-919e-f74d0b67c60e.png",
                            "./screenshots-images-2/chapter_5/section_24/7021798b-34b4-43fb-b0f6-57deecfeccbb.png",
                            "./screenshots-images-2/chapter_5/section_24/546064ed-733b-447e-9412-7d548faf3f8f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Text embeddings\n\nOne-hot encoding is a very useful technique for representing categorical data in ten-\nsors. However, as we have anticipated, one-hot encoding starts to break down when\nthe number of items to encode is effectively unbound, as with words in a corpus. In\njust one book, we had over 7,000 items!\n\nWe certainly could do some work to deduplicate words, condense alternate spell-\nings, collapse past and future tenses into a single token, and that kind of thing. Still, a\ngeneral-purpose English-language encoding would be Auge. Even worse, every time we\nencountered a new word, we would have to add a new column to the vector, which\nwould mean adding a new set of weights to the model to account for that new vocabu-\nlary entry\u2014which would be painful from a training perspective.\n\nHow can we compress our encoding down to a more manageable size and put a\ncap on the size growth? Well, instead of vectors of many zeros and a single one, we can\n\nuse vectors of floating-point numbers. A vector of, say, 100 floating-point numbers can\nindeed represent a large number of words. The trick is to find an effective way to map\nindividual words into this 100-dimensional space in a way that facilitates downstream\nlearning. This is called an embedding.\n\nIn principle, we could simply iterate over our vocabulary and generate a set of 100\nrandom floating-point numbers for each word. This would work, in that we could\ncram a very large vocabulary into just 100 numbers, but it would forgo any concept of\ndistance between words based on meaning or context. A model using this word\nembedding would have to deal with very little structure in its input vectors. An ideal\nsolution would be to generate the embedding in such a way that words used in similar\ncontexts mapped to nearby regions of the embedding.\n\nWell, if we were to design a solution to this problem by hand, we might decide to\nbuild our embedding space by choosing to map basic nouns and adjectives along the\naxes. We can generate a 2D space where axes map to nouns\u2014/ruit (0.0-0.33), flower\n(0.33-0.66), and dog (0.66-1.0)\u2014and adjectives\u2014red (0.0-0.2), orange (0.2-0.4), yellow\n(0.40.6), white (0.6-0.8), and brown (0.8-1.0). Our goal is to take actual fruit, flowers,\nand dogs and lay them out in the embedding.\n\nAs we start embedding words, we can map apple to a number in the fruit and red\nquadrant. Likewise, we can easily map tangerine, lemon, lychee, and kiwi (to round out\nour list of colorful fruits). Then we can start on flowers, and assign rose, poppy, daffodil,\nlily, and ... Hmm. Not many brown flowers out there. Well, sunflower can get flower, yel-\nlow, and brown, and then daisy can get flower, white, and yellow. Perhaps we should\nupdate kiwi to map close to fruit, brown, and green.\u00ae For dogs and color, we can embed\nredbone near red; uh, fox perhaps for orange, golden retriever for yellow, poodle for white, and\n... most kinds of dogs are brown.\n\nNow our embeddings look like figure 4.7. While doing this manually isn\u2019t really\nfeasible for a large corpus, note that although we had an embedding size of 2, we\ndescribed 15 different words besides the base 8 and could probably cram in quite a few\nmore if we took the time to be creative about it.\n\nAs you've probably already guessed, this kind of work can be automated. By pro-\ncessing a large corpus of organic text, embeddings similar to the one we just discussed\ncan be generated. The main differences are that there are 100 to 1,000 elements in\nthe embedding vector and that axes do not map directly to concepts: rather, concep-\ntually similar words map in neighboring regions of an embedding space whose axes\nare arbitrary floating-point dimensions.\n\nWhile the exact algorithms\u2019 used are a bit out of scope for what we're wanting to\nfocus on here, we'd just like to mention that embeddings are often generated using\nneural networks, trying to predict a word from nearby words (the context) in a sen-\ntence. In this case, we could start from one-hot-encoded words and use a (usually\n\n0.6\nYeLLow LEMON DAFFODIL GoLDEN RETRIEVER\nx\nou Rox\nOPRANCE ANGERINE POPPY x\n*\n\n0.2\n\nRep APPLE\nx x\n\nFRUIT\n0.0 x\n\n0.0 0.2 o4 0.6 0.8\n\nFigure 4.7 Our manual word embeddings\n\nrather shallow) neural network to generate the embedding. Once the embedding was\navailable, we could use it for downstream tasks.\n\nOne interesting aspect of the resulting embeddings is that similar words end up not\nonly clustered together, but also having consistent spatial relationships with other\nwords. For example, if we were to take the embedding vector for apple and begin to add\nand subtract the vectors for other words, we could begin to perform analogies like apple\n- red - sweet + yellow + sourand end up with a vector very similar to the one for lemon.\n\nMore contemporary embedding models\u2014with BERT and GPT-2 making headlines\neven in mainstream media\u2014are much more elaborate and are context sensitive: that\nis, the mapping of a word in the vocabulary to a vector is not fixed but depends on the\nsurrounding sentence. Yet they are often used just like the simpler classic embeddings\nwe've touched on here.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.25,
                        "section_name": "Text embeddings as a blueprint",
                        "section_path": "./screenshots-images-2/chapter_5/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_25/be130491-a737-4b6c-8361-b0e9caff5655.png",
                            "./screenshots-images-2/chapter_5/section_25/867ec9d3-76e4-4375-bf1c-82a0e33a3bfe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Text embeddings as a blueprint\n\nEmbeddings are an essential tool for when a large number of entries in the vocabulary\nhave to be represented by numeric vectors. But we won't be using text and text\nembeddings in this book, so you might wonder why we introduce them here. We\nbelieve that how text is represented and processed can also be seen as an example for\ndealing with categorical data in general. Embeddings are useful wherever one-hot\nencoding becomes cumbersome. Indeed, in the form described previously, they are\nan efficient way of representing one-hot encoding immediately followed by multiplica-\ntion with the matrix containing the embedding vectors as rows.\n\nIn non-text applications, we usually do not have the ability to construct the embed-\ndings beforehand, but we will start with the random numbers we eschewed earlier and\nconsider improving them part of our learning problem. This is a standard tech-\nnique\u2014so much so that embeddings are a prominent alternative to one-hot encod-\nings for any categorical data. On the flip side, even when we deal with text, improving\nthe prelearned embeddings while solving the problem at hand has become a common\npractice.'\u00b0\n\nWhen we are interested in co-occurrences of observations, the word embeddings\nwe saw earlier can serve as a blueprint, too. For example, recommender systems\u2014cus-\ntomers who liked our book also bought ...\u2014use the items the customer already inter-\nacted with as the context for predicting what else will spark interest. Similarly,\nprocessing text is perhaps the most common, well-explored task dealing with\nsequences; so, for example, when working on tasks with time series, we might look for\ninspiration in what is done in natural language processing.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.26,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_5/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_26/70ab9e88-f2b1-4e49-8bc5-dcb9cc74d6b1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\nWe've covered a lot of ground in this chapter. We learned to load the most common\ntypes of data and shape them for consumption bya neural network. Of course, there are\nmore data formats in the wild than we could hope to describe in a single volume. Some,\nlike medical histories, are too complex to cover here. Others, like audio and video, were\ndeemed less crucial for the path of this book. If you're interested, however, we provide\nshort examples of audio and video tensor creation in bonus Jupyter Notebooks provided\non the book\u2019s website (www.manning.com/books/deep-learning-with-pytorch) and in\nour code repository (https://github.com/deep-learning-with-pytorch/dlwpt-code/\ntree/master/plch4).\n\nNow that we're familiar with tensors and how to store data in them, we can move on\nto the next step towards the goal of the book: teaching you to train deep neural net-\nworks! The next chapter covers the mechanics of learning for simple linear models.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 6,
                "chapter_name": "The mechanics\nof learning",
                "chapter_path": "./screenshots-images-2/chapter_6",
                "sections": [
                    {
                        "section_id": 6.1,
                        "section_name": "The mechanics\nof learning",
                        "section_path": "./screenshots-images-2/chapter_6/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_1/0445abd0-bf03-4f8c-8b79-eace8401271f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "With the blooming of machine learning that has occurred over the last decade, the\nnotion of machines that learn from experience has become a mainstream theme in\nboth technical and journalistic circles. Now, how is it exactly that a machine learns?\nWhat are the mechanics of this process\u2014or, in words, what is the algorithm behind\nit? From the point of view of an observer, a learning algorithm is presented with\ninput data that is paired with desired outputs. Once learning has occurred, that\nalgorithm will be capable of producing correct outputs when it is fed new data that\nis similar enough to the input data it was trained on. With deep learning, this process\nworks even when the input data and the desired output are far from each other:\nwhen they come from different domains, like an image and a sentence describing\nit, as we saw in chapter 2.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.2,
                        "section_name": "A timeless lesson in modeling",
                        "section_path": "./screenshots-images-2/chapter_6/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_2/7f397191-8a17-4a95-80cc-41a77264df9f.png",
                            "./screenshots-images-2/chapter_6/section_2/f6b26e1e-47d8-41cd-9be4-dc34784241d3.png",
                            "./screenshots-images-2/chapter_6/section_2/f9a1c819-0380-4659-ac28-95c5684a2f98.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A timeless lesson in modeling\n\nBuilding models that allow us to explain input/output relationships dates back centu-\nries at least. When Johannes Kepler, a German mathematical astronomer (1571-1630),\nfigured out his three laws of planetary motion in the early 1600s, he based them on\ndata collected by his mentor Tycho Brahe during naked-eye observations (yep, seen\nwith the naked eye and written on a piece of paper). Not having Newton\u2019s law of grav-\nitation at his disposal (actually, Newton used Kepler\u2019s work to figure things out),\nKepler extrapolated the simplest possible geometric model that could fit the data.\nAnd, by the way, it took him six years of staring at data that didn\u2019t make sense to him,\ntogether with incremental realizations, to finally formulate these laws.' We can see this\nprocess in figure 5.1.\n\nBo KEPLER'S\n\nO (FIRST + SECOND) LAWS\n\n1M eanpiaTe\n\u2018 2 MODELS\nJOHANNES dh NN\n/\nnN an\n. i\u201d . aN :\n(3 ca \u2018.\n(3 rare (ECCENTRICITY\nTAL 4 7 OBSERVATIONS (SA LOT LARGER\n\u201cjt FOR MULTIPLE THAN THE EARTH'S)\n_ PLANETS\n\nFigure 5.1 Johannes Kepler considers multiple candidate models that might fit the data at hand, settling\non an ellipse.\n\nKepler\u2019s first law reads: \u201cThe orbit of every planet is an ellipse with the Sun at one of\nthe two foci.\u201d He didn\u2019t know what caused orbits to be ellipses, but given a set of obser-\nvations for a planet (or a moon of a large planet, like Jupiter), he could estimate the\nshape (the eccentricity) and size (the semi-latus rectum) of the ellipse. With those two\nparameters computed from the data, he could tell where the planet might be during\n\nits journey in the sky. Once he figured out the second law\u2014\u201cA line joining a planet\nand the Sun sweeps out equal areas during equal intervals of time\u201d\u2014he could also tell\nwhen a planet would be at a particular point in space, given observations in time.\u201d\n\nSo, how did Kepler estimate the eccentricity and size of the ellipse without comput-\ners, pocket calculators, or even calculus, none of which had been invented yet? We\ncan learn how from Kepler's own recollection, in his book New Astronomy, or from how\nJ. V. Field put it in his series of articles, \u201cThe origins of proof,\u201d (http://mng.bz/9007):\n\nEssentially, Kepler had to try different shapes, using a certain number of observations to find\n\nthe curve, then use the curve to find some more positions, for times when he had observations\n\navailable, and then check whether these calculated positions agreed with the observed ones.\n\n\u2014J. V. Field\nSo let\u2019s sum things up. Over six years, Kepler\n\na Got lots of good data from his friend Brahe (not without some struggle)\n\n2 Tried to visualize the heck out of it, because he felt there was something fishy\ngoing on\n\n2 Chose the simplest possible model that had a chance to fit the data (an ellipse)\n\n4 Split the data so that he could work on part of it and keep an independent set\nfor validation\n\ns Started with a tentative eccentricity and size for the ellipse and iterated until the\nmodel fit the observations\n\ne Validated his model on the independent observations\n\n7 Looked back in disbelief\n\nThere\u2019s a data science handbook for you, all the way from 1609. The history of science\nis literally constructed on these seven steps. And we have learned over the centuries\nthat deviating from them is a recipe for disaster.\u2019\n\nThis is exactly what we will set out to do in order to learn something from data. In\nfact, in this book there is virtually no difference between saying that we'll fit the data\nor that we'll make an algorithm /earn from data. The process always involves a func-\ntion with a number of unknown parameters whose values are estimated from data: in\nshort, a model.\n\nWe can argue that learning from data presumes the underlying model is not engi-\nneered to solve a specific problem (as was the ellipse in Kepler\u2019s work) and is instead\ncapable of approximating a much wider family of functions. A neural network would\nhave predicted Tycho Brahe\u2019s trajectories really well without requiring Kepler's flash\nof insight to try fitting the data to an ellipse. However, Sir Isaac Newton would have\nhad a much harder time deriving his laws of gravitation from a generic model.\n\nIn this book, we\u2019re interested in models that are not engineered for solving a spe-\ncific narrow task, but that can be automatically adapted to specialize themselves for\nany one of many similar tasks using input and output pairs\u2014in other words, general\nmodels trained on data relevant to the specific task at hand. In particular, PyTorch is\ndesigned to make it easy to create models for which the derivatives of the fitting error,\nwith respect to the parameters, can be expressed analytically. No worries if this last\nsentence didn\u2019t make any sense at all; coming next, we have a full section that hope-\nfully clears it up for you.\n\nThis chapter is about how to automate generic function-fitting. After all, this is\nwhat we do with deep learning\u2014deep neural networks being the generic functions\nwe\u2019re talking about\u2014and PyTorch makes this process as simple and transparent as\npossible. In order to make sure we get the key concepts right, we'll start with a model\nthat is a lot simpler than a deep neural network. This will allow us to understand the\nmechanics of learning algorithms from first principles in this chapter, so we can move\nto more complicated models in chapter 6.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.3,
                        "section_name": "Learning is just parameter estimation",
                        "section_path": "./screenshots-images-2/chapter_6/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_3/43c4606d-eb20-4094-8e60-1952f8fb9cbd.png",
                            "./screenshots-images-2/chapter_6/section_3/aaeb6339-dff9-47f8-a2a8-c57ed737a804.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning is just parameter estimation\n\nIn this section, we'll learn how we can take data, choose a model, and estimate the\nparameters of the model so that it will give good predictions on new data. To do so,\nwe'll leave the intricacies of planetary motion and divert our attention to the second-\nhardest problem in physics: calibrating instruments.\n\nFigure 5.2 shows the high-level overview of what we'll implement by the end of the\nchapter. Given input data and the corresponding desired outputs (ground truth), as\nwell as initial values for the weights, the model is fed input data (forward pass), and a\nmeasure of the error is evaluated by comparing the resulting outputs to the ground\ntruth. In order to optimize the parameter of the model\u2014its weights\u2014the change in\nthe error following a unit change in weights (that is, the gradient of the error with\nrespect to the parameters) is computed using the chain rule for the derivative of a\ncomposite function (backward pass). The value of the weights is then updated in the\ndirection that leads to a decrease in the error. The procedure is repeated until the\nerror, evaluated on unseen data, falls below an acceptable level. If what we just said\nsounds obscure, we've got a whole chapter to clear things up. By the time we\u2019re done,\nall the pieces will fall into place, and this paragraph will make perfect sense.\n\nWe're now going to take a problem with a noisy dataset, build a model, and imple-\nment a learning algorithm for it. When we start, we'll be doing everything by hand,\nbut by the end of the chapter we'll be letting PyTorch do all the heavy lifting for us.\nWhen we finish the chapter, we will have covered many of the essential concepts that\nunderlie training deep neural networks, even if our motivating example is very simple\nand our model isn\u2019t actually a neural network (yet!).\n\nTHE LEARNING PROCESS,\n\nu og\na a DESIRED OUTPUTS\nwees g Opa\nqo uge (GROUND TRUTH)\n0\nZX\nmow L, ie ri Gg ACTUAL outputs\nWARD GIVEN CURRENT\n\u2014_ a4 t uel WEIGHTS\n\nCHANGE WEIGHTS To t\nDECREASE ERRORS \u20ac\u2014 ERRORS (LOSS FUNCTION)\n\n(TERATE\nBACKWARD\n\nnew weots Bg QD vaupation\nOg > a rm\n\nFigure 5.2 Our mental model of the learning process\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.4,
                        "section_name": "A hot problem",
                        "section_path": "./screenshots-images-2/chapter_6/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_4/76dc1b9f-ff6b-47a0-8216-0fa96eb7049f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A hot problem\n\nWe just got back from a trip to some obscure location, and we brought back a fancy,\nwall-mounted analog thermometer. It looks great, and it\u2019s a perfect fit for our living\nroom. Its only flaw is that it doesn\u2019t show units. Not to worry, we've got a plan: we'll\nbuild a dataset of readings and corresponding temperature values in our favorite\nunits, choose a model, adjust its weights iteratively until a measure of the error is low\nenough, and finally be able to interpret the new readings in units we understand.*\n\nLet\u2019s try following the same process Kepler used. Along the way, we'll use a tool he\nnever had available: PyTorch!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.5,
                        "section_name": "Gathering some data",
                        "section_path": "./screenshots-images-2/chapter_6/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_5/d38a04af-e69c-40e5-85ed-300d5f6cc5a5.png",
                            "./screenshots-images-2/chapter_6/section_5/05b4172d-2294-4c19-9dce-3eb7cfb6e842.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Gathering some data\n\nWe'll start by making a note of temperature data in good old Celsius\u00ae and measure-\nments from our new thermometer, and figure things out. After a couple of weeks,\nhere\u2019s the data (code/plch5/1_parameter_estimation.ipynb):\n\n# In(2]:\ntie = (0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0)\n\ntu = (35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4)\ntle = torch.tensor(t_c)\ntu = torch.tensor(t_u)\n\nHere, the t_c values are temperatures in Celsius, and the t_u values are our unknown\nunits. We can expect noise in both measurements, coming from the devices them-\nselves and from our approximate readings. For convenience, we've already put the\ndata into tensors; we'll use it in a minute.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.6,
                        "section_name": "Visualizing the data",
                        "section_path": "./screenshots-images-2/chapter_6/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_6/f448ceca-90c5-44d5-bec9-385bee511072.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "5.2.3 Visualizing the data\n\nA quick plot of our data in figure 5.3 tells us that it\u2019s noisy, but we think there\u2019s a pat-\ntern here.\n\nTEMPERATURE (\u00b0CELSIUS)\n\n\u00b0\n\n25\n\n-5\n\nFigure 5.3 Our unknown\ndata just might follow a\n\n20 30 4o 50 Go To 80\nMEASUREMENT linear model.\n\nNOTE Spoiler alert: we know a linear model is correct because the problem\nand data have been fabricated, but please bear with us. It\u2019s a useful motivating\nexample to build our understanding of what PyTorch is doing under the\nhood.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.7,
                        "section_name": "Choosing a linear model as a first try",
                        "section_path": "./screenshots-images-2/chapter_6/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_7/1306e95a-2be8-472e-a851-fa911a276e5a.png",
                            "./screenshots-images-2/chapter_6/section_7/9d6077bd-f417-4774-b991-ed8d959862e1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Choosing a linear model as a first try\n\nIn the absence of further knowledge, we assume the simplest possible model for con-\nverting between the two sets of measurements, just like Kepler might have done. The\ntwo may be linearly related\u2014that is, multiplying t_u by a factor and adding a constant,\nwe may get the temperature in Celsius (up to an error that we omit):\n\nte=w* tlu+b\n\nIs this a reasonable assumption? Probably; we'll see how well the final model per-\nforms. We chose to name w and b after weight and bias, two very common terms for lin-\near scaling and the additive constant\u2014we\u2019ll bump into those all the time.\u00ae\n\nOK, now we need to estimate wand b, the parameters in our model, based on the data\nwe have. We must do it so that temperatures we obtain from running the unknown tem-\nperatures t_u through the model are close to temperatures we actually measured in Cel-\nsius. If that sounds like fitting a line through a set of measurements, well, yes, because\nthat\u2019s exactly what we're doing. We'll go through this simple example using PyTorch and\nrealize that training a neural network will essentially involve changing the model for a\nslightly more elaborate one, with a few (or a metric ton) more parameters.\n\nLet's flesh it out again: we have a model with some unknown parameters, and we\nneed to estimate those parameters so that the error between predicted outputs and\nmeasured values is as low as possible. We notice that we still need to exactly define a\nmeasure of the error. Such a measure, which we refer to as the loss function, should be\nhigh if the error is high and should ideally be as low as possible for a perfect match.\nOur optimization process should therefore aim at finding w and b so that the loss\nfunction is ata minimum.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.8,
                        "section_name": "Less loss is what we want",
                        "section_path": "./screenshots-images-2/chapter_6/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_8/50d54e82-1cc0-429c-ab09-b42b0ae157a7.png",
                            "./screenshots-images-2/chapter_6/section_8/9d307894-b0dd-418f-ae56-9a78b40bc188.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Less loss is what we want\n\nA loss function (or cost function) is a function that computes a single numerical value\nthat the learning process will attempt to minimize. The calculation of loss typically\ninvolves taking the difference between the desired outputs for some training samples\nand the outputs actually produced by the model when fed those samples. In our case,\nthat would be the difference between the predicted temperatures t_p output by our\nmodel and the actual measurements: t_p - t_c.\n\nWe need to make sure the loss function makes the loss positive both when t_p is\ngreater than and when itis less than the true t_c, since the goal is for t_p to match t_c.\nWe have a few choices, the most straightforward being |t_p - t_c| and (t_p - t_c)*2.\nBased on the mathematical expression we choose, we can emphasize or discount certain\nerrors. Conceptually, a loss function is a way of prioritizing which errors to fix from our\ntraining samples, so that our parameter updates result in adjustments to the outputs for\nthe highly weighted samples instead of changes to some other samples\u2019 output that had\na smaller loss.\n\nBoth of the example loss functions have a clear minimum at zero and grow mono-\ntonically as the predicted value moves further from the true value in either direction.\nBecause the steepness of the growth also monotonically increases away from the mini-\nmum, both of them are said to be convex. Since our model is linear, the loss as a function\nof wand bis also convex.\u2019 Cases where the loss is a convex function of the model param-\neters are usually great to deal with because we can find a minimum very efficiently\n\nthrough specialized algorithms. However, we will instead use less powerful but more\ngenerally applicable methods in this chapter. We do so because for the deep neural net-\nworks we are ultimately interested in, the loss is not a convex function of the inputs.\n\nFor our two loss functions |t_p - t_c| and (t_p - t_c)*2, as shown in figure 5.4,\nwe notice that the square of the differences behaves more nicely around the mini-\nmum: the derivative of the error-squared loss with respect to t_p is zero when t_p\nequals t_c. The absolute value, on the other hand, has an undefined derivative right\nwhere we'd like to converge. This is less of an issue in practice than it looks like, but\nwe'll stick to the square of differences for the time being.\n\n|x-<| (-*)\n\nX x X x\n\nFigure 5.4 Absolute difference versus difference squared\n\nIt\u2019s worth noting that the square difference also penalizes wildly wrong results more than\nthe absolute difference does. Often, having more slightly wrong results is better than hav-\ning a few wildly wrong ones, and the squared difference helps prioritize those as desired.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.9,
                        "section_name": "From problem back to PyTorch",
                        "section_path": "./screenshots-images-2/chapter_6/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_9/a825052c-2432-4dc2-88a5-131684d7bd83.png",
                            "./screenshots-images-2/chapter_6/section_9/7884ba2b-6ae8-40ed-a20e-364376b2c856.png",
                            "./screenshots-images-2/chapter_6/section_9/744e641f-e4d0-4527-91fa-ff236ca084a2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "From problem back to PyTorch\n\nWe've figured out the model and the loss function\u2014we've already got a good part of\nthe high-level picture in figure 5.2 figured out. Now we need to set the learning pro-\ncess in motion and feed it actual data. Also, enough with math notation; let\u2019s switch to\nPyTorch\u2014after all, we came here for the fun.\n\nWe've already created our data tensors, so now let\u2019s write out the model as a\nPython function:\n\n# In(3]:\ndef model(t_u, w, b):\nreturn w * tlu +b\n\nWe\u2019re expecting t_u, w, and b to be the input tensor, weight parameter, and bias\nparameter, respectively. In our model, the parameters will be PyTorch scalars (aka\n\nzero-dimensional tensors), and the product operation will use broadcasting to yield\nthe returned tensors. Anyway, time to define our loss:\n\n# In[4]:\n\ndef loss_fn(t_p, te):\nsquared_diffs = (t_p - t_c)**2\nreturn squared_diffs.mean()\n\nNote that we are building a tensor of differences, taking their square element-wise,\nand finally producing a scalar loss function by averaging all of the elements in the\nresulting tensor. It is a mean square loss.\n\nWe can now initialize the parameters, invoke the model,\n\n# In[5):\nw = torch.ones(())\nb = torch.zeros(())\n\ntp = model(t_u, w, b)\ntp\n\n# Out[5):\ntensor ([(35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n21.8000, 48.4000, 60.4000, 68.4000))\n\nand check the value of the loss:\n\n# In[6):\nloss = loss_fn(t_p, t_c)\nloss\n\n# Out(6):\ntensor (1763.8846)\n\nWe implemented the model and the loss in this section. We've finally reached the\nmeat of the example: how do we estimate w and b such that the loss reaches a mini-\nmum? We'll first work things out by hand and then learn how to use PyTorch\u2019s super-\npowers to solve the same problem in a more general, off-the-shelf way.\n\nBroadcasting\nWe mentioned broadcasting in chapter 3, and we promised to look at it more carefully\nwhen we need it. In our example, we have two scalars (zero-dimensional tensors) w\n\nand b, and we multiply them with and add them to vectors (one-dimensional tensors)\nof length b.\n\nUsually\u2014and in early versions of PyTorch, too\u2014we can only use element-wise binary\noperations such as addition, subtraction, multiplication, and division for arguments\nof the same shape. The entries in matching positions in each of the tensors will be\nused to calculate the corresponding entry in the result tensor.\n\n(continued)\nBroadcasting, which is popular in NumPy and adapted by PyTorch, relaxes this assump-\ntion for most binary operations. It uses the following rules to match tensor elements:\n\n= For each index dimension, counted from the back, if one of the operands is\nsize 1 in that dimension, PyTorch will use the single entry along this dimen-\nsion with each of the entries in the other tensor along this dimension.\n\n= If both sizes are greater than 1, they must be the same, and natural matching\nis used.\n\n= If one of the tensors has more index dimensions than the other, the entirety\nof the other tensor will be used for each entry along these dimensions.\n\nThis sounds complicated (and it can be error-prone if we don't pay close attention, which\nis why we have named the tensor dimensions as shown in section 3.4), but usually,\nwe can either write down the tensor dimensions to see what happens or picture what\nhappens by using space dimensions to show the broadcasting, as in the following figure.\n\nOf course, this would all be theory if we didn\u2019t have some code examples:\n\n# In[(7):\n\ntorch.ones(())\n\ntorch.ones (3,1)\n\ntorch.ones (1,3)\n\ntorch.ones(2, 1, 1)\n\nprint (f\"shapes: x: {x.shape}, y: {y.shape}\")\n\na Do by be Ds \u00bb |\n[= | , .\n\nprint (\u00a3\" z: (z.shape}, a: {a.shape}\")\nprint(\"x * y:\", (x * y).shape)\n\nprint(\"y * z:\", (y * z).shape)\n\nprint(\"y * z * a:\", (y * z * a).shape)\n\nBPN K\n\n. 42> | C43 = | Cae\n44 + Dy| ag + bz| Gq + bs) a4 + Dg\n\n# Out[7):\n\nshapes: x: torch.Size([}), y: torch.Size([3, 1])\nz: torch.Size([1, 3]), a: torch.Size([2, 1, 1)})\nx * y: torch.Size([3, 1))\n\ny * z: torch.Size((3, 3])\ny * z * a: torch.Size([2, 3, 3])\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.1,
                        "section_name": "Down along the gradient",
                        "section_path": "./screenshots-images-2/chapter_6/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_10/807cb692-726a-42fa-a4e4-cd52fbfc8d0a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Down along the gradient\n\nWe'll optimize the loss function with respect to the parameters using the gradient\ndescent algorithm. In this section, we'll build our intuition for how gradient descent\nworks from first principles, which will help us a lot in the future. As we mentioned,\nthere are ways to solve our example problem more efficiently, but those approaches\naren't applicable to most deep learning tasks. Gradient descent is actually a very sim-\nple idea, and it scales up surprisingly well to large neural network models with mil-\n\nlions of parameters.\n\nLet\u2019s start with a mental image, which we\nconveniently sketched out in figure 5.5. Sup-\npose we are in front of a machine sporting two\nknobs, labeled w and b. We are allowed to see\nthe value of the loss on a screen, and we are\ntold to minimize that value. Not knowing the\neffect of the knobs on the loss, we start fid-\ndling with them and decide for each knob\nwhich direction makes the loss decrease. We\ndecide to rotate both knobs in their direction\nof decreasing loss. Suppose we're far from the\noptimal value: we'd likely see the loss decrease\nquickly and then slow down as it gets closer to\nthe minimum. We notice that at some point,\nthe loss climbs back up again, so we invert the\ndirection of rotation for one or both knobs.\nWe also learn that when the loss changes\nslowly, it\u2019s a good idea to adjust the knobs\n\nmore finely, to avoid reaching the point where the loss goes back up. After a while,\n\neventually, we converge to a minimum.\n\nFigure 5.5 A cartoon depiction of the\noptimization process, where a person\nwith knobs for w and b searches for the\ndirection to turn the knobs that makes\nthe loss decrease\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.11,
                        "section_name": "Decreasing loss",
                        "section_path": "./screenshots-images-2/chapter_6/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_11/b8a27255-4122-4419-85a5-58df9cdbc1c9.png",
                            "./screenshots-images-2/chapter_6/section_11/a66c6ac4-6370-4e90-a608-446bd814a045.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Decreasing loss\n\nGradient descent is not that different from the scenario we just described. The idea is\nto compute the rate of change of the loss with respect to each parameter, and modify\neach parameter in the direction of decreasing loss. Just like when we were fiddling\nwith the knobs, we can estimate the rate of change by adding a small number to w and\nband seeing how much the loss changes in that neighborhood:\n\n# In[8):\ndelta = 0.1\n\nloss_rate_of_change_w = \\\n(loss_fn(model(t_u, w + delta, b), tle) -\nloss_fn(model(t_u, w - delta, b), tie)) / (2.0 * delta)\n\nThis is saying that in the neighborhood of the current values of w and b, a unit\nincrease in w leads to some change in the loss. If the change is negative, then we need\nto increase w to minimize the loss, whereas if the change is positive, we need to\ndecrease w. By how much? Applying a change to w that is proportional to the rate of\nchange of the loss is a good idea, especially when the loss has several parameters: we\napply a change to those that exert a significant change on the loss. It is also wise to\nchange the parameters slowly in general, because the rate of change could be dramat-\nically different at a distance from the neighborhood of the current w value. Therefore,\nwe typically should scale the rate of change by a small factor. This scaling factor has\nmany names; the one we use in machine learning is learning_rate:\n\n# In[9]:\nlearning_rate = le-2\n\nw = w - learning_rate * loss_rate_of_change_w\nWe can do the same with b:\n\n# In[10]:\n\nloss_rate_of_change_b = \\\n{loss_fn(model(t_u, w, b + delta), tie) -\nloss_fn(model(t_u, w, b - delta), t_e)) / (2.0 * delta)\n\nb= b - learning_rate * loss_rate_of_change_b\n\nThis represents the basic parameter-update step for gradient descent. By reiterating\nthese evaluations (and provided we choose a small enough learning rate), we will\nconverge to an optimal value of the parameters for which the loss computed on the\ngiven data is minimal. We'll show the complete iterative process soon, but the way we\njust computed our rates of change is rather crude and needs an upgrade before we\nmove on. Let\u2019s see why and how.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.12,
                        "section_name": "Getting analytical",
                        "section_path": "./screenshots-images-2/chapter_6/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_12/72321005-f43b-4dcc-9c10-73e20565be90.png",
                            "./screenshots-images-2/chapter_6/section_12/8d59d8aa-b211-44c4-a1bc-076237f285e7.png",
                            "./screenshots-images-2/chapter_6/section_12/12210fdb-1d22-47bb-8de2-0227af9707ed.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Getting analytical\n\nComputing the rate of change by using repeated evaluations of the model and loss in\norder to probe the behavior of the loss function in the neighborhood of w and b\ndoesn\u2019t scale well to models with many parameters. Also, it is not always clear how\nlarge the neighborhood should be. We chose delta equal to 0.1 in the previous sec-\ntion, but it all depends on the shape of the loss as a function of w and b. If the loss\nchanges too quickly compared to delta, we won't have a very good idea of in which\ndirection the loss is decreasing the most.\n\nWhat if we could make the neighborhood infinitesimally small, as in figure 5.6?\nThat's exactly what happens when we analytically take the derivative of the loss with\nrespect to a parameter. In a model with two or more parameters like the one we\u2019re\ndealing with, we compute the individual derivatives of the loss with respect to each\nparameter and put them in a vector of derivatives: the gradient.\n\nFigure 5.6 Differences in the\nestimated directions for descent\nwhen evaluating them at discrete\nlocations versus analytically\n\nCOMPUTING THE DERIVATIVES\n\nIn order to compute the derivative of the loss with respect to a parameter, we cat\napply the chain rule and compute the derivative of the loss with respect to its inpu\n(which is the output of the model), times the derivative of the model with respect t\nthe parameter:\n\nd@ loss_fn / dw = (d loss_fn / d t_p) * (d t_p / d w)\n\nRecall that our model is a linear function, and our loss is a sum of squares. Let\u2019s figur\nout the expressions for the derivatives. Recalling the expression for the loss:\n\n# In[4):\n\ndef loss_fn(t_p, tc):\nsquared_diffs = (t_p - t_c)**2\nreturn squared_diffs.mean()\n\nRemembering thatd x*2 / d x = 2 x, we get\n\n# In[1i}:\ndef dloss_fn(t_p, t_c): The division is from the\n\ndsq_diffs = 2 * (t_p - t.ec) / t.p.size(0) \u2014/ derivative of mean.\nreturn dsq_diffs\n\nAPPLYING THE DERIVATIVES TO THE MODEL\nFor the model, recalling that our model is\n\n# In[3):\n\ndef model(t_u, w, b):\nreturn w * tu +b\n\nwe get these derivatives:\n\n# In(12]:\ndef dmodel_dw(t_u, w, b):\nreturn t_u\n\n# In(13):\ndef dmodel_db(t_u, w, b):\nreturn 1.0\n\nDEFINING THE GRADIENT FUNCTION\nPutting all of this together, the function returning the gradient of the loss with respect\nto wand b is\n\nThe summation is the reverse of the\n\n# In(14): broadcasting we implicitly do when\ndef grad_fn(t_u, t_c, t_p, w, b): applying the parameters to an entire\ndloss_dtp = dloss_fn(t_p, t_c) vector of inputs in the model.\n\ndloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\ndloss_db = dloss_dtp * dmodel_db(t_u, w, b)\nreturn torch.stack((dloss_dw.sum(), dloss_db.sum(}])\n\nThe same idea expressed in mathematical notation is shown in figure 5.7. Again,\nwe're averaging (that is, summing and dividing by a constant) over all the data points\nto get a single scalar quantity for each partial derivative of the loss.\n\nloss L (wm .00))\n\nVie [e We Wm YW dm\nt\u201d yw | Yb ym ww. Ym Yb\na\n\ngga ent \\ a Tl poramel\u00e9es\ndecvetives MM, ils)\n\nFigure 5.7 The derivative of the loss function with respect to the weights\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.13,
                        "section_name": "Iterating to fit the model",
                        "section_path": "./screenshots-images-2/chapter_6/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_13/47f864a6-b920-490b-a677-7ed00b4bd8ce.png",
                            "./screenshots-images-2/chapter_6/section_13/3f7eaddb-5a0f-4e9e-a68a-eede4b6eab7e.png",
                            "./screenshots-images-2/chapter_6/section_13/e9fe86bc-4558-4fd1-92bd-0717c14af528.png",
                            "./screenshots-images-2/chapter_6/section_13/b46f1fa3-a223-45f1-a663-6c4b0577a98b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Iterating to fit the model\n\nWe now have everything in place to optimize our parameters. Starting from a tentative\nvalue for a parameter, we can iteratively apply updates to it for a fixed number of iter-\nations, or until w and b stop changing. There are several stopping criteria; for now,\nwe'll stick to a fixed number of iterations.\n\nTHE TRAINING LOOP\nSince we're at it, let\u2019s introduce another piece of terminology. We call a training itera-\ntion during which we update the parameters for all of our training samples an epoch.\n\nThe complete training loop looks like this (code/plch5/1_parameter_estimation\n-ipynb):\n\n# In[15):\ndef training_loop(n_epochs, learning_rate, params, t_u, t_c):\nfor epoch in range(1, n_epochs + 1):\nw, b = params\n\nt_p = model(t_u, w, b) \u00abb Forward pass\nloss = loss_fn(t_p, t_c)\ngrad = grad_fn(t_u, t_e, t_p, w, b) \u00abt Backward pass\n\nparams = params - learning_rate * grad\n\nprint('Epoch td, Loss %f' % (epoch, float(loss))) << This logging line can\n\nreturn params be very verbose.\n\nThe actual logging logic used for the output in this text is more complicated (see cell\n15 in the same notebook: http://mng.bz/pBB8), but the differences are unimportant\nfor understanding the core concepts in this chapter.\n\nNow, let\u2019s invoke our training loop:\n\n# In[17):\ntraining_loop(\nn_epochs = 100,\nlearning_rate = le-2,\nparams = torch.tensor({1.0, 0.0)),\n\ntu = tlu,\ntie = tle)\n# Out[1i7):\n\nEpoch i, Loss 1763.884644\nParams: tensor([-44.1730, -0.8260))\nGrad: tensor ([4517.2969, 82.6000))\nEpoch 2, Loss 5802485.500000\nParams: tensor ([2568.4014, 45.1637))\n\nGrad: tensor ([-261257.4219, ~4598.9712))\nEpoch 3, Loss 19408035840.000000\nParams: tensor ([-148527.7344, ~2616.3933))\n\nGrad: tensor ([15109614.0000, 266155.7188))\nEpoch 10, Loss 90901154706620645225508955521810432.000000\nParams: tensor([3.2144e+17, 5.6621e+15))\nGrad: tensor ([-3.2700e+19, -5.7600e+17))\nEpoch 11, Loss inf\nParams: tensor([-1.8590e+19, -3.2746e+17])\nGrad: tensor ([1.8912e+21, 3.3313e+19))\n\ntensor ([-1.8590e+19, -3.2746e+17])\n\nOVERTRAINING\n\nWait, what happened? Our training process literally blew up, leading to losses becom-\ning inf. This is a clear sign that params is receiving updates that are too large, and\ntheir values start oscillating back and forth as each update overshoots and the next\novercorrects even more. The optimization process is unstable: it diverges instead of\nconverging to a minimum. We want to see smaller and smaller updates to params, not\nlarger, as shown in figure 5.8.\n\nA\n\nFigure 5.8 Top: Diverging optimization on a convex function (parabola-like) due to large steps.\nBottom: Converging optimization with small steps.\n\nHow can we limit the magnitude of learning_rate * grad? Well, that looks easy. We\ncould simply choose a smaller learning_rate, and indeed, the learning rate is one of\nthe things we typically change when training does not go as well as we would like.\" We\nusually change learning rates by orders of magnitude, so we might try with le-3 or\nle-4, which would decrease the magnitude of the updates by orders of magnitude.\nLet's go with le-4 and see how it works out:\n\n# In(18]:\ntraining_loop(\nn_epochs = 100,\n\nlearning_rate = le-4,\n\nparams = torch.tensor([{1.0, 0.0)),\ntu = tu,\n\ntle = tle)\n\n# Out[i8):\nEpoch i, Loss 1763.884644\nParams: tensor([ 0.5483, -0.0083))\nGrad: tensor ([4517.2969, 82.6000))\nEpoch 2, Loss 323.090546\nParams: tensor([ 0.3623, -0.0118))\nGrad: tensor ([1859.5493, 35.7843))\nEpoch 3, Loss 78.929634\nParams: tensor([ 0.2858, -0.0135))\nGrad: tensor ([765.4667, 16.5122])\nEpoch 10, Loss 29.105242\nParams: tensor([ 0.2324, -0.0166))\nGrad: tensor([1.4803, 3.0544])\nEpoch 11, Loss 29.104168\nParams: tensor([ 0.2323, -0.0169))\nGrad: tensor([0.5781, 3.0384)])\n\nEpoch 99, Loss 29.023582\nParams: tensor([ 0.2327, -0.0435))\nGrad: tensor([-0.0533, 3.0226))\nEpoch 100, Loss 29.022669\nParams: tensor([ 0.2327, -0.0438))\nGrad: tensor([-0.0532, 3.0226))\n\ntensor([{ 0.2327, -0.0438))\n\nNice\u2014the behavior is now stable. But there\u2019s another problem: the updates to param-\neters are very small, so the loss decreases very slowly and eventually stalls. We could\nobviate this issue by making learning_rate adaptive: that is, change according to the\nmagnitude of updates. There are optimization schemes that do that, and we'll see one\ntoward the end of this chapter, in section 5.5.2.\n\nHowever, there\u2019s another potential troublemaker in the update term: the gradient\nitself. Let\u2019s go back and look at grad at epoch 1 during optimization.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.14,
                        "section_name": "Normalizing inputs",
                        "section_path": "./screenshots-images-2/chapter_6/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_14/339d4f06-e7b9-43fe-8030-2c9dd4993400.png",
                            "./screenshots-images-2/chapter_6/section_14/45fa3604-aa88-4317-b547-3082f26fa3aa.png",
                            "./screenshots-images-2/chapter_6/section_14/c3299af2-3c6e-4b0d-9a50-46af672c5c3d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Normalizing inputs\n\nWe can see that the first-epoch gradient for the weight is about 50 times larger than\nthe gradient for the bias. This means the weight and bias live in differently scaled\nspaces. If this is the case, a learning rate that\u2019s large enough to meaningfully update\none will be so large as to be unstable for the other; and a rate that\u2019s appropriate for\nthe other won\u2019t be large enough to meaningfully change the first. That means we're\nnot going to be able to update our parameters unless we change something about our\nformulation of the problem. We could have individual learning rates for each parame-\nter, but for models with many parameters, this would be too much to bother with; it\u2019s\nbabysitting of the kind we don\u2019t like.\n\nThere\u2019s a simpler way to keep things in check: changing the inputs so that the gra-\ndients aren\u2019t quite so different. We can make sure the range of the input doesn\u2019t get\ntoo far from the range of -1.0 to 1.0, roughly speaking. In our case, we can achieve\nsomething close enough to that by simply multiplying t_u by 0.1:\n\n# In[(19]:\ntun = 0.1 * tlu\n\nHere, we denote the normalized version of t_u by appending an n to the variable\nname. At this point, we can run the training loop on our normalized input:\n\n# In[20]:\n\ntraining_loop(\nn_epochs = 100,\nlearning_rate = le-2,\n\n.\nparams = torch.tensor([1.0, 0.0)), We've updated \u00a2_u to\n\nour new, rescaled t_un.\n\ntu tun,\ntle = te)\n# Out[20]:\n\nEpoch 1, Loss 80.364342\nParams: tensor([(1.7761, 0.1064))\nGrad: tensor ([(-77.6140, -10.6400))\nEpoch 2, Loss 37.574917\nParams: tensor((2.0848, 0.1303))\nGrad: tensor([-30.8623, -2.3864))\nEpoch 3, Loss 30.871077\nParams: tensor([(2.2094, 0.1217))\nGrad: tensor ((-12.4631, 0.8587))\n\nEpoch 10, Loss 29.030487\nParams: tensor([{ 2.3232, -0.0710))\nGrad: tensor([(-0.5355, 2.9295])\n\nEpoch 11, Loss 28.941875\nParams: tensor({ 2.3284, -0.1003)])\nGrad: tensor([-0.5240, 2.9264))\n\nEpoch 99, Loss 22.214186\nParams: tensor([{ 2.7508, -2.4910))\nGrad: tensor([-0.4453, 2.5208))\n\nEpoch 100, Loss 22.148710\nParams: tensor([{ 2.7553, -2.5162))\nGrad: tensor((-0.4446, 2.5165))\n\ntensor([ 2.7553, -2.5162))\n\nEven though we set our learning rate back to 1e-2, parameters don\u2019t blow up during\niterative updates. Let\u2019s take a look at the gradients: they're of similar magnitude, so\nusing a single learning_rate for both parameters works just fine. We could probably\ndo a better job of normalization than a simple rescaling by a factor of 10, but since\ndoing so is good enough for our needs, we're going to stick with that for now.\n\nNOTE The normalization here absolutely helps get the network trained, but\nyou could make an argument that it\u2019s not strictly needed to optimize the\nparameters for this particular problem. That's absolutely true! This problem is\nsmall enough that there are numerous ways to beat the parameters into sub-\nmission. However, for larger, more sophisticated problems, normalization is an\neasy and effective (if not crucial!) tool to use to improve model convergence.\n\nLet\u2019s run the loop for enough iterations to see the changes in params get small. We'll\nchange n_epochs to 5,000:\n\n# In[21):\nparams = training_loop(\nn_epochs = 5000,\nlearning_rate = le-2,\nparams = torch.tensor([{1.0, 0.0)),\ntu = tun,\ntic = tle,\nprint_params = False)\n\nparams\n\n# Out(21):\n\nEpoch 1, Loss 80.364342\nEpoch 2, Loss 37.574917\nEpoch 3, Loss 30.871077\n\nEpoch 10, Loss 29.030487\nEpoch 11, Loss 28.941875\n\nEpoch 99, Loss 22.214186\nEpoch 100, Loss 22.148710\n\nEpoch 4000, Loss 2.927680\nEpoch 5000, Loss 2.927648\n\ntensor({ 5.3671, -17.3012))\n\nGood: our loss decreases while we change parameters along the direction of gradient\ndescent. It doesn\u2019t go exactly to zero; this could mean there aren\u2019t enough iterations to\nconverge to zero, or that the data points don\u2019t sit exactly on a line. As we anticipated, our\nmeasurements were not perfectly accurate, or there was noise involved in the reading.\nBut look: the values for w and b look an awful lot like the numbers we need to use\nto convert Celsius to Fahrenheit (after accounting for our earlier normalization when\nwe multiplied our inputs by 0.1). The exact values would be w=5.5556 and b=-\n17.7778. Our fancy thermometer was showing temperatures in Fahrenheit the whole\ntime. No big discovery, except that our gradient descent optimization process works!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.15,
                        "section_name": "Visualizing (again)",
                        "section_path": "./screenshots-images-2/chapter_6/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_15/747c869a-9b4a-4f8f-8bf8-6512e63352c9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Visualizing (again)\n\nLet's revisit something we did right at the start: plotting our data. Seriously, this is the\nfirst thing anyone doing data science should do. Always plot the heck out of the data:\n\n# In[22]:\n\n$matplotlib inline ,\nfrom matplotlib import pyplot as plt Remember that we\u2019re training on the\nnormalized unknown units. We also\n\nt_p = model(t_un, *params) 4 | use argument unpacking.\n\nfig = plt.figure(dpi=600)\n\nplt.xlabel(\"Temperature (\u00b0Fahrenheit)\")\n\nplt.ylabel (\"Temperature (\u00b0Celsius)\") But we're plotting the\nplt.plot (t_u.numpy(), t_p.detach() .numpy()) <\u2014_! raw unknown values.\nplt.plot(t_u.numpy(), tle.numpy(), 'o')\n\nWe are using a Python trick called argument unpacking here: *params means to pass the\nelements of params as individual arguments. In Python, this is usually done with lists\nor tuples, but we can also use argument unpacking with PyTorch tensors, which are\nsplit along the leading dimension. So here, model(t_un, *params) is equivalent to\nmodel(t_un, params[0], params[1]).\n\nThis code produces figure 5.9. Our linear model is a good model for the data, it\nseems. It also seems Our measurements are somewhat erratic. We should either call\nour optometrist for a new pair of glasses or think about returning our fancy ther-\nmometer.\n\n20 30 4o 50 Go io) 30\nTEMPERATURE (\u00b0FAHRENHEIT)\n\nFigure 5.9 The plot of our linear-fit model (solid line) versus our input data (circles)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.16,
                        "section_name": "PyTorch\u2019s autograd: Backpropagating all things",
                        "section_path": "./screenshots-images-2/chapter_6/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_16/e82ddf6a-f6c2-4b23-93bd-81d01ecc147c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "PyTorch\u2019s autograd: Backpropagating all things\n\nIn our little adventure, we just saw a simple example of backpropagation: we com-\nputed the gradient of a composition of functions\u2014the model and the loss\u2014with\nrespect to their innermost parameters (w and b) by propagating derivatives backward\nusing the chain rule. The basic requirement here is that all functions we're dealing\nwith can be differentiated analytically. If this is the case, we can compute the gradi-\nent\u2014what we earlier called \u201cthe rate of change of the loss\"\u2014with respect to the\nparameters in one sweep.\n\nEven if we have a complicated model with millions of parameters, as long as our\nmodel is differentiable, computing the gradient of the loss with respect to the param-\neters amounts to writing the analytical expression for the derivatives and evaluating\nthem once. Granted, writing the analytical expression for the derivatives of a very deep\ncomposition of linear and nonlinear functions is not a lot of fun.\" It isn\u2019t particularly\nquick, either.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.17,
                        "section_name": "Computing the gradient automatically",
                        "section_path": "./screenshots-images-2/chapter_6/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_17/8220aa12-7d91-495d-8abd-2c71f1877cec.png",
                            "./screenshots-images-2/chapter_6/section_17/2e71c304-cdbe-40e6-b2f0-624816a2e580.png",
                            "./screenshots-images-2/chapter_6/section_17/0aef817e-56f7-4d38-86bf-268501039134.png",
                            "./screenshots-images-2/chapter_6/section_17/d7226c02-2030-42e1-94f1-6f69121a6cd5.png",
                            "./screenshots-images-2/chapter_6/section_17/5203a3e0-d509-440f-be8c-ba670fe6e909.png",
                            "./screenshots-images-2/chapter_6/section_17/c46fe1f5-029b-4860-b3ea-87fd29a55f1e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Computing the gradient automatically\n\nThis is when PyTorch tensors come to the rescue, with a PyTorch component called\nautograd. Chapter 3 presented a comprehensive overview of what tensors are and what\nfunctions we can call on them. We left out one very interesting aspect, however:\nPyTorch tensors can remember where they come from, in terms of the operations and\nparent tensors that originated them, and they can automatically provide the chain of\nderivatives of such operations with respect to their inputs. This means we won't need\nto derive our model by hand;!\u00b0 given a forward expression, no matter how nested,\nPyTorch will automatically provide the gradient of that expression with respect to its\ninput parameters.\n\nAPPLYING AUTOGRAD\n\nAt this point, the best way to proceed is to rewrite our thermometer calibration code,\nthis time using autograd, and see what happens. First, we recall our model and loss\nfunction.\n\nListing 5.1 code/pichS/2_autograd.ipynb\n\n# In[3):\ndef model(t_u, w, b):\nreturn w * tLu+b\n\n# In[4):\n\ndef loss_fn(t_p, te):\nsquared_diffs = (t_p - t_c)**2\nreturn squared _diffs.mean()\n\nLet's again initialize a parameters tensor:\n\n# In(5]:\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\n\nUSING THE GRAD ATTRIBUTE\nNotice the requires_grad=True argument to the tensor constructor? That argument\nis telling PyTorch to track the entire family tree of tensors resulting from operations\non params. In other words, any tensor that will have params as an ancestor will have\naccess to the chain of functions that were called to get from params to that tensor. In\ncase these functions are differentiable (and most PyTorch tensor operations will be),\nthe value of the derivative will be automatically populated as a grad attribute of the\nparams tensor.\n\nIn general, all PyTorch tensors have an attribute named grad. Normally, it\u2019s None:\n\n# In(6]:\nparams.grad is None\n\n# Out[6]:\nTrue\n\nAll we have to do to populate it is to start with a tensor with requires_grad set to\nTrue, then call the model and compute the loss, and then call backward on the loss\ntensor:\n\n# In[(7]:\nloss = loss_fn(model(t_u, *params), t_c)\nloss .backward()\n\nparams.grad\n\n# Out[7]:\ntensor ([4517.2969, 82.6000))\n\nAt this point, the grad attribute of params contains the derivatives of the loss with\nrespect to each element of params.\n\nWhen we compute our loss while the parameters w and b require gradients, in\naddition to performing the actual computation, PyTorch creates the autograd graph\nwith the operations (in black circles) as nodes, as shown in the top row of fig-\nure 5.10. When we call loss. backward(), PyTorch traverses this graph in the reverse\ndirection to compute the gradients, as shown by the arrows in the bottom row of\nthe figure.\n\n\u2014_ 5\n|\n\noe a re loss\nt\n\ngn godsNene teqvies af04 = Teve\nn_o ==\u201d\n\n| ss backward )\n4\nX-8\u2014 702 9 @= Loss\nwl bl\nb Figure 5.10 The forward graph\n\ncode dass gees Yess and backward graph of the model\n3 yw Bb as computed with autograd\n\nACCUMULATING GRAD FUNCTIONS\nWe could have any number of tensors with requires_grad set to True and any compo-\nsition of functions. In this case, PyTorch would compute the derivatives of the loss\nthroughout the chain of functions (the computation graph) and accumulate their val-\nues in the grad attribute of those tensors (the leaf nodes of the graph).\n\nAlert! Big gotcha ahead. This is something PyTorch newcomers\u2014and a lot of more\nexperienced folks, too\u2014trip up on regularly. We just wrote accumulate, not store.\n\nWARNING Calling backward will lead derivatives to accumulate at leaf nodes.\nWe need to zero the gradient explicitly after using it for parameter updates.\n\nLet\u2019s repeat together: calling backward will lead derivatives to accumulate at leaf nodes.\nSo if backward was called earlier, the loss is evaluated again, backward is called again\n(as in any training loop), and the gradient at each leaf is accumulated (that is,\nsummed) on top of the one computed at the previous iteration, which leads to an\nincorrect value for the gradient.\n\nIn order to prevent this from occurring, we need to zero the gradient explicitly at each\niteration. We can do this easily using the in-place zero_ method:\n\n# In[8):\nif params.grad is not None:\nparams .grad.zero_()\n\nNOTE You might be curious why zeroing the gradient is a required step\ninstead of zeroing happening automatically whenever we call backward.\nDoing it this way provides more flexibility and control when working with gra-\ndients in complicated models.\n\nHaving this reminder drilled into our heads, let\u2019s see what our autograd-enabled\ntraining code looks like, start to finish:\n\n# In[9]:\ndef training_loop(n_epochs, learning_rate, params, t_u, tc):\nfor epoch in range(1, n_epochs + 1):\n\nif params dead is not None: t This could be done at any point in the\nparams .grad.zero_() loop prior to calling loss.backward().\n\ntp = model(t_u, *params)\nloss = loss_fn(t_p, t_c)\nloss. backward ()\n\nThis is a somewhat cumbersome bit\nwith torch.no_grad(): <\u2014\u2014\u2014 of code, but as we'll see in the next\nparams -= learning_rate * params.grad section, it\u2019s not an issue in practice.\n\nif epoch % 500 ==\nprint('Epoch td, Loss tf' % (epoch, float(loss)})\n\nreturn params\n\nNote that our code updating params is not quite as straightforward as we might have\nexpected. There are two particularities. First, we are encapsulating the update in a\nno_grad context using the Python with statement. This means within the with block,\nthe PyTorch autograd mechanism should look away:'' that is, not add edges to the for-\nward graph. In fact, when we are executing this bit of code, the forward graph that\nPyTorch records is consumed when we call backward, leaving us with the params leaf\nnode. But now we want to change this leaf node before we start building a fresh for-\nward graph on top of it. While this use case is usually wrapped inside the optimizers\nwe discuss in section 5.5.2, we will take a closer look when we see another common use\nof no_grad in section 5.5.4.\n\nSecond, we update params in place. This means we keep the same params tensor\naround but subtract our update from it. When using autograd, we usually avoid in-\nplace updates because PyTorch\u2019s autograd engine might need the values we would be\nmodifying for the backward pass. Here, however, we are operating without autograd,\nand it is beneficial to keep the params tensor. Not replacing the parameters by assign-\ning new tensors to their variable name will become crucial when we register our\nparameters with the optimizer in section 5.5.2.\n\n\nLet\u2019s see if it works:\n\n# In[10): Adding\ntraining_loop( requires_grad=True is key.\nn_epochs = 5000,\nlearning_rate = le-2,\n\nparams = torch.tensor([1.0, 0.0), requires_grad=True), +t\ntu = tun,\ntle = tle) Again, we\u2019re using the\n\nnormalized t_un instead of t_u.\n# Out[10]:\nEpoch 500, Loss 7.860116\nEpoch 1000, Loss 3.828538\nEpoch 1500, Loss 3.092191\nEpoch 2000, Loss 2.957697\nEpoch 2500, Loss 2.933134\nEpoch 3000, Loss 2.928648\nEpoch 3500, Loss 2.927830\nEpoch 4000, Loss 2.927679\nEpoch 4500, Loss 2.927652\nEpoch 5000, Loss 2.927647\n\ntensor({ 5.3671, -17.3012), requires_grad=True)\n\nThe result is the same as we got previously. Good for us! It means that while we are\ncapable of computing derivatives by hand, we no longer need to.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.18,
                        "section_name": "Optimizers a la carte",
                        "section_path": "./screenshots-images-2/chapter_6/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_18/ab7cb451-6f41-4308-a17a-f0d025f2d9e5.png",
                            "./screenshots-images-2/chapter_6/section_18/0f82302f-3617-464b-8b67-e5ecb2614446.png",
                            "./screenshots-images-2/chapter_6/section_18/300472c4-11bb-431e-b46e-0aab790d89b8.png",
                            "./screenshots-images-2/chapter_6/section_18/2cfe90cc-2b9d-4ec5-a32e-9fcce2eb2199.png",
                            "./screenshots-images-2/chapter_6/section_18/a9dee418-ab01-459c-b2eb-d226afb343a9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optimizers a la carte\n\nIn the example code, we used vanilla gradient descent for optimization, which worked\nfine for our simple case. Needless to say, there are several optimization strategies and\ntricks that can assist convergence, especially when models get complicated.\n\nWe'll dive deeper into this topic in later chapters, but now is the right time to\nintroduce the way PyTorch abstracts the optimization strategy away from user code:\nthat is, the training loop we\u2019ve examined. This saves us from the boilerplate busywork\nof having to update each and every parameter to our model ourselves. The torch\nmodule has an optim submodule where we can find classes implementing different\noptimization algorithms. Here\u2019s an abridged list (code/plch5/3_optimizers.ipynb):\n\n# In[5):\nimport torch.optim as optim\n\ndir (optim)\n\n# Out[5):\n('ASGD\",\n\u2018Adadelta',\n\u2018Adagrad',\n\u2018Adam',\n\u2018Adamax',\n\"LBFGS',\n\u2018Optimizer',\n\n\"RMSprop',\n\"Rprop',\n'SGD',\n'SparseAdam',\n\nEvery optimizer constructor takes a list of parameters (aka PyTorch tensors, typically\nwith requires_grad set to True) as the first input. All parameters passed to the opti-\nmizer are retained inside the optimizer object so the optimizer can update their val-\nues and access their grad attribute, as represented in figure 5.11.\n\nA 8\n\nimpat\n\u00a9 \u00b0 LO\npatenns\n\nFigure 5.11 (A) Conceptual representation of how an optimizer holds a reference to\nparameters. (B) After a loss is computed from inputs, (C) a call to . backward leads to\n. grad being populated on parameters. (D) At that point, the optimizer can access\n.grad and compute the parameter updates.\n\nEach optimizer exposes two methods: zero_grad and step. zero_grad zeroes the\ngrad attribute of all the parameters passed to the optimizer upon construction. step\nupdates the value of those parameters according to the optimization strategy imple-\nmented by the specific optimizer.\n\nUSING A GRADIENT DESCENT OPTIMIZER\nLet's create params and instantiate a gradient descent optimizer:\n\n# In[6]:\n\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\nlearning_rate = le-5\n\noptimizer = optim.SGD([params], lr=learning_rate)\n\nHere SGD stands for stochastic gradient descent. Actually, the optimizer itself is exactly a\n\nvanilla gradient descent (as long as the momentum argument is set to 0.0, which is the\n\ndefault). The term stochastic comes from the fact that the gradient is typically obtained\n\nby averaging over a random subset of all input samples, called a minibatch. However, the\n\noptimizer does not know if the loss was evaluated on all the samples (vanilla) or a ran-\n\ndom subset of them (stochastic), so the algorithm is literally the same in the two cases.\nAnyway, let\u2019s take our fancy new optimizer for a spin:\n\n# In[7):\n\ntp = model(t_u, *params)\nloss = loss_fn(t_p, t_c)\nloss. backward ()\n\noptimizer.step()\nparams\n\n# Out[7):\ntensor([{ 9.5483e-01, -8.2600e-04], requires_grad=True)\n\nThe value of params is updated upon calling step without us having to touch it our-\nselves! What happens is that the optimizer looks into params.grad and updates\nparams, subtracting learning_rate times grad from it, exactly as in our former hand-\nrolled code.\n\nReady to stick this code in a training loop? Nope! The big gotcha almost got us\u2014\nwe forgot to zero out the gradients. Had we called the previous code in a loop, gradi-\nents would have accumulated in the leaves at every call to backward, and our gradient\ndescent would have been all over the place! Here\u2019s the loop-ready code, with the extra\nzero_grad at the correct spot (right before the call to backward):\n\n# In[8]:\n\nparams = torch.tensor([1.0, 0.0), requires_grad=True)\nlearning_rate = le-2\n\noptimizer = optim.SGD(({params], lr=learning_rate)\n\nt_p = model(t_un, *params)\n\nloss = loss_fn(t_p, te) As before, the exact placement of\nthis call is somewhat arbitrary. It\noptimizer.zero_grad() <\u00a2\u2014___| could be earlier in the loop as well.\n\nloss .backward()\noptimizer.step()\n\nparams\n\n# Out [8]:\ntensor((1.7761, 0.1064), requires_grad=True)\n\nPerfect! See how the optim module helps us abstract away the specific optimization\nscheme? All we have to do is provide a list of params to it (that list can be extremely\n\nlong, as is needed for very deep neural network models), and we can forget about the\ndetails.\nLet\u2019s update our training loop accordingly:\n\n# In[9]:\ndef training_loop(n_epochs, optimizer, params, t_u, t_c):\nfor epoch in range(1, n_epochs + 1):\ntp = model(t_u, *params)\nloss = loss_fn(t_p, t_c)\n\noptimizer.zero_grad()\nloss. backward ()\noptimizer.step()\n\nif epoch % 500 == 0:\nprint('Epoch $d, Loss tf' % (epoch, float(loss))})\n\nreturn params\n\n# In(10]:\nparams = torch.tensor([{1.0, 0.0], requires_grad=True)\nlearning_rate = le-2\n\noptimizer = optim.SGD([params}], lr=learning_rate) <+\u2014\nIt\u2019s important that both\ntraining_loop( params are the same object;\nn_epochs = 5000, otherwise the optimizer won't\noptimizer = optimizer, know what parameters were\nparams = params, used by the model.\ntu = t_un,\nte = te)\n# Out[10):\n\nEpoch 500, Loss 7.860118\n\nEpoch 1000, Loss 3.828538\nEpoch 1500, Loss 3.092191\nEpoch 2000, Loss 2.957697\nEpoch 2500, Loss 2.933134\nEpoch 3000, Loss 2.928648\nEpoch 3500, Loss 2.927830\nEpoch 4000, Loss 2.927680\nEpoch 4500, Loss 2.927651\nEpoch 5000, Loss 2.927648\n\ntensor([ 5.3671, -17.3012], requires_grad=True)\n\nAgain, we get the same result as before. Great: this is further confirmation that we\nknow how to descend a gradient by hand!\n\nTESTING OTHER OPTIMIZERS\nIn order to test more optimizers, all we have to do is instantiate a different optimizer,\nsay Adam, instead of SGD. The rest of the code stays as it is. Pretty handy stuff.\n\nWe won't go into much detail about Adam; suffice to say that it is a more sophisti-\ncated optimizer in which the learning rate is set adaptively. In addition, it is a lot less\nsensitive to the scaling of the parameters\u2014so insensitive that we can go back to using\n\nthe original (non-normalized) input t_u, and even increase the learning rate to le-1,\nand Adam won't even blink:\n\n# In[11]:\n\nparams = torch.tensor([1.0, 0.0), requires_grad=True)\n\nlearning_rate = le-1\n\noptimizer = optim.Adam([params], lr=learning_rate) 2 New optimizer class\n\ntraining_loop(\nn_epochs = 2000,\noptimizer = optimizer,\nparams = params,\ntu = tu, +\ntle = tle)\n\nWe're back to the original\nt_u as our input.\n\n# Out[il]:\n\nEpoch 500, Loss 7.612903\nEpoch 1000, Loss 3.086700\nEpoch 1500, Loss 2.928578\nEpoch 2000, Loss 2.927646\n\ntensor({ 0.5367, -17.3021), requires_grad=True)\n\nThe optimizer is not the only flexible part of our training loop. Let\u2019s turn our atten-\ntion to the model. In order to train a neural network on the same data and the same\nloss, all we would need to change is the model function. It wouldn\u2019t make particular\nsense in this case, since we know that converting Celsius to Fahrenheit amounts to a\nlinear transformation, but we'll do it anyway in chapter 6. We'll see quite soon that\nneural networks allow us to remove our arbitrary assumptions about the shape of the\nfunction we should be approximating. Even so, we'll see how neural networks manage\nto be trained even when the underlying processes are highly nonlinear (such in the\ncase of describing an image with a sentence, as we saw in chapter 2).\n\nWe have touched on a lot of the essential concepts that will enable us to train\ncomplicated deep learning models while knowing what\u2019s going on under the hood:\nbackpropagation to estimate gradients, autograd, and optimizing weights of models\nusing gradient descent or other optimizers. Really, there isn\u2019t a lot more. The rest is\nmostly filling in the blanks, however extensive they are.\n\nNext up, we're going to offer an aside on how to split our samples, because that\nsets up a perfect use case for learning how to better control autograd.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.19,
                        "section_name": "Training, validation, and overfitting",
                        "section_path": "./screenshots-images-2/chapter_6/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_19/cfc8762d-24fd-4b89-b259-353355ce2cd5.png",
                            "./screenshots-images-2/chapter_6/section_19/6221644e-6e39-40a3-8156-d91b1308e897.png",
                            "./screenshots-images-2/chapter_6/section_19/5fb063e6-c865-439c-81c2-6b2911b5e588.png",
                            "./screenshots-images-2/chapter_6/section_19/14977aea-02fd-4d93-a8b6-bda2fa1692f8.png",
                            "./screenshots-images-2/chapter_6/section_19/29cc1849-bacc-47fb-a831-ef301d5f9603.png",
                            "./screenshots-images-2/chapter_6/section_19/efa26cc2-a5ab-418e-885a-71ccbb0f8c37.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training, validation, and overfitting\n\nJohannes Kepler taught us one last thing that we didn\u2019t discuss so far, remember? He\nkept part of the data on the side so that he could validate his models on independent\nobservations. This is a vital thing to do, especially when the model we adopt could\npotentially approximate functions of any shape, as in the case of neural networks. In\nother words, a highly adaptable model will tend to use its many parameters to make\nsure the loss is minimal af the data points, but we'll have no guarantee that the model\n\nbehaves well away from or in between the data points. After all, that\u2019s what we\u2019re asking\nthe optimizer to do: minimize the loss af the data points. Sure enough, if we had inde-\npendent data points that we didn\u2019t use to evaluate our loss or descend along its nega-\ntive gradient, we would soon find out that evaluating the loss at those independent\ndata points would yield higher-than-expected loss. We have already mentioned this\nphenomenon, called overfitting.\n\nThe first action we can take to combat overfitting is recognizing that it might hap-\npen. In order to do so, as Kepler figured out in 1600, we must take a few data points\nout of our dataset (the validation set) and only fit our model on the remaining data\npoints (the (raining set), as shown in figure 5.12. Then, while we\u2019re fitting the model,\nwe can evaluate the loss once on the training set and once on the validation set. When\nwe're trying to decide if we\u2019ve done a good job of fitting our model to the data, we\nmust look at both!\n\nmopeL |\u2019: SPERFORMANCE\n\nFigure 5.12 Conceptual representation of a data-\nproducing process and the collection and use of\ntraining data and independent validation data\n\nEVALUATING THE TRAINING LOSS\n\nThe training loss will tell us if our model can fit the training set at all\u2014in other words,\nif our model has enough capacity to process the relevant information in the data. If\nour mysterious thermometer somehow managed to measure temperatures using a log-\narithmic scale, our poor linear model would not have had a chance to fit those mea-\nsurements and provide us with a sensible conversion to Celsius. In that case, our\ntraining loss (the loss we were printing in the training loop) would stop decreasing\nwell before approaching zero.\n\nA deep neural network can potentially approximate complicated functions, pro-\nvided that the number of neurons, and therefore parameters, is high enough. The\nfewer the number of parameters, the simpler the shape of the function our network will\nbe able to approximate. So, rule 1: if the training loss is not decreasing, chances are the\nmodel is too simple for the data. The other possibility is that our data just doesn\u2019t con-\ntain meaningful information that lets it explain the output: if the nice folks at the shop\nsell us a barometer instead of a thermometer, we will have little chance of predicting\ntemperature in Celsius from just pressure, even if we use the latest neural network\narchitecture from Quebec (www.umontreal.ca/en/artificialintelligence).\n\nGENERALIZING TO THE VALIDATION SET\n\nWhat about the validation set? Well, if the loss evaluated in the validation set doesn\u2019t\ndecrease along with the training set, it means our model is improving its fit of the sam-\nples it is seeing during training, but it is not generalizing to samples outside this precise\nset. As soon as we evaluate the model at new, previously unseen points, the values of\nthe loss function are poor. So, rule 2: if the training loss and the validation loss\ndiverge, we're overfitting.\n\nLet\u2019s delve into this phenomenon a little, going back to our thermometer exam-\nple. We could have decided to fit the data with a more complicated function, like a\npiecewise polynomial or a really large neural network. It could generate a model\nmeandering its way through the data points, as in figure 5.13, just because it pushes\nthe loss very close to zero. Since the behavior of the function away from the data\npoints does not increase the loss, there\u2019s nothing to keep the model in check for\ninputs away from the training data points.\n\nFigure 5.13 Rather\nextreme example of\noverfitting\n\nWhat's the cure, though? Good question. From what we just said, overfitting really\nlooks like a problem of making sure the behavior of the model in between data points\nis sensible for the process we're trying to approximate. First of all, we should make\nsure we get enough data for the process. If we collected data from a sinusoidal pro-\ncess by sampling it regularly at a low frequency, we would have a hard time fitting a\nmodel to it.\n\nAssuming we have enough data points, we should make sure the model that is\ncapable of fitting the training data is as regular as possible in between them. There are\nseveral ways to achieve this. One is adding penalization terms to the loss function, to\nmake it cheaper for the model to behave more smoothly and change more slowly (up\nto a point). Another is to add noise to the input samples, to artificially create new data\npoints in between training data samples and force the model to uy to fit those, too.\nThere are several other ways, all of them somewhat related to these. But the best favor\nwe can do to ourselves, at least as a first move, is to make our model simpler. From an\nintuitive standpoint, a simpler model may not fit the training data as perfectly as a\nmore complicated model would, but it will likely behave more regularly in between\ndata points.\n\nWe've got some nice trade-offs here. On the one hand, we need the model to have\nenough capacity for it to fit the training set. On the other, we need the model to avoid\noverfitting. Therefore, in order to choose the right size for a neural network model in\nterms of parameters, the process is based on two steps: increase the size until it fits,\nand then scale it down until it stops overfitting.\n\nWe'll see more about this in chapter 12\u2014we\u2019ll discover that our life will be a bal-\nancing act between fitting and overfitting. For now, let\u2019s get back to our example and\nsee how we can split the data into a training set and a validation set. We'll do it by\nshuffling t_u and t_c the same way and then splitting the resulting shuffled tensors\ninto two parts.\n\nSPLITTING A DATASET\nShuffling the elements of a tensor amounts to finding a permutation of its indices.\nThe randperm function does exactly this:\n\n# In(12]:\nn_samples = t_u.shape[0)\nnival = int(0.2 * n_samples)\n\nshuffled_indices = torch.randperm(n_samples)\n\ntrain_indices = shuffled_indices[:-n_val]\n\nval_indices = shuffled_indices[-n_val:] Since these are random, don\u2019t\n\nbe surprised if your values end\ntrain_indices, val_indices 4. up different from here on out.\n# Out[12]):\n\n(tensor((9, 6, 5, 8, 4, 7, 0, 1, 3)), tensor([ 2, 10))}\n\nWe just got index tensors that we can use to build training and validation sets starting\nfrom the data tensors:\n\n# In[13]:\ntrain_t_u = t_u[train_indices]\ntrain_t_e = t_e[train_indices]\n\nval_t_u = t_u[val_indices]\nval_tlc t_e[val_indices]\n\n* train_t_u\nval_t_u\n\nro\neh\n\nOur training loop doesn\u2019t really change. We just want to additionally evaluate the vali-\ndation loss at every epoch, to have a chance to recognize whether we\u2019re overfitting:\n\n# In[14):\ndef training _loop(n_epochs, optimizer, params, train_t_u, val_t_u,\ntrain_t_c, val_t_c):\nfor epoch in range(1, n_epochs + 1):\ntrain_t_p = model(train_t_u, *params) os\n\ntrain_loss = loss_fn(train_t_p, train_t_c) Tene eumnlt tor of lines ahd\nval_t_p = model(val_t_u, *params) \u201c\u2014 val_* inputs.\n\nval_loss = loss_fn(val_t_p, val_t ic)\n\noptimizer.zero_grad() Note that there is no val_loss.backward()\ntrain_loss.backward() here, since we don\u2019t want to train the\noptimizer.step() model on the validation data.\n\nif epoch <= 3 or epoch % 500 ==\nprint(f\u00a3\"Epoch (epoch), Training loss {train_loss.item():.4f),\"\nf\u00a3\" Validation loss (val_loss.item():.4f\u00a3}\")\n\nreturn params\n\n# In[15):\n\nparams = torch.tensor([1.0, 0.0), requires_grad=True)\nlearning_rate = le-2\n\noptimizer = optim.SGD([params], lr=learning_rate)\n\ntraining_loop(\nn_epochs = 3000,\noptimizer = optimizer,\nparams = params,\ntrain_t_u = train_t_un, Since we're using SGD again, we're\nval_t_u = val_t_un, back to using normalized inputs.\ntrain_t_c = train_t_c,\nval_t_c = val_t_c)\n\n# Out[i5]:\n\nEpoch 1, Training loss 66.5811, Validation loss 142.3890\nEpoch 2, Training loss 38.8626, Validation loss 64.0434\nEpoch 3, Training loss 33.3475, Validation loss 39.4590\nEpoch 500, Training loss 7.1454, Validation loss 9.1252\n\nEpoch 1000, Training loss 3.5940, Validation loss 5.3110\nEpoch 1500, Training loss 3.0942, Validation loss 4.1611\nEpoch 2000, Training loss 3.0238, Validation loss 3.7693\nEpoch 2500, Training loss 3.0139, Validation loss 3.6279\nEpoch 3000, Training loss 3.0125, Validation loss 3.5756\n\ntensor([ 5.1964, -16.7512], requires_grad=True)\n\nHere we are not being entirely fair to our model. The validation set is really small, so\nthe validation loss will only be meaningful up to a point. In any case, we note that the\nvalidation loss is higher than our training loss, although not by an order of magni-\ntude. We expect a model to perform better on the training set, since the model\nparameters are being shaped by the training set. Our main goal is to also see both the\ntraining loss and the validation loss decreasing. While ideally both losses would be\nroughly the same value, as long as the validation loss stays reasonably close to the\ntraining loss, we know that our model is continuing to learn generalized things about\nour data. In figure 5.14, case C is ideal, while D is acceptable. In case A, the model\nisn\u2019t learning at all; and in case B, we see overfitting. We'll see more meaningful exam-\nples of overfitting in chapter 12.\n\nLOSS\n\n(ITERATIONS,\n\nFigure 5.14 Overfitting scenarios when looking at the training (solid line) and validation (dotted line)\nlosses. (A) Training and validation losses do not decrease; the model is not learning due to no\ninformation in the data or insufficient capacity of the model. (B) Training loss decreases while\nvalidation loss increases: overfitting. (C) Training and validation losses decrease exactly in tandem.\nPerformance may be improved further as the model is not at the limit of overfitting. (D) Training and\nvalidation losses have different absolute values but similar trends: overfitting is under control.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.2,
                        "section_name": "Autograd nits and switching it off",
                        "section_path": "./screenshots-images-2/chapter_6/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_20/8ac85744-ec42-42d6-a6f3-ffa6759fc5fd.png",
                            "./screenshots-images-2/chapter_6/section_20/b4f2264a-4ca9-4eff-897a-b0390274b5bd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Autograd nits and switching it off\n\nFrom the previous training loop, we can appreciate that we only ever call backward on\ntrain_loss. Therefore, errors will only ever backpropagate based on the training\nset\u2014the validation set is used to provide an independent evaluation of the accuracy of\nthe model\u2019s output on data that wasn\u2019t used for training.\n\nThe curious reader will have an embryo of a question at this point. The model is\nevaluated twice\u2014once on train_t_u and once on val_t_u\u2014and then backward is\ncalled. Won't this confuse autograd? Won't backward be influenced by the values gen-\nerated during the pass on the validation set?\n\nLuckily for us, this isn\u2019t the case. The first line in the training loop evaluates model\non train_t_u to produce train_t_p. Then train_loss is evaluated from train_t_p.\nThis creates a computation graph that links train_t_u to train_t_p to train_loss.\nWhen model is evaluated again on val_t_u, it produces val_t_p and val_loss. In this\ncase, a separate computation graph will be created that links val_t_u to val_t_p to\nval_loss. Separate tensors have been run through the same functions, model and\nloss_fn, generating separate computation graphs, as shown in figure 5.15.\n\nt > model (x, pers) > Te n\n\nvtain >\nA\na > model (x, or) ~ beak\ntotam > wolel (x, paroms) > Tain ne lossy,\ntyeall > model (x, perms) 2 tava > loss val\nc titsin > > model (x, proms) > Tarn S less ja boekuard\n\ntyval > model (x, poling) Eval = loss al\n\nFigure 5.15 Diagram showing how gradients propagate through a graph with two\nlosses when . backward is called on one of them\n\nThe only tensors these two graphs have in common are the parameters. When we call\nbackward on train_loss, we run backward on the first graph. In other words, we\naccumulate the derivatives of train_loss with respect to the parameters based on the\ncomputation generated from train_t_u.\n\nIf we (incorrectly) called backward on val_loss as well, we would accumulate the\nderivatives of val_loss with respect to the parameters on the same leaf nodes. Remember\nthe zero_grad thing, whereby gradients are accumulated on top of each other every\ntime we call backward unless we zero out the gradients explicitly? Well, here something\n\nvery similar would happen: calling backward on val_loss would lead to gradients accu-\nmulating in the params tensor, on top of those generated during the train_loss.back-\nward() call. In this case, we would effectively train our model on the whole dataset (both\ntraining and validation), since the gradient would depend on both. Pretty interesting.\n\nThere\u2019s another element for discussion here. Since we\u2019re not ever calling back-\nward on val_loss, why are we building the graph in the first place? We could in fact\njust call model and loss_fn as plain functions, without tracking the computation.\nHowever optimized, building the autograd graph comes with additional costs that we\ncould totally forgo during the validation pass, especially when the model has millions\nof parameters.\n\nIn order to address this, PyTorch allows us to switch off autograd when we don\u2019t\nneed it, using the torch.no_grad context manager.\u2019 We won\u2019t see any meaningful\nadvantage in terms of speed or memory consumption on our small problem. How-\never, for larger models, the differences can add up. We can make sure this works by\nchecking the value of the requires_grad attribute on the val_loss tensor:\n\n# In(16]:\ndef training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\ntrain_t_c, val_t ic):\nfor epoch in range(1, n_epochs + 1):\ntrain_t_p = model(train_t_u, *params)\n\nContext train_loss = loss_fn(train_t_p, train_t_c)\nmanager\nhere | with torch.no_grad(): Checks that our output\nval_t_p = model(val_t_u, *params) requires _grad args are\nval_loss = loss_fn(val_t_p, val_t_c) forced to False inside\nassert val_loss.requires_grad == False + | this block\n\noptimizer.zero_grad()\ntrain_loss. backward ()}\noptimizer.step()\n\nUsing the related set_grad_enabled context, we can also condition the code to run\nwith autograd enabled or disabled, according to a Boolean expression\u2014typically indi-\ncating whether we are running in training or inference mode. We could, for instance,\ndefine a calc_forward function that takes data as input and runs model and loss_fn\nwith or without autograd according to a Boolean train_is argument:\n\n# In[(17):\ndef cale_forward(t_u, tic, is_train):\nwith torch.set_grad_enabled(is_train):\ntip = model(t_u, *params)\nloss = loss_fn(t_p, t ce)\nreturn loss\n\n2 We should not think that using torch. no_grad necessarily implies that the outputs do not require gradients.\nThere are particular circumstances (involving views, as discussed in section 3.8.1) in which requires_grad\nis not set to False even when created in a no_grad context. It is best to use the detach function if we need\nto be sure.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.21,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_6/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_21/228e2fac-797f-4bb6-af63-15a367c1a339.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\n\nWe started this chapter with a big question: how is it that a machine can learn from\nexamples? We spent the rest of the chapter describing the mechanism with which a\nmodel can be optimized to fit data. We chose to stick with a simple model in order to\nsee all the moving parts without unneeded complications.\n\nNow that we've had our fill of appetizers, in chapter 6 we'll finally get to the main\ncourse: using a neural network to fit our data. We'll work on solving the same\nthermometer problem, but with the more powerful tools provided by the torch.nn\nmodule. We'll adopt the same spirit of using this small problem to illustrate the\nlarger uses of PyTorch. The problem doesn\u2019t need a neural network to reach a\nsolution, but it will allow us to develop a simpler understanding of what\u2019s required to\ntrain a neural network.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 7,
                "chapter_name": "Using a neural\nnetwork to fit the data",
                "chapter_path": "./screenshots-images-2/chapter_7",
                "sections": [
                    {
                        "section_id": 7.1,
                        "section_name": "Using a neural\nnetwork to fit the data",
                        "section_path": "./screenshots-images-2/chapter_7/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_1/edd55f2b-349e-41fb-828f-3aadff53c205.png",
                            "./screenshots-images-2/chapter_7/section_1/76ed291c-a10d-4a5e-8427-8b02ce5b7387.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "So far, we\u2019ve taken a close look at how a linear model can learn and how to make\nthat happen in PyTorch. We've focused on a very simple regression problem that\nused a linear model with only one input and one output. Such a simple example\nallowed us to dissect the mechanics of a model that learns, without getting overly\ndistracted by the implementation of the model itself. As we saw in the overview dia-\ngram in chapter 5, figure 5.2 (repeated here as figure 6.1), the exact details of a\nmodel are not needed to understand the high-level process that trains the model.\nBackpropagating errors to parameters and then updating those parameters by tak-\ning the gradient with respect to the loss is the same no matter what the underlying\nmodel is.\n\nTHE LEARNING PROCESS\n\nu Og\naa a DESIRED OUTPUTS\nwees g Op a\nqo im UgG (GROUND TRUTH)\n0\nNY\nfe f-% G@g ACTUAL OUTPUTS\nFORWARD 4 WG Given current\n_\u2014\u2014_\u2014\u2014_> 4 4 <i WEIGHTS\nCHANGE weights to\n\nDECREASE ERRORS <\u2014 ERRORS (LOSS FUNCTION)\n\nITERATE\nBACKWARD\n\n\u2014\u2014\u2014\u2014\u2014\u2014\n\nnew weuts Bf yg OD vaupation\nDg > rm\n\nFigure 6.1 Our mental model of the learning process, as implemented in chapter 5\n\nIn this chapter, we will make some changes to our model architecture: we're going to\nimplement a full artificial neural network to solve our temperature-conversion\nproblem. We'll continue using our training loop from the last chapter, along with our\nFahrenheit-to-Celsius samples split into training and validation sets. We could start to\nuse a quadratic model: rewriting model as a quadratic function of its input (for\nexample, y = a * x**2 + b * x + c). Since such a model would be differentiable,\nPyTorch would take care of computing gradients, and the training loop would work as\nusual. That wouldn\u2019t be too interesting for us, though, because we would still be fixing\nthe shape of the function.\n\nThis is the chapter where we begin to hook together the foundational work we've\nput in and the PyTorch features you'll be using day in and day out as you work on your\nprojects. You'll gain an understanding of what's going on underneath the porcelain of\nthe PyTorch API, rather than it just being so much black magic. Before we get into the\nimplementation of our new model, though, let\u2019s cover what we mean by artificial neu-\nral network.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.2,
                        "section_name": "Artificial neurons",
                        "section_path": "./screenshots-images-2/chapter_7/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_2/a587a894-4f7a-457b-8da3-dbdec2ecc237.png",
                            "./screenshots-images-2/chapter_7/section_2/d01e7bb5-54fa-470b-8aac-52c3b74663b4.png",
                            "./screenshots-images-2/chapter_7/section_2/263bcf54-cca3-4cc3-9148-88b580437b78.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Artificial neurons\n\nAt the core of deep learning are neural networks: mathematical entities capable of\nrepresenting complicated functions through a composition of simpler functions. The\nterm neural network is obviously suggestive of a link to the way our brain works. As a\n\nmatter of fact, although the initial models were inspired by neuroscience,! modern\nartificial neural networks bear only a slight resemblance to the mechanisms of neu-\nrons in the brain. It seems likely that both artificial and physiological neural networks\nuse vaguely similar mathematical strategies for approximating complicated functions\nbecause that family of strategies works very effectively.\n\nNOTE We are going to drop the artificial and refer to these constructs as just\nneural networks from here forward.\n\nThe basic building block of these complicated functions is the neuron, as illustrated in\nfigure 6.2. At its core, it is nothing but a linear transformation of the input (for exam-\nple, multiplying the input by a number [the weight] and adding a constant [the bias] )\nfollowed by the application of a fixed nonlinear function (referred to as the activation\nfunction).\n\nMathematically, we can write this out as o = /(w * x + 6), with x as our input, w our\nweight or scaling factor, and b as our bias or offset. fis our activation function, set to\nthe hyperbolic tangent, or tanh function here. In general, x and, hence, o can be sim-\nple scalars, or vector-valued (meaning holding many scalar values); and similarly, w\n\nTHE \"NEURON\"\nLEARNED PARAMETERS\n\u00a9 atarh (wr + b)\n\u2014t \\\u2014_*__weut\n\n| LINEAR TRANSFORMATION\nNONLINEAR FUNCTION (ACTIVATION)\n\nLEARNED\nee2 fw \u201cU1 oatenb())\n1S a 2xle4+024 > tavh (42 )= |\n\n\u201c2.19 \u2014> 2x(2\")4@ = 0% > tanh (0% )s o34A\n\u201c10 \u2014> 2x(-10)40=-4 \u2014 tl, (-\u201c)= -|\n\nFigure 6.2 An artificial neuron: a linear transformation enclosed in a nonlinear function\n\ncan be a single scalar or matrix, while bis a scalar or vector (the dimensionality of the\ninputs and weights must match, however). In the latter case, the previous expression is\n\nreferred to as a layer of neurons, since it represents many neurons via the multidimen-\nsional weights and biases.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.3,
                        "section_name": "Composing a multilayer network",
                        "section_path": "./screenshots-images-2/chapter_7/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_3/9c854d25-6ad4-498f-b67a-22605c395bfe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Composing a multilayer network\n\nA multilayer neural network, as represented in figure 6.3, is made up of a composition\nof functions like those we just discussed\n\n\u00a3(w_0 * x + b_0)\n\u00a3(wl * x1 + b.1)\n\nxl\nx2\ny = f\u00a3(win * x_n + b_n)\n\nwhere the output of a layer of neurons is used as an input for the following layer.\nRemember that w_0 here is a matrix, and x is a vector! Using a vector allows w_0 to\nhold an entire ayer of neurons, not just a single weight.\n\nA NEURAL NETWORK a\n\nOUTPUT\n\nLEARNED PARAMETERS\n\nFigure 6.3 A neural network\nwith three layers\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.4,
                        "section_name": "Understanding the error function",
                        "section_path": "./screenshots-images-2/chapter_7/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_4/d684ca0e-63f8-48bf-8ca1-1d7ca85aa3d8.png",
                            "./screenshots-images-2/chapter_7/section_4/ff8f9df8-9022-4bb0-803b-1e06dd4a8895.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Understanding the error function\n\nAn important difference between our earlier linear model and what we'll actually be\nusing for deep learning is the shape of the error function. Our linear model and\nerror-squared loss function had a convex error curve with a singular, clearly defined\nminimum. If we were to use other methods, we could solve for the parameters mini-\nmizing the error function automatically and definitively. That means that our parame-\nter updates were attempting to estimate that singular correct answer as best they could.\n\nNeural networks do not have that same property of a convex error surface, even\nwhen using the same error-squared loss function! There\u2019s no single right answer for\neach parameter we're attempting to approximate. Instead, we are trying to get all of\nthe parameters, when acting in concert, to produce a useful output. Since that useful\noutput is only going to approximate the truth, there will be some level of imperfection.\nWhere and how imperfections manifest is somewhat arbitrary, and by implication the\nparameters that control the output (and, hence, the imperfections) are somewhat\narbitrary as well. This results in neural network training looking very much like\nparameter estimation from a mechanical perspective, but we must remember that the\ntheoretical underpinnings are quite different.\n\nA big part of the reason neural networks have non-convex error surfaces is due to\nthe activation function. The ability of an ensemble of neurons to approximate a very\nwide range of useful functions depends on the combination of the linear and nonlin-\near behavior inherent to each neuron.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.5,
                        "section_name": "All we need is activation",
                        "section_path": "./screenshots-images-2/chapter_7/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_5/e4ea28e5-2f38-443f-b550-d45b9dd90599.png",
                            "./screenshots-images-2/chapter_7/section_5/cba16f9e-9b2a-4aac-93d9-d2eeb4000c8e.png",
                            "./screenshots-images-2/chapter_7/section_5/e4467294-a9ec-48d3-b29e-0382ae6bc555.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "All we need is activation\n\nAs we have seen, the simplest unit in (deep) neural networks is a linear operation\n(scaling + offset) followed by an activation function. We already had our linear opera-\ntion in our latest model\u2014the linear operation was the entire model. The activation\nfunction plays two important roles:\n\n= In the inner parts of the model, it allows the output function to have different\nslopes at different values\u2014something a linear function by definition cannot do.\nBy trickily composing these differently sloped parts for many outputs, neural\nnetworks can approximate arbitrary functions, as we will see in section 6.1.6.\u201d\n\n= At the last layer of the network, it has the role of concentrating the outputs of\nthe preceding linear operation into a given range.\n\nLet\u2019s talk about what the second point means. Pretend that we're assigning a \u201cgood\ndoggo\u201d score to images. Pictures of retrievers and spaniels should have a high score,\nwhile images of airplanes and garbage trucks should have a low score. Bear pictures\nshould have a lowish score, too, although higher than garbage trucks.\n\nThe problem is, we have to define a \u201chigh score\u201d: we've got the entire range of\nfloat32 to work with, and that means we can go pretty high. Even if we say \u201cit\u2019s a 10-point\nscale,\u201d there\u2019s still the issue that sometimes our model is going to produce a score of 11\nout of 10. Remember that under the hood, it\u2019s all sums of (w*x+b) matrix multiplica-\ntions, and those won't naturally limit themselves to a specific range of outputs.\n\nCAPPING THE OUTPUT RANGE\n\nWe want to firmly constrain the output of our linear operation to a specific range so\nthat the consumer of this output doesn\u2019t have to handle numerical inputs of puppies\nat 12/10, bears at\u201410, and garbage trucks at \u20141,000.\n\nOne possibility is to just cap the output values: anything below 0 is set to 0, and any-\nthing above 10 is set to 10. That\u2019s a simple activation function called torch. nn.Hardtanh\n(https:// pytorch.org/docs/stable/nn.html#hardtanh, but note that the default range\nis\u20141 to +1).\n\nCOMPRESSING THE OUTPUT RANGE\n\nAnother family of functions that work well is torch.nn.Sigmoid, which includes 1 /\n(1 + e ** -x), torch.tanh, and others that we'll see in a moment. These functions\nhave a curve that asymptotically approaches 0 or \u2014-1 as x goes to negative infinity,\napproaches | as x increases, and have a mostly constant slope at x == 0. Conceptually,\nfunctions shaped this way work well because there\u2019s an area in the middle of our lin-\near function\u2019s output that our neuron (which, again, is just a linear function followed\nby an activation) will be sensitive to, while everything else gets lumped next to the\nboundary values. As we can see in figure 6.4, our garbage truck gets a score of -0.97,\nwhile bears and foxes and wolves end up somewhere in the -0.3 to 0.3 range.\n\nONDER- SENSITIVE OVER-\nSATURATED SATURATED\n\nThis results in garbage trucks being flagged as \u201cnot dogs,\u201d our good dog mapping to\n\u201cclearly a dog,\u201d and our bear ending up somewhere in the middle. In code, we can see\nthe exact values:\n\n>>> import math\n\n>>> math. tanh(-2.2) + Garbage truck\n~0.9757431300314515\n\n>>> math. tanh(0.1) 4 Bear\n0.09966799462495582\n\n>>> math. tanh(2.5) + Good doggo\n\n0.9866142981514303\n\nWith the bear in the sensitive range, small changes to the bear will result in a notice-\nable change to the result. For example, we could switch from a grizzly to a polar bear\n(which has a vaguely more traditionally canine face) and see a jump up the Y-axis as\nwe slide toward the \u201cvery much a dog\u201d end of the graph. Conversely, a koala bear\nwould register as less dog-like, and we would see a drop in the activated output. There\nisn\u2019t much we could do to the garbage truck to make it register as dog-like, though:\neven with drastic changes, we might only see a shift from \u20140.97 to -0.8 or so.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.6,
                        "section_name": "More activation functions",
                        "section_path": "./screenshots-images-2/chapter_7/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_6/b082a5e9-6bcd-4f47-9610-d9b19a97886a.png",
                            "./screenshots-images-2/chapter_7/section_6/834ace3b-0013-4998-92f0-4bdf5e983dfb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "More activation functions\n\nThere are quite a few activation functions, some of which are shown in figure 6.5. In\nthe first column, we see the smooth functions Tanh and Softplus, while the second\ncolumn has \u201chard\u201d versions of the activation functions to their left: Hardtanh and\nReLU. ReLU (for rectified linear unit) deserves special note, as it is currently considered\n\nTAN HARDTANK \u2018SIGMOID\n3f 3f 3\n2 2) 2\nty iy \\\no7 oF \u00b0\nAly -l cal\n-24 \u201c2\n-3{ -3\n\nBb -2 4 \u00b0 t 2 3 B24 \u00b0 \\ 2 3\n\nREL LEAKYRELU\n3 3\n27 2\nWy \\\n\u00b0 \u00b0\naly cal\n\u201c27 2\n\u2014 = \u2014\nBb -2 4 \u00b0 t 2 3 B24 \u00b0 \\ 2 3\n\nFigure 6.5 A collection of common and not-so-common activation functions\n\none of the best-performing general activation functions; many state-of-the-art results\nhave used it. The Sigmoid activation function, also known as the logistic function, was\nwidely used in early deep learning work but has since fallen out of common use\nexcept where we explicitly want to move to the 0...1 range: for example, when the out-\nput should be a probability. Finally, the LeakyReLU function modifies the standard\nReLU to have a small positive slope, rather than being strictly zero for negative inputs\n(typically this slope is 0.01, but it\u2019s shown here with slope 0.1 for clarity).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.7,
                        "section_name": "Choosing the best activation function",
                        "section_path": "./screenshots-images-2/chapter_7/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_7/a4aa262c-042e-4b47-a1ef-496b116004c5.png",
                            "./screenshots-images-2/chapter_7/section_7/3c8fed31-ac65-41b9-9b18-4562f2da8468.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Choosing the best activation function\n\nActivation functions are curious, because with such a wide variety of proven successful\nones (many more than shown in figure 6.5), it\u2019s clear that there are few, if any, strict\nrequirements. As such, we\u2019re going to discuss some generalities about activation func-\ntions that can probably be trivially disproved in the specific. That said, by definition,\u2019\nactivation functions\n\n= Are nonlinear. Repeated applications of (w*x+b) without an activation function\nresults in a function of the same (affine linear) form. The nonlinearity allows\nthe overall network to approximate more complex functions.\n\n= Are differentiable, so that gradients can be computed through them. Point dis-\ncontinuities, as we can see in Hardtanh or ReLU, are fine.\n\nWithout these characteristics, the network either falls back to being a linear model or\nbecomes difficult to train.\nThe following are true for the functions:\n\n= They have at least one sensitive range, where nontrivial changes to the input\nresult in a corresponding nontrivial change to the output. This is needed for\ntraining.\n\n= Many of them have an insensitive (or saturated) range, where changes to the\ninput result in little or no change to the output.\n\nBy way of example, the Hardtanh function could easily be used to make piecewise-linear\napproximations of a function by combining the sensitive range with different weights\nand biases on the input.\n\nOften (but far from universally so), the activation function will have at least one of\nthese:\n\n= Alower bound that is approached (or met) as the input goes to negative infinity\n= Asimilar-butinverse upper bound for positive infinity\nThinking of what we know about how backpropagation works, we can figure out that\n\nthe errors will propagate backward through the activation more effectively when the\ninputs are in the response range, while errors will not greatly affect neurons for which\n\nthe input is saturated (since the gradient will be close to zero, due to the flat area\naround the output).\n\nPut together, all this results in a pretty powerful mechanism: we're saying that in a\nnetwork built out of linear + activation units, when different inputs are presented to\nthe network, (a) different units will respond in different ranges for the same inputs,\nand (b) the errors associated with those inputs will primarily affect the neurons oper-\nating in the sensitive range, leaving other units more or less unaffected by the learn-\ning process. In addition, thanks to the fact that derivatives of the activation with\nrespect to its inputs are often close to | in the sensitive range, estimating the parame-\nters of the linear transformation through gradient descent for the units that operate\nin that range will look a lot like the linear fit we have seen previously.\n\nWe are starting to get a deeper intuition for how joining many linear + activation\nunits in parallel and stacking them one after the other leads us to a mathematical\nobject that is capable of approximating complicated functions. Different combina-\ntions of units will respond to inputs in different ranges, and those parameters for\nthose units are relatively easy to optimize through gradient descent, since learning will\nbehave a lot like that of a linear function until the output saturates.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.8,
                        "section_name": "What learning means for a neural network",
                        "section_path": "./screenshots-images-2/chapter_7/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_8/09d92428-1519-465f-be60-ef3bee2e2238.png",
                            "./screenshots-images-2/chapter_7/section_8/fa89b1e4-1ea0-4aa8-9bac-eed6e109db32.png",
                            "./screenshots-images-2/chapter_7/section_8/96afcef8-d79f-4113-a0e2-5a7264a0b49c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What learning means for a neural network\n\nBuilding models out of stacks of linear transformations followed by differentiable acti-\nvations leads to models that can approximate highly nonlinear processes and whose\nparameters we can estimate surprisingly well through gradient descent. This remains\ntrue even when dealing with models with millions of parameters. What makes using\ndeep neural networks so attractive is that it saves us from worrying too much about the\nexact function that represents our data\u2014whether it is quadratic, piecewise polyno-\nmial, or something else. With a deep neural network model, we have a universal\napproximator and a method to estimate its parameters. This approximator can be cus-\ntomized to our needs, in terms of model capacity and its ability to model complicated\ninput/output relationships, just by composing simple building blocks. We can see\nsome examples of this in figure 6.6.\n\nThe four upper-left graphs show four neurons\u2014A, B, C, and D\u2014each with its own\n(arbitrarily chosen) weight and bias. Each neuron uses the Tanh activation function\nwith a min of -] and a max of 1. The varied weights and biases move the center point\nand change how drastically the transition from min to max happens, but they clearly\nall have the same general shape. The columns to the right of those show both pairs of\nneurons added together (A + B and then C + D). Here, we start to see some interest-\ning properties that mimic a single layer of neurons. A + B shows a slight S curve, with\nthe extremes approaching 0, but both a positive bump and a negative bump in the\nmiddle. Conversely, C + D has only a large positive bump, which peaks at a higher\nvalue than our single-neuron max of 1.\n\nIn the third row, we begin to compose our neurons as they would be in a two-layer net-\nwork. Both C(A + B) and D(A + B) have the same positive and negative bumps that A + B\nshows, but the positive peak is more subtle. The composition of C(A + B) + D(A + B)\n\nA: TANK(-2 * X -L25) 8: TANK(| * X + 0.75)\n\naf 3\n\n2 2\n\nVy t\n\n\u00b0 \u00b0\n\nay -l\n\n\u201c2+ 2\n\n. \u2014 \u2014 | \u2014 3 a)\n3-24 =\u00ab0 2 3 32 40+ 3\n\nTANK K-10) D: TANK(-3 \u00a9 X- s)\n\n\u201c+ -l\n-2+ 2\n3 3\n3B 2 4 \u00b0 i 2 3 3B -2 + \u00b0 i 2 3 3B -2 4 \u00b0 2 3\nA 48) DUA +8) C{A+8) + Pty +8)\naf 3 af\n2 2 2\n\\ i t\n\u00b0 \u00b0 \u00b0\n\u201c+ -l \u201c4\n-24 2 -24\n\u201cyc 3 \u2014 J 3\n3B 24 \u00b0 1 2 3 3B -2 4+ \u00b0 t 2 3 3B -2 +4 \u00b0 t 2 3\n\nFigure 6.6 Composing multiple linear units and tanh activation functions to produce nonlinear\noutputs\n\nshows a new property: two clearly negative bumps, and possibly a very subtle second pos-\nitive peak as well, to the left of the main area of interest. All this with only four neurons\nin two layers!\n\nAgain, these neurons\u2019 parameters were chosen only to have a visually interesting\nresult. Training consists of finding acceptable values for these weights and biases so\nthat the resulting network correctly carries out a task, such as predicting likely tem-\nperatures given geographic coordinates and time of the year. By carrying out a task suc-\ncessfully, we mean obtaining a correct output on unseen data produced by the same\ndata-generating process used for training data. A successfully trained network,\nthrough the values of its weights and biases, will capture the inherent structure of the\ndata in the form of meaningful numerical representations that work correctly for pre-\nviously unseen data.\n\nLet's take another step in our realization of the mechanics of learning: deep neural\nnetworks give us the ability to approximate highly nonlinear phenomena without hav-\ning an explicit model for them. Instead, starting from a generic, untrained model, we\nspecialize it on a task by providing it with a set of inputs and outputs and a loss function\nfrom which to backpropagate. Specializing a generic model to a task using examples is\nwhat we refer to as learning, because the model wasn\u2019t built with that specific task in\nmind\u2014no rules describing how that task worked were encoded in the model.\n\nFor our thermometer example, we assumed that both thermometers measured\ntemperatures linearly. That assumption is where we implicitly encoded a rule for our\ntask: we hardcoded the shape of our input/output function; we couldn't have approx-\nimated anything other than data points sitting around a line. As the dimensionality of\na problem grows (that is, many inputs to many outputs) and input/output relation-\nships get complicated, assuming a shape for the input/output function is unlikely to\nwork. The job of a physicist or an applied mathematician is often to come up with a\nfunctional description of a phenomenon from first principles, so that we can estimate\nthe unknown parameters from measurements and get an accurate model of the\nworld. Deep neural networks, on the other hand, are families of functions that have\nthe ability to approximate a wide range of input/output relationships without neces-\nsarily requiring us to come up with an explanatory model of a phenomenon. In a way,\nwe're renouncing an explanation in exchange for the possibility of tackling increas-\ningly complicated problems. In another way, we sometimes lack the ability, informa-\ntion, or computational resources to build an explicit model of what we\u2019re presented\nwith, so data-driven methods are our only way forward.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.9,
                        "section_name": "The PyTorch nn module",
                        "section_path": "./screenshots-images-2/chapter_7/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_9/14fbdab1-ff23-4ec7-bdd5-672ac20defab.png",
                            "./screenshots-images-2/chapter_7/section_9/549ea8cf-6710-4f9f-ad0e-35ea9245dbf4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The PyTorch nn module\n\nAll this talking about neural networks is probably making you really curious about\nbuilding one from scratch with PyTorch. Our first step will be to replace our linear\nmodel with a neural network unit. This will be a somewhat useless step backward from\na correctness perspective, since we've already verified that our calibration only\nrequired a linear function, but it will still be instrumental for starting on a sufficiently\nsimple problem and scaling up later.\n\nPyTorch has a whole submodule dedicated to neural networks, called torch.nn. It\ncontains the building blocks needed to create all sorts of neural network architec-\ntures. Those building blocks are called modules in PyTorch parlance (such building\nblocks are often referred to as /ayers in other frameworks). A PyTorch module is a\nPython class deriving from the nn.Module base class. A module can have one or more\nParameter instances as attributes, which are tensors whose values are optimized\nduring the training process (think w and b in our linear model). A module can also\nhave one or more submodules (subclasses of nn.Module) as attributes, and it will be\nable to track their parameters as well.\n\nNOTE The submodules must be top-level attributes, not buried inside list or\ndict instances! Otherwise, the optimizer will not be able to locate the sub-\nmodules (and, hence, their parameters). For situations where your model\nrequires a list or dict of submodules, PyTorch provides nn.ModuleList and\n\nnn.ModuleDict.\nUnsurprisingly, we can find a subclass of nn.Module called nn.Linear, which applies\nan affine transformation to its input (via the parameter attributes weight and bias)\nand is equivalent to what we implemented earlier in our thermometer experiments.\nWe'll now start precisely where we left off and convert our previous code to a form\n\nthat uses nn.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.1,
                        "section_name": "Using __call__ rather than forward",
                        "section_path": "./screenshots-images-2/chapter_7/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_10/f55a6eb2-691a-4da4-9f94-4228028b8ea3.png",
                            "./screenshots-images-2/chapter_7/section_10/4328115c-1841-453f-a275-a361613aa758.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using __call__ rather than forward\n\nAll PyTorch-provided subclasses of nn.Module have their __call__ method defined.\nThis allows us to instantiate an nn.Linear and call it as if it was a function, like so\n(code/plch6/1_neural_networks.ipynb):\n\n# In[(5]:\n\nimport torch.nn as nn\nWe'll look into the constructor\n\nlinear_model = nn.Linear(1, 1) <a arguments in a moment.\n\nlinear_model (t_un_val)\n\n# Out[5]:\ntensor ([[0.6018],\n(0.2877]], grad_fn=<AddmmBackward>)\n\nCalling an instance of nn.Module with a set of arguments ends up calling a method\nnamed forward with the same arguments. The forward method is what executes the\nforward computation, while __cal1__ does other rather important chores before and\nafter calling forward. So, it is technically possible to call forward directly, and it will\nproduce the same output as __call__, but this should not be done from user code:\n\nmodel (x) <+\u2014 Correct!\n\nmodel . forward (x) \u201ct\nSilent error. Don\u2019t do it!\n\nY\nY\n\nHere's the implementation of Module._call_ (we left out the bits related to the JIT\nand made some simplifications for clarity; torch/nn/modules/module.py, line 483,\nclass: Module):\n\ndef _.call__(self, *input, **kwargs):\nfor hook in self. _forward_pre_hooks.values():\nhook(self, input)\n\nresult = self.forward(*input, **kwargs)\n\nfor hook in self. _forward_hooks.values():\nhook_result = hook(self, input, result)\n\n#o..\n\nfor hook in self. _backward_hooks.values():\n#\n\nreturn result\n\nAs we can see, there are a lot of hooks that won\u2019t get called properly if we just use\n.forward|(..) directly.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.11,
                        "section_name": "Returning to the linear model",
                        "section_path": "./screenshots-images-2/chapter_7/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_11/cb2a20bd-6c45-4140-9f01-1f149e209305.png",
                            "./screenshots-images-2/chapter_7/section_11/64e63202-143e-4b67-90d8-4bf0bb7fac62.png",
                            "./screenshots-images-2/chapter_7/section_11/9d394c14-979b-4e1f-ae8b-59cc1cd5749c.png",
                            "./screenshots-images-2/chapter_7/section_11/31d44a54-1902-4b94-9c2e-a315249a1e86.png",
                            "./screenshots-images-2/chapter_7/section_11/5ec48ed1-3a91-446e-8679-2d5d14fe8c58.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Returning to the linear model\n\nBack to our linear model. The constructor to nn. Linear accepts three arguments: the\nnumber of input features, the number of output features, and whether the linear\nmodel includes a bias or not (defaulting to True, here):\n\n# In[5):\nimport torch.nn as nn\n\nThe arguments are input size, output\nlinear_model = nn.Linear(1, 1) 3 size, and bias defaulting to True.\nlinear_model (t_un_val)\n\n# Out(5):\ntensor ([{[0.6018),\n(0.2877)), grad_fn=<AddmmBackward>)\n\nThe number of features in our case just refers to the size of the input and the output\ntensor for the module, so 1 and 1. If we used both temperature and barometric pres-\nsure as input, for instance, we would have two features in input and one feature in out-\nput. As we will see, for more complex models with several intermediate modules, the\nnumber of features will be associated with the capacity of the model.\n\nWe have an instance of nn.Linear with one input and one output feature. That\nonly requires one weight and one bias:\n\n# In[6):\nlinear_model.weight\n\n# Out[6]:\nParameter containing:\ntensor ([{[-0.0674])], requires_grad=True)\n\n# In[7):\nlinear_model.bias\n\n# Out[7):\nParameter containing:\ntensor((0.7488], requires_grad=True)\n\nWe can call the module with some input:\n\n# In[8]:\nx = torch.ones(1)\nlinear_model (x)\n\n# Out[8]:\ntensor ([0.6814), grad_fn=<AddBackward0>)\n\nAlthough PyTorch lets us get away with it, we don\u2019t actually provide an input with the\nright dimensionality. We have a model that takes one input and produces one output,\nbut PyTorch nn.Module and its subclasses are designed to do so on multiple samples at\nthe same time. To accommodate multiple samples, modules expect the zeroth dimen-\nsion of the input to be the number of samples in the batch. We encountered this con-\ncept in chapter 4, when we learned how to arrange real-world data into tensors.\n\nBATCHING INPUTS\n\nAny module in nn is written to produce outputs for a batch of multiple inputs at the\nsame time. Thus, assuming we need to run nn. Linear on 10 samples, we can create an\ninput tensor of size B x Nin, where B is the size of the batch and Nin is the number of\ninput features, and run it once through the model. For example:\n\n# In(9]:\nx = torch.ones(10, 1)\nlinear_model (x)\n\n# Out [9]:\n\ntensor ([[0.6814],\n(0.6814),\n(0.6814),\n(0.6814),\n(0.6814),\n(0.6814),\n(0.6814),\n(0.6814),\n(0.6814),\n(0.6814]], grad_fn=<AddmmBackward>)\n\nLet's dig into what\u2019s going on here, with figure 6.7 showing a similar situation with\nbatched image data. Our input is B x C x H x Wwith a batch size of 3 (say, images\nof a dog, a bird, and then a car), three channel dimensions (red, green, and blue),\nand an unspecified number of pixels for height and width. As we can see, the out-\nput is a tensor of size B x Nout, where Nout is the number of output features: four, in\nthis case.\n\nFigure 6.7 Three\nRGB images batched\ntogether and fed into\na neural network. The\noutput is a batch of\nthree vectors of size 4.\n\nOPTIMIZING BATCHES\n\nThe reason we want to do this batching is multifaceted. One big motivation is to make\nsure the computation we're asking for is big enough to saturate the computing\nresources we're using to perform the computation. GPUs in particular are highly par-\nallelized, so a single input on a small model will leave most of the computing units idle.\nBy providing batches of inputs, the calculation can be spread across the otherwise-idle\nunits, which means the batched results come back just as quickly as a single result\nwould. Another benefit is that some advanced models use statistical information from\nthe entire batch, and those statistics get better with larger batch sizes.\n\nBack to our thermometer data, t_u and t_c were two 1D tensors of size B. Thanks\nto broadcasting, we could write our linear model as w * x + b, where w and b were\ntwo scalar parameters. This worked because we had a single input feature: if we had\ntwo, we would need to add an extra dimension to turn that 1D tensor into a matrix\nwith samples in the rows and features in the columns.\n\nThat's exactly what we need to do to switch to using nn. Linear. We reshape our B\ninputs to Bx Nin, where Nin is 1. That is easily done with unsqueeze:\n\n# In[2]:\n\ntic = (0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0)\ntu = (35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4)\nt. = t het = t . 1 \" + \u2018,\n\n= ore ensor (t_c) .unsqueeze(1) Adds the extra dimension at axis 1\n\nt_u = torch. tensor (t_u) .unsqueeze(1)\n\nt_u.shape\n\n# Out[2):\n\ntorch.Size({i1, 1))\n\nWe're done; let\u2019s update our training code. First, we replace our handmade model\nwith nn.Linear (1,1), and then we need to pass the linear model parameters to the\noptimizer:\n\n# In{10]: This is just a redefinition\nlinear_model = nn.Linear(1, 1) t . from earlier.\noptimizer = optim.SGD(\n\nlinear_model.parameters(), <+ This method call\n\nlr=le-2) replaces [params].\n\nEarlier, it was our responsibility to create parameters and pass them as the first argu-\nment to optim. SGD. Now we can use the parameters method to ask any nn. Module for\na list of parameters owned by it or any of its submodules:\n\n# In(11]:\nlinear_model .parameters ()\n\n# Out[11):\n<generator object Module.parameters at 0x7\u00a394b4a8a750>\n\n# In(12]:\nlist (linear_model.parameters())\n\n# Out[12):\n[Parameter containing:\ntensor([[{0.7398]], requires_grad=True), Parameter containing:\n\ntensor([0.7974], requires_grad=True) ]\n\nThis call recurses into submodules defined in the module's init constructor and\nreturns a flat list of all parameters encountered, so that we can conveniently pass it to\nthe optimizer constructor as we did previously.\n\nWe can already figure out what happens in the training loop. The optimizer is pro-\nvided with a list of tensors that were defined with requires_grad = True\u2014all Parameters\nare defined this way by definition, since they need to be optimized by gradient descent.\nWhen training_loss.backward() is called, grad is accumulated on the leaf nodes of the\ngraph, which are precisely the parameters that were passed to the optimizer.\n\nAt this point, the SGD optimizer has everything it needs. When optimizer.step()\nis called, it will iterate through each Parameter and change it by an amount propor-\ntional to what is stored in its grad attribute. Pretty clean design.\n\nLet\u2019s take a look a the training loop now:\n\n# In[{13]:\ndef training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,\ntic_train, t_c_val):\nfor epoch in range(1, n_epochs + 1):\nt_p_train = model (t_u_train) co\n\nloss_train = loss_fn(t_p_train, t_c_train) The model is now\n\n| passed in, instead of\n\nt_p_val = model (t_u_val) J the t ual S:\n\nloss_val = loss_fn(t_p_val, t_c_val)\n\noptimizer.zero_grad()\nloss_train.backward()\n\n_\u2014\u2014 \u201c The loss function is also passed\noptimizer.step()\n\nin. We'll use it in a moment.\n\nif epoch == 1 or epoch % 1000 ==\nprint(f\"Epoch (epoch), Training loss {loss_train.item():.4\u00a3},\"\n\u00a3\" Validation loss (loss_val.item():.4f)\")\n\nIt hasn\u2019t changed practically at all, except that now we don\u2019t pass params explicitly to\nmodel since the model itself holds its Parameters internally.\n\nThere\u2019s one last bit that we can leverage from torch.nn: the loss. Indeed, nn comes\nwith several common loss functions, among them nn.MSELoss (MSE stands for Mean\nSquare Error), which is exactly what we defined earlier as our loss_fn. Loss functions\nin nn are still subclasses of nn.Module, so we will create an instance and call it as a\nfunction. In our case, we get rid of the handwritten loss_fn and replace it:\n\n# In[15):\nlinear_model = nn.Linear(1, 1)\noptimizer = optim.SGD(linear_model.parameters(), 1lr=le-2)\n\ntraining_loop(\nn_epochs = 3000,\noptimizer = optimizer,\nmodel = linear_model, We are no longer using our hand-\nloss_fn = nn.MSELoss(), * written loss function from earlier.\n\ntlu_train = t_un_train,\nt_u_val = t_un_val,\ntle_train = t_c_train,\ntic_val = t_c_val)\n\nprint (}\nprint (linear_model.weight)\nprint (linear_model.bias)\n\n# Out[i5]:\n\nEpoch 1, Training loss 134.9599, Validation loss 183.1707\nEpoch 1000, Training loss 4.8053, Validation loss 4.7307\nEpoch 2000, Training loss 3.0285, Validation loss 3.0889\nEpoch 3000, Training loss 2.8569, Validation loss 3.9105\n\nParameter containing:\ntensor ([{[5.4319]], requires_grad=True)\nParameter containing:\ntensor ([{-17.9693], requires_grad=True)\n\nEverything else input into our training loop stays the same. Even our results remain\nthe same as before. Of course, getting the same results is expected, as a difference\nwould imply a bug in one of the two implementations.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.12,
                        "section_name": "Finally a neural network",
                        "section_path": "./screenshots-images-2/chapter_7/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_12/f207d11e-dd97-4895-8cdf-2bfff79f2668.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Finally a neural network\n\nIt\u2019s been a long journey\u2014there has been a lot to explore for these 20-something lines\nof code we require to define and train a model. Hopefully by now the magic involved\nin training has vanished and left room for the mechanics. What we learned so far will\nallow us to own the code we write instead of merely poking at a black box when things\nget more complicated.\n\nThere\u2019s one last step left to take: replacing our linear model with a neural network\nas our approximating function. We said earlier that using a neural network will not\nresult in a higher-quality model, since the process underlying our calibration problem\nwas fundamentally linear. However, it\u2019s good to make the leap from linear to neural\nnetwork in a controlled environment so we won't feel lost later.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.13,
                        "section_name": "Replacing the linear model",
                        "section_path": "./screenshots-images-2/chapter_7/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_13/96d2fdcf-956b-4f5f-b3c8-f5d3ea0f46d0.png",
                            "./screenshots-images-2/chapter_7/section_13/3b2ae596-acbc-4b7a-8736-8b1c374e1808.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Replacing the linear model\nWe are going to keep everything else fixed, including the loss function, and only rede-\nfine model. Let\u2019s build the simplest possible neural network: a linear module, followed\nby an activation function, feeding into another linear module. The first linear + activa-\ntion layer is commonly referred to as a hidden layer for historical reasons, since its out-\nputs are not observed directly but fed into the output layer. While the input and output\nof the model are both of size 1 (they have one input and one output feature), the size\nof the output of the first linear module is usually larger than 1. Recalling our earlier\nexplanation of the role of activations, this can lead different units to respond to different\nranges of the input, which increases the capacity of our model. The last linear layer will\ntake the output of activations and combine them linearly to produce the output value.\nThere is no standard way to depict neural networks. Figure 6.8 shows two ways that\nseem to be somewhat prototypical: the left side shows how our network might be\ndepicted in basic introductions, whereas a style similar to that on the right is often\nused in the more advanced literature and research papers. It is common to make dia-\ngram blocks that roughly correspond to the neural network modules PyTorch offers\n(though sometimes things like the Tanh activation layer are not explicitly shown).\nNote that one somewhat subtle difference between the two is that the graph on the\nleft has the inputs and (intermediate) results in the circles as the main elements. On\nthe right, the computational steps are more prominent.\n\nOUTPUT (Id)\nt\n\nEN\n\u2014\n\nt\n\n\\NPUT (ld) Figure 6.8 Our simplest neural\n\nnetwork in two views. Left: beginner's\nversion. Right: higher-level version.\n\nnn provides a simple way to concatenate modules through the nn.Sequential\n\ncontainer:\n# In(16]: We chose 13 arbitrarily. We wanted a number\nseq_model = nn.Sequential ( that was a different size from the other\nnn.Linear(i, 13), tensor shapes we have floating around.\nnn.Tanh(),\nnn.Linear(13, 1)) oss This 13 must h\nseq_model the first size, however.\n# Out[16):\nSequential (\n(0): Linear(in_features=1, out_features=13, bias=True)\n(1): Tanh()\n\n(2): Linear (in_features=13, out_features=1, bias=True)\n\nThe end result is a model that takes the inputs expected by the first module specified\nas an argument of nn.Sequential, passes intermediate outputs to subsequent mod-\nules, and produces the output returned by the last module. The model fans out from\n1 input feature to 13 hidden features, passes them through a tanh activation, and lin-\nearly combines the resulting 13 numbers into | output feature.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.14,
                        "section_name": "Inspecting the parameters",
                        "section_path": "./screenshots-images-2/chapter_7/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_14/ce986e21-d274-4a5c-aa0d-93bf8eee0cdd.png",
                            "./screenshots-images-2/chapter_7/section_14/08d936a9-b32d-47bc-b46e-827cc9c5c1cd.png",
                            "./screenshots-images-2/chapter_7/section_14/f8644c03-e834-49a4-b68f-492263c0761a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Inspecting the parameters\n\nCalling model.parameters () will collect weight and bias from both the first and sec-\nond linear modules. It\u2019s instructive to inspect the parameters in this case by printing\ntheir shapes:\n\n# In[17]:\n(param.shape for param in seq_model.parameters()]\n\n# Out[i7]:\n(torch.Size((13, 1]), torch.Size([13]), torch.Size((1, 13]), torch.Size([1])]\n\nThese are the tensors that the optimizer will get. Again, after we call model . backward (),\nall parameters are populated with their grad, and the optimizer then updates their val-\nues accordingly during the optimizer .step() call. Not that different from our previous\nlinear model, eh? After all, they\u2019re both differentiable models that can be trained using\ngradient descent.\n\nA few notes on parameters of nn. Modules. When inspecting parameters of a model\nmade up of several submodules, it is handy to be able to identify parameters by name.\nThere\u2019s a method for that, called named_parameters:\n\n# In[18):\nfor name, param in seq_model.named_parameters():\nprint(name, param.shape)\n\n# Out[i8):\nO.weight torch.Size([{13, 1])\n\nO.bias torch.Size([13])\n2.weight torch.Size([1, 13])\n2.bias torch.Size([1])\n\nThe name of each module in Sequential is just the ordinal with which the module\nappears in the arguments. Interestingly, Sequential also accepts an OrderedDict,' in\nwhich we can name each module passed to Sequential:\n\n# In[19]:\nfrom collections import OrderedDict\n\nseq_model = nn.Sequential (OrderedDict ([\n({'hidden_linear\u2019, nn.Linear(1, 8)),\n{'hidden_activation', nn.Tanh()),\n{'output_linear', nn.Linear(8, 1))\n\n)))\nseq_model\n\n# Out[19]):\n\nSequential (\n(hidden_linear): Linear(in_features=1, out_features=8, bias=True)\n(hidden_activation): Tanh()\n(output_linear): Linear(in_features=8, out_features=1, bias=True)\n\nThis allows us to get more explanatory names for submodules:\n\n# In[20]:\nfor name, param in seq_model.named_parameters():\nprint(name, param.shape)\n\n# Out[20):\n\nhidden_linear.weight torch.Size([8, 1])\nhidden_linear.bias torch.Size([8]})\noutput_linear.weight torch.Size([{1, 8])\noutput_linear.bias torch.Size([1)])\n\nThis is more descriptive; but it does not give us more flexibility in the flow of data\nthrough the network, which remains a purely sequential pass-through\u2014the\nnn. Sequential is very aptly named. We will see how to take full control of the process-\ning of input data by subclassing nn .Module ourselves in chapter 8.\n\nWe can also access a particular Parameter by using submodules as attributes:\n\n# In[21):\nseq_model.output_linear.bias\n\n# Out[21):\nParameter containing:\ntensor([-0.0173], requires_grad=True)\n\nThis is useful for inspecting parameters or their gradients: for instance, to monitor\ngradients during training, as we did at the beginning of this chapter. Say we want to\nprint out the gradients of weight of the linear portion of the hidden layer. We can run\nthe training loop for the new neural network model and then look at the resulting\ngradients after the last epoch:\n\n# In[22]:\n\noptimizer = optim.SGD(seq_model.parameters(), lr=le-3) eel We've dropped the\nsoe learning rate a bit to\n\ntraining loop ( help with stability.\n\nn_epochs = 5000,\noptimizer = optimizer,\nmodel = seq_model,\nloss_fn = nn.MSELoss(),\nt_u_train = t_un_train,\nt_u_val = t_un_val,\ntie_train = t_ec_train,\ntic_val = t_c_val)\n\nprint(*output', seq_model (t_un_val))\nprint(\u2018answer', t_c_val)\nprint (*hidden', seq_model.hidden_linear.weight.grad)\n\n# Out[22]:\nEpoch 1, Training loss 182.9724, Validation loss 231.8708\n\nEpoch 1000, Training loss 6.6642, Validation loss 3.7330\nEpoch 2000, Training loss 5.1502, Validation loss 0.1406\nEpoch 3000, Training loss 2.9653, Validation loss 1.0005\nEpoch 4000, Training loss 2.2839, Validation loss 1.6580\nEpoch 5000, Training loss 2.1141, Validation loss 2.0215\n\noutput tensor ([[-1.9930],\n\n(20.8729]], grad_fn=<AddmmBackward>)\nanswer tensor([[-4.],\n\n(21.)])\nhidden tensor([[ 0.0272],\n\n[ 0.0139),\n[ 0.1692),\n{ 0.1735),\n[-0.1697),\n[ 0.1455),\n[-0.0136),\n[-0.0554)])\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.15,
                        "section_name": "Comparing to the linear model",
                        "section_path": "./screenshots-images-2/chapter_7/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_15/8b697376-76a4-46bf-beef-378223b274c9.png",
                            "./screenshots-images-2/chapter_7/section_15/1d0d7212-4f6a-44f1-b1ff-646b6aa0083b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Comparing to the linear model\nWe can also evaluate the model on all of the data and see how it differs from a line:\n\n# In[23):\nfrom matplotlib import pyplot as plt\n\nt_range = torch.arange(20., 90.) .unsqueeze(1)\n\nfig = plt.figure(dpi=600)\n\nplt.\nplt.\nplt.\nplt.\nplt.\n\nxlabel (\"Fahrenheit\")\nylabel (\"Celsius\")\n\nplot(t_u.numpy(), tc.numpy(), 'o')\nplot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), \u2018c-\")\nplot (t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')\n\nThe result is shown in figure 6.9. We can appreciate that the neural network has a ten-\ndency to overfit, as we discussed in chapter 5, since it tries to chase the measurements,\nincluding the noisy ones. Even our tiny neural network has too many parameters to fit\nthe few measurements we have. It doesn\u2019t do a bad job, though, overall.\n\nCELSIUS\n\n20 20 4o 50 Go To 80 40\nFAHRENHETT\n\nFigure 6.9 The plot of our neural network model, with input data (circles) and\nmodel output (Xs). The continuous line shows behavior between samples.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.16,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_7/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_16/cc795360-758a-45f9-90ba-9a781d410956.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\n\nWe've covered a lot in chapters 5 and 6, although we have been dealing with a very\nsimple problem. We dissected building differentiable models and training them using\ngradient descent, first using raw autograd and then relying on nn. By now you should\nhave confidence in your understanding of what\u2019s going on behind the scenes. Hope-\nfully this taste of PyTorch has given you an appetite for more!\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 8,
                "chapter_name": "Telling birds\nfrom airplanes:\nLearning from images",
                "chapter_path": "./screenshots-images-2/chapter_8",
                "sections": [
                    {
                        "section_id": 8.1,
                        "section_name": "Telling birds\nfrom airplanes:\nLearning from images",
                        "section_path": "./screenshots-images-2/chapter_8/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_1/891fb55b-2cdc-4d7c-9743-70617d739de7.png",
                            "./screenshots-images-2/chapter_8/section_1/abcff284-e8cf-4990-b630-2365fc58aec0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The last chapter gave us the opportunity to dive into the inner mechanics of learn-\ning through gradient descent, and the facilities that PyTorch offers to build models\nand optimize them. We did so using a simple regression model of one input and\none output, which allowed us to have everything in plain sight but admittedly was\nonly borderline exciting.\n\nIn this chapter, we'll keep moving ahead with building our neural network foun-\ndations. This time, we'll turn our attention to images. Image recognition is argu-\nably the task that made the world realize the potential of deep learning.\n\nWe will approach a simple image recognition problem step by step, building from\na simple neural network like the one we defined in the last chapter. This time, instead\nof a tiny dataset of numbers, we'll use a more extensive dataset of tiny images. Let\u2019s\ndownload the dataset first and get to work preparing it for use.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.2,
                        "section_name": "A dataset of tiny images",
                        "section_path": "./screenshots-images-2/chapter_8/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_2/4d52faad-f0a0-42f6-bfe8-27bbd7c4315d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A dataset of tiny images\n\nThere is nothing like an intuitive understanding of a subject, and there is nothing to\nachieve that like working on simple data. One of the most basic datasets for image\nrecognition is the handwritten digit-recognition dataset known as MNIST. Here\nwe will use another dataset that is similarly simple and a bit more fun. It\u2019s called\nCIFAR-10, and, like its sibling CIFAR-100, it has been a computer vision classic for\na decade.\n\nCIFAR-10 consists of 60,000 tiny 32 x 32 color (RGB) images, labeled with an inte-\nger corresponding to 1 of 10 classes: airplane (0), automobile (1), bird (2), cat (3),\ndeer (4), dog (5), frog (6), horse (7), ship (8), and truck (9).' Nowadays, CIFAR-10 is\nconsidered too simple for developing or validating new research, but it serves our\nlearning purposes just fine. We will use the torchvision module to automatically\ndownload the dataset and load it as a collection of PyTorch tensors. Figure 7.1 gives us\na taste of CIFAR-10.\n\nATRELANE\n\nAUTOMOBILE\n\nFigure 7.1 Image samples from all CIFAR-10 classes\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.3,
                        "section_name": "Downloading CIFAR-10",
                        "section_path": "./screenshots-images-2/chapter_8/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_3/1e556964-b554-463e-9dcc-95ad568051ce.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "7.L1 Downloading CIFAR-10\n\nAs we anticipated, let\u2019s import torchvision and use the datasets module to down-\nload the CIFAR-10 data:\n\nInstantiates a dataset for the training data;\n\nTorchVision downloads the data if it is not present. With train=False, this gets us a\n\n# In(2]: dataset for the validation data,\nfrom torchvision import datasets again downloading as necessary.\ndata_path = '../data-unversioned/pich7/*\n\n\u201c> cifarl0 = datasets.CIFAR10(data_path, train=True, download=True)\ncifarl0_val = datasets.CIFAR10(data_path, train=False, download=True) %\n\nThe first argument we provide to the CIFAR1O function is the location from which the\ndata will be downloaded; the second specifies whether we're interested in the training\nset or the validation set; and the third says whether we allow PyTorch to download the\ndata if it is not found in the location specified in the first argument.\n\nJust like CIFAR10, the datasets submodule gives us precanned access to the most\npopular computer vision datasets, such as MNIST, Fashion-MNIST, CIFAR-100,\nSVHN, Coco, and Omniglot. In each case, the dataset is returned as a subclass of\ntorch.utils.data.Dataset. We can see that the method-resolution order of our\ncifar10 instance includes it as a base class:\n\n# In[4]:\ntype(cifar10).__mro__\n\n# Out[4]:\n(torchvision.datasets.cifar.CIFAR10,\ntorchvision.datasets.vision.VisionDataset,\ntorch.utils.data.dataset.Dataset,\nobject)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.4,
                        "section_name": "The Dataset class",
                        "section_path": "./screenshots-images-2/chapter_8/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_4/ccc909e1-6c9d-466f-8deb-9ab7a727c133.png",
                            "./screenshots-images-2/chapter_8/section_4/da239d86-3233-4701-ab55-78956b523d7f.png",
                            "./screenshots-images-2/chapter_8/section_4/ae20f2f3-135a-469d-8957-50010d63f9ac.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "7.12\n\nThe Dataset class\nIt\u2019s a good time to discover what being a subclass of torch.utils.data.Dataset\nmeans in practice. Looking at figure 7.2, we see what PyTorch Dataset is all about. It\nis an object that is required to implement two methods: __len__ and __getitem\nThe former should return the number of items in the dataset; the latter should return\nthe item, consisting of a sample and its corresponding label (an integer index).*\n\nIn practice, when a Python object is equipped with the __len__ method, we can\npass it as an argument to the len Python built-in function:\n\n# In(5]:\nlen(cifari0)\n\n# Out[5]:\n50000\n\nACTUAL DATASET @: HOW MANY ELEMENTS 2\n\nPATA y\u00bb len (2-dataset))\nSs ve\nake a :\nWOMAN DOG @ MAY TL GET ITEM 4?\nEl wy adotaset [a]\nR\u201c\u00a27 .\n= (El)\n\nFigure 7.2 Concept of a PyTorch Dataset object: it doesn\u2019t necessarily hold the data, but it\nprovides uniform access to it through len and getitem__\n\nSimilarly, since the dataset is equipped with the __getitem__ method, we can use the\nstandard subscript for indexing tuples and lists to access individual items. Here, we get\na PIL (Python Imaging Library, the PIL package) image with our desired output\u2014an\ninteger with the value 1, corresponding to \u201cautomobile\u201d:\n\n# In[6]:\nimg, label = cifar10[99)\nimg, label, class_names [label]\n\n# Out[6):\n\n(<PIL.Image.Image image mode=RGB size=32x32 at Ox7FB383657390>,\ni,\n\n*automobile')\n\nSo, the sample in the data.CIFAR10 dataset is an instance of an RGB PIL image. We\ncan plot it right away:\n# In[7]}:\n\nplt.imshow (img)\nplt.show()\n\nThis produces the output shown in figure 7.3. It\u2019s a red car!*\n\nFigure 7.3 The 99th image from the\nCIFAR-10 dataset: an automobile\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.5,
                        "section_name": "Dataset transforms",
                        "section_path": "./screenshots-images-2/chapter_8/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_5/9e905732-809f-4aa4-8bdb-aa769dbcb77c.png",
                            "./screenshots-images-2/chapter_8/section_5/bbe7cbaf-8144-40a0-b2b7-e4c72a643576.png",
                            "./screenshots-images-2/chapter_8/section_5/60365fb8-58cc-4eae-a707-115c757e4ec2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "7.13\n\nDataset transforms\n\nThat\u2019s all very nice, but we'll likely need a way to convert the PIL image to a PyTorch\ntensor before we can do anything with it. That\u2019s where torchvision. transforms\ncomes in. This module defines a set of composable, function-like objects that can be\npassed as an argument to a torchvision dataset such as datasets .CIFAR10(...), and\nthat perform transformations on the data after it is loaded but before it is returned by\n__getitem_. We can see the list of available objects as follows:\n\n# In[(8]:\nfrom torchvision import transforms\ndir (transforms)\n\n# Out[8]:\n['CenterCrop',\n\u2018Colorditter',\n\n\u2018Normalize',\n'Pad',\n\u2018RandomAffine',\n\n\u2018RandomResizedCrop',\n'RandomRotation',\n\u2018RandomSizedCrop',\n\n\u2018'TenCrop',\n'ToPILImage',\n'ToTensor',\n\nAmong those transforms, we can spot ToTensor, which turns NumPy arrays and PIL\nimages to tensors. It also takes care to lay out the dimensions of the output tensor as\nCx Hx W (channel, height, width; just as we covered in chapter 4).\n\nLet\u2019s ty out the ToTensor transform. Once instantiated, it can be called like a\nfunction with the PIL image as the argument, returning a tensor as output:\n\n# In[9]:\nfrom torchvision import transforms\n\nto_tensor = transforms.ToTensor()\nimg_t = to_tensor (img)\nimg_t.shape\n\n# Out[9):\ntorch.Size([3, 32, 32])\n\nThe image has been turned into a 3 x 32 x 32 tensor and therefore a 3-channel (RGB)\n32 x 32 image. Note that nothing has happened to label]; it is still an integer.\n\nAs we anticipated, we can pass the transform directly as an argument to dataset\n.CIFAR10:\n\n# In[10):\ntensor_cifari0 = datasets.CIFAR10(data_path, train=True, download=False,\ntransform=transforms.ToTensor ())\n\nAt this point, accessing an element of the dataset will return a tensor, rather than a\nPIL image:\n\n# In[1i}:\nimg_t, _ = tensor_cifar10[99]\ntype (img_t)\n\n# Out(ii):\ntorch.Tensor\n\nAs expected, the shape has the channel as the first dimension, while the scalar type is\nfloat32:\n\n# In[12]:\nimg_t.shape, img_t.dtype\n\n# Out[i2]:\n(torch.Size((3, 32, 32]), torch. float32)\n\nWhereas the values in the original PIL image ranged from 0 to 255 (8 bits per chan-\nnel), the ToTensor transform turns the data into a 32-bit floating-point per channel,\nscaling the values down from 0.0 to 1.0. Let\u2019s verify that:\n\n# In[13]:\nimg_t.min(), img_t.max()\n\n# Out[13):\n(tensor(0.), tensor(1.))\n\nAnd let\u2019s verify that we're getting the same image out:\n\n# In[(14):\n\nplt. imshow (img, t.permute(1, 2, 0)) Changes the order of the axes from\nplt.show() CxHxWtoHxWxC\n\n# Out[14):\n\n<Figure size 432x288 with 1 Axes>\n\nAs we can see in figure 7.4, we get the same output as before.\n\nFigure 7.4 We've seen\nthis one already.\n\n\u00b0 io is 20 25 30\nIt checks. Note how we have to use permute to change the order of the axes from\n\nCx Hx W to H x W x C to match what Matplotlib expects.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.6,
                        "section_name": "Normalizing data",
                        "section_path": "./screenshots-images-2/chapter_8/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_6/3600d1ec-13ef-41e3-bb95-8e5fa9758e7f.png",
                            "./screenshots-images-2/chapter_8/section_6/1de13929-cf8d-4ffb-9f20-ac56c3bb453a.png",
                            "./screenshots-images-2/chapter_8/section_6/edd50535-ce28-4fb8-9cbc-17f5a00519a3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "7.14\n\nNormalizing data\n\nTransforms are really handy because we can chain them using transforms .Compose,\nand they can handle normalization and data augmentation transparently, directly in\nthe data loader. For instance, it\u2019s good practice to normalize the dataset so that each\nchannel has zero mean and unitary standard deviation. We mentioned this in chapter\n4, but now, after going through chapter 5, we also have an intuition for why: by choosing\nactivation functions that are linear around 0 plus or minus 1 (or 2), keeping the data\nin the same range means it\u2019s more likely that neurons have nonzero gradients and,\n\nhence, will learn sooner. Also, normalizing each channel so that it has the same\ndistribution will ensure that channel information can be mixed and updated through\ngradient descent using the same learning rate. This is just like the situation in section\n5.4.4 when we rescaled the weight to be of the same magnitude as the bias in our\ntemperature-conversion model.\n\nIn order to make it so that each channel has zero mean and unitary standard devi-\nation, we can compute the mean value and the standard deviation of each channel\nacross the dataset and apply the following transform: v_n[{c] = (v[c] - mean[c]) /\nstdev[c]. This is what transforms.Normalize does. The values of mean and stdev\nmust be computed offline (they are not computed by the transform). Let\u2019s compute\nthem for the CIFAR-10 training set.\n\nSince the CIFAR-10 dataset is small, we'll be able to manipulate it entirely in mem-\nory. Let\u2019s stack all the tensors returned by the dataset along an extra dimension:\n\n# In[15):\nimgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\nimgs.shape\n\n# Out [15]:\ntorch.Size((3, 32, 32, 50000)\n\nNow we can easily compute the mean per channel:\n\n# In[16): Recall that view(3, -1) keeps the three channels and\n\nimgs.view(3, -1) .mean(dim=1) merges all the remaining dimensions into one, figuring\nout the appropriate size. Here our 3 x 32 X 32 image is\n\n# Out[16]: transformed into a3 x 1,024 vector, and then the mean\n\ntensor ([0.4915, 0.4823, 0.4468)) is taken over the 1,024 elements of each channel.\nComputing the standard deviation is similar:\n\n# In[17):\nimgs.view(3, -1).std(dim=1)\n\n# Out[1i7]:\ntensor ([0.2470, 0.2435, 0.2616))\n\nWith these numbers in our hands, we can initialize the Normalize transform\n\n# In[18]:\ntransforms .Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n\n# Out[i8):\nNormalize (mean=(0.4915, 0.4823, 0.4468), std=(0.247, 0.2435, 0.2616))\n\nand concatenate it after the ToTensor transform:\n\n# In[19]:\ntransformed_cifari0 = datasets.CIFAR10(\ndata_path, train=True, download=False,\n\nNote that, at this point, plotting an image drawn from the dataset won't provide us\nwith a faithful representation of the actual image:\n\n# In[21]\nimg_t = ormed_cifar10[99\nplt.i 1ow(img_t.permute(1, 2, 0))\n\nplt.show()\n\nThe renormalized red car we get is shown in figure 7.5. This is because normalization\nhas shifted the RGB levels outside the 0.0 to 1.0 range and changed the overall magni-\ntudes of the channels. All of the data is still there; it\u2019s just that Matplotlib renders it as\nblack. We'll keep this in mind for the future.\n\nFigure 7.6 Our random CIFAR-10\n\u00b0 Ss \\o is 20 2s 30 image after normalization\n\nStill, we have a fancy dataset loaded that contains tens of thousands of images! That's\n\nquite convenient, because we were going to need something exactly like it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.7,
                        "section_name": "Distinguishing birds from airplanes",
                        "section_path": "./screenshots-images-2/chapter_8/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_7/920e1333-48b4-4aaa-9c0c-609916c0c137.png",
                            "./screenshots-images-2/chapter_8/section_7/9bab30e0-8675-480f-9506-650b9d3cb5aa.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Distinguishing birds from airplanes\n\nJane, our friend at the bird-watching club, has set up a fleet of cameras in the woods\nsouth of the airport. The cameras are supposed to save a shot when something enters\nthe frame and upload it to the club\u2019s real-time bird-watching blog. The problem is\nthat a lot of planes coming and going from the airport end up triggering the camera,\n\ngs\n\nFigure 7.6 The problem at hand: we're going to help our friend tell birds from airplanes\nfor her blog, by training a neural network to do the job.\n\nso Jane spends a lot of time deleting pictures of airplanes from the blog. What she\nneeds is an automated system like that shown in figure 7.6. Instead of manually delet-\ning, she needs a neural network\u2014an AI if we\u2019re into fancy marketing speak\u2014to throw\naway the airplanes right away.\n\nNo worries! We'll take care of that, no problem\u2014we just got the perfect dataset for\nit (what a coincidence, right?). We'll pick out all the birds and airplanes from our\nCIFAR-10 dataset and build a neural network that can tell birds and airplanes apart.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.8,
                        "section_name": "Building the dataset",
                        "section_path": "./screenshots-images-2/chapter_8/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_8/4a44d984-fa9c-4509-b3b4-45d7c5b3c076.png",
                            "./screenshots-images-2/chapter_8/section_8/7a1be334-9790-46ed-a931-cd0cbf1035ed.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "7.2.1\n\nBuilding the dataset\n\nThe first step is to get the data in the right shape. We could create a Dataset subclass\nthat only includes birds and airplanes. However, the dataset is small, and we only need\nindexing and len to work on our dataset. It doesn\u2019t actually have to be a subclass of\ntorch.utils.data.dataset.Dataset! Well, why not take a shortcut and just filter the\ndata in cifar10 and remap the labels so they are contiguous? Here\u2019s how:\n\n# In[5]}:\n\nlabel_map = (0: 0, 2: 1)\nclass_names = [\u2018airplane', \u2018bird']\ncifar2 = [(img, label_map[label))\n\nfor img, label in cifar10\nif label in [0, 2))\ncifar2_val = [(img, label_map[label])\nfor img, label in cifari0_val\nif label in [0, 2)]\n\nThe cifar2 object satisfies the basic requirements for a Dataset\u2014that is, __len__ and\n__getitem__ are defined\u2014so we're going to use that. We should be aware, however,\nthat this is a clever shortcut and we might wish to implement a proper Dataset if we\nhit limitations with it.*\n\nWe have a dataset! Next, we need a model to feed our data to.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.9,
                        "section_name": "A fully connected model",
                        "section_path": "./screenshots-images-2/chapter_8/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_9/65d0ffe1-f35d-4684-b892-cfbfa2aa4ce8.png",
                            "./screenshots-images-2/chapter_8/section_9/597c4b55-8f37-4ca6-929e-cf62c7c858bc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "7.2.2 A fully connected model\n\nWe learned how to build a neural network in chapter 5. We know that it\u2019s a tensor of\nfeatures in, a tensor of features out. After all, an image is just a set of numbers laid out\nin a spatial configuration. OK, we don\u2019t know how to handle the spatial configuration\npart just yet, but in theory if we just take the image pixels and straighten them into a\nlong 1D vector, we could consider those numbers as input features, right? This is what\nfigure 7.7 illustrates.\n\nLet's try that. How many features per sample? Well, 32 x 32 x 3: that is, 3,072 input\nfeatures per sample. Starting from the model we built in chapter 5, our new model\nwould be an nn. Linear with 3,072 input features and some number of hidden features,\n\nie [e]=]w[\u00bb [Ele]\n\no > Prog\n\no \u2014S Proman\n\n[ale\n\n|\n4\noo 9 9 9\n\n\u00b0\noo 09090 0 9 8G G@\n\nFigure 7.7 Treating our image as a 1D vector of values and training a fully connected classifier\non it\n\nfollowed by an activation, and then another nn. Linear that tapers the network down to\nan appropriate output number of features (2, for this use case):\n\n# In[6):\nimport torch.nn as nn\n\nn_out = 2\nmodel = nn.Sequential(\n\nnn.Linear (\nInput features \u2014- 3072,\n\n512, <}\n_ |\nnn.Tanh(), Hidden layer size\nnn.Linear(\n\n512, <t\n\nOutput classes \u2014> n_out,\n)\n\nWe somewhat arbitrarily pick 512 hidden features. A neural network needs at least\none hidden layer (of activations, so two modules) with a nonlinearity in between in\norder to be able to learn arbitrary functions in the way we discussed in section 6.3\u2014\notherwise, it would just be a linear model. The hidden features represent (learned)\nrelations between the inputs encoded through the weight matrix. As such, the model\nmight learn to \u201ccompare\u201d vector elements 176 and 208, but it does not a priori focus\non them because it is structurally unaware that these are, indeed (row 5, pixel 16) and\n(row 6, pixel 16), and thus adjacent.\nSo we have a model. Next we'll discuss what our model output should be.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.1,
                        "section_name": "Output of a classifier",
                        "section_path": "./screenshots-images-2/chapter_8/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_10/c95d71af-0477-4e58-bbdc-92bdba282f22.png",
                            "./screenshots-images-2/chapter_8/section_10/a6125bcf-f172-46e9-a635-c86988fe0be4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Output of a classifier\n\nIn chapter 6, the network produced the predicted temperature (a number with a\nquantitative meaning) as output. We could do something similar here: make our net-\nwork output a single scalar value (so n_out = 1), cast the labels to floats (0.0 for air-\nplane and 1.0 for bird), and use those as a target for MSELoss (the average of squared\ndifferences in the batch). Doing so, we would cast the problem into a regression prob-\nlem. However, looking more closely, we are now dealing with something a bit different\nin nature.\u00ae\n\nWe need to recognize that the output is categorical: it\u2019s either a bird or an air-\nplane (or something else if we had all 10 of the original classes). As we learned in\nchapter 4, when we have to represent a categorical variable, we should switch to a\none-hot-encoding representation of that variable, such as [1, 0] for airplane or [0, 1]\n\nfor bird (the order is arbitrary). This will still work if we have 10 classes, as in the full\nCIFAR-10 dataset; we'll just have a vector of length 10.\u00b0\n\nIn the ideal case, the network would output torch.tensor([1.0, 0.0]) for an air-\nplane and torch.tensor([0.0, 1.0]) fora bird. Practically speaking, since our clas-\nsifier will not be perfect, we can expect the network to output something in between.\nThe key realization in this case is that we can interpret our output as probabilities: the\nfirst entry is the probability of \u201cairplane,\u201d and the second is the probability of \u201cbird.\u201d\n\nCasting the problem in terms of probabilities imposes a few extra constraints on\nthe outputs of our network:\n\n= Each element of the output must be in the [0.0, 1.0] range (a probability of\nan outcome cannot be less than 0 or greater than 1).\n\n= The elements of the output must add up to 1.0 (we're certain that one of the\ntwo outcomes will occur).\n\nIt sounds like a tough constraint to enforce in a differentiable way on a vector of num-\nbers. Yet there\u2019s a very smart trick that does exactly that, and it\u2019s differentiable: it\u2019s\ncalled softmax.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.11,
                        "section_name": "Representing the output as probabilities",
                        "section_path": "./screenshots-images-2/chapter_8/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_11/9197ec64-da0b-4d99-a159-1f103b397a45.png",
                            "./screenshots-images-2/chapter_8/section_11/92323dad-8746-4d2a-ab12-1e60de743ada.png",
                            "./screenshots-images-2/chapter_8/section_11/ea5584a9-5c18-47f2-8827-43b49f5d111e.png",
                            "./screenshots-images-2/chapter_8/section_11/47d106b1-20ae-4790-9a8a-ef915d6b6af2.png",
                            "./screenshots-images-2/chapter_8/section_11/0b609444-f372-43ef-93e3-9d5afdb3bd76.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "7.2.4\n\nRepresenting the output as probabilities\n\nSoftmax is a function that takes a vector of values and produces another vector of the\nsame dimension, where the values satisfy the constraints we just listed to represent\nprobabilities. The expression for softmax is shown in figure 7.8.\n\nThat is, we take the elements of the vector, compute the elementwise exponential,\nand divide each element by the sum of exponentials. In code, it\u2019s something like this:\n\n# In{7]:\ndef softmax(x):\nreturn torch.exp(x) / torch.exp(x) .sum()\n\nLet's test it on an input vector:\n\n# In[(8]:\nx = torch.tensor([1.0, 2.0, 3.0])\n\nsoftmax (x)\n\n# Out[8]:\ntensor([0.0900, 0.2447, 0.6652))\n\nOK gt\ne\u201cse\" e* e\u2122 em\n\u2014_\u2014 \u2014\u2014_ =\u00a7 \u2014\u2014\u2014\"= 1\nEACH ELEMENT e+e\u201d . e* ea es em\nO AND | | SUM OF ELEMENTS\no EQUALS |\n@ @\nsot max (xi,.x2) = (5 x)\nLa ho Xs\n\u00a2 & e\nsett max (x, fa.Xs) = actee\u2122 | Seems | eXee\u00ae a)\n\nx,\n\nx, -\n\n& +.-2\n\nFigure 7.8 Handwritten softmax\n\nAs expected, it satisfies the constraints on probability:\n\n# In[9):\nsoftmax(x) .sum()\n\n# Out[9):\ntensor (i.)\n\nSoftmax is a monotone function, in that lower values in the input will correspond to\nlower values in the output. However, it\u2019s not scale invariant, in that the ratio between\nvalues is not preserved. In fact, the ratio between the first and second elements of the\ninput is 0.5, while the ratio between the same elements in the output is 0.3678. This is\nnota real issue, since the learning process will drive the parameters of the model in a\n\nway that values have appropriate ratios.\n\nThe nn module makes softmax available as a module. Since, as usual, input tensors\nmay have an additional batch Oth dimension, or have dimensions along which they\nencode probabilities and others in which they don\u2019t, nn.Softmax requires us to specify\n\nthe dimension along which the softmax function is applied:\n\n# In[10):\nsoftmax = nn.Softmax(dim=1)\n\nx = torch.tensor([[1.0, 2.0, 3.0],\n(1.0, 2.0, 3.0)])\n\nsoftmax (x)\n\n# Out[10):\ntensor([[0.0900, 0.2447, 0.6652),\n(0.0900, 0.2447, 0.6652)))\n\nIn this case, we have two input vectors in two rows (just like when we work with\nbatches), so we initialize nn. Softmax to operate along dimension 1.\n\nExcellent! We can now add a softmax at the end of our model, and our network\nwill be equipped to produce probabilities:\n\n# In[{11]:\nmodel =\n\n. Sequential (\nnn.Linear(3072, 512),\nnn.Tanh(),\nnn.Linear(512, 2),\nnn.Softmax(dim=1) }\n\nWe can actually try running the model before even training it. Let\u2019s do it, just to see\nwhat comes out. We first build a batch of one image, our bird (figure 7.9):\n\n# In(12]:\n\nimg, _ = cifar2[0]\nplt.imshow(img.permute(1, 2, 0))\nplt.show()\n\nFigure 7.9 A random bird\nfrom the CIFAR-10 dataset\n\u00b0 =} \\o \\5 20 25 30 (after normalization)\n\n\nOh, hello there. In order to call the model, we need to make the input have the right\ndimensions. We recall that our model expects 3,072 features in the input, and that nn\nworks with data organized into batches along the zeroth dimension. So we need to\nturn our 3 x 32 x 32 image into a 1D tensor and then add an extra dimension in the\nzeroth position. We learned how to do this in chapter 3:\n\n# In[13):\nimg_batch = img.view(-1) .unsqueeze(0)\n\nNow we\u2019re ready to invoke our model:\n\n# In[14):\nout = model (img_batch)\nout\n\n# Out (14):\ntensor([{[0.4784, 0.5216]], grad_fn=<SoftmaxBackward>)\n\nSo, we got probabilities! Well, we know we shouldn\u2019t get too excited: the weights and\nbiases of our linear layers have not been trained at all. Their elements are initialized\nrandomly by PyTorch between \u20141.0 and 1.0. Interestingly, we also see grad_fn for the\noutput, which is the tip of the backward computation graph (it will be used as soon as\nwe need to backpropagate).\u201d\n\nIn addition, while we know which output probability is supposed to be which\n(recall our class_names), our network has no indication of that. Is the first entry \u201cair-\nplane\u201d and the second \u201cbird,\u201d or the other way around? The network can\u2019t even tell\nthat at this point. It\u2019s the loss function that associates a meaning with these two num-\nbers, after backpropagation. If the labels are provided as index 0 for \u201cairplane\u201d and\nindex 1 for \u201cbird,\u201d then that\u2019s the order the outputs will be induced to take. Thus,\nafter training, we will be able to get the label as an index by computing the argmax of\nthe output probabilities: that is, the index at which we get the maximum probability.\nConveniently, when supplied with a dimension, torch.max returns the maximum ele-\nment along that dimension as well as the index at which that value occurs. In our case,\nwe need to take the max along the probability vector (not across batches), therefore,\ndimension 1:\n\n# In[15]:\n_, index = torch.max(out, dim=1)\n\nindex\n\n# Out[i5]:\ntensor ({1])\n\nIt says the image is a bird. Pure luck. But we have adapted our model output to the\nclassification task at hand by getting it to output probabilities. We also have now run\nour model against an input image and verified that our plumbing works. Time to get\ntraining. As in the previous two chapters, we need a loss to minimize during training.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.12,
                        "section_name": "A loss for classifying",
                        "section_path": "./screenshots-images-2/chapter_8/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_12/790a4c80-3cda-406d-8482-dd28c54bd552.png",
                            "./screenshots-images-2/chapter_8/section_12/550e8449-a184-43a9-be27-e88da112f6ae.png",
                            "./screenshots-images-2/chapter_8/section_12/0a8eabe5-1749-41a5-9757-ddbad8c96118.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A loss for classifying\n\nWe just mentioned that the loss is what gives probabilities meaning. In chapters 5 and\n6, we used mean square error (MSE) as our loss. We could still use MSE and make our\noutput probabilities converge to [0.0, 1.0] and [1.0, 0.0]. However, thinking about\nit, we\u2019re not really interested in reproducing these values exactly. Looking back at the\nargmax operation we used to extract the index of the predicted class, what we're really\ninterested in is that the first probability is higher than the second for airplanes and vice\nversa for birds. In other words, we want to penalize misclassifications rather than pains-\ntakingly penalize everything that doesn\u2019t look exactly like a 0.0 or 1.0.\n\nWhat we need to maximize in this case is the probability associated with the correct\nclass, out [class_index], where out is the output of softmax and class_index is a vec-\ntor containing 0 for \u201cairplane\u201d and 1 for \u201cbird\u201d for each sample. This quantity\u2014that\nis, the probability associated with the correct class\u2014is referred to as the likelihood (of\nour model\u2019s parameters, given the data) \u201cIn other words, we want a loss function that\nis very high when the likelihood is low: so low that the alternatives have a higher prob-\nability. Conversely, the loss should be low when the likelihood is higher than the alter-\nnatives, and we\u2019re not really fixated on driving the probability up to 1.\n\nThere\u2019s a loss function that behaves that way, and it\u2019s called negative log likelihood\n(NLL). It has the expression NLL = - sum(log(out_i[{c_i])), where the sum is taken\nover Nsamples and c_i is the correct class for sample i. Let's take a look at figure 7.10,\nwhich shows the NLL as a function of predicted probability.\n\n3.0\n25\n\n2.0\n\nNLL LOSS\na\n\n\\.0\n\nos\n\n0.0\nFigure 7.10 The NLL\n0.0 02 om 0.6 0.8 \\.0 joss as a function of the\n\nPREDICTED LIKELIHOOD OF TARGET CLASS, predicted probabilities\n\nThe figure shows that when low probabilities are assigned to the data, the NLL grows\nto infinity, whereas it decreases at a rather shallow rate when probabilities are greater\nthan 0.5. Remember that the NLL takes probabilities as input; so, as the likelihood\ngrows, the other probabilities will necessarily decrease.\n\nSumming up, our loss for classification can be computed as follows. For each sam-\nple in the batch:\n\na Run the forward pass, and obtain the output values from the last (linear) layer.\nCompute their softmax, and obtain probabilities.\nTake the predicted probability corresponding to the correct class (the likeli-\nhood of the parameters). Note that we know what the correct class is because\nit\u2019s a supervised problem\u2014it\u2019s our ground truth.\n\n4 Compute its logarithm, slap a minus sign in front of it, and add it to the loss.\n\nSo, how do we do this in PyTorch? PyTorch has an nn.NLLLoss class. However (gotcha\nahead), as opposed to what you might expect, it does not take probabilities but rather\ntakes a tensor of log probabilities as input. It then computes the NLL of our model\ngiven the batch of data. There\u2019s a good reason behind the input convention: taking\nthe logarithm of a probability is icky when the probability gets close to zero. The\nworkaround is to use nn. LogSoftmax instead of nn.Softmax, which takes care to make\nthe calculation numerically stable.\nWe can now modify our model to use nn. LogSoftmax as the output module:\nmodel = nn.Sequential (\nnn.Linear (3072, 512),\n-Tanh(),\n\nnn\nnn.Linear(512, 2),\nnn.LogSoftmax(dim=1) )\n\nThen we instantiate our NLL loss:\nloss = nn.NLLLoss()\n\nThe loss takes the output of nn.LogSoftmax for a batch as the first argument and a\ntensor of class indices (zeros and ones, in our case) as the second argument. We can\nnow test it with our birdie:\n\nimg, label = cifar2[0]\nout = model (img.view(-1) .unsqueeze(0))\nloss(out, torch.tensor([label]))\n\ntensor (0.6509, grad_fn=<NllLossBackward>)\n\nEnding our investigation of losses, we can look at how using cross-entropy loss\nimproves over MSE. In figure 7.11, we see that the cross-entropy loss has some slope\n\nwhen the prediction is off target (in the low-loss corner, the correct class is assigned a\npredicted probability of 99.97%), while the MSE we dismissed at the beginning satu-\nrates much earlier and\u2014crucially\u2014also for very wrong predictions. The underlying\nreason is that the slope of the MSE is too low to compensate for the flatness of the soft-\nmax function for wrong predictions. This is why the MSE for probabilities is not a\ngood fit for classification work.\n\nSUCCESSFUL AND LESS SUCCESSFUL CLASSIFICATION LOSSES\n\nFigure 7.11 The cross entropy (left) and MSE between predicted probabilities and the target probability vector\n(right) as functions of the predicted scores\u2014that is, before the (log-) softmax\n\n7.2.6 Training the classifier\n\nAll right! We're ready to bring back the training loop we wrote in chapter 5 and see\nhow it trains (the process is illustrated in figure 7.12):\n\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Sequential (\nnn.Linear(3072, 512),\nnn.Tanh(),\nnn.Linear(512, 2),\nnn.LogSoftmax (dim=1) )\n\nlearning_rate = le-2\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.13,
                        "section_name": "Training the classifier",
                        "section_path": "./screenshots-images-2/chapter_8/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_13/fb247236-32c1-4188-9a01-f46dbd405bdd.png",
                            "./screenshots-images-2/chapter_8/section_13/12528910-23cf-4b1a-9d27-b31d7dcfc170.png",
                            "./screenshots-images-2/chapter_8/section_13/5a329038-73be-4a34-a213-21ed3861e118.png",
                            "./screenshots-images-2/chapter_8/section_13/c4b75b07-2227-4646-9c12-4956884ee628.png",
                            "./screenshots-images-2/chapter_8/section_13/7c622176-c2e7-4767-8e59-8198933e174c.png",
                            "./screenshots-images-2/chapter_8/section_13/12f1c6f7-2fe0-4bde-830e-9371deb434b6.png",
                            "./screenshots-images-2/chapter_8/section_13/d82f5d17-bb14-4200-b79d-6b1fe7e6cbdc.png",
                            "./screenshots-images-2/chapter_8/section_13/da17482c-e76a-442e-ae44-df675c78e15a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the classifier\n\nAll right! We're ready to bring back the training loop we wrote in chapter 5 and see\nhow it trains (the process is illustrated in figure 7.12):\n\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Sequential (\nnn.Linear(3072, 512),\nnn.Tanh(),\nnn.Linear(512, 2),\nnn.LogSoftmax (dim=1) )\n\nlearning_rate = le-2\n\noptimizer = optim.SGD(model.parameters(}), lr=learning_rate)\n\nloss_fn = nn.NLLLoss()}\nn_epochs = 100\n\nfor epoch in range (n_epochs):\nfor img, label in cifar2:\nout = model (img.view(-1).unsqueeze(0))\nloss = loss_fn(out, torch.tensor({label)))\n\nPrints the loss for the\noptimizer.zero_grad() last image. In the next\nloss. backward () chapter, we will\noptimizer.step() improve our output to\n\ngive an average over\n\nprint(\"Epoch: td, Loss: t\u00a3\" % (epoch, float(loss))) < the entire epoch.\n\u00ae\nFOR N EPOCHS: FOR N EPOCHS:\n\nWITH EVERY SAMPLE IN DATASET: WITH EVERY SAMPLE IN DATASET:\nEVALUATE MODEL (FORWARD) EVALUATE MODEL (FORWARD)\nCOMPUTE LOSS COMPUTE LOSS\nACCUMULATE GRADIENT OF LOSS COMPUTE GRADIENT OF LOSS\n\n(BACKWARD) (BACKWARD)\n\nUPDATE MODEL WITH ACCUMULATED GRADIENT UPDATE MODEL WITH GRADIENT\n\n\u00a9\nFOR N EPOCHS:\nSPLIT DATASET IN MINIBATCHES EPOCH\nFOR EVERY MINIBATCH:\nWITH EVERY SAMPLE IN MINIBATCH: \"| TTERATION\nEVALUATE MODEL (FORWARD) \u201cTew>\nCOMPUTE LOSS\n\nACCUMULATE GRADIENT OF LOSS (BACKWARD) SwD |\nUPDATE MODEL WITH ACCUMULATED GRADIENT\n\nFigure 7.12 Training loops: (A) averaging updates over the whole dataset; (B) updating the model\nat each sample; (C) averaging updates over minibatches\n\nLooking more closely, we made a small change to the training loop. In chapter 5, we\nhad just one loop: over the epochs (recall that an epoch ends when all samples in the\ntraining set have been evaluated). We figured that evaluating all 10,000 images in a\nsingle batch would be too much, so we decided to have an inner loop where we evalu-\nate one sample at a time and backpropagate over that single sample.\n\nWhile in the first case the gradient is accumulated over all samples before being\napplied, in this case we apply changes to parameters based on a very partial estimation\n\nof the gradient on a single sample. However, what is a good direction for reducing the\nloss based on one sample might not be a good direction for others. By shuffling samples\nat each epoch and estimating the gradient on one or (preferably, for stability) a few\nsamples at a time, we are effectively introducing randomness in our gradient descent.\nRemember SGD? It stands for stochastic gradient descent, and this is what the S is about:\nworking on small batches (aka minibatches) of shuffled data. It turns out that following\ngradients estimated over minibatches, which are poorer approximations of gradients\nestimated across the whole dataset, helps convergence and prevents the optimization\nprocess from getting stuck in local minima it encounters along the way. As depicted in\nfigure 7.13, gradients from minibatches are randomly off the ideal trajectory, which is\npart of the reason why we want to use a reasonably small learning rate. Shuffling the\ndataset at each epoch helps ensure that the sequence of gradients estimated over mini-\nbatches is representative of the gradients computed across the full dataset.\n\nTypically, minibatches are a constant size that we need to set prior to training, just\nlike the learning rate. These are called hyperparameters, to distinguish them from the\nparameters of a model.\n\nLOSS\n\nFigure 7.13 Gradient descent averaged over the whole dataset (light path) versus stochastic\ngradient descent, where the gradient is estimated on randomly picked minibatches\n\nIn our training code, we chose minibatches of size 1 by picking one item ata time from\nthe dataset. The torch. utils.data module has a class that helps with shuffling and\norganizing the data in minibatches: DataLoader. The job of a data loader is to sample\nminibatches from a dataset, giving us the flexibility to choose from different sampling\nstrategies. A very common strategy is uniform sampling after shuffling the data at each\nepoch. Figure 7.14 shows the data loader shuffling the indices it gets from the Dataset.\n\n\u201cnext minibsteh, please!\n\nDATA LOADER aacd\nbatch sizes 4 \u201cnat winitde please! \u2018\nshuffle = True asowod\n\nnext\n\nFigure 7.14 A data loader dispensing minibatches by using a dataset to sample\nindividual data items\n\nLet\u2019s see how this is done. Ata minimum, the DataLoader constructor takes a Dataset\nobject as input, along with batch_size and a shuffle Boolean that indicates whether\nthe data needs to be shuffled at the beginning of each epoch:\n\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\nshuffle=True)\n\nA DataLoader can be iterated over, so we can use it directly in the inner loop of our\nnew training code:\n\nimport torch\nimport torch.nn as nn\n\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\nshuffle=True)\n\nmodel = nn.Sequential(\nnn.Linear (3072, 512),\nnn.Tanh(),\nnn.Linear(512, 2),\nnn.LogSoftmax (dim=1) }\nlearning_rate = le-2\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\nloss_fn = nn.NLLLoss()\n\nn_epochs = 100\n\nfor epoch in range(n_epochs):\nfor imgs, labels in train_loader:\n\nbatch_size = imgs.shape[0]\noutputs = model (imgs.view(batch_size, -1))\nloss = loss_fn(outputs, labels)\n\nDue to the shuffling, this now\noptimizer.zero_grad() prints the loss for a random\nloss. backward () batch\u2014clearly something we\noptimizer.step() want to improve in chapter 8.\n\nprint(\"Epoch: td, Loss: tf\" % (epoch, float(loss))) +t\n\nAt each inner iteration, imgs is a tensor of size 64 x 3 x 32 x 32\u2014that is, a minibatch of\n64 (32 x 32) RGB images\u2014while labels is a tensor of size 64 containing label indices.\nLet\u2019s run our training:\n\nEpoch: 0, Loss: 0.523478\nEpoch: 1, Loss: 0.391083\nEpoch: 2, Loss: 0.407412\nEpoch: 3, Loss: 0.364203\n\nEpoch: 96, Loss: 0.019537\nEpoch: 97, Loss: 0.008973\nEpoch: 98, Loss: 0.002607\nEpoch: 99, Loss: 0.026200\n\nWe see that the loss decreases somehow, but we have no idea whether it\u2019s low enough.\nSince our goal here is to correctly assign classes to images, and preferably do that on\nan independent dataset, we can compute the accuracy of our model on the validation\nset in terms of the number of correct classifications over the total:\n\nval_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\nshuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\nfor imgs, labels in val_loader:\nbatch_size = imgs.shape[0]\noutputs = model (imgs.view(batch_size, -1))\n_, predicted = torch.max(outputs, dim=1)\ntotal += labels.shape[0]\ncorrect += int((predicted == labels) .sum())\n\nprint (\"Accuracy: %\u00a3\", correct / total)\n\nAccuracy: 0.794000\n\nNot a great performance, but quite a lot better than random. In our defense, our\nmodel was quite a shallow classifier; it\u2019s a miracle that it worked at all. It did because\nour dataset is really simple\u2014a lot of the samples in the two classes likely have system-\natic differences (such as the color of the background) that help the model tell birds\nfrom airplanes, based on a few pixels.\n\nWe can certainly add some bling to our model by including more layers, which will\nincrease the model\u2019s depth and capacity. One rather arbitrary possibility is\n\nmodel = nn.Sequential(\nnn.Linear(3072, 1024),\n\nnn.Tanh(),\nnn.Linear(1024, 512),\nnn.Tanh(),\nnn.Linear(512, 128),\nnn.Tanh(),\n\nnn.Linear(i28, 2),\nnn.LogSoftmax (dim=1))\n\nHere we are trying to taper the number of features more gently toward the output, in\nthe hope that intermediate layers will do a better job of squeezing information in\nincreasingly shorter intermediate outputs.\n\nThe combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using\nnn.CrossEntropyLoss. This terminology is a particularity of PyTorch, as the\nnn.NLLoss computes, in fact, the cross entropy but with log probability predictions as\ninputs where nn.CrossEntropyLoss takes scores (sometimes called logits). Techni-\ncally, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass\non the target, and the predicted distribution given by the log probability inputs.\n\nTo add to the confusion, in information theory, up to normalization by sample size,\nthis cross entropy can be interpreted as a negative log likelihood of the predicted dis-\ntribution under the target distribution as an outcome. So both losses are the negative\nlog likelihood of the model parameters given the data when our model predicts the\n(softmax-applied) probabilities. In this book, we won\u2019t rely on these details, but don\u2019t\nlet the PyTorch naming confuse you when you see the terms used in the literature.\n\nIt is quite common to drop the last nn. LogSoftmax layer from the network and use\nnn.CrossEntropyLoss as a loss. Let us try that:\n\nmodel = nn.Sequential (\nnn.Linear(3072, 1024),\n\nnn.Tanh(),\nnn.Linear(1024, 512),\nnn.Tanh(),\nnn.Linear(512, 128),\nnn.Tanh(),\n\nnn.Linear (128, 2))\n\nloss_fn = nn.CrossEntropyLoss ()\n\nNote that the numbers will be exactly the same as with nn. LogSoftmax and nn.NLLLoss.\nIt\u2019s just more convenient to do it all in one pass, with the only gotcha being that the out-\nput of our model will not be interpretable as probabilities (or log probabilities). We'll\nneed to explicitly pass the output through a softmax to obtain those.\n\nTraining this model and evaluating the accuracy on the validation set (0.802000)\nlets us appreciate that a larger model bought us an increase in accuracy, but not that\nmuch. The accuracy on the training set is practically perfect (0.998100). What is this\ntelling us? That we are overfitting our model in both cases. Our fully connected\nmodel is finding a way to discriminate birds and airplanes on the training set by mem-\norizing the training set, but performance on the validation set is not all that great,\neven if we choose a larger model.\n\nPyTorch offers a quick way to determine how many parameters a model has\nthrough the parameters () method of nn.Model (the same method we use to provide\nthe parameters to the optimizer). To find out how many elements are in each tensor\ninstance, we can call the numel method. Summing those gives us our total count.\nDepending on our use case, counting parameters might require us to check whether a\nparameter has requires_grad set to True, as well. We might want to differentiate the\nnumber of trainable parameters from the overall model size. Let's take a look at what\nwe have right now:\n\n# In(7]:\n\nnumel_list = [p.numel()\nfor p in connected_model .parameters()\nif p.requires_grad == True]\n\nsum(numel_list), numel_list\n\n# Out[7]:\n(3737474, (3145728, 1024, 524288, 512, 65536, 128, 256, 2]\n\nWow, 3.7 million parameters! Not a small network for such a small input image, is it?\nEven our first network was pretty large:\n\n# In(9]:\nnumel_list = [p.numel() for p in first_model.parameters() )\nsum(numel_list), numel_list\n\n# Out[9]:\n(1574402, [1572864, 512, 1024, 2)})\n\nThe number of parameters in our first model is roughly half that in our latest model.\nWell, from the list of individual parameter sizes, we start having an idea what's\nresponsible: the first module, which has 1.5 million parameters. In our full network,\nwe had 1,024 output features, which led the first linear module to have 3 million\nparameters. This shouldn't be unexpected: we know that a linear layer computes y =\nweight * x + bias, and if x has length 3,072 (disregarding the batch dimension for\nsimplicity) and y must have length 1,024, then the weight tensor needs to be of size\n1,024 x 3,072 and the bias size must be 1,024. And 1,024 * 3,072 + 1,024 = 3,146,752,\nas we found earlier. We can verify these quantities directly:\n\n# In[10):\nlinear = nn.Linear(3072, 1024)\n\nlinear.weight.shape, linear.bias.shape\n\n# Out[i0):\n(torch.Size((1024, 3072]), torch.Size([1024]))\n\nWhat is this telling us? That our neural network won't scale very well with the number\nof pixels. What if we had a 1,024 x 1,024 RGB image? That\u2019s 3.1 million input values.\nEven abruptly going to 1,024 hidden features (which is not going to work for our clas-\nsifier), we would have over 3 billion parameters. Using 32-bit floats, we're already at 12\nGB of RAM, and we haven\u2019t even hit the second layer, much less computed and stored\nthe gradients. That\u2019s just not going to fit on most present-day GPUs.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.14,
                        "section_name": "The limits of going fully connected",
                        "section_path": "./screenshots-images-2/chapter_8/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_14/48a8ae73-79fb-41a8-9025-3a4ffcef2aa6.png",
                            "./screenshots-images-2/chapter_8/section_14/f4fba463-a2e4-48b2-a1bc-52f90903531c.png",
                            "./screenshots-images-2/chapter_8/section_14/cdd70412-8c5c-470d-bd9a-6b2c12369e07.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The limits of going fully connected\n\nLet\u2019s reason about what using a linear module on a 1D view of our image entails\u2014figure\n7.15 shows what is going on. It\u2019s like taking every single input value\u2014thatis, every single\ncomponent in our RGB image\u2014and computing a linear combination of it with all the\nother values for every output feature. On one hand, we are allowing for the combina-\ntion of any pixel with every other pixel in the image being potentially relevant for our\ntask. On the other hand, we aren't utilizing the relative position of neighboring or far-\naway pixels, since we are treating the image as one big vector of numbers.\n\nOUTPUT LMAGE\n** Souteut NoTe:\n* THERE'S ONE VECTOR OF WEIGHTS,\nNeue PIXEL PER OUTPUT PIXEL.\ned ALL INPUT PIXELS CONTRIBUTE To\nEVERY OUTPUT PIXEL.\n\nWEIGHTS RELATIVE To OUTPUT PLXEL\n\nOVERALL: 4x4\nad TMAGE\n\nNfl =|a\n\n*\n\nFigure 7.15 Using a fully connected module with an input image: every input pixel is combined with\nevery other to produce each element in the output.\n\nAn airplane flying in the sky captured in a 32 x 32 image will be very roughly similar to\na dark, cross-like shape on a blue background. A fully connected network as in figure\n7.15 would need to learn that when pixel 0,1 is dark, pixel 1,1 is also dark, and so on,\nthat\u2019s a good indication of an airplane. This is illustrated in the top half of figure 7.16.\nHowever, shift the same airplane by one pixel or more as in the bottom half of the fig-\nure, and the relationships between pixels will have to be relearned from scratch: this\ntime, an airplane is likely when pixel 0,2 is dark, pixel 1,2 is dark, and so on. In more\ntechnical terms, a fully connected network is not translation invariant. This means a\nnetwork that has been trained to recognize a Spitfire starting at position 4,4 will not\nbe able to recognize the exact same Spitfire starting at position 8,8. We would then have\nto augment the dataset\u2014that is, apply random translations to images during training\u2014\nso the network would have a chance to see Spitfires all over the image, and we would\nneed to do this for every image in the dataset (for the record, we could concatenate a\n\nelele}e[ele[e El ele FFFTeFI\u00b0|\n\nPLANE (TRANSLATED)\n\nef\n\nWELGHTS\n\nFigure 7.16 Translation invariance, or the lack thereof, with fully connected layers\n\ntransform from torchvision. transforms to do this transparently). However, this data\naugmentation strategy comes at a cost: the number of hidden features\u2014that is, of\nparameters\u2014must be large enough to store the information about all of these trans-\nlated replicas.\n\nSo, at the end of this chapter, we have a dataset, a model, and a training loop, and\nour model learns. However, due to a mismatch between our problem and our network\nstructure, we end up overfitting our training data, rather than learning the general-\nized features of what we want the model to detect.\n\nWe've created a model that allows for relating every pixel to every other pixel in\nthe image, regardless of their spatial arrangement. We have a reasonable assumption\nthat pixels that are closer together are in theory a lot more related, though. This\nmeans we are training a classifier that is not translation-invariant, so we're forced to\nuse a lot of capacity for learning translated replicas if we want to hope to do well on\nthe validation set. There has to be a better way, right?\n\nOf course, most such questions in a book like this are rhetorical. The solution to\nour current set of problems is to change our model to use convolutional layers. We'll\ncover what that means in the next chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.15,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_8/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_15/c30332c5-c4ba-4d14-954b-ec1274e3389b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\n\nIn this chapter, we have solved a simple classification problem from dataset, to model,\nto minimizing an appropriate loss in a training loop. All of these things will be stan-\ndard tools for your PyTorch toolbelt, and the skills needed to use them will be useful\nthroughout your PyTorch tenure.\n\nWe've also found a severe shortcoming of our model: we have been treating 2D\nimages as 1D data. Also, we do not have a natural way to incorporate the translation\ninvariance of our problem. In the next chapter, you'll learn how to exploit the 2D\nnature of image data to get much better results.\u201d\n\nWe could use what we have learned right away to process data without this translation\ninvariance. For example, using it on tabular data or the time-series data we met in chap-\nter 4, we can probably do great things already. To some extent, it would also be possible\nto use it on text data that is appropriately represented.!\u201d\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 9,
                "chapter_name": "Using convolutions\nto generalize",
                "chapter_path": "./screenshots-images-2/chapter_9",
                "sections": [
                    {
                        "section_id": 9.1,
                        "section_name": "Using convolutions\nto generalize",
                        "section_path": "./screenshots-images-2/chapter_9/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_1/28b98b6a-efd1-4d50-ab13-207b0022eae5.png",
                            "./screenshots-images-2/chapter_9/section_1/fd3b33c1-655a-4b12-afec-51e979aa81aa.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the previous chapter, we built a simple neural network that could fit (or overfit)\nthe data, thanks to the many parameters available for optimization in the linear lay-\ners. We had issues with our model, however, in that it was better at memorizing the\ntraining set than it was at generalizing properties of birds and airplanes. Based on\nour model architecture, we\u2019ve got a guess as to why that\u2019s the case. Due to the fully\nconnected setup needed to detect the various possible translations of the bird or\nairplane in the image, we have both too many parameters (making it easier for the\nmodel to memorize the training set) and no position independence (making it\nharder to generalize). As we discussed in the last chapter, we could augment our\n\ntraining data by using a wide variety of recropped images to try to force generaliza-\ntion, but that won't address the issue of having too many parameters.\n\nThere is a better way! It consists of replacing the dense, fully connected affine trans-\nformation in our neural network unit with a different linear operation: convolution.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.2,
                        "section_name": "The case for convolutions",
                        "section_path": "./screenshots-images-2/chapter_9/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_2/4cc8b699-352e-409d-ac58-7ec58695e374.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The case for convolutions\n\nLet's get to the bottom of what convolutions are and how we can use them in our neu-\nral networks. Yes, yes, we were in the middle of our quest to tell birds from airplanes,\nand our friend is still waiting for our solution, but this diversion is worth the extra\ntime spent. We'll develop an intuition for this foundational concept in computer\nvision and then return to our problem equipped with superpowers.\n\nIn this section, we'll see how convolutions deliver locality and translation invariance.\nWe'll do so by taking a close look at the formula defining convolutions and applying it\nusing pen and paper\u2014but don\u2019t worry, the gist will be in pictures, not formulas.\n\nWe said earlier that taking a 1D view of our input image and multiplying it by an\nn_output_features x n_input_features weight matrix, as is done in nn.Linear,\nmeans for each channel in the image, computing a weighted sum of all the pixels mul-\ntiplied by a set of weights, one per output feature.\n\nWe also said that, if we want to recognize patterns corresponding to objects, like an\nairplane in the sky, we will likely need to look at how nearby pixels are arranged, and\nwe will be less interested in how pixels that are far from each other appear in combi-\nnation. Essentially, it doesn\u2019t matter if our image of a Spitfire has a tree or cloud or\nkite in the corner or not.\n\nIn order to translate this intuition into mathematical form, we could compute the\nweighted sum of a pixel with its immediate neighbors, rather than with all other pixels\nin the image. This would be equivalent to building weight matrices, one per output\nfeature and output pixel location, in which all weights beyond a certain distance from\na center pixel are zero. This will still be a weighted sum: that is, a linear operation.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.3,
                        "section_name": "What convolutions do",
                        "section_path": "./screenshots-images-2/chapter_9/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_3/f895b033-b621-4427-b466-3b2603a55c6c.png",
                            "./screenshots-images-2/chapter_9/section_3/c9a8353d-b619-478c-ab1c-cc0306b2c531.png",
                            "./screenshots-images-2/chapter_9/section_3/fbbe7e7d-b86e-40e3-b384-1be3a908a514.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What convolutions do\n\nWe identified one more desired property earlier: we would like these localized patterns\nto have an effect on the output regardless of their location in the image: that is, to be\ntranslation invariant. To achieve this goal in a matrix applied to the image-as-a-vector we\nused in chapter 7 would require implementing a rather complicated pattern of weights\n(don\u2019t worry if it is too complicated; it'll get better shortly): most of the weight matrix\nwould be zero (for entries corresponding to input pixels too far away from the output\npixel to have an influence). For other weights, we would have to find a way to keep\nentries in sync that correspond to the same relative position of input and output pixels.\nThis means we would need to initialize them to the same values and ensure that all these\ntied weights stayed the same while the network is updated during training. This way, we\nwould ensure that weights operate in neighborhoods to respond to local patterns, and\nlocal patterns are identified no matter where they occur in the image.\n\nOf course, this approach is more than impractical. Fortunately, there is a readily\navailable, local, translation-invariant linear operation on the image: a convolution. We\ncan come up with a more compact description of a convolution, but what we are going\nto describe is exactly what we just delineated\u2014only taken from a different angle.\n\nConvolution, or more precisely, discrete convolution\u2019 (there\u2019s an analogous continu-\nous version that we won't go into here), is defined for a 2D image as the scalar prod-\nuct of a weight matrix, the kernel, with every neighborhood in the input. Consider a\n3 x 3 kernel (in deep learning, we typically use small kernels; we'll see why later on) as\na 2D tensor\n\nweight = torch.tensor([[w00, w01, w02],\n{[w10, wll, wil2),\n[w20, w21, w22]])\n\nand a 1-channel, MxN image:\n\nimage = torch.tensor([{[i00, i01, i102, i03, ion],\n(110, i11, i12, 113, iin),\n(120, i21, i122, 123, . i2N),\n(130, i31, i132, 133, ..., i3N),\n(imo, iMim iM2, iM3, ..., iMN]}))\n\nWe can compute an element of the output image (without bias) as follows:\n\no11 = i11 * wOO + 112 * w01 + 122 * w02 +\ni21 * wl0 + i22 * wll + i23 * wi2 +\ni31 * w20 + i32 * w21 + i133 * w22\n\nFigure 8.1 shows this computation in action.\n\nThat is, we \u201ctranslate\u201d the kernel on the i11 location of the input image, and we\nmultiply each weight by the value of the input image at the corresponding location.\nThus, the output image is created by translating the kernel on all input locations and\nperforming the weighted sum. For a multichannel image, like our RGB image, the\nweight matrix would be a 3 x 3 x 3 matrix: one set of weights for every channel, con-\ntributing together to the output values.\n\nNote that, just like the elements in the weight matrix of nn.Linear, the weights in\nthe kernel are not known in advance, but they are initialized randomly and updated\nthrough backpropagation. Note also that the same kernel, and thus each weight in the\nkernel, is reused across the whole image. Thinking back to autograd, this means the use\nof each weight has a history spanning the entire image. Thus, the derivative of the loss\nwith respect to a convolution weight includes contributions from the entire image.\n\noli fol}o oljolifo\nKERNEL DonG oon\no|i fe lofife le ofofifo|\noo0o00 0000\n\nali ores\nBog S fz) <\n! Be\n\nKERNEL\nWEIGHTS\no1 0,0 o10 0\nojifo| i jofvfo\nDone oft fa\nlelife|e o[efsfe|\nSCALAR PRODUCT\nSAME KERNEL WEIGHTS\nservsen TeALareD CReb AcnoSe we Inne\n(ZEROS OUTSIDE THE KERNEL) b\nLOCALITY TRANSLATION\nINVARIANCE\n\nFigure 8.1 Convolution: locality and translation invariance\n\nIt\u2019s now possible to see the connection to what we were stating earlier: a convolution is\nequivalent to having multiple linear operations whose weights are zero almost every-\nwhere except around individual pixels and that receive equal updates during training.\n\nSummarizing, by switching to convolutions, we get\n\n= Local operations on neighborhoods\n\n= Translation invariance\n\n= Models with a lot fewer parameters\n\nThe key insight underlying the third point is that, with a convolution layer, the num-\nber of parameters depends not on the number of pixels in the image, as was the case\nin our fully connected model, but rather on the size of the convolution kernel (3 x 3,\n5x 5, and so on) and on how many convolution filters (or output channels) we decide\nto use in our model.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.4,
                        "section_name": "Convolutions in action",
                        "section_path": "./screenshots-images-2/chapter_9/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_4/986cd036-6960-49ed-89e5-b60b8fed0dcb.png",
                            "./screenshots-images-2/chapter_9/section_4/791a2d90-bf10-41bf-bcf5-933328987268.png",
                            "./screenshots-images-2/chapter_9/section_4/5aa2deb4-c6ab-48d1-a6ec-07aff265db62.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Convolutions in action\nWell, it looks like we've spent enough time down a rabbit hole! Let\u2019s see some PyTorch\nin action on our birds versus airplanes challenge. The torch. nn module provides con-\nvolutions for 1, 2, and 3 dimensions: nn.Conv1d for time series, nn .Conv2d for images,\nand nn.Conv3d for volumes or videos.\n\nFor our CIFAR-10 data, we'll resort to nn.Conv2d. Ata minimum, the arguments we\nprovide to nn.Conv2dare the number of input features (or channels, since we're dealing\n\nwith multichannel images: that is, more than one value per pixel), the number of output\nfeatures, and the size of the kernel. For instance, for our first convolutional module,\nwe'll have 3 input features per pixel (the RGB channels) and an arbitrary number of\nchannels in the output\u2014say, 16. The more channels in the output image, the more the\ncapacity of the network. We need the channels to be able to detect many different types\nof features. Also, because we are randomly initializing them, some of the features we'll\nget, even after training, will turn out to be useless.\u201d Let\u2019s stick to a kernel size of 3 x 3.\n\nIt is very common to have kernel sizes that are the same in all directions, so\nPyTorch has a shortcut for this: whenever kernel_size=3 is specified for a 2D convo-\nlution, it means 3 x 3 (provided as a tuple (3, 3) in Python). For a 3D convolution, it\nmeans 3 x 3 x 3. The CT scans we will see in part 2 of the book have a different voxel\n(volumewic pixel) resolution in one of the three axes. In such a case, it makes sense to\nconsider kernels that have a different size for the exceptional dimension. But for now,\nwe stick with having the same size of convolutions across all dimensions:\n\n# In[1l1i}:\n\nconv = nn.Conv2d(3, 16, kernel_size=3) <4 Instead of the shortcut kernel_size=3, we\n\u2014_ could equivalently pass in the tuple that we\n# out (11): see in the output: kernel_size=(3, 3).\n\nConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n\nWhat do we expect to be the shape of the weight tensor? The kernel is of size 3 x 3, so\nwe want the weight to consist of 3 x 3 parts. For a single output pixel value, our kernel\nwould consider, say, in_ch = 3 input channels, so the weight component for a single\noutput pixel value (and by translation the invariance for the entire output channel) is\nof shape in_ch x 3 x 3. Finally, we have as many of those as we have output channels,\nhere out_ch = 16, so the complete weight tensor is out_ch x in_ch x 3 x 3, in our case\n16 x 3 x 3 x 3. The bias will have size 16 (we haven\u2019t talked about bias for a while for\nsimplicity, but just as in the linear module case, it\u2019s a constant value we add to each\nchannel of the output image). Let\u2019s verify our assumptions:\n\n# In[12):\nconv.weight.shape, conv.bias.shape\n\n# Out[12]:\n({torch.Size((16, 3, 3, 3]), torch.Size([16]))\n\nWe can see how convolutions are a convenient choice for learning from images. We\nhave smaller models looking for local patterns whose weights are optimized across the\nentire image.\n\nA 2D convolution pass produces a 2D image as output, whose pixels are a weighted\nsum over neighborhoods of the input image. In our case, both the kernel weights and\n\nthe bias conv. weight are initialized randomly, so the output image will not be particu-\nlarly meaningful. As usual, we need to add the zeroth batch dimension with\nunsqueeze if we want to call the conv module with one input image, since nn.Conv2d\nexpects a Bx Cx Hx Wshaped tensor as input:\n\n# In(13]:\n\nimg, _ = cifar2[0)\n\noutput = conv(img.unsqueeze(0)\nimg.unsqueeze(0).shape, output.shape\n\n# Out[13):\n(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30)}))\n\nWe're curious, so we can display the output, shown in figure 8.2:\n# In(15]:\n\nplt.imshow(output[0, 0].detach(), cmap='gray')\nplt.show()\n\nOUTPUT INPUT\n\n\u00b0 V] 20 25 30 \u00b0 5 \\o is 20 25 30\n\nFigure 8.2 Our bird after a random convolution treatment. (We cheated a little with the code\nto show you the input, too.)\n\nWait a minute. Let\u2019s take a look a the size of output: it\u2019s torch.Size([1, 16, 30,\n30]). Huh; we lost a few pixels in the process. How did that happen?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.5,
                        "section_name": "Padding the boundary",
                        "section_path": "./screenshots-images-2/chapter_9/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_5/23fd07b9-a554-488e-9e3a-319a686dd7ef.png",
                            "./screenshots-images-2/chapter_9/section_5/8e0936cf-90ee-40d1-8ce9-aeeb1f7061d7.png",
                            "./screenshots-images-2/chapter_9/section_5/d456aa66-61ea-4675-9aab-80c8b9722155.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Padding the boundary\n\nThe fact that our output image is smaller than the inputis a side effect of deciding what\nto do at the boundary of the image. Applying a convolution kernel as a weighted sum\nof pixels in a 3 x 3 neighborhood requires that there are neighbors in all directions. If\nwe are at i00, we only have pixels to the right of and below us. By default, PyTorch will\nslide the convolution kernel within the input picture, getting width - kernel_width + 1\nhorizontal and vertical positions. For odd-sized kernels, this results in images that are\n\none-half the convolution kernel\u2019s width (in our case, 3//2 = 1) smaller on each side.\nThis explains why we're missing two pixels in each dimension.\n\nHowever, PyTorch gives us the possibility of padding the image by creating ghost pix-\nels around the border that have value zero as far as the convolution is concerned. Fig-\nure 8.3 shows padding in action.\n\nIn our case, specifying padding=1 when kernel_size=3 means i00 has an extra set\nof neighbors above it and to its left, so that an output of the convolution can be com-\nputed even in the corner of our original image.* The net result is that the output has\nnow the exact same size as the input:\n\n# In[16]:\n\nconv = nn.Conv2d(3, 1, kernel_size=3, padding=1) <\u2014 Now with padding\noutput = conv(img.unsqueeze (0) )\n\nimg.unsqueeze(0).shape, output.shape\n\n# Out[16):\n{torch.Size({1, 3, 32, 32])), torch.Size([1, 1, 32, 32]))\n\no1\na0 \u00b0 \u00a5\n\\ L4\nuo \u00b0 0106\nJ 2100 gh ere bbb b 0000\nZEROS baddao ooloo\u00b0o eo op bd\nOUTSIDE\n44 o} i] o| o \u00b0 o}\n1, Lif af ifo \\ Lt\n4-9 ol i] of \u00b0 o |\n@aqo oo 00 oo00 0000\n\n\u00b0\n\u00b0\n\neo, or=-'o\n[IETS -\nia ali\nbil\n\n\u00a3 FER\noo) =\n2 9 990\n[ol lo\n[-[-[-| -\nLol-lel \u00a9\neo0 0\n\neo 9 9\n\u00b0\n\u00b0\n\na)\noo \u00a9\no4\n\nFigure 8.3 Zero padding to preserve the image size in the output\n\nNote that the sizes of weight and bias don\u2019t change, regardless of whether padding is\nused.\n\nThere are two main reasons to pad convolutions. First, doing so helps us separate\nthe matters of convolution and changing image sizes, so we have one less thing to\nremember. And second, when we have more elaborate structures such as skip con-\nnections (discussed in section 8.5.3) or the U-Nets we'll cover in part 2, we want the\ntensors before and after a few convolutions to be of compatible size so that we can\nadd them or take differences.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.6,
                        "section_name": "Detecting features with convolutions",
                        "section_path": "./screenshots-images-2/chapter_9/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_6/4d52f682-6ccf-45d4-8eb4-9ce76d459d0d.png",
                            "./screenshots-images-2/chapter_9/section_6/094344ec-10f5-47e8-831b-2896f35612e6.png",
                            "./screenshots-images-2/chapter_9/section_6/18f8da79-5189-41d5-9ee0-b753c6bc0444.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Detecting features with convolutions\nWe said earlier that weight and bias are parameters that are learned through back-\npropagation, exactly as it happens for weight and bias in nn. Linear. However, we can\nplay with convolution by setting weights by hand and see what happens.\n\nLet's first zero out bias, just to remove any confounding factors, and then set\nweights to a constant value so that each pixel in the output gets the mean of its neigh-\nbors. For each 3 x 3 neighborhood:\n\n# In(17]:\nwith torch.no_grad():\nconv.bias.zero_()\n\nwith torch.no_grad():\nconv.weight.fill_(1.0 / 9.0)\n\nWe could have gone with conv.weight .one_{)\u2014that would result in each pixel in the\noutput being the sum of the pixels in the neighborhood. Not a big difference, except\nthat the values in the output image would have been nine times larger.\n\nAnyway, let\u2019s see the effect on our CIFAR image:\n\n# In{18]:\n\noutput = conv(img.unsqueeze(0))\nplt.imshow(output[0, 0] .detach(), cmap=\"gray')\nplt.show()\n\nAs we could have predicted, the filter produces a blurred version of the image, as shown\nin figure 8.4. After all, every pixel of the output is the average of a neighborhood of the\ninput, so pixels in the output are correlated and change more smoothly.\n\nNext, let\u2019s try something different. The following kernel may look a bit mysterious\nat first:\n\n# In(19]:\nconv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n\nwith torch.no_grad():\nconv.weight[:] = torch.tensor([\n\nconv.bias.zero_()\n\nSs 20 30\n\n\u00b0 5 io\n\n\u00b0 5 to S 20 25 30\n\nFigure 8.4 Our bird, this time blurred thanks to a constant convolution kernel\n\nWorking out the weighted sum for an arbitrary pixel in position 2,2, as we did earlier\nfor the generic convolution kernel, we get\n\n022 = i113 - ill +\ni23 - i21 +\ni33 - i31\n\nwhich performs the difference of all pixels on the right of i22 minus the pixels on the\nleft of i22. If the kernel is applied on a vertical boundary between two adjacent regions\nof different intensity, 022 will have a high value. If the kernel is applied on a region of\nuniform intensity, 022 will be zero. It\u2019s an edge-detection kernel: the kernel highlights the\nvertical edge between two horizontally adjacent regions.\n\nApplying the convolution kernel to our image, we see the result shown in figure\n8.5. As expected, the convolution kernel enhances the vertical edges. We could build\n\nINPUT\n\n30 20 20\n\nFigure 8.5 Vertical edges throughout our bird, courtesy of a handcrafted convolution kere!\n\nlots more elaborate filters, such as for detecting horizontal or diagonal edges, or cross-\nlike or checkerboard patterns, where \u201cdetecting\u201d means the output has a high magni-\ntude. In fact, the job of a computer vision expert has historically been to come up with\nthe most effective combination of filters so that certain features are highlighted in\nimages and objects can be recognized.\n\nWith deep learning, we let kernels be estimated from data in whatever way the dis-\ncrimination is most effective: for instance, in terms of minimizing the negative cross-\nentropy loss between the output and the ground truth that we introduced in section\n7.2.5. From this angle, the job of a convolutional neural network is to estimate the ker-\nnel of a set of filter banks in successive layers that will transform a multichannel image\ninto another multichannel image, where different channels correspond to different\nfeatures (such as one channel for the average, another channel for vertical edges, and\nso on). Figure 8.6 shows how the training automatically learns the kernels.\n\noy\n\na -_\na | RE\nHes\n\n(re nets)\nCHANNELS\n\nFigure 8.6 The process of learning with convolutions by estimating the gradient at the kernel weights and\nupdating them individually in order to optimize for the loss\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.7,
                        "section_name": "Looking further with depth and pooling",
                        "section_path": "./screenshots-images-2/chapter_9/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_7/e938686e-fa1f-4157-a9b8-88809c229fea.png",
                            "./screenshots-images-2/chapter_9/section_7/6315fc73-9350-4aa7-b683-1db01c0df491.png",
                            "./screenshots-images-2/chapter_9/section_7/2184bddc-01ff-4d81-ba52-a69f1e9a49bf.png",
                            "./screenshots-images-2/chapter_9/section_7/bc276764-b86a-4109-b98d-79f63580c5aa.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Looking further with depth and pooling\n\nThis is all well and good, but conceptually there\u2019s an elephant in the room. We got all\nexcited because by moving from fully connected layers to convolutions, we achieve\nlocality and translation invariance. Then we recommended the use of small kernels,\nlike 3 x 3, or 5 x 5: that\u2019s peak locality, all right. What about the big picture? How do we\nknow that all structures in our images are 3 pixels or 5 pixels wide? Well, we don\u2019t,\nbecause they aren't. And if they aren\u2019t, how are our networks going to be equipped to\nsee those patterns with larger scope? This is something we'll really need if we want to\n\nsolve our birds versus airplanes problem effectively, since although CIFAR-10 images\nare small, the objects still have a (wing-)span several pixels across.\n\nOne possibility could be to use large convolution kernels. Well, sure, at the limit we\ncould get a 32 x 32 kernel for a 32 x 32 image, but we would converge to the old fully\nconnected, affine transformation and lose all the nice properties of convolution.\nAnother option, which is used in convolutional neural networks, is stacking one con-\nvolution after the other and at the same time downsampling the image between suc-\ncessive convolutions.\n\nFROM LARGE TO SMALL: DOWNSAMPLING\n\nDownsampling could in principle occur in different ways. Scaling an image by half is\nthe equivalent of taking four neighboring pixels as input and producing one pixel as\noutput. How we compute the value of the output based on the values of the input is\nup to us. We could\n\n= Average the four pixels. This average pooling was a common approach early on but\nhas fallen out of favor somewhat.\n= Take the maximum of the four pixels. This approach, called max pooling, is currently\nthe most commonly used approach, but it has a downside of discarding the\nother three-quarters of the data.\n= Perform a strided convolution, where only every Nth pixel is calculated. A 3 x 4 convolu-\ntion with stride 2 still incorporates input from all pixels from the previous layer.\nThe literature shows promise for this approach, but it has not yet supplanted\nmax pooling.\nWe will be focusing on max pooling, illustrated in figure 8.7, going forward. The fig-\nure shows the most common setup of taking non-overlapping 2 x 2 tiles and taking the\nmaximum over each of them as the new pixel at the reduced scale.\nIntuitively, the output images from a convolution layer, especially since they are fol-\nlowed by an activation just like any other linear layer, tend to have a high magnitude\n\nmr Ea\n(OUTPUT OF CONV + ACTIVATION) outeut on\n\nMAKPOOL mares 2\n\nMAK=2 (\u2014 <\n\nFigure 8.7 Max pooling in detail\n\nwhere certain features corresponding to the estimated kernel are detected (such as\nvertical lines). By keeping the highest value in the 2 x 2 neighborhood as the downs-\nampled output, we ensure that the features that are found survive the downsampling,\nat the expense of the weaker responses.\n\nMax pooling is provided by the nn .MaxPoo12d module (as with convolution, there are\nversions for 1D and 3D data). It takes as input the size of the neighborhood over which\nto operate the pooling operation. If we wish to downsample our image by half, we'll want\nto use a size of 2. Let\u2019s verify that it works as expected directly on our input image:\n\n# In(21):\npool = nn.MaxPool2d(2)\noutput = pool(img.unsqueeze(0))\n\nimg.unsqueeze(0).shape, output.shape\n\n# Out[21):\n(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16})})\n\nCOMBINING CONVOLUTIONS AND DOWNSAMPLING FOR GREAT GOOD\n\nLet\u2019s now see how combining convolutions and downsampling can help us recognize\nlarger structures. In figure 8.8, we start by applying a set of 3 x 3 kernels on our 8 x 8\nimage, obtaining a multichannel output image of the same size. Then we scale down\nthe output image by half, obtaining a 4 x 4 image, and apply another set of 3 x 3 ker-\nnels to it. This second set of kernels operates on a 3 x 3 neighborhood of something\nthat has been scaled down by half, so it effectively maps back to 8 x 8 neighborhoods\nof the input. In addition, the second set of kernels takes the output of the first set of\nkernels (features like averages, edges, and so on) and extracts additional features on\ntop of those.\n\nSo, on one hand, the first set of kernels operates on small neighborhoods on first-\norder, low-level features, while the second set of kernels effectively operates on wider\nneighborhoods, producing features that are compositions of the previous features.\nThis is a very powerful mechanism that provides convolutional neural networks with\nthe ability to see into very complex scenes\u2014much more complex than our 32 x 32\nimages from the CIFAR-10 dataset.\n\nCONV OUTPUT MAK POOL CONV CONV\n\nFigure 8.8 More convolutions by hand, showing the effect of stacking convolutions and downsampling: a large\ncross is highlighted using two small, cross-shaped kernels and max pooling.\n\nThe receptive field of output pixels\n\nWhen the second 3 x 3 convolution kernel produces 21 in its conv output in figure\n8.8, this is based on the top-left 3 x 3 pixels of the first max pool output. They, in turn,\ncorrespond to the 6 x 6 pixels in the top-left corner in the first conv output, which in\nturn are computed by the first convolution from the top-left 7 x 7 pixels. So the pixel\nin the second convolution output is influenced by a 7 x 7 input square. The first\nconvolution also uses an implicitly \u201cpadded\u201d column and row to produce the output in\nthe corner; otherwise, we would have an 8 x 8 square of input pixels informing a given\npixel (away from the boundary) in the second convolution\u2019s output. In fancy language,\nwe say that a given output neuron of the 3 x 3-conv, 2 x 2-max-pool, 3 x 3-conv\nconstruction has a receptive field of 8 x 8.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.8,
                        "section_name": "Putting it all together for our network",
                        "section_path": "./screenshots-images-2/chapter_9/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_8/c19c41c9-254a-4769-9661-04e6995dc472.png",
                            "./screenshots-images-2/chapter_9/section_8/29bfa48c-5468-4dd5-9e5e-7bd8fd9e3a14.png",
                            "./screenshots-images-2/chapter_9/section_8/2f07ee63-9d1a-4598-bb9b-680fdce822a6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Putting it all together for our network\n\nWith these building blocks in our hands, we can now proceed to build our convolu-\ntional neural network for detecting birds and airplanes. Let\u2019s take our previous fully\nconnected model as a starting point and introduce nn.Conv2d and nn.MaxPoo12d as\ndescribed previously:\n\n# In[22]:\nmodel = nn.Sequential(\nnn.Conv2d(3, 16, kernel_size=3, padding=1),\nnn.Tanh(),\nnn.MaxPool2d(2),\nnn.Conv2d(16, 8, kernel_size=3, padding=1),\nnn.Tanh(),\nnn.MaxPool2d(2),\n)\n\nThe first convolution takes us from 3 RGB channels to 16, thereby giving the network\na chance to generate 16 independent features that operate to (hopefully) discrimi-\nnate low-level features of birds and airplanes. Then we apply the Tanh activation func-\ntion. The resulting 16-channel 32 x 32 image is pooled to a 16-channel 16 x 16 image\nby the first MaxPoo13d. At this point, the downsampled image undergoes another con-\nvolution that generates an 8-channel 16 x 16 output. With any luck, this output will\nconsist of higher-level features. Again, we apply a Tanh activation and then pool to an\n8-channel 8 x 8 output.\n\nWhere does this end? After the input image has been reduced to a set of 8 x 8 fea-\ntures, we expect to be able to output some probabilities from the network that we can\nfeed to our negative log likelihood. However, probabilities are a pair of numbers in a\n1D vector (one for airplane, one for bird), but here we\u2019re still dealing with multichan-\nnel 2D features.\n\nThinking back to the beginning of this chapter, we already know what we need to\ndo: turn the 8-channel 8 x 8 image into a 1D vector and complete our network with a\nset of fully connected layers:\n\n# In(23):\nmodel = nn.Sequential (\n\nnn.Conv2d(3, 16, kernel_size=3, padding=1),\nnn.Tanh(),\n\nnn.MaxPool2d(2),\n\nnn.Conv2d(16, 8, kernel_size=3, padding=1),\nnn.Tanh(),\n\nnn.MaxPool2d(2),\n\noe: Warning: Something\nnn.Linear(8 * 8 * 8, 32), important is missing here!\nnn.Tanh(),\n\nnn\n\n-Linear(32, 2))\n\nThis code gives us a neural network as shown in figure 8.9.\n\nFRR\n\n\u00b0.\n\u00b0\n\u00b0\n\noS, \u2014 Paro = 0.8\n3/~ Preeane = 0.2\n\n\\ cecceee\n\neoo00D00000000000\n\nFigure 8.9 Shape of a typical convolutional network, including the one we're building. An image is fed to a series\nof convolutions and max pooling modules and then straightened into a 1D vector and fed into fully connected modules.\n\nIgnore the \u201csomething missing\u201d comment for a minute. Let\u2019s first notice that the size\nof the linear layer is dependent on the expected size of the output of MaxPoo12d: 8 x 8\nx 8 = 512. Let\u2019s count the number of parameters for this small model:\n\n# In[24]:\nnumel_list = [p.numel() for p in model.parameters () ]\nsum(numel_list), numel_list\n\n# Out [24]:\n(18090, [432, 16, 1152, 8, 16384, 32, 64, 2))\n\nThat\u2019s very reasonable for a limited dataset of such small images. In order to increase\nthe capacity of the model, we could increase the number of output channels for the\nconvolution layers (that is, the number of features each convolution layer generates) ,\nwhich would lead the linear layer to increase its size as well.\n\nWe put the \u201cWarning\u201d note in the code for a reason. The model has zero chance of\nrunning without complaining:\n\n# In[25):\nmodel (img.unsqueeze(0))\n\n# Out(25):\n\nRuntimeError: size mismatch, ml:\nwe [64 x 8), m2: [512 x 32] at \u00a2:\\...\\THTensorMath.cpp:940\n\nAdmittedly, the error message is a bit obscure, but not too much so. We find refer-\nences to linear in the traceback: looking back at the model, we see that only module\nthat has to have a 512 x 32 tensor is nn.Linear (512, 32), the first linear module after\nthe last convolution block.\n\nWhat's missing there is the reshaping step from an 8-channel 8 x 8 image to a 512-\nelement, 1D vector (1D if we ignore the batch dimension, that is). This could be\nachieved by calling view on the output of the last nn .MaxPoo12d, but unfortunately, we\ndon\u2019t have any explicit visibility of the output of each module when we use\nnn. Sequential.*\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.9,
                        "section_name": "Subclassing nn.Module",
                        "section_path": "./screenshots-images-2/chapter_9/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_9/a7090559-119d-4485-a14e-7cba4d2dbc93.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Subclassing nn.Module\n\nAtsome point in developing neural networks, we will find ourselves in a situation where\nwe want to compute something that the premade modules do not cover. Here, itis some-\nthing very simple like reshaping,\u201d; but in section 8.5.3, we use the same construction to\nimplement residual connections. So in this section, we learn how to make our own\nnn.Module subclasses that we can then use just like the prebuilt ones or nn. Sequential.\n\nWhen we want to build models that do more complex things than just applying\none layer after another, we need to leave nn.Sequential for something that gives us\nadded flexibility. PyTorch allows us to use any computation in our model by subclass-\ning nn.Module.\n\nIn order to subclass nn .Module, ata minimum we need to define a forward function\nthat takes the inputs to the module and returns the output. This is where we define our\nmodule\u2019s computation. The name forward here is reminiscent of a distant past, when\nmodules needed to define both the forward and backward passes we met in section\n5.5.1. With PyTorch, if we use standard torch operations, autograd will take care of the\nbackward pass automatically; and indeed, an nn.Module never comes with a backward.\n\nTypically, our computation will use other modules\u2014premade like convolutions or\ncustomized. To include these submodules, we typically define them in the constructor\n__init__ and assign them to self for use in the forward function. They will, at the\nsame time, hold their parameters throughout the lifetime of our module. Note that you\nneed to call super () .__init__() before you can do that (or PyTorch will remind you).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.1,
                        "section_name": "Our network as an nn.Module",
                        "section_path": "./screenshots-images-2/chapter_9/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_10/c28c09c0-05e9-43ed-b58b-a1c7f9af8744.png",
                            "./screenshots-images-2/chapter_9/section_10/c41356fc-ce13-4c5b-b576-ef158ad5cdf7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8.3.1 Our network as an nn.Module\n\nLet\u2019s write our network as a submodule. To do so, we instantiate all the nn.Conv2d,\nnn.Linear, and so on that we previously passed to nn.Sequential in the constructor,\n\nand then use their instances one after another in forward:\n\n# In[26]:\nclass Net (nn.Module):\ndef __init_ (self):\nsuper().__init__()\n\nself.convl = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n\nself.actl = nn.Tanh()\nself.pooll = nn.MaxPool2d(2)\n\nself.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n\nself.act2 = nn.Tanh()\n\nself.pool2 = nn.MaxPool2d(2)\nself.fcl = nn.Linear(& * 8 * 8, 32)\nself.act3 = nn.Tanh()\n\nself.fc2 = nn.Linear(32, 2)\n\ndef forward(self, x):\nout = self.pooll(self.act1(self.convi(x)))\nout = self.pool2 (self.act2(self.conv2 (out) ))\nThis reshape \u00bb out = out.view(-1, 8 * 8 * 8)\n\nNET\nis what we out = self.act3(self.fel(out)) Te\nwere missing out = self. fe2(out) UT (2D)\nearlier. return out\n\nLINEAR (32D->2D)\nThe Net class is equivalent to the nn. Sequential model TANK\nwe built earlier in terms of submodules; but by writing\nthe forward function explicitly, we can manipulate the LINEAR (SI2D->32D)\n\noutput of self .poo13 directly and call view on itto turn\nit into a B x N vector. Note that we leave the batch\ndimension as \u2014] in the call to view, since in principle we\ndon\u2019t know how many samples will be in the batch.\n\nHere we use a subclass of nn.Module to contain\nour entire model. We could also use subclasses to\ndefine new building blocks for more complex net-\nworks. Picking up on the diagram style in chapter 6,\nour network looks like the one shown in figure 8.10.\nWe are making some ad hoc choices about what infor-\nmation to present where.\n\nRecall that the goal of classification networks typi-\ncally is to compress information in the sense that we\nstart with an image with a sizable number of pixels\nand compress it into (a vector of probabilities of)\nclasses. Two things about our architecture deserve\n\nWew (5125)\nBCKBXS\n\nMAXPOOL (2x2)\na\n\nTANK\nx\n\nCONV2D (3x3, I6C->8\u00a2)\n\nWOCKIOKIO\n\nMAXKPOOL (242)\n\nTANK\nW@CX32432\nCONV2D (3x3, 3C->16C)\n\nINPUT (3\u00a2, 32x32)\n\nFigure 8.10 Our baseline convolu-\n\nsome commentary with respect to this goal. tional network architecture\n\nFirst, our goal is reflected by the size of our intermediate values generally\nshrinking\u2014this is done by reducing the number of channels in the convolutions, by\nreducing the number of pixels through pooling, and by having an output dimension\nlower than the input dimension in the linear layers. This is a common trait of\nclassification networks. However, in many popular architectures like the ResNets we saw\nin chapter 2 and discuss more in section 8.5.3, the reduction is achieved by pooling in\nthe spatial resolution, but the number of channels increases (still resulting in a\nreduction in size). It seems that our pattern of fast information reduction works well\nwith networks of limited depth and small images; but for deeper networks, the decrease\nis typically slower.\n\nSecond, in one layer, there is not a reduction of output size with regard to input\nsize: the initial convolution. If we consider a single output pixel as a vector of 32 ele-\nments (the channels), it is a linear transformation of 27 elements (as a convolution of\n3 channels x 3 x 3 kernel size)\u2014only a moderate increase. In ResNet, the initial con-\nvolution generates 64 channels from 147 elements (3 channels x 7 x 7 kernel size).\u00b0\nSo the first layer is exceptional in that it greatly increases the overall dimension (as in\nchannels times pixels) of the data flowing through it, but the mapping for each out-\nput pixel considered in isolation still has approximately as many outputs as inputs.\u2019\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.11,
                        "section_name": "How PyTorch keeps track of parameters and submodules",
                        "section_path": "./screenshots-images-2/chapter_9/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_11/79868fce-494a-44b0-ad37-441f27377e5a.png",
                            "./screenshots-images-2/chapter_9/section_11/356c276b-dab9-4a7a-870f-5a0b7508de57.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8.3.2. How PyTorch keeps track of parameters and submodules\n\nInterestingly, assigning an instance of nn.Module to an attribute in an nn.Module, as\nwe did in the earlier constructor, automatically registers the module as a submodule.\n\nNOTE The submodules must be top-level attributes, not buried inside list or\ndict instances! Otherwise the optimizer will not be able to locate the sub-\nmodules (and, hence, their parameters). For situations where your model\nrequires a list or dict of submodules, PyTorch provides nn.ModuleList and\nnn.ModuleDict.\n\nWe can call arbitrary methods of an nn.Module subclass. For example, for a model\nwhere training is substantially different than its use, say, for prediction, it may make\nsense to have a predict method. Be aware that calling such methods will be similar to\ncalling forward instead of the module itself\u2014they will be ignorant of hooks, and the\nJIT does not see the module structure when using them because we are missing the\nequivalent of the __cal1__ bits shown in section 6.2.1.\n\nThis allows Net to have access to the parameters of its submodules without further\naction by the user:\n\n# In(27]:\nmodel = Net()\n\nnumel_list = [p.numel() for p in model.parameters() ]\nsum(numel_list), numel_list\n\n# Out [27):\n(18090, (432, 16, 1152, 8, 16384, 32, 64, 2))\n\nWhat happens here is that the parameters () call delves into all submodules assigned\nas attributes in the constructor and recursively calls parameters () on them. No mat-\nter how nested the submodule, any nn.Module can access the list of all child parame-\nters. By accessing their grad attribute, which has been populated by autograd, the\noptimizer will know how to change parameters to minimize the loss. We know that\nstory from chapter 5.\n\nWe now know how to implement our own modules\u2014and we will need this a lot for\npart 2. Looking back at the implementation of the Net class, and thinking about the\nutility of registering submodules in the constructor so that we can access their param-\neters, it appears a bit of a waste that we are also registering submodules that have no\nparameters, like nn.Tanh and nn.MaxPool2d. Wouldn't it be easier to call these\ndirectly in the forward function, just as we called view?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.12,
                        "section_name": "The functional API",
                        "section_path": "./screenshots-images-2/chapter_9/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_12/6f5f7083-4ec3-43ce-a297-a03d88838aec.png",
                            "./screenshots-images-2/chapter_9/section_12/d9398d77-96d2-464f-8b5e-591693f5177c.png",
                            "./screenshots-images-2/chapter_9/section_12/f2956714-f3bc-4716-a1b6-329cdf2a14c6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The functional API\n\nIt sure would! And that\u2019s why PyTorch has functional counterparts for every nn module.\nBy \u201cfunctional\u201d here we mean \u201chaving no internal state\"\u2014in other words, \u201cwhose out-\nput value is solely and fully determined by the value input arguments.\u201d Indeed, torch\n-nn.functional provides many functions that work like the modules we find in nn.\nBut instead of working on the input arguments and stored parameters like the mod-\nule counterparts, they take inputs and parameters as arguments to the function call.\nFor instance, the functional counterpart of nn.Linear is nn.functional.linear,\nwhich is a function that has signature linear(input, weight, bias=None). The\nweight and bias parameters are arguments to the function.\n\nBack to our model, it makes sense to keep using nn modules for nn.Linear and\nnn.Conv2d so that Net will be able to manage their Parameters during training. How-\never, we can safely switch to the functional counterparts of pooling and activation,\nsince they have no parameters:\n\n# In[(28]:\nimport torch.nn.functional as F\n\nclass Net (nn.Module):\ndef __init (self):\nsuper().__init__()\nself.convl = nn.Conv2d(3, 16, kernel_size=3, padding=1)\nself.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\nself.fcl = nn.Linear(& * 8 * 8, 32)\nself.fc2 = nn.Linear(32, 2)\n\ndef forward(self, x):\n\nout = F.max_pool2d(torch.tanh(self.convi(x)), 2)\nout = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\nout = out.view(-1, 8 * 8 * 8)\n\nout = torch.tanh(self.fcl(out))\n\nout = self.fc2(out)\n\nreturn out\n\nThis is a lot more concise than and fully equivalent to our previous definition of Net\nin section 8.3.1. Note that it would still make sense to instantiate modules that require\nseveral parameters for their initialization in the constructor.\n\nTIP While general-purpose scientific functions like tanh still exist in\ntorch.nn.functional in version 1.0, those entry points are deprecated in\nfavor of functions in the top-level torch namespace. More niche functions\nlike max_poo124 will remain in torch.nn. functional.\n\nThus, the functional way also sheds light on what the nn.Module API is all about: a\nModule is a container for state in the forms of Parameters and submodules combined\nwith the instructions to do a forward.\n\nWhether to use the functional or the modular API is a decision based on style and\ntaste. When part of a network is so simple that we want to use nn. Sequential, we\u2019re in\nthe modular realm. When we are writing our own forwards, it may be more natural to\nuse the functional interface for things that do not need state in the form of parameters.\n\nIn chapter 15, we will briefly touch on quantization. Then stateless bits like activa-\ntions suddenly become stateful because information about the quantization needs to\nbe captured. This means if we aim to quantize our model, it might be worthwhile to\nstick with the modular API if we go for non-JITed quantization. There is one style mat-\nter that will help you avoid surprises with (originally unforeseen) uses: if you need sev-\neral applications of stateless modules (like nn.HardTanh or nn.ReLU), it is probably a\ngood idea to have a separate instance for each. Reusing the same module appears to\nbe clever and will give correct results with our standard Python usage here, but tools\nanalyzing your model may trip over it.\n\nSo now we can make our own nn.Module if we need to, and we also have the func-\ntional API for cases when instantiating and then calling an nn .Module is overkill. This\nhas been the last bit missing to understand how the code organization works in just\nabout any neural network implemented in PyTorch.\n\nLet\u2019s double-check that our model runs, and then we'll get to the training loop:\n\n# In[29]:\n\nmodel = Net ()\nmodel (img.unsqueeze (0) )}\n\n# Out[29]:\ntensor({[-0.0157, 0.1143)], grad_fn=<AddmmBackward>)\n\nWe got two numbers! Information flows correctly. We might not realize it right now,\nbut in more complex models, getting the size of the first linear layer right is some-\ntimes a source of frustration. We've heard stories of famous practitioners putting in\narbitrary numbers and then relying on error messages from PyTorch to backtrack the\ncorrect sizes for their linear layers. Lame, eh? Nah, it\u2019s all legit!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.13,
                        "section_name": "Training our convnet",
                        "section_path": "./screenshots-images-2/chapter_9/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_13/ae846891-88bc-4495-b9bd-40c739d9e0d9.png",
                            "./screenshots-images-2/chapter_9/section_13/68414965-989b-4efc-a190-36e95192f976.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8.4 Training our convnet\n\nWe\u2019re now at the point where we can assemble our complete training loop. We already\ndeveloped the overall structure in chapter 5, and the training loop looks much like\nthe one from chapter 6, but here we will revisit it to add some details like some track-\ning for accuracy. After we run our model, we will also have an appetite for a little more\nspeed, so we will learn how to run our models fast on a GPU. But first let\u2019s look at the\ntraining loop.\n\nRecall that the core of our convynet is two nested loops: an outer one over the\nepochs and an inner one of the DataLoader that produces batches from our Dataset.\nIn each loop, we then have to\n\na Feed the inputs through the model (the forward pass).\n\n2 Compute the loss (also part of the forward pass).\n\n3 Zero any old gradients.\n\n4 Call loss.backward() to compute the gradients of the loss with respect to all\n\nparameters (the backward pass) .\ns Have the optimizer take a step in toward lower loss.\n\nAlso, we collect and print some information. So here is our training loop, looking\nalmost as it does in the previous chapter\u2014but it is good to remember what each thing\n\nis doing:\nUses the datetime module\nincluded with Python Our loop over the epochs,\n# In[30]: numbered from 1 to n_epochs\nrather than starting at 0\n\nft import datetime\n\ndef training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n\nfor epoch in range(1, n_epochs + 1): +\nloss_train = 0.0 Loops over our dataset in\nFeeds a batch for imgs, labels in train_loader: the batches the data loader\ncreates for us\nthrones a * outputs = model (imgs)\nloss = loss_fn(outputs, labels) << ee and computes the loss\nAfter getting rid of : | we wish to minimize\nthe gradients from optimizer.zero_grad()\nthe last round ...\n\nloss. backward () + .-. performs the backward step. That is, we\n\ncompute the gradients of all parameters we\nwant the network to learn.\n\nUpdates > optimizer.step()\nthe model\n\n> loss_train += loss.item()\n\nif epoch == 1 or epoch % 10 ==\n\nSums the losses poch print('{} Epoch {}, Training loss {)}'.format (\n\nsaw over the epoch. :\n\nReal ht is impertant oe ee eee | Dh y the eg of the\n\nto transform the loss to a - ~ training data loader to get the\number with .item(), average loss per isisa\n\npoole the gradients. mt much more intuitive measure than\n\nthe sum.\n\nWe use the Dataset from chapter 7; wrap it into a DataLoader; instantiate our net-\nwork, an optimizer, and a loss function as before; and call our training loop.\n\nThe substantial changes in our model from the last chapter are that now our\nmodel is a custom subclass of nn.Module and that we\u2019re using convolutions. Let\u2019s run\ntraining for 100 epochs while printing the loss. Depending on your hardware, this\nmay take 20 minutes or more to finish!\n\nThe DataLoader batches up the examples of our cifar2 dataset.\nShuffling randomizes the order of the examples from the dataset.\n\n# In[31):\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch _size=64,\n\nshuffle=True)\n\nmodel = Net() # <_| Instantiates our network ... descent optiaze: ve hare\noptimizer = optim.SGD(model.parameters(), lr=le-2) # been working with ...\nloss_fn = nn.CrossEntropyLoss() # a,\ntraining_loop( 4 ) Calls the training jg and the cross entropy\n\nn_epochs = 100, loop we defined\n\noptimizer = optimizer, earlier\n\nmodel = model,\n\nloss_fn = loss_fn,\n\ntrain_loader = train_loader,\n\n)\n\n# Out[31]):\n\n2020-01-16 23:07:21.889707 Epoch 1, Training loss 0.5634813266954605\n2020-01-16 23:07:37.560610 Epoch 10, Training loss 0.3277610331109375\n2020-01-16 23:07:54.966180 Epoch 20, Training loss 0.3035225479086493\n2020-01-16 23:08:12.361597 Epoch 30, Training loss 0.28249378549824855\n2020-01-16 23:08:29.769820 Epoch 40, Training loss 0.2611226033253275\n2020-01-16 23:08:47.185401 Epoch 50, Training loss 0.24105800626574048\n2020-01-16 23:09:04.644522 Epoch 60, Training loss 0.21997178820477928\n2020-01-16 23:09:22.079625 Epoch 70, Training loss 0.20370126601047578\n2020-01-16 23:09:39.593780 Epoch 80, Training loss 0.18939699422401987\n2020-01-16 23:09:57.111441 Epoch 90, Training loss 0.17283396527266046\n2020-01-16 23:10:14.632351 Epoch 100, Training loss 0.1614033816868712\n\nSo now we can train our network. But again, our friend the bird watcher will likely not\nbe impressed when we tell her that we trained to very low training loss.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.14,
                        "section_name": "Measuring accuracy",
                        "section_path": "./screenshots-images-2/chapter_9/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_14/4fdda7a1-56dd-4fe4-a998-dcfc77e87c3b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "84.1 Measuring accuracy\nIn order to have a measure that is more interpretable than the loss, we can take a look\nat our accuracies on the training and validation datasets. We use the same code as in\nchapter 7:\n# In(32):\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch _size=64,\nshuffle=False)\nval_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\nshuffle=False)\ndef validate(model, train_loader, val_loader):\nfor name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\nt= 0\norn 0 We do not want gradients\n- here, as we will not want to\nwith torch.no_grad(): \u201cF the p eters.\nfor imgs, labels in loader: Counts the number of\noutputs = model (imgs) examples, so total is\nGives us the index ., predicted = torch.max(outputs, dim=1) increased by the batch\nvalue as output total += labels.shape[0) ee size\ncorrect += int((predicted == labels) .sum()) t\n\nprint (\"Accuracy {}: {:.2\u00a3}\".format(mame , correct / total))\n\nvalidate(model, train_loader, val_loader) Comparing the predicted class that had the\n\nmaximum probability and the ground-truth\n# Out [32]: labels, we first get a Boolean array. Taking the\nAccuracy train: 0.93 sum gives the number of items in the batch\nAccuracy val: 0.89 where the prediction and ground truth agree.\n\nWe cast to a Python int\u2014for integer tensors, this is equivalent to using .item(), simi-\nlar to what we did in the training loop.\n\nThis is quite a lot better than the fully connected model, which achieved only 79%\naccuracy. We about halved the number of errors on the validation set. Also, we used\nfar fewer parameters. This is telling us that the model does a better job of generalizing\nits task of recognizing the subject of images from a new sample, through locality and\ntranslation invariance. We could now let it run for more epochs and see what perfor-\nmance we could squeeze out.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.15,
                        "section_name": "Saving and loading our model",
                        "section_path": "./screenshots-images-2/chapter_9/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_15/91148777-2228-4f44-8f7e-c00dd8ebcb25.png",
                            "./screenshots-images-2/chapter_9/section_15/2a1e0922-1909-48a2-8934-e349e539590e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Saving and loading our model\nSince we\u2019re satisfied with our model so far, it would be nice to actually save it, right?\n\nIt\u2019s easy to do. Let\u2019s save the model to a file:\n\n# In(33]:\ntorch. save (model.state_dict(), data_path + \u2018birds_vs_airplanes.pt')\n\nThe birds_vs_airplanes.pt file now contains all the parameters of model: that is,\nweights and biases for the two convolution modules and the two linear modules. So,\n\nno structure\u2014just the weights. This means when we deploy the model in production\nfor our friend, we'll need to keep the model class handy, create an instance, and then\n\nload the parameters back into it:\nWe will have to make sure we don\u2019t change\nthe definition of Net between saving and\n\n# In(34): later loading the model state.\n\nloaded_model = Net () 2\nloaded_model . load_state_dict (torch. load(data_path\n+ *birds_vs_airplanes.pt'))\n\n# Out[34]:\n<All keys matched successfully>\n\nWe have also included a pretrained model in our code repository, saved to ../data/\nplch7/birds_vs_airplanes.pt.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.16,
                        "section_name": "Training on the GPU",
                        "section_path": "./screenshots-images-2/chapter_9/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_16/616cef70-82d9-45b4-977b-caa80e55c5bc.png",
                            "./screenshots-images-2/chapter_9/section_16/701b6cb9-52a7-433f-895b-44228754041b.png",
                            "./screenshots-images-2/chapter_9/section_16/0dae0892-0fc9-4cc5-8af6-4c7827ebad89.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8.4.3\n\nTraining on the GPU\n\nWe have a net and can train it! But it would be good to make it a bit faster. It is no sur-\nprise by now that we do so by moving our training onto the GPU. Using the .to\nmethod we saw in chapter 3, we can move the tensors we get from the data loader to\nthe GPU, after which our computation will automatically take place there. But we also\nneed to move our parameters to the GPU. Happily, nn.Module implements a . to func-\ntion that moves all of its parameters to the GPU (or casts the type when you pass a\ndtype argument).\n\nThere is a somewhat subtle difference between Module.to and Tensor.to.\nModule. to is in place: the module instance is modified. But Tensor .to is out of place\n(in some ways computation, just like Tensor.tanh), returning a new tensor. One\nimplication is that it is good practice to create the Optimizer after moving the param-\neters to the appropriate device.\n\nIt is considered good style to move things to the GPU if one is available. A good\npattern is to set the a variable device depending on torch. cuda.is_available:\n\n# In[35):\n\ndevice = (torch.device('cuda') if torch.cuda.is_available()\nelse torch.device('cpu'))\n\nprint(f*Training on device {device}.\")\n\nThen we can amend the training loop by moving the tensors we get from the data\nloader to the GPU by using the Tensor.to method. Note that the code is exactly like\nour first version at the beginning of this section except for the two lines moving the\ninputs to the GPU:\n\n# In[36):\nimport datetime\n\ndef training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\nfor epoch in range(1, n_epochs + 1):\nloss_train = 0.0\n\nfor imgs, labels in train_loader:\n\nimgs = imgs.to(device=device) = These two lines that move imgs and\nlabels = labels.to(device=device) labels to the device we are training\noutputs = model (imgs) on are the only difference from our\nloss = loss_fn(outputs, labels) previous version.\n\noptimizer.zero_grad()\nloss. backward ()\noptimizer.step()\n\nloss_train += loss.item()\n\nif epoch == 1 or epoch $ 10 ==\nprint('{} Epoch {}, Training loss {}'.format(\ndatetime.datetime.now(), epoch,\nloss_train / len(train_loader)))\n\nThe same amendment must be made to the validate function. We can then instanti-\nate our model, move it to device, and run it as before:\n\n# In(37]:\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\nshuffle=True)\n\nmodel = Net().to(device=device) Moves our model (all\noptimizer = optim.SGD(model.parameters(), lr=le-2) parameters) to the GPU. If\nloss_fn = nn.CrossEntropyLoss () you forget to move either the\nmodel or the inputs to the\ntraining_loop({ GPU, you will get errors about\nn_epochs = 100, tensors not being on the same\noptimizer = optimizer, device, because the PyTorch\nmodel = model, operators do not support\nloss_fn = loss_fn, mixing GPU and CPU inputs.\n\ntrain_loader = train_loader,\n)\n\n# Out (37):\n\n2020-01-16 23:10:35.563216 Epoch 1, Training loss 0.5717791349265227\n2020-01-16 23:10:39.730262 Epoch 10, Training loss 0.3285350770137872\n2020-01-16 23:10:45.906321 Epoch 20, Training loss 0.29493294959994637\n2020-01-16 23:10:52.086905 Epoch 30, Training loss 0.26962305994550134\n2020-01-16 23:10:56.551582 Epoch 40, Training loss 0.24709946277794564\n2020-01-16 23:11:00.991432 Epoch 50, Training loss 0.22623272664892446\n2020-01-16 23:11:05.421524 Epoch 60, Training loss 0.20996672821462534\n2020-01-16 23:11:09.951312 Epoch 70, Training loss 0.1934866009719053\n2020-01-16 23:11:14.499484 Epoch 80, Training loss 0.1799132404908253\n2020-01-16 23:11:19.047609 Epoch 90, Training loss 0.16620008706761774\n2020-01-16 23:11:23.590435 Epoch 100, Training loss 0.15667157247662544\n\n\u00ae There is a pin_memory option for the data loader that will cause the data loader to use memory pinned to\nthe GPU, with the goal of speeding up transfers. Whether we gain something varies, though, so we will not\npursue this here.\n\nEven for our small network here, we do see a sizable increase in speed. The advantage\nof computing on GPUs is more visible for larger models.\n\nThere is a slight complication when loading network weights: PyTorch will attempt\nto load the weight to the same device it was saved from\u2014that is, weights on the GPU\nwill be restored to the GPU. As we don\u2019t know whether we want the same device, we\nhave two options: we could move the network to the CPU before saving it, or move it\nback after restoring. It is a bit more concise to instruct PyTorch to override the device\ninformation when loading weights. This is done by passing the map_location keyword\nargument to torch. load:\n\n# In[39]:\n\nloaded_model = Net() .to(device=device)\n\nloaded_model . load_state_dict (torch. load(data_path\n+ \u2018birds_vs_airplanes.pt',\nmap_location=device) )\n\n# Out[39]:\n<All keys matched successfully>\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.17,
                        "section_name": "Model design",
                        "section_path": "./screenshots-images-2/chapter_9/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_17/bc6fb719-ffdf-4153-9b07-3c66d621928e.png",
                            "./screenshots-images-2/chapter_9/section_17/d7fafbca-8caf-4d93-b0f9-5045b000a4d9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8.5\n\nModel design\n\nWe built our model as a subclass of nn.Module, the de facto standard for all but the\nsimplest models. Then we trained it successfully and saw how to use the GPU to train\nour models. We've reached the point where we can build a feed-forward convolutional\nneural network and train it successfully to classify images. The natural question is,\nwhat now? What if we are presented with a more complicated problem? Admittedly,\nour birds versus airplanes dataset wasn\u2019t that complicated: the images were very small,\nand the object under investigation was centered and took up most of the viewport.\n\nIf we moved to, say, ImageNet, we would find larger, more complex images, where\nthe right answer would depend on multiple visual clues, often hierarchically orga-\nnized. For instance, when trying to predict whether a dark brick shape is a remote\ncontrol or a cell phone, the network could be looking for something like a screen.\n\nPlus images may not be our sole focus in the real world, where we have tabular\ndata, sequences, and text. The promise of neural networks is sufficient flexibility to\nsolve problems on all these kinds of data given the proper architecture (that is, the\ninterconnection of layers or modules) and the proper loss function.\n\nPyTorch ships with a very comprehensive collection of modules and loss functions\nto implement state-of-the-art architectures ranging from feed-forward components to\nlong short-term memory (LSTM) modules and transformer networks (two very popu-\nlar architectures for sequential data). Several models are available through PyTorch\nHub or as part of torchvision and other vertical community efforts.\n\nWe'll see a few more advanced architectures in part 2, where we'll walk through an\nend-to-end problem of analyzing CT scans, but in general, it is beyond the scope of this\nbook to explore variations on neural network architectures. However, we can build on\nthe knowledge we\u2019ve accumulated thus far to understand how we can implement\n\nalmost any architecture thanks to the expressivity of PyTorch. The purpose of this\nsection is precisely to provide conceptual tools that will allow us to read the latest\nresearch paper and start implementing it in PYTorch\u2014or, since authors often release\n\nPyTorch implementations of their papers, to read the implementations without chok-\ning on our coffee.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.18,
                        "section_name": "Adding memory capacity: Width",
                        "section_path": "./screenshots-images-2/chapter_9/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_18/ebd73d5f-233e-42f1-8269-74ed8792db80.png",
                            "./screenshots-images-2/chapter_9/section_18/402d2e45-be06-47e3-983c-b4164f9877b1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adding memory capacity: Width\n\nGiven our feed-forward architecture, there are a couple of dimensions we'd likely\nwant to explore before getting into further complications. The first dimension is the\nwidth of the network: the number of neurons per layer, or channels per convolution.\nWe can make a model wider very easily in PyTorch. We just specify a larger number of\noutput channels in the first convolution and increase the subsequent layers accord-\ningly, taking care to change the forward function to reflect the fact that we'll now\nhave a longer vector once we switch to fully connected layers:\n\n# In[40]:\nclass NetWidth(nn.Module) :\ndef __init__ (self):\n\nsuper().__init__()\nself.convl = nn.Conv2d(3, 32, kernel_size=3, padding=1)\nself.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\nself.fcl = nn.Linear(16 * 8 * 8, 32)\nself.fc2 = nn.Linear(32, 2)\n\ndef forward(self, x):\nout = F.max_pool2d(torch.tanh(self.convl(x)), 2)\nout = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\nout = out.view(-1, 16 * 8 * 8)\nout = torch.tanh(self.fcl(out))\nout = self. fe2(out)\nreturn out\n\nIf we want to avoid hardcoding numbers in the definition of the model, we can easily\npass a parameter to init and parameterize the width, taking care to also parameterize\nthe call to view in the forward function:\n\n# In[42]:\nclass NetWidth(nn.Module):\ndef __init__ (self, n_chans1=32):\nsuper().__init__()\nself.n_chansl = n_chans1l\nself.convl = nn.Conv2d(3, n_chansl, kernel_size=3, padding=1)\nself.conv2 = nn.Conv2d(n_chansl1, n_chansl // 2, kernel_size=3,\npadding=1)\n\nself.fcl = nn.Linear(8 * 8 * n_chansi // 2, 32)\nself.fc2 = nn.Linear(32, 2)\n\ndef forward(self, x):\nout = F.max_pool2d(torch.tanh(self.convl(x)), 2)\nout = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n\nout = out.view(-1, 8 * 8 * self.n_chansi // 2)\nout = torch.tanh(self.fel(out))\n\nout = self.fc2(out)\n\nreturn out\n\nThe numbers specifying channels and features for each layer are directly related to\nthe number of parameters in a model; all other things being equal, they increase the\ncapacity of the model. As we did previously, we can look at how many parameters our\nmodel has now:\n\n# In[44):\nsum(p.numel() for p in model.parameters())\n\n# Out[44]:\n38386\n\nThe greater the capacity, the more variability in the inputs the model will be able to\nmanage; but at the same time, the more likely overfitting will be, since the model can\nuse a greater number of parameters to memorize unessential aspects of the input. We\nalready went into ways to combat overfitting, the best being increasing the sample size\nor, in the absence of new data, augmenting existing data through artificial modifica-\ntions of the same data.\n\nThere are a few more tricks we can play at the model level (without acting on the\ndata) to control overfitting. Let\u2019s review the most common ones.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.19,
                        "section_name": "Helping our model to converge and generalize: Regularization",
                        "section_path": "./screenshots-images-2/chapter_9/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_19/1b7c2a40-c95f-4c52-9ecb-12a47675cd97.png",
                            "./screenshots-images-2/chapter_9/section_19/00cf5461-e1c6-460e-9bbe-a78c94c7f42a.png",
                            "./screenshots-images-2/chapter_9/section_19/c482855f-3283-4bb0-a6a2-f56afefe8e26.png",
                            "./screenshots-images-2/chapter_9/section_19/3b2656de-908e-487d-9d4c-9f07a118e1ac.png",
                            "./screenshots-images-2/chapter_9/section_19/df576725-ab65-4a2a-88e0-27557089989c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8.5.2\n\nHelping our model to converge and generalize: Regularization\n\nTraining a model involves two critical steps: optimization, when we need the loss to\ndecrease on the training set; and generalization, when the model has to work not only\non the training set but also on data it has not seen before, like the validation set. The\nmathematical tools aimed at easing these two steps are sometimes subsumed under\nthe label regularization.\n\nKEEPING THE PARAMETERS IN CHECK: WEIGHT PENALTIES\n\nThe first way to stabilize generalization is to add a regularization term to the loss. This\nterm is crafted so that the weights of the model tend to be small on their own, limiting\nhow much training makes them grow. In other words, it is a penalty on larger weight\nvalues. This makes the loss have a smoother topography, and there's relatively less to\ngain from fitting individual samples.\n\nThe most popular regularization terms of this kind are L2 regularization, which is\nthe sum of squares of all weights in the model, and L1 regularization, which is the sum\nof the absolute values of all weights in the model.\u00ae Both of them are scaled by a\n(small) factor, which is a hyperparameter we set prior to training.\n\nL2 regularization is also referred to as weight decay. The reason for this name is that,\nthinking about SGD and backpropagation, the negative gradient of the L2 regulariza-\ntion term with respect to a parameter w_i is - 2 * lambda * w_i, where lambda is the\naforementioned hyperparameter, simply named weight decayin PyTorch. So, adding L2\nregularization to the loss function is equivalent to decreasing each weight by an\namount proportional to its current value during the optimization step (hence, the\nname weight decay). Note that weight decay applies to all parameters of the network,\nsuch as biases.\n\nIn PyTorch, we could implement regularization pretty easily by adding a term to\nthe loss. After computing the loss, whatever the loss function is, we can iterate the\nparameters of the model, sum their respective square (for L2) or abs (for L1), and\nbackpropagate:\n\n# In[45]:\ndef training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\ntrain_loader):\nfor epoch in range(1, n_epochs + 1):\nloss_train = 0.0\nfor imgs, labels in train_loader:\nimgs = imgs.to(device=device)\nlabels = labels.to(device=device)\noutputs = model (imgs)\nloss = loss_fn(outputs, labels)\n\n12_lambda = 0.001 Replaces pow(2.0)\n12_norm = sum(p.pow(2.0).sum() with abs() for L1\nfor p in model.parameters()) <\u2014 regularization\n\nloss = loss + 12_lambda * 12_norm\n\noptimizer. zero_grad()\nloss. backward ()\noptimizer.step()\n\nloss_train += loss.item()\nif epoch == 1 or epoch % 10 == 0:\nprint('{} Epoch {}, Training loss {}'. format (\ndatetime.datetime.now(), epoch,\nloss_train / len(train_loader)))\n\nHowever, the SGD optimizer in PyTorch already has a weight_decay parameter that\ncorresponds to 2 * lambda, and it directly performs weight decay during the update\nas described previously. It is fully equivalent to adding the L2 norm of weights to the\nloss, without the need for accumulating terms in the loss and involving autograd.\n\nNOT RELYING TOO MUCH ON A SINGLE INPUT: DROPOUT\n\nAn effective strategy for combating overfitting was originally proposed in 2014 by Nit-\nish Srivastava and coauthors from Geoff Hinton\u2019s group in Toronto, in a paper aptly\nentitled \u201cDropout: a Simple Way to Prevent Neural Networks from Overfitting\u201d\n(http://mng.bz/nPMa). Sounds like pretty much exactly what we're looking for,\n\nright? The idea behind dropout is indeed simple: zero out a random fraction of out-\nputs from neurons across the network, where the randomization happens at each\ntraining iteration.\n\nThis procedure effectively generates slightly different models with different neu-\nron topologies at each iteration, giving neurons in the model less chance to coordi-\nnate in the memorization process that happens during overfitting. An alternative\npoint of view is that dropout perturbs the features being generated by the model,\nexerting an effect that is close to augmentation, but this time throughout the network.\n\nIn PyTorch, we can implement dropout in a model by adding an nn. Dropout mod-\nule between the nonlinear activation function and the linear or convolutional module\nof the subsequent layer. As an argument, we need to specify the probability with which\ninputs will be zeroed out. In case of convolutions, we'll use the specialized nn. Drop-\nout2d or nn. Dropout3d, which zero out entire channels of the input:\n\n# In[47):\nclass NetDropout (nn.Module) :\ndef __init__ (self, n_chans1=32):\nsuper ().__init__()\nself.n_chansl = n_chansi\nself.convl = nn.Conv2d(3, n_chansi, kernel_size=3, padding=1)\nself.convl_dropout = nn.Dropout2d(p=0.4)\nself.conv2 = nn.Conv2d(n_chansl, n_chansl // 2, kernel_size=3,\npadding=1)\n\nself.conv2_dropout = nn.Dropout2d(p=0.4)\nself.fcl = nn.Linear(8 * 8 * n_chansl // 2, 32)\nself.fe2 = nn.Linear(32, 2)\n\ndef forward(self, x):\nout = F.max_pool2d(torch.tanh(self.convl(x)), 2)\nout = self.convl_dropout (out)\nout = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\nout = self.conv2_dropout (out)\nout = out.view(-1, 8 * 8 * self.n_chansi // 2)\nout = torch.tanh(self.fel(out))\nout = self. fe2(out)\nreturn out\n\nNote that dropout is normally active during training, while during the evaluation of a\ntrained model in production, dropout is bypassed or, equivalently, assigned a proba-\nbility equal to zero. This is controlled through the train property of the Dropout\nmodule. Recall that PyTorch lets us switch between the two modalities by calling\n\nmodel .train()\nor\n\nmodel .eval({)\n\non any nn.Model subclass. The call will be automatically replicated on the submodules\nso that if Dropout is among them, it will behave accordingly in subsequent forward\nand backward passes.\n\nKEEPING ACTIVATIONS IN CHECK: BATCH NORMALIZATION\n\nDropout was all the rage when, in 2015, another seminal paper was published by\nSergey Ioffe and Christian Szegedy from Google, entitled \u201cBatch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift\u201d\n(https: //arxiv.org/abs/1502.03167). The paper described a technique that had mul-\ntiple beneficial effects on training: allowing us to increase the learning rate and make\ntraining less dependent on initialization and act as a regularizer, thus representing an\nalternative to dropout.\n\nThe main idea behind batch normalization is to rescale the inputs to the activa-\ntions of the network so that minibatches have a certain desirable distribution. Recall-\ning the mechanics of learning and the role of nonlinear activation functions, this\nhelps avoid the inputs to activation functions being too far into the saturated portion\nof the function, thereby killing gradients and slowing training.\n\nIn practical terms, batch normalization shifts and scales an intermediate input\nusing the mean and standard deviation collected at that intermediate location over\nthe samples of the minibatch. The regularization effect is a result of the fact that an\nindividual sample and its downstream activations are always seen by the model as\nshifted and scaled, depending on the statistics across the randomly extracted mini-\nbatch. This is in itself a form of principled augmentation. The authors of the paper\nsuggest that using batch normalization eliminates or at least alleviates the need\nfor dropout.\n\nBatch normalization in PyTorch is provided through the nn.BatchNorm1D,\nnn.BatchNorm2d, and nn.BatchNorm3d modules, depending on the dimensionality of\nthe input. Since the aim for batch normalization is to rescale the inputs of the activa-\ntions, the natural location is after the linear transformation (convolution, in this case)\nand the activation, as shown here:\n\n# In[49]:\nclass NetBatchNorm(nn.Module):\ndef __init__(self, n_chans1=32):\nsuper().__init__()\nself.n_chans1 = n_chans1\nself.convl = nn.Conv2d(3, n_chansl, kernel_size=3, padding=1)\nself.convl_batchnorm = nn. BatchNorm2d(num_features=n_chans1)\nself.conv2 = nn.Conv2d(n_chansl, n_chansl // 2, kernel_size=3,\npadding=1)\n\nself.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chansl // 2)\nself.fcl = nn.Linear(8 * 8 * n_chansl // 2, 32)\nself.fc2 = nn.Linear(32, 2)\n\ndef forward(self, x):\nout = self.convl_batchnorm(self.convi(x))\nout = F.max_pool2d(torch.tanh(out), 2)\n\nout = self.conv2_batchnorm(self.conv2 (out) )\nout = F.max_pool2d(torch.tanh(out), 2)\n\nout = out.view(-1, 8 * 8 * self.n_chansi // 2)\nout = torch.tanh(self.fcl(out))\n\nout = self.fc2(out)\n\nreturn out\n\nJust as for dropout, batch normalization needs to behave differently during training\nand inference. In fact, at inference time, we want to avoid having the output for a spe-\ncific input depend on the statistics of the other inputs we're presenting to the model.\nAs such, we need a way to still normalize, but this time fixing the normalization\nparameters once and for all.\n\nAs minibatches are processed, in addition to estimating the mean and standard\ndeviation for the current minibatch, PyTorch also updates the running estimates for\nmean and standard deviation that are representative of the whole dataset, as an\napproximation. This way, when the user specifies\n\nmodel .eval()\n\nand the model contains a batch normalization module, the running estimates are fro-\nzen and used for normalization. To unfreeze running estimates and return to using\nthe minibatch statistics, we call model .train(), just as we did for dropout.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.2,
                        "section_name": "Going deeper to learn more complex structures: Depth",
                        "section_path": "./screenshots-images-2/chapter_9/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_20/13923821-86a0-4c9f-bc07-43fad7c25f92.png",
                            "./screenshots-images-2/chapter_9/section_20/9f44093b-2c67-40ca-a72f-baec4161490a.png",
                            "./screenshots-images-2/chapter_9/section_20/e53a9f50-2bc9-4d27-9102-4405c37f1e56.png",
                            "./screenshots-images-2/chapter_9/section_20/65f27a17-9ddb-4751-be59-712527b86f54.png",
                            "./screenshots-images-2/chapter_9/section_20/f2d9423e-61b7-4732-baca-dac82b4db7f5.png",
                            "./screenshots-images-2/chapter_9/section_20/c79f16ef-5e8e-4fd8-8fba-af459702087c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Going deeper to learn more complex structures: Depth\n\nEarlier, we talked about width as the first dimension to act on in order to make a\nmodel larger and, in a way, more capable. The second fundamental dimension is obvi-\nously depth. Since this is a deep learning book, depth is something we're supposedly\ninto. After all, deeper models are always better than shallow ones, aren't they? Well, it\ndepends. With depth, the complexity of the function the network is able to approxi-\nmate generally increases. In regard to computer vision, a shallower network could\nidentify a person\u2019s shape in a photo, whereas a deeper network could identify the per-\nson, the face on their top half, and the mouth within the face. Depth allows a model\nto deal with hierarchical information when we need to understand the context in\norder to say something about some input.\n\nThere\u2019s another way to think about depth: increasing depth is related to increasing\nthe length of the sequence of operations that the network will be able to perform\nwhen processing input. This view\u2014of a deep network that performs sequential opera-\ntions to carry out a task\u2014is likely fascinating to software developers who are used to\nthinking about algorithms as sequences of operations like \u201cfind the person\u2019s boundar-\nies, look for the head on top of the boundaries, look for the mouth within the head.\u201d\n\nSKIP CONNECTIONS\n\nDepth comes with some additional challenges, which prevented deep learning models\nfrom reaching 20 or more layers until late 2015. Adding depth to a model generally\nmakes training harder to converge. Let\u2019s recall backpropagation and think about it in\n\nthe context of a very deep network. The derivatives of the loss function with respect to\nthe parameters, especially those in early layers, need to be multiplied by a lot of other\nnumbers originating from the chain of derivative operations between the loss and the\nparameter. Those numbers being multiplied could be small, generating ever-smaller\nnumbers, or large, swallowing smaller numbers due to floating-point approximation.\nThe bottom line is that a long chain of multiplications will tend to make the contribu-\ntion of the parameter to the gradient vanish, leading to ineffective training of that layer\nsince that parameter and others like it won't be properly updated.\nIn December 2015, Kaiming He and Ne / NETRES\n\ncoauthors presented residual networks (2p)\n\n(ResNets), an architecture that uses a\nsimple trick to allow very deep networks\nto be successfully trained (https://\narxiv.org/abs/1512.03385). That work\n\nLINEAR (32D->2D)\n\nReL\n\nopened the door to networks ranging\nfrom tens oflayers to 100 layers in depth,\nsurpassing the then state of the art in\ncomputer vision benchmark problems.\nWe encountered residual networks\nwhen we were playing with pretrained\n\nLINEAR ((N/2*1G)D->32D)\n\nVIEW ((N/2*1G)D)\n\nN/Z2CX4K4 *\n\n< .\nCONV2D(3x3, N/2C->N/20) | _} Eonwecrion\n\nmodels in chapter 2. The trick we men- N/2Cx8x8_fN------=-====-=~ \u201c\ntioned is the following: using a skip con-\nnection to short-circuit blocks of layers, as\n\nshown in figure 8.11.\n\nA skip connection is nothing but\nthe addition of the input to the output\nof a block of layers. This is exactly how\nit is done in PyTorch. Let\u2019s add one\nlayer to our simple convolutional\nmodel, and let's use ReLU as the acti-\nvation for a change. The vanilla mod-\nule with an extra layer looks like this:\n\nZz\nfed\n*\nS\n=\n\nMAXPOOL (2x2)\nNCx32x32_\nCONV2D (3x3, 3C->NC)\n\ni\n\nINPUT (3\u00a2, 32x32)\n\nFigure 8.11 The architecture of our network with\nthree convolutional layers. The skip connection is\nwhat differentiates Net Res from NetDepth.\n\n# In(51]:\nclass NetDepth(nn.Module):\ndef __init (self, n_chans1=32):\nsuper().__init__()\nself.n_chans1 = n_chansl\nself.convl = nn.Conv2d(3, n_chansl, kernel_size=3, padding=1)\nself.conv2 = nn.Conv2d(n_chansl, n_chansl // 2, kernel_size=3,\npadding=1)\nself.conv3 = nn.Conv2d(n_chansl // 2, n_chansl // 2,\nkernel_size=3, padding=1)\nnn.Linear(4 * 4 * n_chansi // 2, 32)\nnn.Linear(32, 2)\n\nself.fel\nself.fc2\n\ndef forward(self, x):\nout = F.max_pool2d(torch.relu(self.convi(x)), 2)\nout = F.max_pool2d(torch.relu(self.conv2(out)), 2)\nout = F.max_pool2d(torch.relu(self.conv3(out)), 2)\nout = out.view(-1, 4 * 4 * self.n_chansi // 2)\nout = torch.relu(self.fcl(out))\nout = self. fc2(out)\nreturn out\n\nAdding a skip connection a la ResNet to this model amounts to adding the output of\nthe first layer in the forward function to the input of the third layer:\n\n# In[53):\nclass NetRes(nn.Module):\ndef __init__ (self, n_chans1=32):\nsuper().__init__()\nself.n_chansl = n_chansi\nself.convl = nn.Conv2d(3, n_chansl, kernel_size=3, padding=1)\nself.conv2 = nn.Conv2d(n_chansl, n_chansl // 2, kernel_size=3,\npadding=1)\nself.conv3 = nn.Conv2d(n_chansl // 2, n_chansl // 2,\nkernel_size=3, padding=1)\nself.fcl = nn.Linear(4 * 4 * n_chansl // 2, 32)\nself. fc2 nn.Linear (32, 2)\n\ndef forward(self, x):\nout = F.max_pool2d(torch.relu(self.convi(x)), 2)\nout = F.max_pool2d(torch.relu(self.conv2(out)), 2)\nouti = out\nout = F.max_pool2d(torch.relu(self.conv3(out)) + outl, 2)\n\nout = out.view(-1, 4 * 4 * self.n_chansl // 2)\nout = torch.relu(self.fcl(out))\nout = self. fce2(out)\n\nreturn out\n\nIn other words, we're using the output of the first activations as inputs to the last, in\naddition to the standard feed-forward path. This is also referred to as identity mapping.\nSo, how does this alleviate the issues with vanishing gradients we were mentioning\nearlier?\n\nThinking about backpropagation, we can appreciate that a skip connection, or a\nsequence of skip connections in a deep network, creates a direct path from the deeper\nparameters to the loss. This makes their contribution to the gradient of the loss more\ndirect, as partial derivatives of the loss with respect to those parameters have a chance\nnot to be multiplied by a long chain of other operations.\n\nIt has been observed that skip connections have a beneficial effect on convergence\nespecially in the initial phases of training. Also, the loss landscape of deep residual\nnetworks is a lot smoother than feed-forward networks of the same depth and width.\n\nIt is worth noting that skip connections were not new to the world when ResNets\ncame along. Highway networks and U-Net made use of skip connections of one form\n\nor another. However, the way ResNets used skip connections enabled models of\ndepths greater than 100 to be amenable to training.\n\nSince the advent of ResNets, other architectures have taken skip connections to\nthe next level. One in particular, DenseNet, proposed to connect each layer with sev-\neral other layers downstream through skip connections, achieving state-of-the-art\nresults with fewer parameters. By now, we know how to implement something like\nDenseNets: just arithmetically add earlier intermediate outputs to downstream inter-\nmediate outputs.\n\nBUILDING VERY DEEP MODELS IN PYTORCH\n\nWe talked about exceeding 100 layers in a convolutional neural network. How can we\nbuild that network in PyTorch without losing our minds in the process? The standard\nstrategy is to define a building block, such as a (Conv2d, ReLU, Conv2d) + skip\nconnection block, and then build the network dynamically in a for loop. Let\u2019s see it\ndone in practice. We will create the network depicted in figure 8.12.\n\nRESBLOCK NETRESDEEP\nOUTPUT (2D, Nn CHANNELS) OUTPUT (2D)\n\nREL\n\nLINEAR ((N*G4)D->32D)\n\nx\n\n[errenenna5 we) |\na NCKBxS a\u201d\n[comantess. wee)\nui NOxioxie\nINPUT (2D, N CHANNELS) RESBLOCK(NC)\nNCKOXIG t RESBLOCKS\nRESBLOCK(NC)\nNexioxie\nNCX32x32 new\n\nCONV2D (3x3, 3C->NC)\n\nINPUT (3\u00a2, 32x32)\n\nFigure 8.12 Our deep architecture with residual connections. On the left, we define a simplistic\nresidual block. We use it as a building block in our network, as shown on the right.\n\nWe first create a module subclass whose sole job is to provide the computation for one\nblock\u2014that is, one group of convolutions, activation, and skip connection:\n\n# In[55):\nclass ResBlock(nn.Module) : The BatchNorm layer would\n\ndef _init__(self, n_chans): it is customarily left out.\nsuper (ResBlock, self).__init__()\n\nself.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\npadding=1, bias=False) <t\nself.batch_norm = nn.BatchNorm2d(num_features=n_chans)\ntorch.nn.init.kaiming_normal_(self.conv.weight,\nnonlinearity='relu')\ntorch.nn.init.constant_(self.batch_norm.weight, 0.5)\ntorch.nn.init.zeros_(self.batch_norm.bias)\n\nUses custom initializations\n\ndef forward(self, x): . kaiming_normal_ initializes with\nout = self.conv(x) normal random elements with standard\nout = self.batch_norm(out) deviation as computed in the ResNet paper.\nout = torch.relu(out) The batch norm is initialized to produce output\nreturn out + x distributions that initially have 0 mean and 0.5 variance.\n\nSince we're planning to generate a deep model, we are including batch normalization\nin the block, since this will help prevent gradients from vanishing during training.\nWe'd now like to generate a 100-block network. Does this mean we have to prepare for\nsome serious cutting and pasting? Not at all; we already have the ingredients for imag-\nining how this could look like.\n\nFirst, in init, we create nn.Sequential containing a list of ResBlock instances.\nnn. Sequential will ensure that the output of one block is used as input to the next. It\nwill also ensure that all the parameters in the block are visible to Net. Then, in forward,\nwe just call the sequential to traverse the 100 blocks and generate the output:\n\n# In[56):\nclass NetResDeep(nn.Module) :\ndef __init__ (self, n_chans1=32, n_blocks=10):\nsuper().__init__()\nself.n_chansl = n_chansi\nself.convl = nn.Conv2d(3, n_chansi, kernel_size=3, padding=1)\nself.resblocks = nn.Sequential (\n*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n\nself.fcl = nn.Linear(8 * 8 * n_chansl, 32)\nself.fc2 = nn.Linear(32, 2)\n\ndef forward(self, x):\nout = F.max_pool2d(torch.relu(self.convi(x)), 2)\nout = self.resblocks (out)\nout = F.max_pool2d(out, 2)\nout = out.view(-1, 8 * 8 * self.n_chans1)\nout = torch.relu(self.fel(out))\nout = self. fe2(out)\nreturn out\n\nIn the implementation, we parameterize the actual number of layers, which is import-\nant for experimentation and reuse. Also, needless to say, backpropagation will work as\nexpected. Unsurprisingly, the network is quite a bit slower to converge. It is also more\n\nfragile in convergence. This is why we used more-detailed initializations and trained\nour NetRes with a learning rate of 3e \u2014 3 instead of the le \u2014 2 we used for the other\nnetworks. We trained none of the networks to convergence, but we would not have\ngotten anywhere without these tweaks.\n\nAll this shouldn't encourage us to seek depth on a dataset of 32 x 32 images, but it\nclearly demonstrates how this can be achieved on more challenging datasets like Image-\nNet. It also provides the key elements for understanding existing implementations for\nmodels like ResNet, for instance, in torchvision.\n\nINITIALIZATION\n\nLet's briefly comment about the earlier initialization. Initialization is one of the\nimportant tricks in training neural networks. Unfortunately, for historical reasons,\nPyTorch has default weight initializations that are not ideal. People are looking at fix-\ning the situation; if progress is made, it can be tracked on GitHub (https://\ngithub.com/ pytorch/pytorch/issues/18182). In the meantime, we need to fix the\nweight initialization ourselves. We found that our model did not converge and looked\nat what people commonly choose as initialization (a smaller variance in weights; and\nzero mean and unit variance outputs for batch norm), and then we halved the output\nvariance in the batch norm when the network would not converge.\n\nWeight initialization could fill an entire chapter on its own, but we think that\nwould be excessive. In chapter 11, we'll bump into initialization again and use what\narguably could be PyTorch defaults without much explanation. Once you've pro-\ngressed to the point where the details of weight initialization are of specific interest to\nyou\u2014probably not before finishing this book\u2014you might revisit this topic.\u201d\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.21,
                        "section_name": "Comparing the designs from this section",
                        "section_path": "./screenshots-images-2/chapter_9/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_21/5eb07869-eb88-4e82-8713-a8523514ffe4.png",
                            "./screenshots-images-2/chapter_9/section_21/714a4664-bdea-44f6-bb35-d41a0f2aa57b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Comparing the designs from this section\n\nWe summarize the effect of each of our design modifications in isolation in figure\n8.13. We should not overinterpret any of the specific numbers\u2014our problem setup\nand experiments are simplistic, and repeating the experiment with different random\nseeds will probably generate variation at least as large as the differences in validation\naccuracy. For this demonstration, we left all other things equal, from learning rate to\nnumber of epochs to train; in practice, we would try to get the best results by varying\nthose. Also, we would likely want to combine some of the additional design elements.\n\nBut a qualitative observation may be in order: as we saw in section 5.5.3, when dis-\ncussing validatioin and overfitting, The weight decay and dropout regularizations,\nwhich have a more rigorous statistical estimation interpretation as regularization than\nbatch norm, have a much narrower gap between the two accuracies. Batch norm, which\n\nFigure 8.13 The modified networks all perform similarly.\n\nserves more as a convergence helper, lets us train the network to nearly 100% training\naccuracy, so we interpret the first two as regularization.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.22,
                        "section_name": "It's already outdated",
                        "section_path": "./screenshots-images-2/chapter_9/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_22/7993fa6e-80a7-47d7-ab05-55f8076dd104.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8.5.5\n\nIt\u2019s already outdated\n\nThe curse and blessing of a deep learning practitioner is that neural network architec-\ntures evolve at a very rapid pace. This is not to say that what we've seen in this chapter\nis necessarily old school, but a thorough illustration of the latest and greatest architec-\ntures is a matter for another book (and they would cease to be the latest and the great-\nest pretty quickly anyway). The take-home message is that we should make every effort\nto proficiently translate the math behind a paper into actual PyTorch code, or at least\nunderstand the code that others have written with the same intention. In the last few\nchapters, you have hopefully gathered quite a few of the fundamental skills to trans-\nlate ideas into implemented models in PyTorch.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.23,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_9/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_23/7335f248-f3a4-42d5-961b-05987c5e4ed7.png",
                            "./screenshots-images-2/chapter_9/section_23/937553b6-c042-4808-ae44-d6533d9ed4ae.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8.6\n\nConclusion\nAfter quite a lot of work, we now have a model that our fictional friend Jane can use to\nfilter images for her blog. All we have to do is take an incoming image, crop and resize\nit to 32 x 32, and see what the model has to say about it. Admittedly, we have solved\nonly part of the problem, but it was a journey in itself.\n\nWe have solved just part of the problem because there are a few interesting\nunknowns we would still have to face. One is picking out a bird or airplane from a\n\nlarger image. Creating bounding boxes around objects in an image is something a\nmodel like ours can\u2019t do.\n\nAnother hurdle concerns what happens when Fred the cat walks in front of the\ncamera. Our model will not refrain from giving its opinion about how bird-like the cat\nis! It will happily output \u201cairplane\u201d or \u201cbird,\u201d perhaps with 0.99 probability. This issue\nof being very confident about samples that are far from the training distribution is\ncalled overgeneralization. It\u2019s one of the main problems when we take a (presumably\ngood) model to production in those cases where we can\u2019t really trust the input (which,\nsadly, is the majority of real-world cases).\n\nIn this chapter, we have built reasonable, working models in PyTorch that can\nlearn from images. We did it in a way that helped us build our intuition around convo-\nlutional networks. We also explored ways in which we can make our models wider and\ndeeper, while controlling effects like overfitting. Although we still only scratched the\nsurface, we have taken another significant step ahead from the previous chapter. We\nnow have a solid basis for facing the challenges we'll encounter when working on\ndeep learning projects.\n\nNow that we're familiar with PyTorch conventions and common features, we're\nready to tackle something bigger. We're going to transition from a mode where each\nchapter or two presents a small problem, to spending multiple chapters breaking\ndown a bigger, real-world problem. Part 2 uses automatic detection of lung cancer as\nan ongoing example; we will go from being familiar with the PyTorch API to being\nable to implement entire projects using PyTorch. We'll start in the next chapter by\nexplaining the problem from a high level, and then we'll get into the details of the\ndata we'll be using.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 10,
                "chapter_name": "Using PyTorch\nto fight cancer",
                "chapter_path": "./screenshots-images-2/chapter_10",
                "sections": [
                    {
                        "section_id": 10.1,
                        "section_name": "Using PyTorch\nto fight cancer",
                        "section_path": "./screenshots-images-2/chapter_10/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_1/e65dc5d0-0401-4dc8-b6f3-ea05e7765798.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "We have two main goals for this chapter. We'll start by covering the overall plan for\npart 2 of the book so that we have a solid idea of the larger scope the following indi-\nvidual chapters will be building toward. In chapter 10, we will begin to build out the\ndata-parsing and data-manipulation routines that will produce data to be con-\nsumed in chapter 11 while training our first model. In order to do what's needed\nfor those upcoming chapters well, we'll also use this chapter to cover some of the\ncontext in which our project will be operating: we'll go over data formats, data\nsources, and exploring the constraints that our problem domain places on us. Get\nused to performing these tasks, since you'll have to do them for any serious deep\nlearning project!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.2,
                        "section_name": "Introduction to the use case",
                        "section_path": "./screenshots-images-2/chapter_10/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_2/94c4cf17-8d3e-4c08-affe-b918ecc874c9.png",
                            "./screenshots-images-2/chapter_10/section_2/049b9522-9c6e-4dee-a3dd-b42f5a9db6d0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction to the use case\nOur goal for this part of the book is to give you the tools to deal with situations where\nthings aren\u2019t working, which is a far more common state of affairs than part 1 might have\nled you to believe. We can\u2019t predict every failure case or cover every debugging tech-\nnique, but hopefully we'll give you enough to not feel stuck when you encounter a new\nroadblock. Similarly, we want to help you avoid situations with your own projects where\nyou have no idea what you could do next when your projects are under-performing.\nInstead, we hope your ideas list will be so long that the challenge will be to prioritize!\nIn order to present these ideas and techniques, we need a context with some\nnuance and a fair bit of heft to it. We\u2019ve chosen automatic detection of malignant\ntumors in the lungs using only a CT scan of a patient\u2019s chest as input. We'll be focus-\ning on the technical challenges rather than the human impact, but make no mis-\ntake\u2014even from just an engineering perspective, part 2 will require a more serious,\nstructured approach than we needed in part | in order to have the project succeed.\n\nNOTE CT scans are essentially 3D X-rays, represented as a 3D array of single-\nchannel data. We'll cover them in more detail soon.\n\nAs you might have guessed, the title of this chapter is more eye-catching, implied hyper-\nbole than anything approaching a serious statement of intent. Let us be precise: our\nproject in this part of the book will take three-dimensional CT scans of human torsos as\ninput and produce as output the location of suspected malignant tumors, if any exist.\n\nDetecting lung cancer early has a huge impact on survival rate, but is difficult to do\nmanually, especially in any comprehensive, whole-population sense. Currently, the\nwork of reviewing the data must be performed by highly trained specialists, requires\npainstaking attention to detail, and it is dominated by cases where no cancer exists.\n\nDoing that job well is akin to being placed in front of 100 haystacks and being told,\n\u201cDetermine which of these, if any, contain a needle.\u201d Searching this way results in the\npotential for missed warning signs, particularly in the early stages when the hints are\nmore subtle. The human brain just isn\u2019t built well for that kind of monotonous work.\nAnd that, of course, is where deep learning comes in.\n\nAutomating this process is going to give us experience working in an uncoopera-\ntive environment where we have to do more work from scratch, and there are fewer\neasy answers to problems that we might run into. Together, we'll get there, though!\nOnce you're finished reading part 2, we think you'll be ready to start working on a\nreal-world, unsolved problem of your own choosing.\n\nWe chose this problem of lung tumor detection for a few reasons. The primary rea-\nson is that the problem itself is unsolved! This is important, because we want to make\nit clear that you can use PyTorch to tackle cutting-edge projects effectively. We hope\nthat increases your confidence in PyTorch as a framework, as well as in yourself as a\ndeveloper. Another nice aspect of this problem space is that while it\u2019s unsolved, a lot\nof teams have been paying attention to it recently and have seen promising results.\nThat means this challenge is probably right at the edge of our collective ability to\nsolve; we won't be wasting our time on a problem that\u2019s actually decades away from\n\nreasonable solutions. That attention on the problem has also resulted in a lot of high-\nquality papers and open source projects, which are a great source of inspiration and\nideas. This will be a huge help once we conclude part 2 of the book, if you are inter-\nested in continuing to improve on the solution we create. We'll provide some links to\nadditional information in chapter 14.\n\nThis part of the book will remain focused on the problem of detecting lung\ntumors, but the skills we'll teach are general. Learning how to investigate, preprocess,\nand present your data for training is important no matter what project you're working\non. While we'll be covering preprocessing in the specific context of lung tumors, the\ngeneral idea is that this is what you should be prepared to do for your project to succeed.\nSimilarly, setting up a training loop, getting the right performance metrics, and tying\nthe project\u2019s models together into a final application are all general skills that we'll\nemploy as we go through chapters 9 through 14.\n\nNOTE While the end result of part 2 will work, the output will not be accurate\nenough to use clinically. We're focusing on using this as a motivating example\nfor teaching PyTorch, not on employing every last trick to solve the problem.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.3,
                        "section_name": "Preparing for a large-scale project",
                        "section_path": "./screenshots-images-2/chapter_10/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_3/e5ddee63-eb68-48e4-ba3e-2526ee8793d2.png",
                            "./screenshots-images-2/chapter_10/section_3/f67e6ca6-4552-4734-87d7-553ca7a01f4e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Preparing for a large-scale project\n\nThis project will build off of the foundational skills learned in part 1. In particular, the\ncontent covering model construction from chapter 8 will be directly relevant.\nRepeated convolutional layers followed by a resolution-reducing downsampling layer\nwill still make up the majority of our model. We will use 3D data as input to our\nmodel, however. This is conceptually similar to the 2D image data used in the last few\nchapters of part 1, but we will not be able to rely on all of the 2D-specific tools avail-\nable in the PyTorch ecosystem.\n\nThe main differences between the work we did with convolutional models in chap-\nter 8 and what we'll do in part 2 are related to how much effort we put into things out-\nside the model itself. In chapter 8, we used a provided, off-the-shelf dataset and did\nlittle data manipulation before feeding the data into a model for classification. Almost\nall of our time and attention were spent building the model itself, whereas now we're\nnot even going to begin designing the first of our two model architectures until chap-\nter 11. That is a direct consequence of having nonstandard data without prebuilt\nlibraries ready to hand us training samples suitable to plug into a model. We'll have to\nlearn about our data and implement quite a bit ourselves.\n\nEven when that\u2019s done, this will not end up being a case where we convert the CT to\na tensor, feed it into a neural network, and have the answer pop out the other side. As\nis common for real-world use cases such as this, a workable approach will be more com-\nplicated to account for confounding factors such as limited data availability, finite\ncomputational resources, and limitations on our ability to design effective models. Please\nkeep that in mind as we build to a high-level explanation of our project architecture.\n\nSpeaking of finite computational resources, part 2 will require access to a GPU to\nachieve reasonable training speeds, preferably one with at least 8 GB of RAM. Trying\n\nto train the models we will build on CPU could take weeks!! If you don\u2019t have a GPU\nhandy, we provide pretrained models in chapter 14; the nodule analysis script there\ncan probably be run overnight. While we don\u2019t want to tie the book to proprietary ser-\nvices if we don\u2019t have to, we should note that at the time of writing, Colaboratory\n(https://colab.research.google.com) provides free GPU instances that might be of\nuse. PyTorch even comes preinstalled! You will also need to have at least 220 GB of\nfree disk space to store the raw training data, cached data, and trained models.\n\nNOTE Many of the code examples presented in part 2 have complicating\ndetails omitted. Rather than clutter the examples with logging, error han-\ndling, and edge cases, the text of this book contains only code that expresses\nthe core idea under discussion. Full working code samples can be found on\nthe book\u2019s website (www.manning.com/books/deep-learning-with-pytorch)\nand GitHub (https://github.com/deep-learning-with-pytorch/dlwpt-code).\n\nOK, we\u2019ve established that this is a hard, multifaceted problem, but what are we going\nto do about it? Instead of looking at an entire CT scan for signs of tumors or their\npotential malignancy, we're going to solve a series of simpler problems that will com-\nbine to provide the end-to-end result we're interested in. Like a factory assembly line,\neach step will take raw materials (data) and/or output from previous steps, perform\nsome processing, and hand off the result to the next station down the line. Not every\nproblem needs to be solved this way, but breaking off chunks of the problem to solve\nin isolation is often a great way to start. Even if it turns out to be the wrong approach\nfor a given project, it\u2019s likely we'll have learned enough while working on the individ-\nual chunks that we'll have a good idea how to restructure our approach into some-\nthing successful.\n\nBefore we get into the details of how we'll break down our problem, we need to\nlearn some details about the medical domain. While the code listings will tell you what\nwe're doing, learning about radiation oncology will explain why. Learning about the\nproblem space is crucial, no matter what domain it is. Deep learning is powerful, but\nit\u2019s not magic, and trying to apply it blindly to nontrivial problems will likely fail.\nInstead, we have to combine insights into the space with intuition about neural net-\nwork behavior. From there, disciplined experimentation and refinement should give\nus enough information to close in on a workable solution.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.4,
                        "section_name": "What is a CT scan, exactly?",
                        "section_path": "./screenshots-images-2/chapter_10/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_4/57c176ec-32e0-425b-bcdd-1fa959cc402e.png",
                            "./screenshots-images-2/chapter_10/section_4/e36a8625-6eed-4661-a792-019c2fe13ad9.png",
                            "./screenshots-images-2/chapter_10/section_4/86e9a08b-b6af-4ac2-99bb-af8e24135892.png",
                            "./screenshots-images-2/chapter_10/section_4/4ccd6b8c-6ac9-476f-bc47-317e5fa2ba79.png",
                            "./screenshots-images-2/chapter_10/section_4/307b8be6-190f-4302-81db-95aa61db66eb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is a CT scan, exactly?\n\nBefore we get too far into the project, we need to take a moment to explain what a CT\nscan is. We will be using data from CT scans extensively as the main data format for\nour project, so having a working understanding of the data format\u2019s strengths, weak-\nnesses, and fundamental nature will be crucial to utilizing it well. The key point we\nnoted earlier is this: CT scans are essentially 3D X-rays, represented as a 3D array of\n\nsingle-channel data. As we might recall from chapter 4, this is like a stacked set of gray-\nscale PNG images.\n\nVoxel\n\nA voxel is the 3D equivalent to the familiar two-dimensional pixel. It encloses a vol-\nume of space (hence, \u201cvolumetric pixel\"), rather than an area, and is typically\narranged in a 3D grid to represent a field of data. Each of those dimensions will have\na measurable distance associated with it. Often, voxels are cubic, but for this chap-\nter, we will be dealing with voxels that are rectangular prisms.\n\nIn addition to medical data, we can see similar voxel data in fluid simulations, 3D\nscene reconstructions from 2D images, light detection and ranging (LIDAR) data for\nself-driving cars, and many other problem spaces. Those spaces all have their individ-\nual quirks and subtleties, and while the APIs that we're going to cover here apply gen-\nerally, we must also be aware of the nature of the data we\u2019re using with those APIs if we\nwant to be effective.\n\nEach voxel of a CT scan has a numeric value that roughly corresponds to the aver-\nage mass density of the matter contained inside. Most visualizations of that data show\nhigh-density material like bones and metal implants as white, low-density air and lung\ntissue as black, and fat and tissue as various shades of gray. Again, this ends up looking\nsomewhat similar to an X-ray, with some key differences.\n\nThe primary difference between CT scans and X-rays is that whereas an X-ray is a\nprojection of 3D intensity (in this case, tissue and bone density) onto a 2D plane, a CT\nscan retains the third dimension of the data. This allows us to render the data in a vari-\nety of ways: for example, as a grayscale solid, which we can see in figure 9.1.\n\nFigure 9.1 A CT scan of a human torso\nshowing, from the top, skin, organs,\nspine, and patient support bed. Source:\nhttp://mng.bz/04r6; Mindways CT\nSoftware / CC BY-SA 3.0 (https://\ncreativecommons.org/licenses/by-sa/\n3.0/deed.en).\n\n\nNOTE CT scans actually measure radiodensity, which is a function of both\nmass density and atomic number of the material under examination. For our\npurposes here, the distinction isn\u2019t relevant, since the model will consume\nand learn from the CT data no matter what the exact units of the input hap-\npen to be.\n\nThis 3D representation also allows us to \u201csee inside\u201d the subject by hiding tissue types\nwe are not interested in. For example, we can render the data in 3D and restrict visibil-\nity to only bone and lung tissue, as in figure 9.2.\n\nFigure 9.2 ACT scan\n400.0 showing ribs, spine, and\nX 500.0 lung structures\n\nCT scans are much more difficult to acquire than X-rays, because doing so requires a\nmachine like the one shown in figure 9.3 that typically costs upward of a million dol-\nlars new and requires trained staff to operate it. Most hospitals and some well-\nequipped clinics have a CT scanner, but they aren\u2019t nearly as ubiquitous as X-ray\nmachines. This, combined with patient privacy regulations, can make it somewhat dif-\nficult to get CT scans unless someone has already done the work of gathering and\norganizing a collection of them.\n\nFigure 9.3 also shows an example bounding box for the area contained in the CT\nscan. The bed the patient is resting on moves back and forth, allowing the scanner to\nimage multiple slices of the patient and hence fill the bounding box. The scanner\u2019s\ndarker, central ring is where the actual imaging equipment is located.\n\nAfinal difference between a CT scan and an X-ray is that the data isa digital-only format.\nCT stands for computed tomography (https://en.wikipedia.org/wiki/CT_scan#Process).\n\nFigure 9.3 A patient\ninside a CT scanner, with\nthe CT scan\u2019s bounding\nbox overlaid. Other than\nin stock photos, patients\ndon\u2019t typically wear\nstreet clothes while\n\nin the machine.\n\nThe raw output of the scanning process doesn\u2019t look particularly meaningful to the human\neye and must be properly reinterpreted by a computer into something we can understand.\nThe settings of the CT scanner when the scan is taken can have a large impact on the result-\ning data.\n\nWhile this information might not seem particularly relevant, we have actually\nlearned something that is: from figure 9.3, we can see that the way the CT scanner\nmeasures distance along the head-to-foot axis is different than the other two axes. The\npatient actually moves along that axis! This explains (or at least is a strong hint as to)\nwhy our voxels might not be cubic, and also ties into how we approach massaging our\ndata in chapter 12. This is a good example of why we need to understand our problem\nspace if we're going to make effective choices about how to solve our problem. When\nstarting to work on your own projects, be sure you do the same investigation into the\n\nFigure 9.3 A patient\ninside a CT scanner, with\nthe CT scan\u2019s bounding\nbox overlaid. Other than\nin stock photos, patients\ndon\u2019t typically wear\nstreet clothes while\n\nin the machine.\n\nThe raw output of the scanning process doesn\u2019t look particularly meaningful to the human\neye and must be properly reinterpreted by a computer into something we can understand.\nThe settings of the CT scanner when the scan is taken can have a large impact on the result-\ning data.\n\nWhile this information might not seem particularly relevant, we have actually\nlearned something that is: from figure 9.3, we can see that the way the CT scanner\nmeasures distance along the head-to-foot axis is different than the other two axes. The\npatient actually moves along that axis! This explains (or at least is a strong hint as to)\nwhy our voxels might not be cubic, and also ties into how we approach massaging our\ndata in chapter 12. This is a good example of why we need to understand our problem\nspace if we're going to make effective choices about how to solve our problem. When\nstarting to work on your own projects, be sure you do the same investigation into the\ndetails of your data.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.5,
                        "section_name": "The project: An end-to-end detector for lung cancer",
                        "section_path": "./screenshots-images-2/chapter_10/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_5/0f976094-eeb8-4b99-99e3-d525b2f2d342.png",
                            "./screenshots-images-2/chapter_10/section_5/fe887f56-26ec-4815-84ad-3ec84b1a9535.png",
                            "./screenshots-images-2/chapter_10/section_5/dbaa66dd-832d-44d5-9648-a4898d235b34.png",
                            "./screenshots-images-2/chapter_10/section_5/68565749-f7e2-4270-988f-03da1d83a015.png",
                            "./screenshots-images-2/chapter_10/section_5/0b13e749-8f1f-4b2f-a9ac-c47d4419fe5d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The project: An end-to-end detector for lung cancer\n\nNow that we've got our heads wrapped around the basics of CT scans, let\u2019s discuss the\nstructure of our project. Most of the bytes on disk will be devoted to storing the CT\nscans\u2019 3D arrays containing density information, and our models will primarily con-\nsume various subslices of those 3D arrays. We're going to use five main steps to go\nfrom examining a whole-chest CT scan to giving the patient a lung cancer diagnosis.\n\nOur full, end-to-end solution shown in figure 9.4 will load CT data files to produce\na Ct instance that contains the full 3D scan, combine that with a module that per-\nforms segmentation (flagging voxels of interest), and then group the interesting voxels\ninto small lumps in the search for candidate nodules.\n\nThe nodule locations are combined back with the CT voxel data to produce nod-\nule candidates, which can then be examined by our nodule classification model to\ndetermine whether they are actually nodules in the first place and, eventually, whether\n\nSTEP 2 (OH. 13):\nSEGMENTATION\n\nSTEP 4 (CH. lH12):\nSTEP | (CH. 10): CLASSIFICATION\nDATA LOADING\nCANDIDATE CANDIDATE e G\nLOCATIONS SAMPLE [IN ook\n\nslr URC), + POS,\n\ne204\n\nSEGMENTATION oor\nMODEL + CLASSIFICATION\nSTEP 3 (CH. 14): \u201crr mops ]\nGROUPING ] STEP 5 (OK. 14):\nNODULE ANALYSIS\nAND DIAGNOSIS\n\nMAL/BEN\n\nP2084\n\nFigure 9.4 The end-to-end process of taking a full-chest CT scan and determining whether the patent has a\nmalignant tumor\n\nAmass of tissue made of proliferating cells in the lung is a tumor. A tumor can be benign\nor it can be malignant, in which case it is also referred to as cancer. A small tumor in\nthe lung (just a few millimeters wide) is called a nodule. About 40% of lung nodules turn\nout to be malignant\u2014small cancers. It is very important to catch those as early as pos-\nsible, and this depends on medical imaging of the kind we are looking at here.\n\nthey're malignant. This latter task is particularly difficult because malignancy might\nnot be apparent from CT imaging alone, but we'll see how far we get. Last, each of\nthose individual, per-nodule classifications can then be combined into a whole-patient\ndiagnosis.\n\nIn more detail, we will do the following:\n\na Load our raw CT scan data into a form that we can use with PyTorch. Putting\nraw data into a form usable by PyTorch will be the first step in any project you\nface. The process is somewhat less complicated with 2D image data and simpler\nstill with non-image data.\n\n2 Identify the voxels of potential tumors in the lungs using PyTorch to implement\na technique known as segmentation. This is roughly akin to producing a heatmap\nof areas that should be fed into our classifier in step 3. This will allow us to focus\non potential tumors inside the lungs and ignore huge swaths of uninteresting\nanatomy (a person can\u2019t have lung cancer in the stomach, for example).\n\nGenerally, being able to focus on a single, small task is best while learning.\nWith experience, there are some situations where more complicated model\nstructures can yield superlative results (for example, the GAN game we saw in\nchapter 2), but designing those from scratch requires extensive mastery of the\nbasic building blocks first. Gotta walk before you run, and all that.\n\n3 Group interesting voxels into lumps: that is, candidate nodules (see figure 9.5\nfor more information on nodules). Here, we will find the rough center of each\nhotspot on our heatmap.\n\nEach nodule can be located by the index, row, and column of its center point.\nWe do this to present a simple, constrained problem to the final classifier.\nGrouping voxels will not involve PyTorch directly, which is why we've pulled this\nout into a separate step. Often, when working with multistep solutions, there will\nbe non-deep-learning glue steps between the larger, deep-learning-powered\nportions of the project.\n\n4 Classify candidate nodules as actual nodules or non-nodules using 3D convolution.\n\nThis will be similar in concept to the 2D convolution we covered in chapter 8.\nThe features that determine the nature ofa tumor from a candidate structure are\nlocal to the tumor in question, so this approach should provide a good balance\nbetween limiting input data size and excluding relevant information. Making\nscope-limiting decisions like this can keep each individual task constrained,\nwhich can help limit the amount of things to examine when troubleshooting.\n\ns Diagnose the patient using the combined per-nodule classifications.\n\nSimilar to the nodule classifier in the previous step, we will attempt to deter-\nmine whether the nodule is benign or malignant based on imaging data alone. We\nwill take a simple maximum of the per-tumor malignancy predictions, as only one\ntumor needs to be malignant for a patient to have cancer. Other projects might\nwant to use different ways of aggregating the per-instance predictions into a file\nscore. Here, we are asking, \u201cIs there anything suspicious?\u201d so maximum is a good\nfit for aggregation. If we were looking for quantitative information like \u201cthe ratio\nof type A tissue to type B tissue,\u201d we might take an appropriate mean instead.\n\nFigure 9.4 only depicts the final path through the system once we've built and trained\nall of the requisite models. The actual work required to train the relevant models will\nbe detailed as we get closer to implementing each step.\n\nThe data we'll use for training provides human-annotated output for both steps 3\nand 4, This allows us to treat steps 2 and 3 (identifying voxels and grouping them into\nnodule candidates) as almost a separate project from step 4 (nodule candidate\n\nOn the shoulders of giants\n\nWe are standing on the shoulders of giants when deciding on this five-step approach.\nWe'll discuss these giants and their work more in chapter 14. There isn't any partic-\nular reason why we should know in advance that this project structure will work well\nfor this problem; instead, we're relying on others who have actually implemented sim-\nilar things and reported success when doing so. Expect to have to experiment to find\nworkable approaches when transitioning to a different domain, but always try to learn\nfrom earlier efforts in the space and from those who have worked in similar areas and\nhave discovered things that might transfer well. Go out there, look for what others\nhave done, and use that as a benchmark. At the same time, avoid getting code and\nrunning it blindly, because you need to fully understand the code you're running in\norder to use the results to make progress for yourself.\n\nclassification). Human experts have annotated the data with nodule locations, so we can\nwork on either steps 2 and 3 or step 4 in whatever order we prefer.\n\nWe will first work on step 1 (data loading), and then jump to step 4 before we come\nback and implement steps 2 and 3, since step 4 (classification) requires an approach\nsimilar to what we used in chapter 8, using multiple convolutional and pooling layers to\naggregate spatial information before feeding it into a linear classifier. Once we've got\na handle on our classification model, we can start working on step 2 (segmentation).\nSince segmentation is the more complicated topic, we want to tackle it without having\nto learn both segmentation and the fundamentals of CT scans and malignant tumors at\nthe same time. Instead, we'll explore the cancer-detection space while working on a\nmore familiar classification problem.\n\nThis approach of starting in the middle of the problem and working our way out\nprobably seems odd. Starting at step 1 and working our way forward would make more\nintuitive sense. Being able to carve up the problem and work on steps independently\nis useful, however, since it can encourage more modular solutions; in addition, it\u2019s eas-\nier to partition the workload between members of a small team. Also, actual clinical\nusers would likely prefer a system that flags suspicious nodules for review rather than\nprovides a single binary diagnosis. Adapting our modular solution to different use\ncases will probably be easier than if we\u2019d done a monolithic, from-the-top system.\n\nAs we work our way through implementing each step, we'll be going into a fair bit\nof detail about lung tumors, as well as presenting a lot of fine-grained detail about CT\nscans. While that might seem off-topic for a book that\u2019s focused on PyTorch, we're\ndoing so specifically so that you begin to develop an intuition about the problem\nspace. That\u2019s crucial to have, because the space of all possible solutions and\napproaches is too large to effectively code, train, and evaluate.\n\nIf we were working on a different project (say, the one you tackle after finishing\nthis book), we'd still need to do an investigation to understand the data and problem\nspace. Perhaps you're interested in satellite mapping, and your next project needs to\nconsume pictures of our planet taken from orbit. You'd need to ask questions about\nthe wavelengths being collected\u2014do you get only normal RGB, or something more\n\nexotic? What about infrared or ultraviolet? In addition, there might be impacts on the\nimages based on time of day, or if the imaged location isn\u2019t directly under the satellite,\nskewing the image. Will the image need correction?\n\nEven if your hypothetical third project\u2019s data type remains the same, it\u2019s probable\nthat the domain you'll be working in will change things, possibly drastically. Processing\ncamera output for self-driving cars still involves 2D images, but the complications and\ncaveats are wildly different. For example, it\u2019s much less likely that a mapping satellite\nwill need to worry about the sun shining into the camera, or getting mud on the lens!\n\nWe must be able to use our intuition to guide our investigation into potential opti-\nmizations and improvements. That\u2019s true of deep learning projects in general, and\nwe'll practice using our intuition as we go through part 2. So, let\u2019s do that. Take a\nquick step back, and do a gut check. What does your intuition say about this\napproach? Does it seem overcomplicated to you?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.6,
                        "section_name": "Why can\u2019t we just throw data at a neural network until it works?",
                        "section_path": "./screenshots-images-2/chapter_10/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_6/e3e02c86-bd6d-497f-9bbc-09b2378cde69.png",
                            "./screenshots-images-2/chapter_10/section_6/56778149-8689-48d5-a590-56381b69cba6.png",
                            "./screenshots-images-2/chapter_10/section_6/7c7ffafd-2337-40ad-9eef-c11631b618e2.png",
                            "./screenshots-images-2/chapter_10/section_6/6ed9968e-78c0-4058-8ce0-13a8f08098f8.png",
                            "./screenshots-images-2/chapter_10/section_6/f1f08627-155f-42d2-ba89-2c5695368318.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Why can\u2019t we just throw data at a neural network until it works?\n\nAfter reading the last section, we couldn\u2019t blame you for thinking, \u201cThis is nothing\nlike chapter 8!\u201d You might be wondering why we've got two separate model architec-\ntures or why the overall data flow is so complicated. Well, our approach is different\nfrom that in chapter 8 for a reason. It\u2019s a hard task to automate, and people haven't\nfully figured it out yet. That difficulty wanslates to complexity; once we as a society\nhave solved this problem definitively, there will probably be an off-the-shelf library\npackage we can grab to have it Just Work, but we're not there just yet.\n\nWhy so difficult, though?\n\nWell, for starters, the majority of a CT scan is fundamentally uninteresting with\nregard to answering the question, \u201cDoes this patient have a malignant tumor?\u201d This\nmakes intuitive sense, since the vast majority of the patient\u2019s body will consist of\nhealthy cells. In the cases where there is a malignant tumor, up to 99.9999% of the\nvoxels in the CT still won\u2019t be cancer. That ratio is equivalent to a two-pixel blob of\nincorrectly tinted color somewhere on a high-definition television, or a single mis-\nspelled word out of a shelf of novels.\n\nCan you identify the white dot in the three views of figure 9.5 that has been flagged\nas a nodule?*\n\nIf you need a hint, the index, row, and column values can be used to help find the\nrelevant blob of dense tissue. Do you think you could figure out the relevant proper-\nties of tumors given only images (and that means only the images\u2014no index, row, and\ncolumn information!) like these? What if you were given the entire 3D scan, not just\nthree slices that intersect the interesting part of the scan?\n\nNOTE Don\u2019t fret if you can\u2019t locate the tumor! We're trying to illustrate just\nhow subtle this data can be\u2014the fact that it is hard to identify visually is the\nentire point of this example.\n\nINDEX 522 ROW 267 COL 367\n\noF : \u00b0\n200 300 400 \u00a9 \u00ab6500 100 200 300 400 500 \u00a9 060 200 300 400 500\n\nFigure 9.6 ACT scan with approximately 1,000 structures that look like tumors to the untrained eye. Exactly one\nhas been identified as a nodule when reviewed by a human specialist. The rest are normal anatomical structures\nlike blood vessels, lesions, and other non-problematic lumps.\n\nYou might have seen elsewhere that end-to-end approaches for detection and classi-\nfication of objects are very successful in general vision tasks. TorchVision includes end-\nto-end models like Fast RCCNN/Mask R-CNN, but these are typically trained on\nhundreds of thousands of images, and those datasets aren\u2019t constrained by the number\nof samples from rare classes. The project architecture we will use has the benefit of\nworking well with a more modest amount of data. So while it\u2019s certainly theoretically\npossible to just throw an arbitrarily large amount of data at a neural network until it\nlearns the specifics of the proverbial lost needle, as well as how to ignore the hay, it\u2019s\ngoing to be practically prohibitive to collect enough data and wait for a long enough\ntime to train the network properly. That won't be the best approach since the results are\npoor, and most readers won't have access to the compute resources to pull it off at all.\n\nTo come up with the best solution, we could investigate proven model designs that\ncan better integrate data in an end-to-end manner.* These complicated designs are\ncapable of producing high-quality results, but they're not the best because understand-\ning the design decisions behind them requires having mastered fundamental con-\ncepts first. That makes these advanced models poor candidates to use while teaching\nthose same fundamentals!\n\nThat\u2019s not to say that our multistep design is the best approach, either, but that\u2019s\nbecause \u201cbest\u201d is only relative to the criteria we chose to evaluate approaches. There are\nmany \u201cbest\u201d approaches, just as there are many goals we could have in mind as we work\non a project. Our self-contained, multistep approach has some disadvantages as well.\n\nRecall the GAN game from chapter 2. There, we had two networks cooperating to\nproduce convincing forgeries of old master artists. The artist would produce a candi-\ndate work, and the scholar would critique it, giving the artist feedback on how to\n\n\u2018For example, Retina U-Net (https://arxiv.org/pdf/1811.08661.pdf) and FishNet (http://mng.bz/K240).\n\nimprove. Put in technical terms, the structure of the model allowed gradients to back-\npropagate from the final classifier (fake or real) to the earliest parts of the project\n(the artist).\n\nOur approach for solving the problem won't use end-to-end gradient backpropa-\ngation to directly optimize for our end goal. Instead, we'll optimize discrete chunks of\nthe problem individually, since our segmentation model and classification model\nwon't be trained in tandem with each other. That might limit the top-end effectiveness\nof our solution, but we feel that this will make for a much better learning experience.\n\nWe feel that being able to focus on a single step at a time allows us to zoom in and\nconcentrate on the smaller number of new skills we're learning. Each of our two mod-\nels will be focused on performing exactly one task. Similar to a human radiologist as\nthey review slice after slice of CT, the job gets much easier to train for if the scope is\nwell contained. We also want to provide tools that allow for rich manipulation of the\ndata. Being able to zoom in and focus on the detail of a particular location will have a\nhuge impact on overall productivity while training the model compared to having to\nlook at the entire image at once. Our segmentation model is forced to consume the\nentire image, but we will structure things so that our classification model gets a\nzoomed-in view of the areas of interest.\n\nStep 3 (grouping) will produce and step 4 (classification) will consume data simi-\nlar to the image in figure 9.6 containing sequential transverse slices of a tumor. This\nimage is a close-up view of a (potentially malignant, or at least indeterminate) tumor,\nand it is what we\u2019re going to train the step 4 model to identify, and the step 5 model to\nclassify as either benign or malignant. While this lump may seem nondescript to an\nuntrained eye (or untrained convolutional network), identifying the warning signs of\nmalignancy in this sample is at least a far more constrained problem than having to\nconsume the entire CT we saw earlier. Our code for the next chapter will provide rou-\ntines to produce zoomed-in nodule images like figure 9.6.\n\nWe will perform the step 1 data-loading work in chapter 10, and chapters 11 and\n12 will focus on solving the problem of classifying these nodules. After that, we'll back\nup to work on step 2 (using segmentation to find the candidate tumors) in chapter 13,\nand then we'll close out part 2 of the book in chapter 14 by implementing the end-to-\nend project with step 3 (grouping) and step 5 (nodule analysis and diagnosis).\n\nNOTE Standard rendering of CTs places the superior at the top of the image\n(basically, the head goes up), but CTs order their slices such that the first slice\nis the inferior (toward the feet). So, Matplotlib renders the images upside\ndown unless we take care to flip them. Since that flip doesn\u2019t really matter to\nour model, we won't complicate the code paths between our raw data and the\nmodel, but we will add a flip to our rendering code to get the images right-\nside up. For more information about CT coordinate systems, see section 10.4.\n\nSLICE 7\n\n20 30 4o lo 20 30 40\n\nSLICE I SLICE 13\n\n\u00b0o \\o 20 30 4o \u00b0 \\o 20 30 40 \u00b0\n\nFigure 9.6 A close-up, multislice crop of the tumor from the CT scan in figure 9.5\n\nLet\u2019s repeat our high-level overview in figure 9.7.\n\n\nSTEP 2 (CH. 13):\n\nner \\ (oh. wy SEGMENTATION STEP 4 (OM. 112):\nSTEP LCCH. 10} CLASSIFICATION\nDATA LOADING\n|\nCANDIDATE CANDIDATE\nom LOCATIONS \u2018SAMPI [NEG,\n+ DATA\n\n5 + tc RC), L + POS,\n[~\\ (ERC), NEG,\n\nP02\nSEGMENTATION (LR), ooo\nwepeL f CLASSIFICATION\nSTEP 3 (CH. 14) \u201crr MODEL J\nGROUPING ]\n\nSTEP 5 (OH. IH):\nNODULE ANALYSIS.\nfe} fe} AND DIAGNOSIS.\nfol MAL/BEN\n\n20.4\nFigure 9.7 The end-to-end process of taking a full-chest CT scan and determining whether the patient has a\nmalignant tumor\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.7,
                        "section_name": "What is a nodule?",
                        "section_path": "./screenshots-images-2/chapter_10/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_7/b991aa50-c3bc-4810-9ad0-1ee7a18835b4.png",
                            "./screenshots-images-2/chapter_10/section_7/6f760350-3607-461d-b533-61aa43ba84b6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is a nodule?\n\nAs we've said, in order to understand our data well enough to use it effectively, we\nneed to learn some specifics about cancer and radiation oncology. One last key thing\nwe need to understand is what a nodule is. Simply put, a nodule is any of the myriad\nlumps and bumps that might appear inside someone\u2019s lungs. Some are problematic\nfrom a health-of-the-patient perspective; some are not. The precise definition* limits\nthe size of a nodule to 3 cm or less, with a larger lump being a lung mass; but we're\ngoing to use nodule interchangeably for all such anatomical structures, since it\u2019s a\nsomewhat arbitrary cutoff and we're going to deal with lumps on both sides of 3 cm\nusing the same code paths. A nodule\u2014a small mass in the lung\u2014can turn out to be\nbenign or a malignant tumor (also referred to as cancer). From a radiological perspec-\ntive, a nodule is really similar to other lumps that have a wide variety of causes: infec-\ntion, inflammation, blood-supply issues, malformed blood vessels, and diseases other\nthan tumors.\n\nThe key part is this: the cancers that we are trying to detect will always be nodules,\neither suspended in the very non-dense tissue of the lung or attached to the lung wall.\nThat means we can limit our classifier to only nodules, rather than have it examine all\ntissue. Being able to restrict the scope of expected inputs will help our classifier learn\nthe task at hand.\n\nThis is another example of how the underlying deep learning techniques we'll use\nare universal, but they can\u2019t be applied blindly.\u00ae We'll need to understand the field\nwe\u2019re working in to make choices that will serve us well.\n\nIn figure 9.8, we can see a stereotypical example of a malignant nodule. The smallest\nnodules we'll be concerned with are only a few millimeters across, though the one in\nfigure 9.8 is larger. As we discussed earlier in the chapter, this makes the smallest nod-\nules approximately a million times smaller than the CT scan as a whole. More than half\nof the nodules detected in patients are not malignant.\u201d\n\nINDEX 522\n\n200 300 400\n\nINDEX 93\n\n4o\n\n\u00b0 io 20 30 4o\n\nFigure 9.8 A CT scan with a malignant nodule displaying a visual discrepancy from other nodules\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.8,
                        "section_name": "Our data source: The LUNA Grand Challenge",
                        "section_path": "./screenshots-images-2/chapter_10/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_8/79803c9e-0052-4b86-be63-d0122d9e7a5b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Our data source: The LUNA Grand Challenge\n\nThe CT scans we were just looking at come from the LUNA (LUng Nodule Analysis)\nGrand Challenge. The LUNA Grand Challenge is the combination of an open dataset\nwith high-quality labels of patient CT scans (many with lung nodules) and a public\nranking of classifiers against the data. There is something of a culture of publicly shar-\ning medical datasets for research and analysis; open access to such data allows\nresearchers to use, combine, and perform novel work on this data without having to\nenter into formal research agreements between institutions (obviously, some data is\nkept private as well). The goal of the LUNA Grand Challenge is to encourage\nimprovements in nodule detection by making it easy for teams to compete for high\npositions on the leader board. A project team can test the efficacy of their detection\nmethods against standardized criteria (the dataset provided). To be included in the\npublic ranking, a team must provide a scientific paper describing the project architec-\nture, training methods, and so on. This makes for a great resource to provide further\nideas and inspiration for project improvements.\n\nNOTE Many CT scans \u201cin the wild\u201d are incredibly messy, in terms of idiosyn-\ncrasies between various scanners and processing programs. For example,\nsome scanners indicate areas of the CT scan that are outside of the scanner\u2019s\nfield of view by setting the density of those voxels to something negative. CT\nscans can also be acquired with a variety of settings on the CT scanner, which\ncan change the resulting image in ways ranging from subtly to wildly differ-\nent. Although the LUNA data is generally clean, be sure to check your\nassumptions if you incorporate other data sources.\n\nWe will be using the LUNA 2016 dataset. The LUNA site (https://lunal6.grand-challenge\n.org/Description) describes two tracks for the challenge: the first track, \u201cNodule detec-\ntion (NDET),\u201d roughly corresponds to our step 1 (segmentation); and the second track,\n\u201cFalse positive reduction (FPRED),\u201d is similar to our step 3 (classification). When the site\ndiscusses \u201clocations of possible nodules,\u201d it is talking about a process similar to what we'll\ncover in chapter 13.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.9,
                        "section_name": "Downloading the LUNA data",
                        "section_path": "./screenshots-images-2/chapter_10/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_9/19263e16-ec64-4a29-89ac-4436095355d5.png",
                            "./screenshots-images-2/chapter_10/section_9/bb89b807-5fe8-4c77-b912-219cbb0a52db.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Downloading the LUNA data\n\nBefore we go any further into the nuts and bolts of our project, we'll cover how to get\nthe data we'll be using. It\u2019s about 60 GB of data compressed, so depending on your\ninternet connection, it might take a while to download. Once uncompressed, it takes\nup about 120 GB of space; and we'll need another 100 GB or so of cache space to\nstore smaller chunks of data so that we can access it more quickly than reading in the\nwhole CT.\u2019\n\nNavigate to https://lunal6.grand-challenge.org/download and either register\nusing email or use the Google OAuth login. Once logged in, you should see two down-\nload links to Zenodo data, as well as a link to Academic Torrents. The data should be\nthe same from either.\n\nTIP The luna.grand-challenge.org domain does not have links to the data\ndownload page as of this writing. If you are having issues finding the down-\nload page, double-check the domain for lunal6., not luna., and reenter the\nURL if needed.\n\nThe data we will be using comes in 10 subsets, aptly named subset 0 through subset9.\nUnzip each of them so you have separate subdirectories like code/data-unversioned/\npart2/luna/subset0, and so on. On Linux, you'll need the 7z decompression utility\n(Ubuntu provides this via the p7zip-full package). Windows users can get an\nextractor from the 7-Zip website (www.7-zip.org). Some decompression utilities will\nnot be able to open the archives; make sure you have the full version of the extractor\nif you get an error.\n\nIn addition, you need the candidates.csv and annotations.csv files. We\u2019ve included\nthese files on the book\u2019s website and in the GitHub repository for convenience, so\nthey should already be present in code/data/part2/luna/*.csv. They can also be\ndownloaded from the same location as the data subsets.\n\nNOTE If you do not have easy access to ~220 GB of free disk space, it\u2019s possi-\nble to run the examples using only 1 or 2 of the 10 subsets of data. The\nsmaller training set will result in the model performing much more poorly,\nbut that\u2019s better than not being able to run the examples at all.\n\nOnce you have the candidates file and at least one subset downloaded, uncompressed,\nand put in the correct location, you should be able to start running the examples in\nthis chapter. If you want to jump ahead, you can use the code/p2ch09_explore_data\n-ipynb Jupyter Notebook to get started. Otherwise, we'll return to the notebook in\nmore depth later in the chapter. Hopefully your downloads will finish before you start\nreading the next chapter!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.1,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_10/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_10/7d6203ad-d23d-4d5c-b496-3e3962cbcd07.png",
                            "./screenshots-images-2/chapter_10/section_10/2d34ff43-7949-40c2-9b90-52dbad2b0568.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\nWe've made major strides toward finishing our project! You might have the feeling\nthat we haven't accomplished much; after all, we haven't implemented a single line of\ncode yet. But keep in mind that you'll need to do research and preparation as we have\nhere when you tackle projects on your own.\n\nIn this chapter, we set out to do two things:\n\na Understand the larger context around our lung cancer-detection project\n\n2 Sketch out the direction and structure of our project for part 2\n\nIf you still feel that we haven\u2019t made real progress, please recognize that mindset as a\ntrap\u2014understanding the space your project is working in is crucial, and the design\n\nwork we've done will pay off handsomely as we move forward. We'll see those divi-\ndends shortly, once we start implementing our data-loading routines in chapter 10.\n\nSince this chapter has been informational only, without any code, we\u2019ll skip the\nexercises for now.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 11,
                "chapter_name": "Combining data sources\ninto a unified dataset",
                "chapter_path": "./screenshots-images-2/chapter_11",
                "sections": [
                    {
                        "section_id": 11.1,
                        "section_name": "Combining data sources\ninto a unified dataset",
                        "section_path": "./screenshots-images-2/chapter_11/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_1/ef52dd0c-a990-4a97-883a-4321394ab73d.png",
                            "./screenshots-images-2/chapter_11/section_1/764d9156-dae0-42fe-a72a-c0165fbfa539.png",
                            "./screenshots-images-2/chapter_11/section_1/2b7d906a-1fcb-43c5-be30-71858e881b99.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Now that we've discussed the high-level goals for part 2, as well as outlined how the\ndata will flow through our system, let\u2019s get into specifics of what we\u2019re going to do in\nthis chapter. It\u2019s time to implement basic data-loading and data-processing routines\nfor our raw data. Basically, every significant project you work on will need something\nanalogous to what we cover here.\u2019 Figure 10.1 shows the high-level map of our proj-\nect from chapter 9. We'll focus on step 1, data loading, for the rest of this chapter.\nOur goal is to be able to produce a training sample given our inputs of raw CT\nscan data and a list of annotations for those CTs. This might sound simple, but\nquite a bit needs to happen before we can load, process, and extract the data we\u2019re\n\nFigure10.1 Ourend-to-end lung cancer detection project, with a focus on this chapter's topic: step 1, data loading\n\ninterested in. Figure 10.2 shows what we'll need to do to turn our raw data into a train-\n\ning sample. Luckily, we got a head start on understanding our data in the last chapter,\nbut we have more work to do on that front as well.\n\nCT FILES Pong\n\nReh SAMPLE TUPLE\n+ \u2014\u2014_\n| =| ( Piecnaed veal\n\n7\n} IS NODULE?\nwie | (ERO, 0 \u201cE\nmerce, CLRC),\n0100 (LR), \"Ls 23\"\n0010 \u201cont\n0001 \u201c YO\npa. + we ] CANDIDATE\none SRO)\nNv (KN,2Z)\n\nFigure 10.2 The data transforms required to\nmake a sample tuple. These sample tuples will\nbe used as input to our model training routine.\n\nThis is a crucial moment, when we begin to transmute the leaden raw data, if not into\ngold, then at least into the stuff that our neural network will spin into gold. We first dis-\ncussed the mechanics of this transformation in chapter 4.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.2,
                        "section_name": "Raw CT data files",
                        "section_path": "./screenshots-images-2/chapter_11/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_2/b4c0d622-77ef-41d1-b8c1-b64d401b0e6d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Raw CT data files\n\nOur CT data comes in two files: a .mhd file containing metadata header information,\nand a .raw file containing the raw bytes that make up the 3D array. Each file\u2019s name starts\nwith a unique identifier called the series UID (the name comes from the Digital Imaging\nand Communications in Medicine [DICOM] nomenclature) for the CT scan in ques-\ntion. For example, for series UID 1.2.3, there would be two files: 1.2.3.mhd and 1.2.3.raw.\n\nOur Ct class will consume those two files and produce the 3D array, as well as the\ntransformation matrix to convert from the patient coordinate system (which we will\ndiscuss in more detail in section 10.6) to the index, row, column coordinates needed\nby the array (these coordinates are shown as (I,R,C) in the figures and are denoted\nwith _irc variable suffixes in the code). Don\u2019t sweat the details of all this right now;\njust remember that we've got some coordinate system conversion to do before we can\napply these coordinates to our CT data. We'll explore the details as we need them.\n\nWe will also load the annotation data provided by LUNA, which will give us a list of\nnodule coordinates, each with a malignancy flag, along with the series UID of the rel-\nevant CT scan. By combining the nodule coordinate with coordinate system transfor-\nmation information, we get the index, row, and column of the voxel at the center of\nour nodule.\n\nUsing the (I,R,C) coordinates, we can crop a small 3D slice of our CT data to use as\nthe input to our model. Along with this 3D sample array, we must construct the rest of\nour training sample tuple, which will have the sample array, nodule status flag, series\nUID, and the index of this sample in the CT list of nodule candidates. This sample\ntuple is exactly what PyTorch expects from our Dataset subclass and represents the\nlast section of our bridge from our original raw data to the standard structure of\nPyTorch tensors.\n\nLimiting or cropping our data so as not to drown our model in noise is important,\nas is making sure we're not so aggressive that our signal gets cropped out of our\ninput. We want to make sure the range of our data is well behaved, especially after\nnormalization. Clamping our data to remove outliers can be useful, especially if our\ndata is prone to extreme outliers. We can also create handcrafted, algorithmic trans-\nformations of our input; this is known as feature engineering; and we discussed it briefly\nin chapter 1. We'll usually want to let the model do most of the heavy lifting; feature\nengineering has its uses, but we won't use it here in part 2.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.3,
                        "section_name": "Parsing LUNA\u2019s annotation data",
                        "section_path": "./screenshots-images-2/chapter_11/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_3/7a55eb1e-f6d0-4100-98ca-7a66c089552e.png",
                            "./screenshots-images-2/chapter_11/section_3/52128339-82cc-452a-bd37-db867a37206e.png",
                            "./screenshots-images-2/chapter_11/section_3/0ea97f99-4ca1-450a-8f70-3b4e2900c7ca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Parsing LUNA\u2019s annotation data\n\nThe first thing we need to do is begin loading our data. When working on a new proj-\nect, that\u2019s often a good place to start. Making sure we know how to work with the raw\ninput is required no matter what, and knowing how our data will look after it loads\n\ncan help inform the structure of our early experiments. We could try loading individ-\nual CT scans, but we think it makes sense to parse the CSV files that LUNA provides,\nwhich contain information about the points of interest in each CT scan. As we can see\nin figure 10.3, we expect to get some coordinate information, an indication of\nwhether the coordinate is a nodule, and a unique identifier for the CT scan. Since\nthere are fewer types of information in the CSV files, and they're easier to parse, we're\nhoping they will give us some clues about what to look for once we start loading CTs.\n\nFigure 10.3 The LUNA annotations in candidates.csv contain the CT series, the nodule\ncandidate's position, and a flag indicating if the candidate is actually a nodule or not.\n\nThe candidates.csv file contains information about all lumps that potentially look like\nnodules, whether those lumps are malignant, benign tumors, or something else alto-\ngether. We'll use this as the basis for building a complete list of candidates that can\nthen be split into our training and validation datasets. The following Bash shell ses-\n\nsion shows what the file contains:\n\nCounts the number\n$ we -1 candidates.csv < of lines in the file\n551066 candidates.csv\n\nPrints the first few\n$ head data/part2/luna/candidates.cev <;\u2014! lines of the file\nseriesuid, coordX, coordY, coordZ,class <\n1.3...6860,-56.08,-67.85,-311.92,0 The first line of the .csv file\n1.3...6860,53.21,-244.41,-245.17,0 defines the column headers.\n\n1.3...6860,103.66,-121.8,-286.62,0\n\n1.3...6860,-33.66,-72.75,-308.41,0\nCounts the number of lines\n\nthat end with 1, which\n$ grep ',1$' candidates.csv | we -1 indicates malignancy\n1351\n\nNOTE The values in the seriesuid column have been elided to better fit the\nprinted page.\n\nSo we have 551,000 lines, each with a seriesuid (which we'll call series_uid in the\ncode), some (X,Y,Z) coordinates, and a class column that corresponds to the nodule\nstatus (it\u2019s a Boolean value: 0 for a candidate that is not an actual nodule, and 1 fora\ncandidate that is a nodule, either malignant or benign). We have 1,351 candidates\nflagged as actual nodules.\n\nThe annotations.csv file contains information about some of the candidates that\nhave been flagged as nodules. We are interested in the diameter_mm information in\nparticular:\n\nThis is a different\n$ we -1 annotations.csv number than in the\n1187 annotations.csv  < candidates.csv file.\n- The last column\n$ head data/part2/luna/annotations.csv is also different.\n\nseriesuid, coordxX, coordY, coordZ,diameter_mm\n1.3.6...6860,-128.6994211,-175.3192718, -298.3875064,5.651470635\n1.3.6...6860,103.7836509, -211.9251487, -227.12125,4.224708481\n1.3.6...5208,69.63901724, -140.9445859, 876.3744957,5.786347814\n1.3.6...0405,-24.0138242,192.1024053,-391.0812764,8.143261683\n\nWe have size information for about 1,200 nodules. This is useful, since we can use it to\nmake sure our training and validation data includes a representative spread of nodule\nsizes. Without this, it\u2019s possible that our validation set could end up with only extreme\nvalues, making it seem as though our model is underperforming.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.4,
                        "section_name": "1 Training and validation sets",
                        "section_path": "./screenshots-images-2/chapter_11/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_4/2696258c-cb5a-448c-935d-e2de7c72ab08.png",
                            "./screenshots-images-2/chapter_11/section_4/68781ea1-3dba-4a3f-b98e-8f9113cfb415.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "For any standard supervised learning task (classification is the prototypical example),\nwe'll split our data into training and validation sets. We want to make sure both sets\nare representative of the range of real-world input data we're expecting to see and han-\ndle normally. If either set is meaningfully different from our real-world use cases, it\u2019s\npretty likely that our model will behave differently than we expect\u2014all of the training\nand statistics we collect won\u2019t be predictive once we transfer over to production use!\nWe're not trying to make this an exact science, but you should keep an eye out in\nfuture projects for hints that you are training and testing on data that doesn\u2019t make\nsense for your operating environment.\n\nLet\u2019s get back to our nodules. We're going to sort them by size and take every Nth\none for our validation set. That should give us the representative spread we're looking\n\nfor. Unfortunately, the location information provided in annotations.csv doesn\u2019t\nalways precisely line up with the coordinates in candidates.csv:\n\n$ grep 100225287222365663678666836860 annotations.csv\n\n1.3.6. ..6860,-128.6994211, -175.3192718, -298.3875064,5.651470635 \u00ab 7\n\n1.3.6...6860,103.7836509, -211.9251487, -227.12125,4.224708481 These two\ncoordinates\n\n$ grep '100225287222365663678666836860.*,1$' candidates.csv are very close\n\n1.3.6...6860,104.16480444, -211.685591018, -227.011363746,1 to each other.\n\n1.3.6...6860,-128.94,-175.04,-297.87,1 + 4\n\nIf we truncate the corresponding coordinates from each file, we end up with (-128.70,\n-175.32,-298.39) versus (-128.94,-175.04,-297.87). Since the nodule in question has\na diameter of 5 mm, both of these points are clearly meant to be the \u201ccenter\u201d of the\nnodule, but they don\u2019t line up exactly. It would be a perfectly valid response to decide\nthat dealing with this data mismatch isn\u2019t worth it, and to ignore the file. We are going\nto do the legwork to make things line up, though, since real-world datasets are often\nimperfect this way, and this is a good example of the kind of work you will need to do\nto assemble data from disparate data sources.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.5,
                        "section_name": "Unifying our annotation and candidate data",
                        "section_path": "./screenshots-images-2/chapter_11/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_5/7714f666-de57-417f-a1b4-bac7742b3867.png",
                            "./screenshots-images-2/chapter_11/section_5/d39c1cf1-4d82-4297-b206-f835f19fb525.png",
                            "./screenshots-images-2/chapter_11/section_5/3b664ce0-dc5b-4efa-b73b-a8b0568d475b.png",
                            "./screenshots-images-2/chapter_11/section_5/d84c7fca-3788-4f89-9e46-46affa520031.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": ". Unifying our annotation and candidate data\n\nNow that we know what our raw data files look like, let\u2019s build a getCandidateInfo-\nList function that will stitch it all together. We'll use a named tuple that is defined at\nthe top of the file to hold the information for each nodule.\n\nListing 10.1 dsets.py:7\n\nfrom collections import namedtuple\n# ... line 27\nCandidateInfoTuple = namedtuple(\n*\u201cCandidateInfoTuple',\n*\u2018isNodule_bool, diameter_mm, series_uid, center_xyz',\n)\n\nThese tuples are not our training samples, as they\u2019re missing the chunks of CT data we\nneed. Instead, these represent a sanitized, cleaned, unified interface to the human-\nannotated data we're using. It\u2019s very important to isolate having to deal with messy\ndata from model training. Otherwise, your training loop can get cluttered quickly,\nbecause you have to keep dealing with special cases and other distractions in the mid-\ndle of code that should be focused on training.\n\nTIP Clearly separate the code that\u2019s responsible for data sanitization from\nthe rest of your project. Don\u2019t be afraid to rewrite your data once and save it\nto disk if needed.\n\nOur list of candidate information will have the nodule status (what we're going to be\ntraining the model to classify), diameter (useful for getting a good spread in training,\n\nsince large and small nodules will not have the same features), series (to locate the\ncorrect CT scan), and candidate center (to find the candidate in the larger CT). The\nfunction that will build a list of these NoduleInfoTuple instances starts by using an in-\nmemory caching decorator, followed by getting the list of files present on disk.\n\nListing 10.2 dsets.py:32\n\nndard library in- requireOnDisk_bool defaults to\noe caching \" screening out series from data\nsubsets that aren\u2019t in place yet.\n\n@functools.1lru_cache(1)\n\ndef getCandidateInfoList (requireOnDisk_bool=True) : <\nmhd_list = glob.glob('data-unversioned/part2/luna/subset*/*.mhd')\npresentOnDisk_set = {os.path.split(p)[-1][:-4]) for p in mhd_list}\n\nSince parsing some of the data files can be slow, we'll cache the results of this function\ncall in memory. This will come in handy later, because we'll be calling this function\nmore often in future chapters. Speeding up our data pipeline by carefully applying in-\nmemory or on-disk caching can result in some pretty impressive gains in training\nspeed. Keep an eye out for these opportunities as you work on your projects.\n\nEarlier we said that we'll support running our training program with less than the\nfull set of training data, due to the long download times and high disk space require-\nments. The requireOnDisk_bool parameter is what makes good on that promise;\nwe're detecting which LUNA series UIDs are actually present and ready to be loaded\nfrom disk, and we'll use that information to limit which entries we use from the CSV\nfiles we're about to parse. Being able to run a subset of our data through the training\nloop can be useful to verify that the code is working as intended. Often a model\u2019s\ntraining results are bad to useless when doing so, but exercising our logging, metrics,\nmodel check-pointing, and similar functionality is beneficial.\n\nAfter we get our candidate information, we want to merge in the diameter infor-\nmation from annotations.csy. First we need to group our annotations by series_uid,\nas that\u2019s the first key we'll use to cross-reference each row from the two files.\n\ndiameter_dict = {}\nwith open('data/part2/luna/annotations.csv', \"r\") as f:\nfor row in list(csv.reader(f))[1:]:\nseries_uid = row[0]\nannotationCenter_xyz = tuple([float(x) for x in row[1:4]])\nannotationDiameter_mm = float (row[4])\n\ndiameter_dict.setdefault(series_uid, []) .append(\n(annotationCenter_xyz, annotationDiameter_mm)\n)\n\nNow we'll build our full list of candidates using the information in the candidates.csv\nfile.\n\nsting 10.4 dsets.py:51, def getCandidateInfoList\n\ncandidateInfo_list = [] If a series_uid isn\u2019t\nwith open('data/part2/luna/candidates.csv', \"r*) as f\u00a3: present, it\u2019s ina subset\nfor row in list(csv.reader(f))[1:): we don\u2019t have on disk,\nseries_uid = row[0] so we should skip it.\n\nif series_uid not in presentOnDisk_set and requireOnDisk_bool:\ncontinue\n\nisNodule_bool = bool (int (row[4]))\ncandidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n\ncandidateDiameter_mm = 0.0\nfor annotation_tup in diameter_dict.get(series_uid, []):\nannotationCenter_xyz, annotationDiameter_mm = annotation_tup\nfor i in range(3):\ndelta_mm = abs (candidateCenter_xyz[i] - annotationCenter_xyz[i))\nif delta_mm > annotationDiameter_mm / 4:\n\nbreak Divides the diameter by 2 to get\nelse: the radius, and divides the\ncandidateDiameter_mm = annotationDiameter_mm radius by 2 to require that the\nbreak two nodule center points not be\ntoo far apart relative to the size\ncand Savernfes ist.append (CandidateInfoTuple ( sounding ben Gen meteuen\nisNodule_bool, d ce check.)\n\ncandidateDiameter_mm,\n\nseries_uid,\n\ncandidateCenter_xyz,\n)\n\nFor each of the candidate entries for a given series_uid, we loop through the annota-\ntions we collected earlier for the same series_uid and see if the two coordinates are\nclose enough to consider them the same nodule. If they are, great! Now we have diam-\neter information for that nodule. If we don\u2019t find a match, that\u2019s fine; we'll just treat\nthe nodule as having a 0.0 diameter. Since we're only using this information to get a\ngood spread of nodule sizes in our training and validation sets, having incorrect diam-\neter sizes for some nodules shouldn\u2019t be a problem, but we should remember we're\ndoing this in case our assumption here is wrong.\n\nThat's a lot of somewhat fiddly code just to merge in our nodule diameter. Unfor-\ntunately, having to do this kind of manipulation and fuzzy matching can be fairly com-\nmon, depending on your raw data. Once we get to this point, however, we just need to\nsort the data and return it.\n\nListing 10.5 dsets.py:80, def getCandidateInfoList\n\ncandidateInfo_list.sort(reverse=True) \u00ab\n\nreturn candidateInfo_list This we have all of the I module\n\nsamples starting with the largest first, followed\nby all of the non-nodule samples (which don\u2019t\nhave nodule size information).\n\nThe ordering of the tuple members in noduleInfo_list is driven by this sort. We're\nusing this sorting approach to help ensure that when we take a slice of the data, that\nslice gets a representative chunk of the actual nodules with a good spread of nodule\ndiameters. We'll discuss this more in section 10.5.3.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.6,
                        "section_name": "Loading individual CT scans",
                        "section_path": "./screenshots-images-2/chapter_11/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_6/c9ff26ef-d066-4fc9-82ab-4314e7f5c9f2.png",
                            "./screenshots-images-2/chapter_11/section_6/dc791899-5338-4fa7-b1e8-ba453940c4b6.png",
                            "./screenshots-images-2/chapter_11/section_6/5146f402-9813-4d1f-ab44-526a9e5900f4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Loading individual CT scans\n\nNext up, we need to be able to take our CT data from a pile of bits on disk and turn it\ninto a Python object from which we can extract 3D nodule density data. We can see this\npath from the .mhd and .raw files to Ct objects in figure 10.4. Our nodule annotation\ninformation acts like a map to the interesting parts of our raw data. Before we can follow\nthat map to our data of interest, we need to get the data into an addressable form.\n\nTIP Having a large amount of raw data, most of which is uninteresting, is a\ncommon situation; look for ways to limit your scope to only the relevant data\nwhen working on your own projects.\n\nFigure 10.4 Loading a CT scan produces a voxel array and a transformation from\npatient coordinates to array indices.\n\nThe native file format for CT scans is DICOM (www.dicomstandard.org). The first ver-\nsion of the DICOM standard was authored in 1984, and as we might expect from any-\nthing computing-related that comes from that time period, it\u2019s a bit of a mess (for\nexample, whole sections that are now retired were devoted to the data link layer pro-\ntocol to use, since Ethernet hadn't won yet).\n\nNOTE We've done the legwork of finding the right library to parse these raw\ndata files, but for other formats you've never heard of, you'll have to find a\nparser yourself. We recommend taking the time to do so! The Python ecosys-\ntem has parsers for just about every file format under the sun, and your time\nis almost certainly better spent working on the novel parts of your project\nthan writing parsers for esoteric data formats.\n\nHappily, LUNA has converted the data we're going to be using for this chapter into\nthe MetalO format, which is quite a bit easier to use (https://itk.org/Wiki/MetalO/\nDocumentation#Quick_Start). Don\u2019t worry if you've never heard of the format\nbefore! We can treat the format of the data files as a black box and use SimpleITK to\nload them into more familiar NumPy arrays.\n\nListing 10.6 dsets.py:9\n\nimport SimpleITK as sitk\n\n# ... line 83 We don\u2019t care to track which\nclass Ct: subset a given series_uid is in,\ndef __init (self, series_uid): so we wildcard the subset.\nmhd_path = glob.glob(\n*data-unversioned/part2/luna/subset*/{)}.mhd'.format(series_uid) +\n\n(0) sitk.ReadImage implicitly consumes the .raw\n\nct_mhd = sitk.ReadImage (mha_path) | file in addition to the passed-in .mhd file.\n\net_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32) Bas\n\nRecreates an np.array since we want\nto convert the value type to np.float3\n\nFor real projects, you'll want to understand what types of information are contained\nin your raw data, but it\u2019s perfectly fine to rely on third-party code like SimpleITK to\nparse the bits on disk. Finding the right balance of knowing everything about your\ninputs versus blindly accepting whatever your data-loading library hands you will prob-\nably take some experience. Just remember that we're mostly concerned about data,\nnot bits. It\u2019s the information that matters, not how it\u2019s represented.\n\nBeing able to uniquely identify a given sample of our data can be useful. For exam-\nple, clearly communicating which sample is causing a problem or is getting poor clas-\nsification results can drastically improve our ability to isolate and debug the issue.\nDepending on the nature of our samples, sometimes that unique identifier is an atom,\nlike a number or a string, and sometimes it\u2019s more complicated, like a tuple.\n\nWe identify specific CT scans using the series instance UID (series_uid) assigned\nwhen the CT scan was created. DICOM makes heavy use of unique identifiers (UIDs)\n\nfor individual DICOM files, groups of files, courses of treatment, and so on. These\nidentifiers are similar in concept to UUIDs (https://docs.python.org/3.6/library/\nuuid.html), but they have a different creation process and are formatted differently.\nFor our purposes, we can treat them as opaque ASCII strings that serve as unique keys\nto reference the various CT scans. Officially, only the characters 0 through 9 and the\nperiod (.) are valid characters in a DICOM UID, but some DICOM files in the wild\nhave been anonymized with routines that replace the UIDs with hexadecimal (0-9\nand a-f) or other technically out-ofspec values (these out-of-spec values typically\naren't flagged or cleaned by DICOM parsers; as we said before, it\u2019s a bit of a mess).\n\nThe 10 subsets we discussed earlier have about 90 CT scans each (888 in total),\nwith every CT scan represented as two files: one with a .mhd extension and one with a\nraw extension. The data being split between multiple files is hidden behind the sitk\nroutines, however, and is not something we need to be directly concerned with.\n\nAt this point, ct_a is a three-dimensional array. All three dimensions are spatial,\nand the single intensity channel is implicit. As we saw in chapter 4, in a PyTorch ten-\nsor, the channel information is represented as a fourth dimension with size 1.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.7,
                        "section_name": "Hounsfield Units",
                        "section_path": "./screenshots-images-2/chapter_11/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_7/e539cb7f-f210-465f-848d-688d7633dda9.png",
                            "./screenshots-images-2/chapter_11/section_7/92a109ac-5886-4dd5-9cb1-d6e7274a0884.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Hounsfield Units\n\nRecall that earlier, we said that we need to understand our data, not the bits that store\nit. Here, we have a perfect example of that in action. Without understanding the\nnuances of our data\u2019s values and range, we'll end up feeding values into our model\nthat will hinder its ability to learn what we want it to.\n\nContinuing the __init__ method, we need to do a bit of cleanup on the ct_a val-\nues. CT scan voxels are expressed in Hounsfield units (HU; https://en.wikipedia.org/\nwiki/Hounsfield_scale), which are odd units; air is\u20141,000 HU (close enough to 0 g/cc\n[grams per cubic centimeter] for our purposes), water is 0 HU (1 g/cc), and bone is\nat least +1,000 HU (2-3 g/cc).\n\nNOTE HU values are typically stored on disk as signed 12-bit integers (shoved\ninto 16-bit integers), which fits well with the level of precision CT scanners\ncan provide. While this is perhaps interesting, it\u2019s not particularly relevant to\nthe project.\n\nSome CT scanners use HU values that correspond to negative densities to indicate\nthat those voxels are outside of the CT scanner\u2019s field of view. For our purposes, every-\nthing outside of the patient should be air, so we discard that field-of-view information\nby setting a lower bound of the values to -1,000 HU. Similarly, the exact densities of\nbones, metal implants, and so on are not relevant to our use case, so we cap density at\nroughly 2 g/cc (1,000 HU) even though that\u2019s not biologically accurate in most cases.\n\nListing 10.7 dsets.py:96,Ct._ init _\n\nct_a.clip(-1000, 1000, ct_a)\n\nValues above 0 HU don\u2019t scale perfectly with density, but the tumors we\u2019re interested\nin are typically around 1 g/cc (0 HU), so we\u2019re going to ignore that HU doesn\u2019t map\nperfectly to common units like g/cc. That's fine, since our model will be trained to\nconsume HU directly.\n\nWe want to remove all of these outlier values from our data: they aren\u2019t directly rel-\nevant to our goal, and having those outliers can make the model\u2019s job harder. This can\nhappen in many ways, but a common example is when batch normalization is fed\nthese outlier values and the statistics about how to best normalize the data are skewed.\nAlways be on the lookout for ways to clean your data.\n\nAll of the values we've built are now assigned to self.\n\nListing 10.8 dsets.py:98,ct. init _\n\nself.series_uid = series_uid\nself.hu_a = ct_a\n\nIt\u2019s important to know that our data uses the range of \u20141,000 to +1,000, since in chap-\nter 13 we end up adding channels of information to our samples. If we don\u2019t account\nfor the disparity between HU and our additional data, those new channels can easily\nbe overshadowed by the raw HU values. We won\u2019t add more channels of data for the\nclassification step of our project, so we don\u2019t need to implement special handling\nright now.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.8,
                        "section_name": "Locating a nodule using the patient coordinate system",
                        "section_path": "./screenshots-images-2/chapter_11/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_8/9bbb90b3-60ac-4a3d-9637-706c1dd1e051.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Locating a nodule using the patient coordinate system\n\nDeep learning models typically need fixed-size inputs,* due to having a fixed number\nof input neurons. We need to be able to produce a fixed-size array containing the can-\ndidate so that we can use it as input to our classifier. We'd like to tain our model\nusing a crop of the CT scan that has a candidate nicely centered, since then our\nmodel doesn\u2019t have to learn how to notice nodules tucked away in the corner of the\ninput. By reducing the variation in expected inputs, we make the model's job easier.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.9,
                        "section_name": "The patient coordinate system",
                        "section_path": "./screenshots-images-2/chapter_11/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_9/e2822be3-6ec0-49fc-be81-84743a93ea90.png",
                            "./screenshots-images-2/chapter_11/section_9/ee5657a2-ab50-4d5c-a270-19b845fb1165.png",
                            "./screenshots-images-2/chapter_11/section_9/c7815eb2-5799-4fee-b969-1ff5dd9b32f6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The patient coordinate system\n\nUnfortunately, all of the candidate center data we loaded in section 10.2 is expressed\nin millimeters, not voxels! We can\u2019t just plug locations in millimeters into an array\nindex and expect everything to work out the way we want. As we can see in figure 10.5,\nwe need to transform our coordinates from the millimeter-based coordinate system\n(X,Y,Z) they're expressed in, to the voxel-address-based coordinate system (I,R,C)\nused to take array slices from our CT scan data. This is a classic example of how it\u2019s\nimportant to handle units consistently!\n\nAs we have mentioned previously, when dealing with CT scans, we refer to the array\ndimensions as index, row, and column, because a separate meaning exists for X, Y, and Z,\n\nRAIN \u2014_ >\nCANDIDATE\n\nTRANSFORM (i R iC),\n\nooo\n0100 (L,R,C),\n\noo\\0\n\nFigure 10.5 Using the transformation information to convert a nodule center\ncoordinate in patient coordinates (X,Y,Z) to an array index (index,Row,Column).\n\nas illustrated in figure 10.6. The patient coordinate system defines positive X to be patient-\nleft (left), positive Y to be patient-behind (posterior) , and positive Z to be toward-patient-\nhead (superior). Left-posterior-superior is sometimes abbreviated LPS.\n\nFigure 10.6 Our inappropriately clothed patient demonstrating the axes of the patient coordinate system\n\nARRAY PATIENT\n\nCOORDINATES COORDINATES\n(0, 0) '\n\n(Sit, Sit)\n\nFigure 10.7 Array coordinates and patient coordinates have different origins and scaling.\n\nThe patient coordinate system is measured in millimeters and has an arbitrarily posi-\ntioned origin that does not correspond to the origin of the CT voxel array, as shown in\nfigure 10.7.\n\nThe patient coordinate system is often used to specify the locations of interesting\nanatomy in a way that is independent of any particular scan. The metadata that\ndefines the relationship between the CT array and the patient coordinate system is\nstored in the header of DICOM files, and that meta-image format preserves the data\nin its header as well. This metadata allows us to construct the transformation from\n(X,Y,Z) to (I,R,C) that we saw in figure 10.5. The raw data contains many other fields\nof similar metadata, but since we don\u2019t have a use for them right now, those unneeded\nfields will be ignored.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.1,
                        "section_name": "CT scan shape and voxel sizes",
                        "section_path": "./screenshots-images-2/chapter_11/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_10/4cbaa438-0bd1-4608-a451-a4b0694f9c27.png",
                            "./screenshots-images-2/chapter_11/section_10/37e2c719-17e2-4a57-9e3e-23d0fbb61003.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "CT scan shape and voxel sizes\n\nOne of the most common variations between CT scans is the size of the voxels; typi-\ncally, they are not cubes. Instead, they can be 1.125 mm x 1.125 mm x 2.5 mm or simi-\nlar. Usually the row and column dimensions have voxel sizes that are the same, and\nthe index dimension has a larger value, but other ratios can exist.\n\nWhen plotted using square pixels, the non-cubic voxels can end up looking some-\nwhat distorted, similar to the distortion near the north and south poles when using a\nMercator projection map. That's an imperfect analogy, since in this case the distortion\nis uniform and linear\u2014the patient looks far more squat or barrel-chested in figure\n10.8 than they would in reality. We will need to apply a scaling factor if we want the\nimages to depict realistic proportions.\n\nKnowing these kinds of details can help when trying to interpret our results visually.\nWithout this information, it would be easy to assume that something was wrong with our\ndata loading: we might think the data looked so squat because we were skipping half of\n\nINDEX 41\n\nROW 229 COL 457\n\noRSaS RSA\n\n00 200 300 400 S00\n\n8\n\n200 800 \u00ab4400 \u00ab6500\n\n200 800 4400 \u00a9 \u00a96S00\n\nFigure 10.8 A CT scan with non-cubic voxels along the index-axis. Note how compressed the lungs are from top\nto bottom.\n\nthe slices by accident, or something along those lines. It can be easy to waste a lot of time\ndebugging something that\u2019s been working all along, and being familiar with your data\ncan help prevent that.\n\nCTs are commonly 512 rows by 512 columns, with the index dimension ranging from\naround 100 total slices up to perhaps 250 slices (250 slices times 2.5 millimeters is\ntypically enough to contain the anatomical region of interest). This results in a lower\nbound of approximately 2* voxels, or about 32 million data points. Each CT specifies\nthe voxel size in millimeters as part of the file metadata; for example, we'll call ct_mhd\n-GetSpacing() in listing 10.10.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.11,
                        "section_name": "| Converting between millimeters and voxel addresses",
                        "section_path": "./screenshots-images-2/chapter_11/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_11/07808cf9-601e-4ee7-9483-758e72046c4d.png",
                            "./screenshots-images-2/chapter_11/section_11/c831e2cb-d428-4970-bce2-cf387d5a5a67.png",
                            "./screenshots-images-2/chapter_11/section_11/c59f6cc0-3ceb-4a80-90ac-9373eb0e7299.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Converting between millimeters and voxel addresses\n\nWe will define some utility code to assist with the conversion between patient coordi-\nnates in millimeters (which we will denote in the code with an _xyz suffix on variables\nand the like) and (I,R,C) array coordinates (which we will denote in code with an\n_irc suffix).\n\nYou might wonder whether the SimpleITK library comes with utility functions to\nconvert these. And indeed, an Image instance does feature two methods\u2014Trans form-\nIndexToPhysicalPoint and TransformPhysicalPointToIndex\u2014to do just that\n(except shuffling from CRI [column,row,index] IRC). However, we want to be able to\ndo this computation without keeping the Image object around, so we'll perform the\nmath manually here.\n\nFlipping the axes (and potentially a rotation or other transforms) is encoded in a\n3 x 3 matrix returned as a tuple from ct_mhd.GetDirections(). To go from voxel\nindices to coordinates, we need to follow these four steps in order:\n\na Filip the coordinates from IRC to CRI, to align with XYZ.\n\n2 Scale the indices with the voxel sizes.\n\n3 Matrix-multiply with the directions matrix, using @ in Python.\n\n\u00ab Add the offset for the origin.\n\nTo go back from XYZ to IRC, we need to perform the inverse of each step in the\nreverse order.\nWe keep the voxel sizes in named tuples, so we convert these into arrays.\n\nListing 10.9 util.p\n\nSwaps the order while we\n\nconvert to a NumPy array\nIreTuple = collections.namedtuple('IrcTuple', ['index', \u2018row\u2019, \u2018col'])\nXyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z']})\n\ndef irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n\ncri_a = np.array(coord_irc) [::-1]\n\norigin_a = np.array(origin_xyz) The bottom three steps of\nvxSize_a = np.array(vxSize_xyz) our plan, all in one line\ncoords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a <1\n\nreturn XyzTuple(*coords_xyz)\n\ndef xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\norigin_a = np.array(origin_xyz)\n\nvxSize_a = np.array(vxSize_xyz) Inverse of the last three steps\n\ncoord_a = np.array(coord_xyz) |\n\neri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n\neri_a = np.round(cri_a)\n\nreturn IrcTuple(int(cri_a[2]), int(eri_a[1]), int(eri_a[0])) < Shuffles and\nSneaks in proper rounding converts to\nbefore converting to integers integers\n\nPhew. If that was a bit heavy, don\u2019t worry. Just remember that we need to convert and\nuse the functions as a black box. The metadata we need to convert from patient coor-\ndinates (_xyz) to array coordinates (_irc) is contained in the MetalO file alongside\nthe CT data itself. We pull the voxel sizing and positioning metadata out of the .mhd\nfile at the same time we get the ct_a.\n\nListing 10.10 dsets.py:72, c\n\nclass Ct:\ndef __init (self, series_uid):\nmhd_path = glob.glob( 'data-\nunversioned/part2/luna/subset*/{}.mhd' . format (series_uid)) [0]\nConverts the directions to an array, and\nct_mhd = sitk.ReadImage (mhd_path) reshapes the nine-element array to its\n\n# ... line 91 proper 3 x 3 matrix shape\nself.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n\nself.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\nself.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3) +\n\nThese are the inputs we need to pass into our xyz2irc conversion function, in addition\nto the individual point to covert. With these attributes, our CT object implementation\n\nnow has all the data needed to convert a candidate center from patient coordinates to\narray coordinates.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.12,
                        "section_name": "Extracting a nodule from a CT scan",
                        "section_path": "./screenshots-images-2/chapter_11/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_12/a8e9421f-b52d-4561-a9fa-0836967da8bb.png",
                            "./screenshots-images-2/chapter_11/section_12/37a634c2-cfe7-4912-b0bf-10b8160f2155.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "1 Extracting a nodule from a CT scan\n\nAs we mentioned in chapter 9, up to 99.9999% of the voxels in a CT scan of a patient\nwith a lung nodule won't be part of the actual nodule (or cancer, for that matter).\nAgain, that ratio is equivalent to a two-pixel blob of incorrectly tinted color some-\nwhere on a high-definition television, or a single misspelled word out of a shelf of nov-\nels. Forcing our model to examine such huge swaths of data looking for the hints of\nthe nodules we want it to focus on is going to work about as well as asking you to find\na single misspelled word from a set of novels written in a language you don\u2019t know!*\n\nInstead, as we can see in figure 10.9, we will extract an area around each candidate\nand let the model focus on one candidate at a time. This is akin to letting you read\nindividual paragraphs in that foreign language: still not an easy task, but far less\ndaunting! Looking for ways to reduce the scope of the problem for our model can\nhelp, especially in the early stages of a project when we're trying to get our first work-\ning implementation up and running.\n\nLOCATION\n\n[are 7\n\nFigure 10.9 Cropping a candidate sample out of the larger CT voxel array using the\ncandidate center's array coordinate information (Index,Row,Column)\n\nThe getRawNodule function takes the center expressed in the patient coordinate sys-\ntem (X,Y,Z), just as it\u2019s specified in the LUNA CSV data, as well as a width in voxels. It\nreturns a cubic chunk of CT, as well as the center of the candidate converted to array\ncoordinates.\n\nListing 10.11 dsets.py:105, Ct .getRawCandidate\n\ndef getRawCandidate(self, center_xyz, width_irec):\ncenter_ire = xyz2irc(\ncenter_xyz,\nself.origin_xyz,\nself.vxSize_xyz,\nself.direction_a,\n)\n\nslice_list = []\n\nfor axis, center_val in enumerate(center_irc):\nstart_ndx = int(round(center_val - width_irc[axis] /2))\nend_ndx = int(start_ndx + width_irc[axis])\nslice_list.append(slice(start_ndx, end_ndx))\n\net_chunk = self.hu_a[tuple(slice_list)]\n\nreturn ct_chunk, center_ire\n\nThe actual implementation will need to deal with situations where the combination of\ncenter and width puts the edges of the cropped areas outside of the array. But as\nnoted earlier, we will skip complications that obscure the larger intent of the function.\nThe full implementation can be found on the book's website (www.manning.com/\nbooks/deep-learning-with-pytorch?query=pytorch) and in the GitHub repository\n(https: //github.com/deep-learning-with-pytorch/dlwpt-code).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.13,
                        "section_name": "A straightforward dataset implementation",
                        "section_path": "./screenshots-images-2/chapter_11/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_13/7985fd10-36dc-4535-8233-1e35f92c472b.png",
                            "./screenshots-images-2/chapter_11/section_13/563a1010-8668-4fd6-9b04-a794241fe199.png",
                            "./screenshots-images-2/chapter_11/section_13/f5c41784-527f-47e0-b0e2-f294dd8e1b33.png",
                            "./screenshots-images-2/chapter_11/section_13/3b92b369-8938-40fe-852f-9bfb54d7c9c7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A straightforward dataset implementation\n\nWe first saw PyTorch Dataset instances in chapter 7, but this will be the first time\nwe've implemented one ourselves. By subclassing Dataset, we will take our arbitrary\ndata and plug it into the rest of the PyTorch ecosystem. Each Ct instance represents\nhundreds of different samples that we can use to train our model or validate its effec-\ntiveness. Our LunaDataset class will normalize those samples, flattening each CT\u2019s\nnodules into a single collection from which samples can be retrieved without regard\nfor which Ct instance the sample originates from. This flattening is often how we want\nto process data, although as we'll see in chapter 12, in some situations a simple flatten-\ning of the data isn\u2019t enough to train a model well.\n\nIn terms of implementation, we are going to start with the requirements imposed\nfrom subclassing Dataset and work backward. This is different from the datasets we've\nworked with earlier; there we were using classes provided by external libraries,\nwhereas here we need to implement and instantiate the class ourselves. Once we have\ndone so, we can use it similarly to those earlier examples. Luckily, the implementation\n\nof our custom subclass will not be too difficult, as the PyTorch API only requires that\nany Dataset subclasses we want to implement must provide these two functions:\n= An implementation of __len__ that must return a single, constant value after\ninitialization (the value ends up being cached in some use cases)\n\n= The __getitem__ method, which takes an index and returns a tuple with sam-\nple data to be used for training (or validation, as the case may be)\n\nFirst, let\u2019s see what the function signatures and return values of those functions look like.\n\nListing 10.12 dsets.py:176, LunaDatas:\n\ndef len__(self):\n\nreturn len(self.candidateInfo_list)\n\ndef __getitem__(self, ndx):\n# ... line 200\nreturn (\ncandidate_t, 1((C010-1))\npos_t, 1((CO10-2))\ncandidateInfo_tup.series_uid,\n\ntorch.tensor(center_irc), This is our training sample.\n\nOur __len__ implementation is straightforward: we have a list of candidates, each can-\ndidate is a sample, and our datasetis as large as the number of samples we have. We don\u2019t\nhave to make the implementation as simple as it is here; in later chapters, we'll see this\nchange!* The only rule is that if __len__ returns a value of N, then __getitem__needs\nto return something valid for all inputs 0 to N- 1.\n\nFor __getitem_., we take ndx (typically an integer, given the rule about support-\ning inputs 0 to N- 1) and return the four-item sample tuple as depicted in figure 10.2.\nBuilding this tuple is a bit more complicated than getting the length of our dataset,\nhowever, so let\u2019s take a look.\n\nThe first part of this method implies that we need to construct self .candidateInfo\n_list as well as provide the getCtRawNodule function.\n\ndef __getitem__(self, ndx):\ncandidateInfo_tup = self.candidateInfo_list [ndx]\nwidth_ire = (32, 48, 48)\n\ncandidate_a, center_irc = getCtRawCandidate ( The return value candidate ahas\ncandidateInfo_tup.series_uid, shape (32,48,48); the axes are\ncandidateInfo_tup.center_xyz, depth, height, and width.\n\nwidth_ire,\n)\n\nWe will get to those in a moment in sections 10.5.1 and 10.5.2.\n\nThe next thing we need to do in the __getitem__ method is manipulate the data\ninto the proper data types and required array dimensions that will be expected by\ndownstream code.\n\nListing 10.14 dsets.py:189, LunaDataset. getitem__\n\ncandidate_t\ncandidate_t\ncandidate_t\n\ntorch. from_numpy (candidate_a)\ncandidate_t.to(torch. float32) sunsqueeze(0) adds the\ncandidate_t.unsqueeze (0) \u2018Channel\u2019 dimension.\n\nDon\u2019t worry too much about exactly why we are manipulating dimensionality for now;\nthe next chapter will contain the code that ends up consuming this output and impos-\ning the constraints we're proactively meeting here. This will be something you should\nexpect for every custom Dataset you implement. These conversions are a key part of\ntransforming your Wild West data into nice, orderly tensors.\n\nFinally, we need to build our classification tensor.\n\nListing 10.15 dsets.py:193, LunaDataset. getitem__\n\npos_t = torch.tensor([\nnot candidateInfo_tup.isNodule_bool,\ncandidateInfo_tup.isNodule_bool\n1,\ndtype=torch. long,\n)\n\nThis has two elements, one each for our possible candidate classes (nodule or non-\nnodule; or positive or negative, respectively). We could have a single output for the\nnodule status, but nn.CrossEntropyLoss expects one output value per class, so that\u2019s\nwhat we provide here. The exact details of the tensors you construct will change based\non the type of project you're working on.\n\nLet\u2019s take a look at our final sample tuple (the larger nodule_t output isn\u2019t partic-\nularly readable, so we elide most of it in the listing).\n\nListing p2chi0_explore_data.ipynb\n\n# In[10):\nLunaDataset () [0]\n\n# Out[10]:\n(tensor ([[[[-899., -903., -825., ..., -901., -898., -893.],\nceey candidate_t\n[ -92., -63., 4., ..., 63., 70.,  52.))))),\nds t \u2014t tensor((0, 1]),\n1.3.6. ..287966244644280690737019247886\"', | <\u2014 candidate_tup.series_uid (elided)\n\ntensor([{ 91, 360, 341])) \u2014\ncenter_irc\n\nHere we see the four items from our __getitem__ return statement.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.14,
                        "section_name": "Caching candidate arrays with the getCtRawCandidate function",
                        "section_path": "./screenshots-images-2/chapter_11/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_14/51de6c2e-f19b-4a98-bb96-0a39e2411103.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Caching candidate arrays with the getCtRawCandidate function\n\nIn order to get decent performance out of LunaDataset, we'll need to invest in some\non-disk caching. This will allow us to avoid having to read an entire CT scan from disk\nfor every sample. Doing so would be prohibitively slow! Make sure you're paying atten-\ntion to bottlenecks in your project and doing what you can to optimize them once\nthey start slowing you down. We\u2019re kind of jumping the gun here since we haven't\ndemonstrated that we need caching here. Without caching, the LunaDataset is easily\n50 times slower! We'll revisit this in the chapter's exercises.\n\nThe function itself is easy. It\u2019s a file-cache-backed (https://pypi.python.org/pypi/\ndiskcache) wrapper around the Ct .getRawCandidate method we saw earlier.\n\nListing 10.17 dset: 139\n\n@functools.lru_cache(1, typed=True)\ndef getCt (series_uid):\nreturn Ct(series_uid)\n\n@raw_cache.memoize(typed=True)\n\ndef getCtRawCandidate(series_uid, center_xyz, width_irc):\net = getCt(series_uid)\net_chunk, center_ire = ct.getRawCandidate(center_xyz, width_irc)\nreturn ct_chunk, center_ire\n\nWe use a few different caching methods here. First, we\u2019re caching the getCt return\nvalue in memory so that we can repeatedly ask for the same Ct instance without hay-\ning to reload all of the data from disk. That\u2019s a huge speed increase in the case of\nrepeated requests, but we\u2019re only keeping one CT in memory, so cache misses will be\nfrequent if we're not careful about access order.\n\nThe getCtRawCandidate function that calls getCt also has its outputs cached, how-\never; so after our cache is populated, getCt won't ever be called. These values are\ncached to disk using the Python library diskcache. We'll discuss why we have this spe-\ncific caching setup in chapter 11. For now, it\u2019s enough to know that it\u2019s much, much\nfaster to read in 2'\u00b0 \u00a31oat32 values from disk than it is to read in 2\u201d\u00b0 int 16 values, con-\nvert to \u00a3loat32, and then select a 2\u2019 subset. From the second pass through the data\nforward, I/O times for input should drop to insignificance.\n\nNOTE If the definitions of these functions ever materially change, we will\nneed to remove the cached values from disk. If we don\u2019t, the cache will con-\ntinue to return them, even if now the function will not map the given inputs\nto the old output. The data is stored in the data-unversioned/cache directory.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.15,
                        "section_name": "Constructing our dataset in LunaDataset.__init__",
                        "section_path": "./screenshots-images-2/chapter_11/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_15/1eff347b-0df1-4cd8-8a62-4930e819016e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Constructing our dataset in LunaDataset.__init__\n\nJust about every project will need to separate samples into a training set and a valida-\ntion set. We are going to do that here by designating every tenth sample, specified by\nthe val_stride parameter, as a member of the validation set. We will also accept an\nisValSet_bool parameter and use it to determine whether we should keep only the\ntraining data, the validation data, or everything.\n\nListing 10. dsets.py:149, class LunaDatase\n\nclass LunaDataset (Dataset) :\ndef __init (self, Copies the return value so the\nval_stride=0, cached copy won\u2019t be im;\nisValSet_bool=None, altering self.candidatelnfo_list\nseries_uid=None,\n):\nself.candidateInfo_list = copy.copy(getCandidateInfoList ())\n\nif series_uid:\nself.candidateInfo_list = [\nx for x in self.candidateInfo_list if x.series_uid == series_uid\n\n]\n\nIf we pass in a truthy series_uid, then the instance will only have nodules from that\nseries. This can be useful for visualization or debugging, by making it easier to look at,\nfor instance, a single problematic CT scan.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.16,
                        "section_name": "A training/validation split",
                        "section_path": "./screenshots-images-2/chapter_11/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_16/3eb68415-77f0-4a8c-abcb-566c503ac7ed.png",
                            "./screenshots-images-2/chapter_11/section_16/46707b78-95bb-43bf-b2af-e167703bc2ac.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "3 A training/validation split\n\nWe allow for the Dataset to partition out 1/Nth of the data into a subset used for vali-\ndating the model. How we will handle that subset is based on the value of the isvalSet\n\n_bool argument.\n\nListing 10.19 dsets.py:162, LunaDataset.___\n\nif isValSet_bool:\nassert val_stride > 0, val_stride\nself.candidateInfo_list = self.candidateInfo_list[::val_stride]\nassert self.candidateInfo_list\n\nelif val_stride > 0: Deletes bag hyremny ay wy =r sind\ndel self.candidateInfo_list[::val_stride] va! m one from\nn ~ self.candidatelnfo_list. We made a\nassert self.candidateInfo_list copy e so that we don\u2019t alter\nthe original list.\n\nThis means we can create two Dataset instances and be confident that there is strict\nsegregation between our training data and our validation data. Of course, this\ndepends on there being a consistent sorted order to self.candidateInfo_list,\nwhich we ensure by having there be a stable sorted order to the candidate info tuples,\nand by the getCandidateInfoList function sorting the list before returning it.\n\nThe other caveat regarding separation of training and validation data is that,\ndepending on the task at hand, we might need to ensure that data from a single\npatient is only present either in training or in testing but not both. Here this is not a\nproblem; otherwise, we would have needed to split the list of patients and CT scans\nbefore going to the level of nodules.\n\nLet's take a look at the data using p2ch10_explore_data.ipynb:\n\n# In[2]:\nfrom p2chi0.dsets import getCandidateInfoList, getCt, LunaDataset\ncandidateInfo_list = getCandidateInfoList (requireOnDisk_bool=False)\n\npositiveInfo_list = [x for x in candidateInfo_list if x[0])\ndiameter_list = [x[{1] for x in positiveInfo_list)\n# In[4]:\n\nfor i in range(0, len(diameter_list), 100):\nprint('{:4) {:4.1\u00a3) mm'.format(i, diameter_list[i]))\n\n# Out[4]:\n\n0 32.3 mm\n100 17.7 mm\n200 13.0 mm\n300 10.0 mm\n400 8.2 mm\n500 7.0 mm\n600 6.3 mm\n700 5.7 mm\n800 5.1 mm\n900 4.7 mm\n1000 4.0 mm\n1100 0.0 mm\n1200 0.0 mm\n1300 0.0 mm\n\nWe have a few very large candidates, starting at 32 mm, but they rapidly drop off to\nhalf that size. The bulk of the candidates are in the 4 to 10 mm range, and several\nhundred don\u2019t have size information at all. This looks as expected; you might recall\nthat we had more actual nodules than we had diameter annotations. Quick sanity\nchecks on your data can be very helpful; catching a problem or mistaken assumption\nearly may save hours of effort!\nThe larger takeaway is that our training and validation splits should have a few\nproperties in order to work well:\n= Both sets should include examples of all variations of expected inputs.\n= Neither set should have samples that aren\u2019t representative of expected inputs\nunless they have a specific purpose like training the model to be robust to outliers.\n= The training set shouldn\u2019t offer unfair hints about the validation set that\nwouldn't be true for real-world data (for example, including the same sample in\nboth sets; this is known as a leak in the training set).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.17,
                        "section_name": "Rendering the data",
                        "section_path": "./screenshots-images-2/chapter_11/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_17/e5288e51-f666-45b7-9488-671a4b653cd4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Rendering the data\nAgain, either use p2ch10_explore_data.ipynb directly or start Jupyter Notebook and\nenter\n\nThis magic line sets up the ability for images\n# In(7): to be displayed inline via the notebook.\n\n%matplotlib inline <>\nfrom p2ch10.vis import findNoduleSamples, showNodule\nnoduleSample_list = findNoduleSamples()\n\nTIP For more information about Jupyter\u2019s matplotlib inline magic,\u2019 please\nsee http://mng.bz/rrmD.\n\n# In[8):\nseries_uid = positiveSample_list [11] [2]\nshowCandidate (series_uid)\n\nThis produces images akin to those showing CT and nodule slices earlier in this chapter.\n\nIf you\u2019re interested, we invite you to edit the implementation of the rendering\ncode in p2ch10/vis.py to match your needs and tastes. The rendering code makes\nheavy use of Matplotib (https://matplotlib.org), which is too complex a library for us\nto attempt to cover here.\n\nRemember that rendering your data is not just about getting nifty-looking pictures.\nThe point is to get an intuitive sense of what your inputs look like. Being able to tell at\na glance \u201cThis problematic sample is very noisy compared to the rest of my data\u201d or\n\u201cThat's odd, this looks pretty normal\u201d can be useful when investigating issues. Effec-\ntive rendering also helps foster insights like \u201cPerhaps if I modify things like so, I can\nsolve the issue I\u2019m having.\u201d That level of familiarity will be necessary as you start tack-\nling harder and harder projects.\n\nNOTE Due to the way each subset has been partitioned, combined with the\nsorting used when constructing LunaDataset.candidateInfo_list, the\nordering of the entries in noduleSample_list is highly dependent on which\nsubsets are present at the time the code is executed. Please remember this\nwhen trying to find a particular sample a second time, especially after decom-\npressing more subsets.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.18,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_11/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_18/50024947-f2f1-425b-b168-a7de32037c21.png",
                            "./screenshots-images-2/chapter_11/section_18/dd9838c5-e06b-420b-90ea-95a182263973.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\nIn chapter 9, we got our heads wrapped around our data. In this chapter, we got\nPyTorch\u2019s head wrapped around our data! By transforming our DICOM-via-meta-image\nraw data into tensors, we've set the stage to start implementing a model and a training\nloop, which we'll see in the next chapter.\n\nIt\u2019s important not to underestimate the impact of the design decisions we've\nalready made: the size of our inputs, the structure of our caching, and how we\u2019re par-\ntitioning our training and validation sets will all make a difference to the success or\n\nfailure of our overall project. Don\u2019t hesitate to revisit these decisions later, especially\nonce you're working on your own projects.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 12,
                "chapter_name": "Training a\nclassification model\nto detect suspected tumors",
                "chapter_path": "./screenshots-images-2/chapter_12",
                "sections": [
                    {
                        "section_id": 12.1,
                        "section_name": "Training a\nclassification model\nto detect suspected tumors",
                        "section_path": "./screenshots-images-2/chapter_12/section_1",
                        "images": [],
                        "code_images": [],
                        "status": "images testing in progress",
                        "errors": [],
                        "extracted-text": "",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.2,
                        "section_name": "Training a\nclassification model\nto detect suspected tumors",
                        "section_path": "./screenshots-images-2/chapter_12/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_2/ae261819-2741-419a-b5e6-d666ea782869.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the previous chapters, we set the stage for our cancer-detection project. We cov-\nered medical details of lung cancer, took a look at the main data sources we will use\nfor our project, and transformed our raw CT scans into a PyTorch Dataset\ninstance. Now that we have a dataset, we can easily consume our training data. So\n\nlet\u2019s do that!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.3,
                        "section_name": "A foundational model and training loop",
                        "section_path": "./screenshots-images-2/chapter_12/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_3/0ace4d48-afe2-42cb-8d43-21d869b9ff09.png",
                            "./screenshots-images-2/chapter_12/section_3/17f99bfd-2b68-491c-9f39-e29782cbc024.png",
                            "./screenshots-images-2/chapter_12/section_3/129970f4-b7f4-4ccb-b4a5-3c53b47f84da.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A foundational model and training loop\n\nWe're going to do two main things in this chapter. We'll start by building the nodule\nclassification model and training loop that will be the foundation that the rest of part 2\nuses to explore the larger project. To do that, we'll use the Ct and LunaDataset classes\nwe implemented in chapter 10 to feed DataLoader instances. Those instances, in turn,\nwill feed our classification model with data via training and validation loops.\n\nWe'll finish the chapter by using the results from running that training loop to intro-\nduce one of the hardest challenges in this part of the book: how to get high-quality\nresults from messy, limited data. In later chapters, we'll explore the specific ways in\nwhich our data is limited, as well as mitigate those limitations.\n\nLet's recall our high-level roadmap from chapter 9, shown here in figure 11.1.\nRight now, we'll work on producing a model capable of performing step 4: classifica-\ntion. As a reminder, we will classify candidates as nodules or non-nodules (we'll build\nanother classifier to attempt to tell malignant nodules from benign ones in chapter\n14). That means we're going to assign a single, specific label to each sample that we\npresent to the model. In this case, those labels are \u201cnodule\u201d and \u201cnon-nodule,\u201d since\neach sample represents a single candidate.\n\nGetting an early end-to-end version of a meaningful part of your project is a great\nmilestone to reach. Having something that works well enough for the results to be\nevaluated analytically let\u2019s you move forward with future changes, confident that you\n\nFigure 11.1 Our end-to-end project to detect lung cancer, with a focus on this chapter\u2019s topic:\nstep 4, classification\n\nare improving your results with each change\u2014or at least that you're able to set aside\nany changes and experiments that don\u2019t work out! Expect to have to do a lot of exper-\nimentation when working on your own projects. Getting the best results will usually\nrequire considerable tinkering and tweaking.\n\nBut before we can get to the experimental phase, we must lay our foundation. Let\u2019s\nsee what our part 2 training loop looks like in figure 11.2: it should seem generally\nfamiliar, given that we saw a similar set of core steps in chapter 5. Here we will also use\na validation set to evaluate our training progress, as discussed in section 5.5.3.\n\nINIT MODEL\noerauzes .\nwr nano rau\nments, Tao trameD\nINIT Ve LOADERS\n\u201ca es OVER EPOCHS\nwameuerOC\u2014=<C\u00ab~S Loop\nLOAD BATCH TUPLE\n\u2014_\u2014\n( G \\e CLASSIFY BATCH SEB \u2014- VALIDATION Loop\nCALCULATE LOSS __ 9838 \u2014 LOAD BATCH TUPLE\nRECORD METRICS CLASSIFY BATCH\nUPDATE WEIGHTS CALCULATE LOSS\ntm RECORD METRICS\n(Low METRICS\nCONSOLE\nTENSORBOARD\n\nFigure 11.2 The training and validation script we will implement in this chapter\n\nThe basic structure of what we\u2019re going to implement is as follows:\n\n= Initialize our model and data loading.\n= Loop over a semi-arbitrarily chosen number of epochs.\n\u2014 Loop over each batch of training data returned by LunaDataset.\n\u2014 The datatoader worker process loads the relevant batch of data in the\nbackground.\n\u2014 Pass the batch into our classification model to get results.\n\u2014 Calculate our loss based on the difference between our predicted results and\nour ground-truth data.\n\n\u2014 Record metrics about our model\u2019s performance into a temporary data\nstructure.\n\n\u2014 Update the model weights via backpropagation of the error.\n\n\u2014 Loop over each batch of validation data (in a manner very similar to the\ntraining loop).\n\n\u2014 Load the relevant batch of validation data (again, in the background worker\nprocess).\n\n\u2014 Classify the batch, and compute the loss.\n\n\u2014 Record information about how well the model performed on the validation\ndata.\n\n\u2014 Print out progress and performance information for this epoch.\n\nAs we go through the code for the chapter, keep an eye out for two main differences\nbetween the code we're producing here and what we used for a training loop in part 1.\nFirst, we'll put more structure around our program, since the project as a whole is quite\na bit more complicated than what we did in earlier chapters. Without that extra struc-\nture, the code can get messy quickly. And for this project, we will have our main train-\ning application use a number of well-contained functions, and we will further separate\ncode for things like our dataset into self-contained Python modules.\n\nMake sure that for your own projects, you match the level of structure and design\nto the complexity level of your project. Too little structure, and it will become difficult\nto perform experiments cleanly, troubleshoot problems, or even describe what you're\ndoing! Conversely, too much structure means you're wasting time writing infrastruc-\nture that you don\u2019t need and most likely slowing yourself down by having to conform\nto it after all that plumbing is in place. Plus it can be tempting to spend time on infra-\nstructure as a procrastination tactic, rather than digging into the hard work of making\nactual progress on your project. Don\u2019t fall into that trap!\n\nThe other big difference between this chapter's code and part 1 will be a focus on\ncollecting a variety of metrics about how training is progressing. Being able to accu-\nrately determine the impact of changes on training is impossible without having good\nmetrics logging. Without spoiling the next chapter, we'll also see how important it is\nto collect not just metrics, but the right metrics for the job. We'll lay the infrastructure for\ntracking those metrics in this chapter, and we'll exercise that infrastructure by collect-\ning and displaying the loss and percent of samples correctly classified, both overall\nand per class. That\u2019s enough to get us started, but we'll cover a more realistic set of\nmetrics in chapter 12.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.4,
                        "section_name": "The main entry point for our application",
                        "section_path": "./screenshots-images-2/chapter_12/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_4/95e8eb9a-990d-4d31-a434-bfce55bc910d.png",
                            "./screenshots-images-2/chapter_12/section_4/b6cdc50a-30c8-48c8-b26f-496b4cdde8bb.png",
                            "./screenshots-images-2/chapter_12/section_4/76a4b9b7-6224-4392-b61d-5fa473777ebd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The main entry point for our application\n\nOne of the big structural differences from earlier training work we've done in this\nbook is that part 2 wraps our work in a fully fledged command-line application. It will\nparse command-line arguments, have a full-featured --help command, and be easy to\nrun in a wide variety of environments. All this will allow us to easily invoke the training\nroutines from both Jupyter and a Bash shell.!\n\nOur application\u2019s functionality will be implemented via a class so that we can\ninstantiate the application and pass it around if we feel the need. This can make test-\ning, debugging, or invocation from other Python programs easier. We can invoke the\napplication without needing to spin up a second OS-level process (we won't do\nexplicit unit testing in this book, but the structure we create can be helpful for real\nprojects where that kind of testing is appropriate) .\n\nOne way to take advantage of being able to invoke our training by either function\ncall or OS-level process is to wrap the function invocations into a Jupyter Notebook so\nthe code can easily be called from either the native CLI or the browser.\n\nListing 11.1 code/p2_run_everything.ipynb\n\n# In[2]):w\n\ndef run(app, *argv) : We assume you have a four-core, eight-\n\nargv = list (argv) thread CPU. Change the 4 if needed.\nargv.insert(0, '--num-workers=4') wo \u00b0 .\n\nlog.info(\"Running: {}({!r}).main()\".format(app, argv))\n\napp_cls = importstr(*app.rsplit(*.', 1)) This is a slightly cleaner\napp_cls(argv) .main() eallto i rt\n\nlog.info(\"Finished: {}.{!r}).main()\".format(app, argv))\n\n# In[6]:\nrun('p2chl1l.training.LunaTrainingApp', '--epochs=1')\n\nNOTE The training here assumes that you're on a workstation that has a four-\ncore, eightthread CPU, 16 GB of RAM, and a GPU with 8 GB of RAM. Reduce\n--batch-size if your GPU has less RAM, and --num-workers if you have fewer\nCPU cores, or less CPU RAM.\n\nLet\u2019s get some semistandard boilerplate code out of the way. We'll start at the end of\nthe file with a pretty standard if main stanza that instantiates the application object\nand invokes the main method.\n\nListing 11.2  training.py:386\n\nif name. == '__main_':\n\nLunaTrainingApp() .main()\n\nFrom there, we can jump back to the top of the file and have a look at the application class\nand the two functions we just called, __init__ and main. We'll want to be able to accept\ncommand-line arguments, so we'll use the standard argparse library (https://docs\n-python.org/3/library/argparse.hunl) in the application\u2019s___init__ function. Note that\nwe can pass in custom arguments to the initializer, should we wish to do so. The main\nmethod will be the primary entry point for the core logic of the application.\n\nListing\n\n.3 training.py:31, c\n\ns LunaTrainingApp\n\nclass LunaTrainingApp: If the caller doesn\u2019t provide\ndat \u2014init__ (self , S8yS_argv=None): arguments, we get them from\nif sys_argy is None:\n\nthe command line.\nsys_argv = sys.argv[1:]\n\nparser = argparse.ArgumentParser ()\nparser .add_argument (*--num-workers',\nhelp='Number of worker processes for background data loading',\n\ndefault=8,\n\ntype=int, \u2018We'll use the timestamp to\n) help identify training runs.\n# ... line 63\n\nself.cli_args = parser.parse_args(sys_argv)\n\nself.time_str = datetime.datetime.now() .strftime('tyY-%m-td_tH.%M.tS')\n# ... line 137\ndef main(self):\n\nlog.info(\"Starting {}, {}\".format(type(self)...name__, self.cli_args))\n\nThis structure is pretty general and could be reused for future projects. In particular,\nparsing arguments in __init__ allows us to configure the application separately from\ninvoking it.\n\nIf you check the code for this chapter on the book\u2019s website or GitHub, you might\n\nnotice some extra lines mentioning TensorBoard. Ignore those for now; we'll discuss\nthem in detail later in the chapter, in section 11.9.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.5,
                        "section_name": "Pretraining setup and initialization",
                        "section_path": "./screenshots-images-2/chapter_12/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_5/fd6b5c79-c76f-4455-8a48-d67d9ad12db4.png",
                            "./screenshots-images-2/chapter_12/section_5/d24b23fe-9afa-46c2-9669-9fba99eb32ca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Pretraining setup and initialization\n\nBefore we can begin iterating over each batch in our epoch, some initialization work\nneeds to happen. After all, we can\u2019t train a model if we haven't even instantiated one\nyet! We need to do two main things, as we can see in figure 11.3. The first, as we just\nmentioned, is to initialize our model and optimizer; and the second is to initialize\nour Dataset and DataLoader instances. LunaDataset will define the randomized\nset of samples that will make up our training epoch, and our DataLoader instance\nwill perform the work of loading the data out of our dataset and providing it to\nour application.\n\nINIT DATA LOADERS\n\nVe\n\nFigure 11.3. The training and validation script we will implement in this chapter, with\na focus on the preloop variable initialization\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.6,
                        "section_name": "Initializing the model and optimizer",
                        "section_path": "./screenshots-images-2/chapter_12/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_6/8b727755-1128-497e-85b7-581531314651.png",
                            "./screenshots-images-2/chapter_12/section_6/fb7690ce-3010-494d-8ff4-04756c4b614b.png",
                            "./screenshots-images-2/chapter_12/section_6/8daa0100-84ef-4aae-a33e-52102f607acb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Initializing the model and optimizer\n\nFor this section, we are treating the details of LunaModel as a black box. In section 11.4,\nwe will detail the internal workings. You are welcome to explore changes to the imple-\nmentation to better meet our goals for the model, although that\u2019s probably best done\nafter finishing at least chapter 12.\n\nLet\u2019s see what our starting point looks like.\n\nListing 11.4 training.py:31, class LunaTr\n\nclass LunaTrainingApp:\ndef __init (self, sys_argv=None):\n# ... line 70\nself.use_cuda = torch.cuda.is_available()\nself.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n\nself.model = self.initModel()\nself.optimizer = self.initOptimizer()\n\ndef initModel (self):\nmodel = LunaModel ()\n\nif self.use_cuda:\nlog.info(\"Using CUDA; {) devices.\". format (torch.cuda.device_count()))\nif torch.cuda.device_count() > 1:\n\nmca\nmultiple model = nn.DataParallel(model) + Wraps the model\nGPUs model = model.to(self.device) <\nreturn model Sends model\nparameters to the GPU\ndef initOptimizer(self):\nreturn SGD(self.model.parameters(), 1lr=0.001, momentum=0.99)\n\nIf the system used for training has more than one GPU, we will use the nn. DataParallel\nclass to distribute the work between all of the GPUs in the system and then collect and\nresync parameter updates and so on. This is almost entirely transparent in terms of both\nthe model implementation and the code that uses that model.\n\nDataParallel vs. DistributedDataParallel\n\nIn this book, we use DataParallei to handle utilizing multiple GPUs. We chose Data-\nParallel because it\u2019s a simple drop-in wrapper around our existing models. It is not\nthe best-performing solution for using multiple GPUs, however, and it is limited to work-\ning with the hardware available in a single machine.\n\nPyTorch also provides DistributedDataParallel, which is the recommended wrap-\nper class to use when you need to spread work between more than one GPU or\nmachine. Since the proper setup and configuration are nontrivial, and we suspect that\nthe vast majority of our readers won't see any benefit from the complexity, we won't\ncover DistributedDataParall1el1 in this book. If you wish to learn more, we suggest\nreading the official documentation: https://pytorch.org/tutorials/intermediate/\nddp_tutorial.html.\n\nAssuming that self.use_cuda is true, the call self.model.to(device) moves the\nmodel parameters to the GPU, setting up the various conyolutions and other calcula-\ntions to use the GPU for the heavy numerical lifting. It\u2019s important to do so before\nconstructing the optimizer, since, otherwise, the optimizer would be left looking at\nthe CPU-based parameter objects rather than those copied to the GPU.\n\nFor our optimizer, we'll use basic stochastic gradient descent (SGD;\nhttps://pytorch.org/docs/stable/optim.huml#torch.optim.SGD) with momentum.\nWe first saw this optimizer in chapter 5. Recall from part 1 that many different opti-\nmizers are available in PyTorch; while we won't cover most of them in any detail, the\nofficial documentation (https://pytorch.org/docs/stable/optim.html#\u00a5algorithms)\ndoes a good job of linking to the relevant papers.\n\nUsing SGD is generally considered a safe place to start when it comes to picking an\noptimizer; there are some problems that might not work well with SGD, but they\u2019re\nrelatively rare. Similarly, a learning rate of 0.001 and a momentum of 0.9 are pretty\nsafe choices. Empirically, SGD with those values has worked reasonably well for a wide\nrange of projects, and it\u2019s easy to ty a learning rate of 0.01 or 0.0001 if things aren't\nworking well right out of the box.\n\nThat's not to say any of those values is the best for our use case, but trying to find bet-\nter ones is getting ahead of ourselves. Systematically trying different values for learning\nrate, momentum, network size, and other similar configuration settings is called a hyper-\nparameter search. There are other, more glaring issues we need to address first in the com-\ning chapters. Once we address those, we can begin to fine-tune these values. As we\nmentioned in the section \u201cTesting other optimizers\u201d in chapter 5, there are also other,\nmore exotic optimizers we might choose; but other than perhaps swapping\ntorch.optim.SGD for torch.optim.Adam, understanding the trade-offs involved is a\ntopic too advanced for this book.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.7,
                        "section_name": "Care and feeding of data loaders",
                        "section_path": "./screenshots-images-2/chapter_12/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_7/ee1445d7-e13c-4058-8550-14bd23f9ed24.png",
                            "./screenshots-images-2/chapter_12/section_7/50ba5967-8107-49e3-b355-82ad1bb3a6ff.png",
                            "./screenshots-images-2/chapter_12/section_7/a38615c5-f7e4-41fa-ace6-2b8acc868295.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Care and feeding of data loaders\n\nThe LunaDataset class that we built in the last chapter acts as the bridge between\nwhatever Wild West data we have and the somewhat more structured world of tensors\nthat the PyTorch building blocks expect. For example, torch.nn.Conv3d (https://\npytorch.org/docs/stable/nn.html#conv3d) expects five-dimensional input: (N, C, D,\nH, W): number of samples, channels per sample, depth, height, and width. Quite dif-\nferent from the native 3D our CT provides!\n\nYou may recall the ct_t.unsqueeze(0) callin LunaDataset.__getitem__ from the\nlast chapter; it provides the fourth dimension, a \u201cchannel\u201d for our data. Recall from\nchapter 4 that an RGB image has three channels, one each for red, green, and blue.\nAstronomical data could have dozens, one each for various slices of the electromag-\nnetic spectrum\u2014gamma rays, X-rays, ultraviolet light, visible light, infrared, micro-\nwaves, and/or radio waves. Since CT scans are single-intensity, our channel dimension\nis only size 1.\n\nAlso recall from part 1 that training on single samples at a time is typically an inef-\nficient use of computing resources, because most processing platforms are capable of\nmore parallel calculations than are required by a model to process a single training or\nvalidation sample. The solution is to group sample tuples together into a batch tuple,\nas in figure 11.4, allowing multiple samples to be processed at the same time. The fifth\ndimension (N) differentiates multiple samples in the same batch.\n\nLUNA DATASET DATA LOADER,\n\n(\u201cwaearmay TUPLES BATCH TUPLE\nSAMPLE ARRAY SAMPLE ARRAY \u2018BD FP32 ARRAY\nCTF a\nGA. | GG,\n1S NODULE? (S NODULE? ID BOOL ARRAY\n= T ia] Ie&.]\nSERIES_UID SERIES_UID LIST OF STRINGS\n. \"2.3\", \u201c4.5.6\", ['123\", \u201c450\"...1,\n. CANDIDATES CANDIDATE LIST OF TRC\nLOCATION LOCATION TUPLES:\n(LRC)) (LRC)) [TRe, TRe...])\n\nFigure 11.4 Sample tuples being collated into a single batch tuple inside a data loader\n\nConveniently, we don\u2019t have to implement any of this batching: the PyTorch Data-\nLoader class will handle all of the collation work for us. We've already built the bridge\nfrom the CT scans to PyTorch tensors with our LunaDataset class, so all that remains\nis to plug our dataset into a data loader.\n\nListing 11.5 training.py:89, LunaTrainingApp.initTrainDl\n\ndef initTrainDl (self):\ntrain_ds = LunaDataset ( <b Our custom dataset\nval_stride=10,\nisValSet_bool=False,\n)\n\nbatch_size = self.cli_args.batch_size\nif self.use_cuda:\nbatch_size *= torch.cuda.device_count()\n\ntrain_dl = DataLoader ( < An off-the-shelf class\ntrain_ds,\nbatch_size=batch_size, \u201c Batching is done automatically.\nnum_workers=self.cli_args.num_workers,\npin_memory=self.use_cuda, t\n) \u201c| Pinned memory transfers\nto GPU quickly.\n\nreturn train_dl\n\n# ... line 137\n\ndef main(self):\ntrain dl = self.initTrainDl() The validation data loader\nval_dl = self.initvalpl() <\u2014t Is very similar to training.\n\nIn addition to batching individual samples, data loaders can also provide parallel\nloading of data by using separate processes and shared memory. All we need to do is\nspecify num_workers=.. when instantiating the data loader, and the rest is taken care of\nbehind the scenes. Each worker process produces complete batches as in figure 11.4.\nThis helps make sure hungry GPUs are well fed with data. Our validation_ds and\nvalidation_dl instances look similar, except for the obvious isValSet_bool=True.\n\nWhen we iterate, like for batch_tup in self.train_dl:, we won't have to wait\nfor each Ct to be loaded, samples to be taken and batched, and so on. Instead, we'll\nget the already loaded batch_tup immediately, and a worker process will be freed up\nin the background to begin loading another batch to use on a later iteration. Using\nthe data-loading features of PyTorch can help speed up most projects, because we can\noverlap data loading and processing with GPU calculation.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.8,
                        "section_name": "Our first-pass neural network design",
                        "section_path": "./screenshots-images-2/chapter_12/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_8/8b50d26c-5f3e-49df-beb2-67ad374a24d7.png",
                            "./screenshots-images-2/chapter_12/section_8/bca7c2c1-560b-41fc-b56a-124e4cfd4826.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Our first-pass neural network design\n\nThe possible design space for a convolutional neural network capable of detecting\ntumors is effectively infinite. Luckily, considerable effort has been spent over the past\ndecade or so investigating effective models for image recognition. While these have\nlargely focused on 2D images, the general architecture ideas transfer well to 3D, so\nthere are many tested designs that we can use as a starting point. This helps because\nalthough our first network architecture is unlikely to be our best option, right now we\nare only aiming for \u201cgood enough to get us going.\u201d\n\nWe will base the network design on what we used in chapter 8. We will have to\nupdate the model somewhat because our input data is 3D, and we will add some com-\nplicating details, but the overall structure shown in figure 11.5 should feel familiar.\nSimilarly, the work we do for this project will be a good base for your future projects,\nalthough the further you get from classification or segmentation projects, the more\nyou'll have to adapt this base to fit. Let\u2019s dissect this architecture, starting with the four\nrepeated blocks that make up the bulk of the network.\n\nsata) LUNA MODEL ARCHITECTURE\n\nBACKBONE\n\nCH a\n\n8 an ]\nZ\nTMAGE! Vox xa\n{| Stock{bizck I]\n~~\n\nce\n\n| TMAGE: 82140548 paver\n= Cn\n~ \u2018INPUT\n\nIMAGE\n\n\u2018\n\nTAIL\n\nvee\n\nFigure 11.5 The architecture of the LunaModel class consisting of a batch-normalization tail,\na four-block backbone, and a head comprised of a linear layer followed by softmax\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.9,
                        "section_name": "The core convolutions",
                        "section_path": "./screenshots-images-2/chapter_12/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_9/2aae29f4-afd5-4e7d-9ae7-e75bb6d79c5b.png",
                            "./screenshots-images-2/chapter_12/section_9/8d3668b2-f851-4837-a967-3bf1a35505a2.png",
                            "./screenshots-images-2/chapter_12/section_9/56627067-6320-4674-b99c-40c10e59363f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "_ The core convolutions\n\nClassification models often have a structure that consists of a tail, a backbone (or\nbody), and a head. The Jail is the first few layers that process the input to the network.\nThese early layers often have a different structure or organization than the rest of the\nnetwork, as they must adapt the input to the form expected by the backbone. Here we\nuse a simple batch normalization layer, though often the tail contains convolutional\nlayers as well. Such convolutional layers are often used to aggressively downsample the\nsize of the image; since our image size is already small, we don\u2019t need to do that here.\n\nNext, the backbone of the network typically contains the bulk of the layers, which\nare usually arranged in series of blocks. Each block has the same (or at least a similar)\nset of layers, though often the size of the expected input and the number of filters\nchanges from block to block. We will use a block that consists of two 3 x 3 convolu-\ntions, each followed by an activation, with a max-pooling operation at the end of the\nblock. We can see this in the expanded view of figure 11.5 labeled Block[{block1].\nHere's what the implementation of the block looks like in code.\n\nListing 11.6 model.py:67, class LunaBlock\n\nclass LunaBlock(nn.Module) :\ndef __init_ (self, in_channels, conv_channels):\nsuper ().__init__()\n\nself.convl = nn.Conv3d(\nin_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n)\nself.relul = nn.ReLU(inplace=True) 1((CO5-1))\nself.conv2 = nn.Conv3d(\nconv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n)\n\nself.relu2 = nn.ReLU(inplace=True) Sc]\n\nself.maxpool = nn.MaxPool3d(2, 2)\n\nThese could be\ndef forward(self, input_batch): implemented as calls\nblock_out = sel\u00a3.convi (input_batch) to the functional API\nblock_out = self.relul (block_out) a\u2014| instead.\nblock_out = self.conv2(block_out)\nblock_out = self.relu2(block_out) a\n\nreturn self.maxpool (block_out)\n\nFinally, the head of the network takes the output from the backbone and converts it\ninto the desired output form. For convolutional networks, this often involves flatten-\ning the intermediate output and passing it to a fully connected layer. For some net-\nworks, it makes sense to also include a second fully connected layer, although that is\nusually more appropriate for classification problems in which the imaged objects have\nmore structure (think about cars versus trucks having wheels, lights, grill, doors, and\nso on) and for projects with a large number of classes. Since we are only doing binary\nclassification, and we don\u2019t seem to need the additional complexity, we have only a\nsingle flattening layer.\n\nUsing a structure like this can be a good first building block for a convolutional\nnetwork. There are more complicated designs out there, but for many projects they\u2019re\noverkill in terms of both implementation complexity and computational demands. It\u2019s\na good idea to start simple and add complexity only when there\u2019s a demonstrable\nneed for it.\n\nWe can see the convolutions of our block represented in 2D in figure 11.6. Since\nthis is a small portion of a larger image, we ignore padding here. (Note that the ReLU\nactivation function is not shown, as applying it does not change the image sizes.)\n\nLet\u2019s walk through the information flow between our input voxels and a single voxel\nof output. We want to have a strong sense of how our output will respond when the\ninputs change. It might be a good idea to review chapter 8, particularly sections 8.1\nthrough 8.3, just to make sure you're 100% solid on the basic mechanics of convolutions.\n\nWe're using 3 x 3 x 3 convolutions in our block. A single 3 x 3 x 3 convolution has\na receptive field of 3 x 3 x 3, which is almost tautological. Twenty-seven voxels are fed\nin, and one comes out.\n\nIt gets interesting when we use two 3 x 3 x 3 convolutions stacked back to back. Stack-\ning convolutional layers allows the final output voxel (or pixel) to be influenced by an\ninput further away than the size of the convolutional kernel suggests. If that output\n\nFigure 11.6\n\nThe convolutional\narchitecture of a\nLunaModel block\nconsisting of two 3 x 3\nconvolutions followed\n\\xl OUTPUT by a max pool. The final\npixel has a receptive\nfield of 6 x 6.\n\nvoxel is fed into another 3 x 3 x 3 kernel as one of the edge voxels, then some of the\ninputs to the first layer will be outside of the 3 x 3 x 3 area of input to the second. The\nfinal output of those two stacked layers has an effective receptive field of 5 x 5 x 5. That\nmeans that when taken together, the stacked layers act as similar to a single convolu-\ntional layer with a larger size.\n\nPut another way, each 3 x 3 x 3 convolutional layer adds an additional one-voxel-\nper-edge border to the receptive field. We can see this if we trace the arrows in fig-\nure 11.6 backward; our 2 x 2 output has a receptive field of 4 x 4, which in turn has a\nreceptive field of 6 x 6. Two stacked 3 x 3 x 3 layers uses fewer parameters than a full\n5 x 5 x 5 convolution would (and so is also faster to compute).\n\nThe output of our two stacked convolutions is fed into a 2 x 2 x 2 max pool, which\nmeans we\u2019re taking a 6 x 6 x 6 effective field, throwing away seven-eighths of the data,\nand going with the one 5 x 5 x 5 field that produced the largest value.\u201d Now, those\n\u201cdiscarded\u201d input voxels still have a chance to contribute, since the max pool that\u2019s\none output voxel over has an overlapping input field, so it\u2019s possible they'll influence\nthe final output that way.\n\nNote that while we show the receptive field shrinking with each convolutional\nlayer, we're using padded convolutions, which add a virtual one-pixel border around\nthe image. Doing so keeps our input and output image sizes the same.\n\nThe nn.ReLU layers are the same as the ones we looked at in chapter 6. Outputs\ngreater than 0.0 will be left unchanged, and outputs less than 0.0 will be clamped to\nzero.\n\nThis block will be repeated multiple times to form our model\u2019s backbone.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.1,
                        "section_name": "The full model",
                        "section_path": "./screenshots-images-2/chapter_12/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_10/4613a353-3ddc-4c06-bacc-4cbf1c1a8089.png",
                            "./screenshots-images-2/chapter_12/section_10/f5b08e5c-c12e-4a8f-bd70-9ea33a95edaa.png",
                            "./screenshots-images-2/chapter_12/section_10/cbfbdc69-a0c5-4d54-b10f-95e509245ede.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The full model\n\nLet\u2019s take a look at the full model implementation. We'll skip the block definition,\nsince we just saw that in listing 11.6.\n\nListing 11.7 model.py:13, 8 LunaModel\n\nclass LunaModel (nn.Module) :\ndef __init (self, in_channels=1, conv_channels=8):\nsuper().__init__()\n\nself.tail_batchnorm = nn.BatchNorm3d (1) <\u2014 Tail\n\nself.blockl\nself. block2\nself.block3\nself. block4\n\nLunaBlock(in_channels, conv_channels)\nLunaBlock(conv_channels, conv_channels * 2)\nLunaBlock(conv_channels * 2, conv_channels * 4)\nLunaBlock(conv_channels * 4, conv_channels * 8)\n\nBackbone\n\nself.head_linear = nn.Linear(1152, 2) Head\nself.head_softmax = nn.Softmax(dim=1)\n\nHere, our tail is relatively simple. We are going to normalize our input using\nnn. BatchNorm3d, which, as we saw in chapter 8, will shift and scale our input so that it\nhas a mean of 0 and a standard deviation of 1. Thus, the somewhat odd Hounsfield\nunit (HU) scale that our input is in won't really be visible to the rest of the network.\nThis is a somewhat arbitrary choice; we know what our input units are, and we know\nthe expected values of the relevant tissues, so we could probably implement a fixed\nnormalization scheme pretty easily. It\u2019s not clear which approach would be better.*\n\nOur backbone is four repeated blocks, with the block implementation pulled outinto\nthe separate nn. Module subclass we saw earlier in listing 11.6. Since each block ends with\na 2x22 max-pool operation, after 4 layers we will have decreased the resolution of the\nimage 16 times in each dimension. Recall from chapter 10 that our data is returned in\nchunks that are 32 x 48 x 48, which will become 2 x 3 x 3 by the end of the backbone.\n\nFinally, our tail is just a fully connected layer followed by a call to nn. Softmax. Soft-\nmax is a useful function for single-label classification tasks and has a few nice proper-\nties: it bounds the output between 0 and 1, it\u2019s relatively insensitive to the absolute\nrange of the inputs (only the relative values of the inputs matter), and it allows our\nmodel to express the degree of certainty it has in an answer.\n\nThe function itself is relatively simple. Every value from the input is used to expo-\nnentiate e, and the resulting series of values is then divided by the sum of all the\nresults of exponentiation. Here\u2019s what it looks like implemented in a simple fashion as\na nonoptimized softmax implementation in pure Python:\n\n>>> logits = [1, -2, 3]\n>>> exp = [e ** x for x in logits]\n>>> exp\n\n(2.718, 0.135, 20.086)\n\n>>> softmax = [x / sum(exp) for x in exp]\n>>> softmax\n(0.118, 0.006, 0.876)\n\nOf course, we use the PyTorch version of nn.Softmax for our model, as it natively\nunderstands batches and tensors and will perform autograd quickly and as expected.\n\nCOMPLICATION: CONVERTING FROM CONVOLUTION TO LINEAR\n\nContinuing on with our model definition, we come to a complication. We can\u2019t just\nfeed the output of self .block4 into a fully connected layer, since that output is a per-\nsample 2 x 3 x 3 image with 64 channels, and fully connected layers expect a 1D vector\nas input (well, technically they expect a batch of 1D vectors, which is a 2D array, but the\nmismatch remains either way). Let\u2019s take a look at the forward method.\n\nListing\n\nmodel.py:50, LunaModel . forward\n\ndef forward(self, input_batch):\nbn_output = self.tail_batchnorm(input_batch)\n\nblock_out self.blocki(bn_output)\nblock_out self. block2 (block_out)\nblock_out = self.block3 (block_out)\nblock_out = self.block4 (block_out)\n\nconv_flat = block_out.view(\nblock_out.size(0), <i\u2014 The batch size\n-1,\n\n)\n\nlinear_output = self.head_linear(conv_flat)\n\nreturn linear_output, self.head_softmax(linear_output)\n\nNote that before we pass data into a fully connected layer, we must flatten it using the\nview function. Since that operation is stateless (it has no parameters that govern its\nbehavior), we can simply perform the operation in the forward function. This is\nsomewhat similar to the functional interfaces we discussed in chapter 8. Almost every\nmodel that uses convolution and produces classifications, regressions, or other non-\nimage outputs will have a similar component in the head of the network.\n\nFor the return value of the forward method, we return both the raw /ogits and the\nsoftmax-produced probabilities. We first hinted at logits in section 7.2.6: they are the\nnumerical values produced by the network prior to being normalized into probabili-\nties by the softmax layer. That might sound a bit complicated, but logits are really just\nthe raw input to the softmax layer. They can have any real-valued input, and the soft-\nmax will squash them to the range 0-1.\n\nWe'll use the logits when we calculate the nn.CrossEntropyLoss during training,*\nand we'll use the probabilities for when we want to actually classify the samples. This\nkind of slight difference between what's used for training and what's used in produc-\ntion is fairly common, especially when the difference between the two outputs is a sim-\nple, stateless function like softmax.\n\nINITIALIZATION\n\nFinally, let\u2019s talk about initializing our network\u2019s parameters. In order to get well-\nbehaved performance out of our model, the network's weights, biases, and other\nparameters need to exhibit certain properties. Let\u2019s imagine a degenerate case, where\nall of the network\u2019s weights are greater than 1 (and we do not have residual connec-\ntions). In that case, repeated multiplication by those weights would result in layer out-\nputs that became very large as data flowed through the layers of the network.\nSimilarly, weights less than 1 would cause all layer outputs to become smaller and van-\nish. Similar considerations apply to the gradients in the backward pass.\n\nMany normalization techniques can be used to keep layer outputs well behaved, but\none of the simplest is to just make sure the network's weights are initialized such that\nintermediate values and gradients become neither unreasonably small nor unreasonably\nlarge. As we discussed in chapter 8, PyTorch does not help us as much as it should here,\nso we need to do some initialization ourselves. We can treat the following _init_weights\nfunction as boilerplate, as the exact details aren\u2019t particularly important.\n\nListing 11.9 model.py:30, LunaModel._init_weights\n\ndef _init_weights(self):\nfor m in self.modules():\nif type(m) in {\nnn.Linear,\nnn.Conv3d,\n}:\nnn.init.kaiming_normal_(\nm.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n)\nif m.bias is not None:\nfan_in, fan_out = \\\nnn.init._calculate_fan_in_and_fan_out (m.weight.data)\nbound = 1 / math.sart (fan_out)\nnn.init.normal_(m.bias, -bound, bound)\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.11,
                        "section_name": "Training and validating the model",
                        "section_path": "./screenshots-images-2/chapter_12/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_11/e48618c3-eecb-46fd-9fa0-62ab4d16ab7b.png",
                            "./screenshots-images-2/chapter_12/section_11/ca5c9401-79f5-4256-afe8-ad0dd7783831.png",
                            "./screenshots-images-2/chapter_12/section_11/d8d5f8bd-d8b1-47f2-a3fc-be8171a1ffed.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training and validating the model\n\nNow it\u2019s time to take the various pieces we've been working with and assemble them\ninto something we can actually execute. This training loop should be familiar\u2014we saw\nloops like figure 11.7 in chapter 5.\n\nINIT MODEL\nOo\n\nINIT DATA LOADERS\n\nLOOP OVER EPOCHS\n\nS/F\n\nTRAINING LOOP\n\nCLASSIFY BATCH 988 \u2014~\nCALCULATE LOSS __ 9838 \u2014\nRECORD METRICS\n\nVALIDATION LOOP\nLOAD BATCH TUPLE\nIFY BATCH\n\nRECORD METRICS\n\nLOG METRICS\n\nCONS\n\nTENSORBOARD\nFigure 11.7 The training and validation script we will implement in this chapter, with\na focus on the nested loops over each epoch and batches in the epoch\n\nThe code is relatively compact (the doTraining function is only 12 statements; it\u2019s lon-\nger here due to line-length limitations).\n\nListing 11.10 training.py:137, LunaTra.\n\ndef main(self):\n# ... line 143\nfor epoch_ndx in range(1, self.cli_args.epochs + 1):\ntrnMetrics_t = self.doTraining(epoch_ndx, train_dl)\nself.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n\n# ... line 165\n\ndef doTraining(self, epoch_ndx, train_dl):\nself.model.train()\ntrnMetrics_g = torch.zeros(\n\nInitializes an empty\nMETRICS_SIZE, metrics array\nlen(train_dl.dataset),\ndevice=self.device,\n)\nbatch_iter = enumerateWithEstimate( Sets up our batch looping\ntrain_dl, with time estimate\n\n*E{} Training\". format (epoch_ndx),\nstart_ndx=train_dl.num_workers,\n\n)\n\nfor batch_ndx, batch_tup in batch_iter: Frees any leftover\nself.optimizer.zero_grad() gradient tensors\n\nloss_var = self.computeBatchLoss(\nbatch_ndx,\nbatch_tup,\ntrain_dl.batch_size,\ntrnMetrics_g\n\n)\n\n\u2014 We'll discuss this method in\ndetail in the next section.\n\nloss_var . backward () Actually updates\nself.optimizer.step() the model weights\n\nself.totalTrainingSamples_count += len(train_dl.dataset)\n\nreturn trnMetrics_g.to('cpu')\n\nThe main differences that we see from the training loops in earlier chapters are as\nfollows:\n\n= The trnMetrics_g tensor collects detailed per-class metrics during training.\nFor larger projects like ours, this kind of insight can be very nice to have.\n\n= We don\u2019t directly iterate over the train_dl data loader. We use enumerateWith-\nEstimate to provide an estimated time of completion. This isn\u2019t crucial; it\u2019s just\na stylistic choice.\n\n= The actual loss computation is pushed into the computeBatchLoss method.\nAgain, this isn\u2019t strictly necessary, but code reuse is typically a plus.\n\nWe'll discuss why we\u2019ve wrapped enumerate with additional functionality in section\n11.7.2; for now, assume it\u2019s the same as enumerate (train_dl).\n\nThe purpose of the trnMetrics_g tensor is to transport information about how\nthe model is behaving on a per-sample basis from the computeBatchLoss function to\nthe logMetrics function. Let\u2019s take a look at computeBatchLoss next. We'll cover\nlogMetrics after we're done with the rest of the main training loop.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.12,
                        "section_name": "The computeBatchLoss function",
                        "section_path": "./screenshots-images-2/chapter_12/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_12/03c2eaf9-8059-4854-9ee1-5993364bc886.png",
                            "./screenshots-images-2/chapter_12/section_12/e0aed86e-e4cd-4503-a9a0-1800db1defb9.png",
                            "./screenshots-images-2/chapter_12/section_12/85d82afa-9654-4946-b5c7-c0b6b046343a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "_ The computeBatchLoss function\n\nThe computeBatchLoss function is called by both the training and validation loops. As\nthe name suggests, it computes the loss over a batch of samples. In addition, the func-\ntion also computes and records per-sample information about the output the model is\nproducing. This lets us compute things like the percentage of correct answers per\nclass, which allows us to hone in on areas where our model is having difficulty.\n\nOf course, the function\u2019s core functionality is around feeding the batch into the\nmodel and computing the per-batch loss. We\u2019re using CrossEntropyLoss (https://\npytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss), just like in chapter 7.\nUnpacking the batch tuple, moving the tensors to the GPU, and invoking the model\nshould all feel familiar after that earlier training work.\n\nListii\n\n11.11 | training.py:225, . compu\u2019 chLoss\n\ndef computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\ninput_t, label_t, _series_list, _center_list = batch_tup\n\ninput_g\nlabel_g\n\ninput_t.to(self.device, non_blocking=True)\nlabel_t.to(self.device, non_blocking=True)\n\nlogits_g, probability_g = self.model (input_g)\n\u2018 reduction=\u2018none\u2019 gives\n\nloss_fune = nn.CrossEntropyLoss (reduction=\u2018none') the loss per sample.\nloss_g = loss_func(\nlogits_g, Index of the one-\nlabel_g[:,1], \u201c hot-encoded class\n)\n# ... line 238 Recombines the loss per\nreturn loss_g.mean() sample into a single value\n\nHere we are not using the default behavior to get a loss value averaged over the batch.\nInstead, we get a tensor of loss values, one per sample. This lets us track the individual\nlosses, which means we can aggregate them as we wish (per class, for example). We'll\nsee that in action in just a moment. For now, we'll return the mean of those per-sample\nlosses, which is equivalent to the batch loss. In situations where you don\u2019t want to keep\nstatistics per sample, using the loss averaged over the batch is perfectly fine. Whether\nthat\u2019s the case is highly dependent on your project and goals.\n\nOnce that\u2019s done, we've fulfilled our obligations to the calling function in terms of\nwhat's required to do backpropagation and weight updates. Before we do that, how-\never, we also want to record our per-sample stats for posterity (and later analysis).\nWe'll use the metrics_g parameter passed in to accomplish this.\n\nListing 11.12  training.py:26\n\nMETRICS_LABEL_NDX=0 These named array indexes are\n\nMETRICS_PRED_NDX=1 declared at module-level scope.\nMETRICS_LOSS_NDX=2\n\nMETRICS_SIZE = 3\n\n# ... line 225\ndef computeBatchLoss(self, batch _ndx, batch _tup, batch_size, metrics_g):\n# ... line 238\n\nstart_ndx = batch_ndx * batch_size\nend_ndx = start_ndx + label_t.size(0)\n\nmetrics_g[(METRICS_LABEL_NDX, start_ndx:end_ndx] = \\\n\nlabel_g[:,1] .detach() We use detach since\nmetrics_g(METRICS_PRED_NDX, start_ndx:end_ndx] = \\ none of our metrics\nprobability_g[:,1].detach() need to hold on to\n\nmetrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx]\nloss_g.detach()\n\n\"\n-\n\ngradients.\n\nAgain, this is the loss\nreturn loss_g.mean() over the entire batch.\n\nBy recording the label, prediction, and loss for each and every training (and later, val-\nidation) sample, we have a wealth of detailed information we can use to investigate\nthe behavior of our model. For now, we're going to focus on compiling per-class statis-\ntics, but we could easily use this information to find the sample that is classified the\nmost wrongly and start to investigate why. Again, for some projects, this kind of infor-\nmation will be less interesting, but it\u2019s good to remember that you have these kinds of\noptions available.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.13,
                        "section_name": "The validation loop is similar",
                        "section_path": "./screenshots-images-2/chapter_12/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_13/561b45e4-6602-4af0-93b4-330b685cdf90.png",
                            "./screenshots-images-2/chapter_12/section_13/c65aabe0-e144-4fcd-966b-17d2911dd614.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The validation loop is similar\n\nThe validation loop in figure 11.8 looks very similar to training but is somewhat sim-\nplified. The key difference is that validation is read-only. Specifically, the loss value\nreturned is not used, and the weights are not updated.\n\nINIT wee oO.\nOO a= X52 =\n\nINIT DATA LOADERS\n\nLOOP OVER EPOCHS\n\nTRAINING LOOP\nLOAD BATCH TUPLE\n( es] 7 ) CLASSIFY BATCH 9B\n\u2019 CALCULATE LOSS __ 98\nRECORD Me\u2019\n\nUPDATE WEIGHTS\n\nLOG METRICS\nCONSOLE\nTENSORBOARD\n\nFigure 11.8 The training and validation script we will implement in this chapter, with a\nfocus on the per-epoch validation loop\n\nNothing about the model should have changed between the start and end of the func-\ntion call. In addition, it\u2019s quite a bit faster due to the with torch.no_grad() context\nmanager explicitly informing PyTorch that no gradients need to be computed.\n\nListing 11.13 training.py:137, LunaTrainingApp.main\n\ndef main(self):\nfor epoch_ndx in range(1, self.cli_args.epochs + 1):\n# =... line 157\nvalMetrics_t = self.doValidation(epoch_ndx, val_dl)\n\nself.logMetrics(epoch_ndx, \u2018'val', valMetrics_t)\n\n# ... line 203\ndef doValidation(self, epoch_ndx, val_dl):\nwith torch.no_grad():\nself.model.eval() \u00ab>\u2014 Turns off training-time behavior\nvalMetrics_g = torch. zeros(\nMETRICS_SIZE,\nlen(val_dl.dataset),\ndevice=self.device,\n)\n\nbatch_iter = enumerateWithEstimate(\nval_dl,\n\"E() Validation \". format (epoch_ndx),\nstart_ndx=val_dl.num_workers,\n)\nfor batch_ndx, batch _tup in batch_iter:\nself.computeBatchLoss (\nbatch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n\nreturn valMetrics_g.to('cpu')\n\nWithout needing to update network weights (recall that doing so would violate the\nentire premise of the validation set; something we never want to do!), we don\u2019t need\nto use the loss returned from computeBatchLoss, nor do we need to reference the\noptimizer. All that\u2019s left inside the loop is the call to computeBatchLoss. Note that we\nare still collecting metrics in valMetrics_g asa side effect of the call, even though we\naren't using the overall per-batch loss returned by computeBatchLoss for anything.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.14,
                        "section_name": "Outputting performance metrics",
                        "section_path": "./screenshots-images-2/chapter_12/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_14/cfee3850-fafd-48e6-aae5-545f84ed86e4.png",
                            "./screenshots-images-2/chapter_12/section_14/b04020f6-72d6-4fc4-8731-3d8d339ecdf5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Outputting performance metrics\n\nThe last thing we do per epoch is log our performance metrics for this epoch. As\nshown in figure 11.9, once we've logged metrics, we return to the training loop for the\nnext epoch of training. Logging results and progress as we go is important, since if\ntraining goes off the rails (\u201cdoes not converge\u201d in the parlance of deep learning), we\nwant to notice this is happening and stop spending time training a model that\u2019s not\nworking out. In less catastrophic cases, it\u2019s good to be able to keep an eye on how your\nmodel behaves.\n\nEarlier, we were collecting results in trnMetrics_g and valMetrics_g for logging\nprogress per epoch. Each of these two tensors now contains everything we need to\ncompute our percent correct and average loss per class for our training and validation\nruns. Doing this per epoch is a common choice, though somewhat arbitrary. In future\nchapters, we'll see how to manipulate the size of our epochs such that we get feedback\nabout training progress at a reasonable rate.\n\nINIT MODEL\n\nINIT DATA LOADERS,\n\nLOOP OVER EPOCHS\n\nTRAINING LOOP\n\nLOAD BATCH TUPLE\n2\nB, . \\ CLASSIFY BATCH \u2014 SRE VALIDATION Loop\n\nCALCULATE LOSS __ 968 LOAD BATCH TUPLE\nRECORD METRICS CLASSIFY BATCH\n\nUPDATE WEIGHTS, CALCULATE LOSS,\n\nRECORD METRICS\n\nLOG METRICS\nCONSOLE\nTENSORSOARD\n\nFigure 11.9 The training and validation script we will implement in this chapter, with\na focus on the metrics logging at the end of each epoch\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.15,
                        "section_name": "The logMetrics function",
                        "section_path": "./screenshots-images-2/chapter_12/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_15/2cfe6a83-ad13-4652-a52d-bd5f7bef81e2.png",
                            "./screenshots-images-2/chapter_12/section_15/17c9501e-4deb-48f3-a01d-a15c8eb41a7d.png",
                            "./screenshots-images-2/chapter_12/section_15/3bb10a0d-7c86-468f-891f-602909b2988d.png",
                            "./screenshots-images-2/chapter_12/section_15/5f15c1f1-2607-4fea-89ac-7bef5e4e32bf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "L The logMetrics function\n\nLet\u2019s talk about the high-level structure of the logMetrics function. The signature\nlooks like this.\n\nListing 11.14 training.py:251, LunaTrainingApp.logMetrics\n\ndef logMetrics(\nself,\nepoch_ndx,\nmode_str,\nmetrics_t,\nclassificationThreshold=0.5,\nd:\n\nWe use epoch_ndx purely for display while logging our results. The mode_str argu-\nment tells us whether the metrics are for training or validation.\n\nWe consume either trnMetrics_t or valMetrics_t, which is passed in as the metrics\n_t parameter. Recall that both of those inputs are tensors of floating-point values that we\nfilled with data during computeBatchLoss and then transferred back to the CPU right\nbefore we returned them from doTraining and doValidation. Both tensors have three\nrows and as many columns as we have samples (training samples or validation samples,\ndepending). As a reminder, those three rows correspond to the following constants.\n\nListing 11.15  training.py:26\n\nMETRICS_LABEL_NDX=0 These are declared at\n\nMETRICS_PRED_NDX=1 module-level scope.\nMETRICS_LOSS_NDX=2\n\nMETRICS_SIZE = 3\n\nTensor masking and Boolean indexing\nMasked tensors are a common usage pattern that might be opaque if you have not\n\nencountered them before. You may be familiar with the NumPy concept called\nmasked arrays; tensor and array masks behave the same way.\n\nIf you aren't familiar with masked arrays, an excellent page in the NumPy documen-\ntation (http://mng.bz/XPra) describes the behavior well. PyTorch purposely uses the\nsame syntax and semantics as NumPy.\n\nCONSTRUCTING MASKS\n\nNext, we're going to construct masks that will let us limit our metrics to only the nod-\nule or non-nodule (aka positive or negative) samples. We will also count the total sam-\nples per class, as well as the number of samples we classified correctly.\n\nListing 11.16 training.py:264, LunaTrainingApp. logMetrics\n\nnegLabel_mask = metrics_t [METRICS_LABEL_NDX] <= classificationThreshold\nnegPred_mask = metrics_t (METRICS_PRED_NDX] <= classificationThreshold\n\nposLabel_mask = ~negLabel_mask\nposPred_mask = ~negPred_mask\n\nWhile we don\u2019t assert it here, we know that all of the values stored in metrics\n_t [METRICS_LABEL_NDX] belong to the set {0.0, 1.0} since we know that our nodule\nstatus labels are simply True or False. By comparing to classificationThreshold,\nwhich defaults to 0.5, we get an array of binary values where a True value corresponds\nto a non-nodule (aka negative) label for the sample in question.\n\nWe do a similar comparison to create the negPred_mask, but we must remember\nthat the METRICS_PRED_NDX values are the positive predictions produced by our model\nand can be any floating-point value between 0.0 and 1.0, inclusive. That doesn\u2019t\nchange our comparison, but it does mean the actual value can be close to 0.5. The\npositive masks are simply the inverse of the negative masks.\n\nNOTE While other projects can utilize similar approaches, it\u2019s important to\nrealize that we\u2019re taking some shortcuts that are allowed because this is a\nbinary classification problem. If your next project has more than two classes\nor has samples that belong to multiple classes at the same time, you'll have to\nuse more complicated logic to build similar masks.\n\nNext, we use those masks to compute some per-label statistics and store them in a dic-\ntionary, metrics_dict.\n\nListing 11.17 training.py:270, LunaTrainingApp.1\n\nneg_count = int (negLabel_mask.sum()) <\u2014 Converts to a normal\npos_count = int (posLabel_mask.sum()) Python integer\n\nneg_correct = int((negLabel_mask & negPred_mask).sum())\npos_correct = int((posLabel_mask & posPred_mask).sum())\n\nmetrics dict = {}\n\nmetrics_dict['loss/all'] = \\\nmetrics_t[(METRICS_LOSS_NDX] .mean()\n\nmetrics _dict['loss/neg'] = \\\nmetrics_t(METRICS_LOSS_NDX, negLabel_mask] .mean()\n\nmetrics _dict['loss/pos'] = \\\n\nmetrics_t(METRICS_LOSS_NDX, posLabel_mask] .mean() Avoids i by\n: : converting to\nmetrics_dict['correct/all'] = (pos_correct + neg_correct) \\ np.float32\n\n/ np.float32(metrics_t.shape[1]) * 100 ae\nmetrics_dict['correct/neg') = neg_correct / np.float32(neg_count) * 100\nmetrics_dict['correct/pos'] = pos_correct / np.float32(pos_count) * 100\n\nFirst we compute the average loss over the entire epoch. Since the loss is the single\nmetric that is being minimized during training, we always want to be able to keep\ntrack of it. Then we limit the loss averaging to only those samples with a negative label\nusing the negLabel_mask we just made. We do the same with the positive loss. Com-\nputing a per-class loss like this can be useful if one class is persistently harder to classify\nthan another, since that knowledge can help drive investigation and improvements.\nWe'll close out the calculations with determining the fraction of samples we classi-\nfied correctly, as well as the fraction correct from each label. Since we will display\nthese numbers as percentages in a moment, we also multiply the values by 100. Similar\nto the loss, we can use these numbers to help guide our efforts when making improve-\nments. After the calculations, we then log our results with three calls to log. info.\n\nListing 11.18 training.py:289, LunaTrainingApp.logMetrics\n\nlog.info(\n(\"E() (:8) {loss/all:.4\u00a3) loss, \"\n+ \"{correct/all:-5.1\u00a3)% correct, \"\n) . format (\nepoch_ndx,\nmode_str,\n**metrics_dict,\n)\n)\nlog. info(\n(\"E{} {:8) {loss/neg:.4\u00a3} loss, \"\n+ \"{correct/neg:-5.1\u00a3)% correct ({neg_correct:} of (neg_count:})\"\n\n). format (\nepoch_ndx,\nmode_str + '_neg',\nneg_correct=neg_correct,\nneg_count=neg_count,\n**metrics_dict,\n\n) The \u2018pos\u2019 logging is similar\nlog. info( to the \u2018neg\u2019 logging earlier.\n\n# ... line 319\n)\n\nThe first log has values computed from all of our samples and is tagged /all, while\nthe negative (non-nodule) and positive (nodule) values are tagged /neg and /pos,\nrespectively. We don\u2019t show the third logging statement for positive values here; it\u2019s\nidentical to the second except for swapping neg for pos in all cases.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.16,
                        "section_name": "Running the training script",
                        "section_path": "./screenshots-images-2/chapter_12/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_16/9717bce4-ba12-4d2d-901f-7666d18243ed.png",
                            "./screenshots-images-2/chapter_12/section_16/3e0fccc5-8195-4700-ba61-939cc6949e53.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Running the training script\n\nNow that we've completed the core of the training.py script, we'll actually start run-\nning it. This will initialize and train our model and print statistics about how well the\ntraining is going. The idea is to get this kicked off to run in the background while\nwe\u2019re covering the model implementation in detail. Hopefully we'll have results to\nlook at once we're done.\n\nWe're running this script from the main code directory; it should have subdirecto-\nries called p2ch11, util, and so on. The python environment used should have all the\nlibraries listed in requirements.txt installed. Once those libraries are ready, we can run:\n\nThis is the command line for Linux/Bash. Windows\nusers will probably need to invoke Python\n$ python -m p2ch11.training q\u2014 differently, depending on the install method used.\nStarting LunaTrainingApp,\n\nNamespace (batch_size=256, channels=8, epochs=20, layers=3, num_workers=8)\n<p2ch11.dsets.LunaDataset object at 0x7?fa53ai28710>: 495958 training samples\n<p2ch1l.dsets.LunaDataset object at 0x7fa537325198>: 55107 validation samples\nEpoch 1 of 20, 1938/216 batches of size 256\nEl Training ----/1938, starting\nEl Training 16/1938, done at 2018-02-28 20:52:54, 0:02:57\n\nAs a reminder, we also provide a Jupyter Notebook that contains invocations of the\ntraining application.\n\nListing 11.19 code/p2_run_everything.ipynb\n\n# In(5]:\nrun('p2chil.prepcache.LunaPrepCacheApp' )\n\n# In[6]:\nrun('p2chil.training.LunaTrainingApp', '--epochs=1')\n\nIf the first epoch seems to be taking a very long time (more than 10 or 20 minutes), it\nmight be related to needing to prepare the cached data required by LunaDataset. See\nsection 10.5.1 for details about the caching. The exercises for chapter 10 included\nwriting a script to pre-stuff the cache in an efficient manner. We also provide the\nprepcache.py file to do the same thing; it can be invoked with python -m p2chil\n.prepcache. Since we repeat our dsets.py files per chapter, the caching will need to be\nrepeated for every chapter. This is somewhat space and time inefficient, but it means we\ncan keep the code for each chapter much more well contained. For your future proj-\nects, we recommend reusing your cache more heavily.\n\nOnce training is underway, we want to make sure we're using the computing\nresources at hand the way we expect. An easy way to tell if the bottleneck is data loading\nor computation is to wait a few moments after the script starts to train (look for output\nlike E1 Training 16/7750, done at..) and then check both top and nvidia-smi:\n\n= Ifthe eight Python worker processes are consuming >80% CPU, then the cache\nprobably needs to be prepared (we know this here because the authors have\nmade sure there aren\u2019t CPU bottlenecks in this project's implementation; this\nwon't be generally true).\n\n= Ifnvidia-smi reports that GPU-Util is >80%, then you're saturating your GPU.\nWe'll discuss some strategies for efficient waiting in section 11.7.2.\n\nThe intent is that the GPU is saturated; we want to use as much of that computing\npower as we can to complete epochs quickly. A single NVIDIA GTX 1080 Ti should\ncomplete an epoch in under 15 minutes. Since our model is relatively simple, it\ndoesn\u2019t take a lot of CPU preprocessing for the CPU to be the bottleneck. When work-\ning with models with greater depth (or more needed calculations in general), process-\ning each batch will take longer, which will increase the amount of CPU processing we\ncan do before the GPU runs out of work before the next batch of input is ready.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.17,
                        "section_name": "Needed data for training",
                        "section_path": "./screenshots-images-2/chapter_12/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_17/f6ef24d3-60b5-40e5-ac07-b7e662348b0a.png",
                            "./screenshots-images-2/chapter_12/section_17/1d34869b-7c4d-4762-9f03-e67f4384d030.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Needed data for training\n\nIf the number of samples is less than 495,958 for training or 55,107 for validation, it\nmight make sense to do some sanity checking to be sure the full data is present and\naccounted for. For your future projects, make sure your dataset returns the number of\n\nsamples that you expect.\nFirst, let\u2019s take a look at the basic directory structure of our data-unversioned/\n\npart2/luna directory:\n\n$ ls -ip data-unversioned/part2/luna/\nsubset 0/\n\nsubseti/\n\nsubset 9/\n\nNext, let\u2019s make sure we have one .mhd file and one .raw file for each series UID\n\np data-unversioned/part2/luna/subset0/\n\n~1.14519.5.2.1.6279.6001.105756658031515062000744821260.mha\n-1.14519.5.2.1.6279.6001.105756658031515062000744821260. raw\n-1.14519.5.2.1.6279.6001.108197895896446896160048741492.mha\n-1.14519.5.2.1.6279.6001.108197895896446896160048741492. raw\n\nand that we have the overall correct number of files:\n\n$ ls -1 data-unversioned/part2/luna/subset?/* | we -1\n1776\n$ ls -1 data-unversioned/part2/luna/subset0/* | we -1\n178\n\n$ ls -1 data-unversioned/part2/luna/subset9/* | we -1\n176\n\nIf all of these seem right but things still aren\u2019t working, ask on Manning LiveBook\n(https:/ /livebook.manning.com/book/deep-learning-with-pytorch/chapter-11) and\nhopefully someone can help get things sorted out.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.18,
                        "section_name": "Interlude: The enumerateWithEstimate function",
                        "section_path": "./screenshots-images-2/chapter_12/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_18/d5a7efd2-a45b-4e39-a3ec-4c217dbe45dc.png",
                            "./screenshots-images-2/chapter_12/section_18/f06a7751-7bf9-44c9-9f8c-b4c91a7bd310.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "2 Interlude: The enumerateWithEstimate function\n\nWorking with deep learning involves a lot of waiting. We're talking about real-world,\nsitting around, glancing at the clock on the wall, a watched pot never boils (but you\ncould fry an egg on the GPU), straight up boredom.\n\nThe only thing worse than sitting and staring at a blinking cursor that hasn't\nmoved for over an hour is flooding your screen with this:\n\n2020-01-01 10:00:00,056 INFO training batch 1234\n\n2020-01-01 10:00:00,067 INFO training batch 1235\n\n2020-01-01 10:00:00,077 INFO training batch 1236\n\n2020-01-01 10:00:00,087 INFO training batch 1237\nete...\n\nAt least the quietly blinking cursor doesn\u2019t blow out your scrollback buffer!\n\nFundamentally, while doing all this waiting, we want to answer the question \u201cDo I\nhave time to go refill my water glass?\u201d along with follow-up questions about having\ntime to\n\n= Brew a cup of coffee\n= Grab dinner\n= Grab dinner in Paris?\n\nTo answer these pressing questions, we\u2019re going to use our enumerateWithEstimate\nfunction. Usage looks like the following:\n\n>>> for i, _ in enumerateWithEstimate(list(range(234)), \"sleeping\"):\nnae time.sleep (random. random())\n\n11:12:41,892 WARNING sleeping ----/234, starting\n\n11:12:44,542 WARNING sleeping 4/234, done at 2020-01-01 0:02:35\n\n11: WARNING sleeping 8/234, done at 2020-01-01 0:02:17\n11: WARNING sleeping 16/234, done at 2020-01-01 4:33, 0:01:51\n11: WARNING sleeping 32/234, done at 2020-01-01 141, 0:01:59\n11: WARNING sleeping 64/234, done at 2020-01-01 143, 0:02:01\n11: WARNING sleeping 128/234, done at 2020-01-01 135, 0:01:53\n11:14:40,083 WARNING sleeping ----/234, done at 2020-01-01 240\n\n>>>\n\nThat's 8 lines of output for over 200 iterations lasting about 2 minutes. Even given the\nwide variance of random.random(), the function had a pretty decent estimate after 16\niterations (in less than 10 seconds). For loop bodies with more constant timing, the\nestimates stabilize even more quickly.\n\nIn terms of behavior, enumerateWithEstimate is almost identical to the standard\nenumerate (the differences are things like the fact that our function returns a genera-\ntor, whereas enumerate returns a specialized <enumerate object at 0x..>).\n\nListing 11. util.py:143, def enume\n\ndef enumerateWithEstimate (\niter,\ndesec_str,\nstart_ndx=0,\nprint_ndx=4,\nbackoff=None,\niter_len=None,\n):\nfor (current_ndx, item) in enumerate (iter):\nyield (current_ndx, item)\n\nHowever, the side effects (logging, specifically) are what make the function interest-\ning. Rather than get lost in the weeds trying to cover every detail of the implementa-\ntion, if you're interested, you can consult the function docstring (https://github\n.com/deep-learning-with-pytorch/dlwpt-code/blob/master/util/util.py#L143) to get\ninformation about the function parameters and desk-check the implementation.\n\nDeep learning projects can be very time intensive. Knowing when something is\nexpected to finish means you can use your time until then wisely, and it can also clue\nyou in that something isn\u2019t working properly (or an approach is unworkable) if the\nexpected time to completion is much larger than expected.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.19,
                        "section_name": "Evaluating the model: Getting 99.7% correct means\nwe\u2019re done, right?",
                        "section_path": "./screenshots-images-2/chapter_12/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_19/e370e0df-f961-449d-b57f-37af1b4671c0.png",
                            "./screenshots-images-2/chapter_12/section_19/fd59cdb0-0e9e-4364-8f24-808e8e275c5c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Evaluating the model: Getting 99.7% correct means\nwe\u2019re done, right?\n\nLet's take a look at some (abridged) output from our training script. As a reminder,\nwe've run this with the command line python -m p2ch11.training:\n\nEl Training ----/969, starting\n\nEl LunaTrainingApp\nEl trn 2.4576 loss, 99.7% correct\n\nEl val 0.0172 loss, 99.8% correct\n\nAfter one epoch of training, both the training and validation set show at least 99.7%\ncorrect results. That's an A+! Time for a round of high-fives, or at least a satisfied nod\nand smile. We just solved cancer! ... Right?\n\nWell, no.\n\nLet\u2019s take a closer (less-abridged) look at that epoch 1 output:\n\nEl LunaTrainingApp\n\nEl trn 2.4576 loss, 99.7% correct,\n\nEl trnineg 0.1936 loss, 99.9% correct (494289 of 494743)\nEl trn_pos 924.34 loss, 0.2% correct (3 of 1215)\n\nEl val 0.0172 loss, 99.8% correct,\nEl val_neg 0.0025 loss, 100.0% correct (494743 of 494743)\nEl val_pos 5.9768 loss, 0.0% correct (0 of 1215)\n\nOn the validation set, we're getting non-nodules 100% correct, but the actual nodules\nare 100% wrong. The network is just classifying everything as not-a-nodule! The value\n99.7% just means only approximately 0.3% of the samples are nodules.\n\nAfter 10 epochs, the situation is only marginally better:\n\nE10 LunaTrainingApp\nE10 trn 0.0024 loss, 99.8% correct\n\nE10 trn_neg 0.0000 loss, 100.0% correct\nE10 trn_pos 0.9915 loss, 0.0% correct\nE10 val 0.0025 loss, 99.7% correct\nE10 val_neg 0.0000 loss, 100.0% correct\nE10 val_pos 0.9929 loss, 0.0% correct\n\nThe classification output remains the same\u2014none of the nodule (aka positive) sam-\nples are correctly identified. It\u2019s interesting that we're starting to see some decrease in\nthe val_pos loss, however, while not seeing a corresponding increase in the val_neg\nloss. This implies that the network is learning something. Unfortunately, it\u2019s learning\nvery, very slowly.\n\nEven worse, this particular failure mode is the most dangerous in the real world!\nWe want to avoid the situation where we classify a tumor as an innocuous structure,\n\nbecause that would not facilitate a patient getting the evaluation and eventual treat-\nment they might need. It\u2019s important to understand the consequences for misclassifi-\ncation for all your projects, as that can have a large impact on how you design, train,\nand evaluate your model. We'll discuss this more in the next chapter.\n\nBefore we get to that, however, we need to upgrade our tooling to make the results\neasier to understand. We\u2019re sure you love to squint at columns of numbers as much as\nanyone, but pictures are worth a thousand words. Let\u2019s graph some of these metrics.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.2,
                        "section_name": "Graphing training metrics with TensorBoard",
                        "section_path": "./screenshots-images-2/chapter_12/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_20/afbb4271-0697-480f-a33a-4790bf0516b6.png",
                            "./screenshots-images-2/chapter_12/section_20/afbe8e7e-dd80-4c02-9ef1-1e33098af34f.png",
                            "./screenshots-images-2/chapter_12/section_20/4c069544-408c-4a6d-a3d6-f1dd6aab20e6.png",
                            "./screenshots-images-2/chapter_12/section_20/728b95fc-eafa-4e13-8618-da9c59c92f5f.png",
                            "./screenshots-images-2/chapter_12/section_20/21259afe-9458-4545-a5d4-78ad8960a195.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Graphing training metrics with TensorBoard\n\nWe're going to use a tool called TensorBoard as a quick and easy way to get our train-\ning metrics out of our training loop and into some pretty graphs. This will allow us to\nfollow the trends of those metrics, rather than only look at the instantaneous values per\nepoch. It gets much, much easier to know whether a value is an outlier or just the lat-\nest in a trend when you're looking at a visual representation.\n\n\u201cHey, wait,\u201d you might be thinking, \u201cisn\u2019t TensorBoard part of the TensorFlow proj-\nect? What\u2019s it doing here in my PyTorch book?\u201d\n\nWell, yes, it is part of another deep learning framework, but our philosophy is \u201cuse\nwhat works.\u201d There\u2019s no reason to restrict ourselves by not using a tool just because it\u2019s\nbundled with another project we're not using. Both the PyTorch and TensorBoard\ndevs agree, because they collaborated to add official support for TensorBoard into\nPyTorch. TensorBoard is great, and it\u2019s got some easy-to-use PyTorch APIs that let us\nhook data from just about anywhere into it for quick and easy display. If you stick with\ndeep learning, you'll probably be seeing (and using) a /ot of TensorBoard.\n\nIn fact, if you've been running the chapter examples, you should already have\nsome data on disk ready and waiting to be displayed. Let\u2019s see how to run Tensor-\nBoard, and look at what it can show us.\n\nRunning TensorBoard\n\nBy default, our training script will write metrics data to the runs/ subdirectory. If you\nlist the directory content, you might see something like this during your Bash shell\nsession:\n\nThe single-epoch\n$ ls -1A runs/p2chl1l1/ run from earlier\ntotal 24\ndrwxrwxr-x 2 elis elis 4096 Sep 15 13:22 2020-01-01_12.55.27-trn-dlwpt/\ndrwxrwxr-x 2 elis elis 4096 Sep 15 13:22 2020-01-01_12.55.27-val-dlwpt/\ndrwxrwxr-x 2 elis elis 4096 Sep 15 15:14 2020-01-01_13.31.23-trn-dwlpt/\ndrwxrwxr-x 2 elis elis 4096 Sep 15 15:14 2020-01-01_13.31.23-val-dwlpt/\n\nThe more recent 10-epoch\ntraining run\n\nTo get the tensorboard program, install the tensorflow (https://pypi.org/project/\ntensorflow) Python package. Since we're not actually going to use TensorFlow proper,\nit\u2019s fine if you install the default CPU-only package. If you have another version of\n\nTensorBoard installed already, using that is fine too. Either make sure the appropriate\ndirectory is on your path, or invoke it with . . /path/to/tensorboard --logdir runs/.\nIt doesn\u2019t really matter where you invoke it from, as long as you use the --logdir argu-\nment to point it at where your data is stored. It\u2019s a good idea to segregate your data into\nseparate folders, as TensorBoard can get a bit unwieldy once you get over 10 or 20\nexperiments. You'll have to decide the best way to do that for each project as you go.\nDon\u2019t be afraid to move data around after the fact if you need to.\nLet\u2019s start TensorBoard now:\n\nThese messages might be different\n$ tensorboard --logdir runs/ or not present for you; that\u2019s fine.\n2020-01-01 12:13:16.163044: I tensorflow/core/platform/cpu_feature_guard.cc:140] <~\n\nYour CPU supports instructions that this TensorFlow binary was not\n\n> compiled to use: AVX2 FMA 1((CO17-2))\nTensorBoard 1.14.0 at http://localhost:6006/ (Press CTRL+C to quit)\n\nOnce that\u2019s done, you should be able to point your browser at http://localhost:6006\nand see the main dashboard.\u2019 Figure 11.10 shows us what that looks like.\n\nTensorBoard\n\nShon dana downoad inks rites tog\n\u2018arore cs\n\njean chart acalng\n\nTosti sorseg metho deteutt\n\nSrc\n\nes\nsassaaaee:\ni\nBO MeV e2ee 10HHb an deen ees\nowe at mn \u201c oa on\n\\ Ke \\\nos KS -\n\nFigure 11.10 The main TensorBoard UI, showing a paired set of training and validation runs\n\nAlong the top of the browser window, you should see the orange header. The right\nside of the header has the typical widgets for settings, a link to the GitHub repository,\nand the like. We can ignore those for now. The left side of the header has items for the\ndata types we've provided. You should have at least the following:\n\n\u00ab Scalars (the default tab)\n= Histograms\n\u00ab Precision-Recall Curves (shown as PR Curves)\n\nYou might see Distributions as well as the second UI tab (to the right of Scalars in fig-\nure 11.10). We won't use or discuss those here. Make sure you've selected Scalars by\nclicking it.\n\nOn the left is a set of controls for display options, as well as a list of runs that are\npresent. The smoothing option can be useful if you have particularly noisy data; it will\ncalm things down so that you can pick out the overall tend. The original non-\nsmoothed data will still be visible in the background as a faded line in the same color.\nFigure 11.11 shows this, although it might be difficult to discern when printed in black\nand white.\n\nDepending on how many times you've run the training script, you might have mul-\ntiple runs to select from. With too many runs being rendered, the graphs can get\noverly noisy, so don\u2019t hesitate to deselect runs that aren\u2019t of interest at the moment.\n\nIf you want to permanently remove a run, the data can be deleted from disk while\nTensorBoard is running. You can do this to get rid of experiments that crashed, had\n\nhe t . ink Q loss\n\nSMOOTHED\nTREND LINES\n\nFigure 11.11 The TensorBoard sidebar with Smoothing set to 0.6 and two runs selected for display\n\nbugs, didn\u2019t converge, or are so old they're no longer interesting. The number of runs\ncan grow pretty quickly, so it can be helpful to prune it often and to rename runs or\nmove runs that are particularly interesting to a more permanent directory so they\ndon\u2019t get deleted by accident. To remove both the train and validation runs, exe-\ncute the following (after changing the chapter, date, and time to match the run you\nwant to remove):\n\n$ rm -rf runs/p2ch11/2020-01-01_12.02.15_*\n\nKeep in mind that removing runs will cause the runs that are later in the list to move\nup, which will result in them being assigned new colors.\n\nOK, let\u2019s get to the point of TensorBoard: the pretty graphs! The main part of the\nscreen should be filled with data from gathering training and validation metrics, as\nshown in figure 11.12.\n\nShon daca download inks Q. Fiter tgs (reguar expremsons eupsoried\n\nTosttip serveg mettot deteutt bd 7 ate CORRECT... i O% co CT\n\ntemoctting ms\n\u2014\u2014\new os me 1\u201c\n\nee ee Ce ee ee)\n\nOMB an deems eee\n4 sera eente\n\n386\nAPPROXIMATELY HUGE\nZERO AND BAD\n\nFigure 11.12 The main TensorBoard data display area showing us that our results on actual nodules are\ndownright awful\n\nThat's much easier to parse and absorb than E1 trn_pos 924.34 loss, 0.2% correct\n(3 of 1215)! Although we\u2019re going to save discussion of what these graphs are telling\nus for section 11.10, now would be a good time to make sure it\u2019s clear what these num-\nbers correspond to from our training program. Take a moment to cross-reference the\n\nnumbers you get by mousing over the lines with the numbers spit out by training.py\nduring the same training run. You should see a direct correspondence between the\nValue column of the tooltip and the values printed during training. Once you're com-\nfortable and confident that you understand exactly what TensorBoard is showing you,\nlet\u2019s move on and discuss how to get these numbers to appear in the first place.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.21,
                        "section_name": "Adding TensorBoard support to the metrics logging function",
                        "section_path": "./screenshots-images-2/chapter_12/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_21/8fca35c5-fda6-45e4-8865-98db8187f92d.png",
                            "./screenshots-images-2/chapter_12/section_21/6e3edac2-2551-4306-a3da-e66447dd24a3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adding TensorBoard support to the metrics logging function\n\nWe are going to use the torch. utils.tensorboard module to write data in a format\nthat TensorBoard will consume. This will allow us to write metrics for this and any\nother project quickly and easily. TensorBoard supports a mix of NumPy arrays and\nPyTorch tensors, but since we don\u2019t have any reason to put our data into NumPy\narrays, we'll use PyTorch tensors exclusively.\n\nThe first thing we need do is to create our SummaryWriter objects (which we\nimported from torch.utils.tensorboard). The only parameter we're going to pass\nin is log_dir, which we will initialize to something like runs/p2ch11/2020-01-01_12\n-55.27-trn-dlwpt. We can add a comment argument to our training script to change\ndlwpt to something more informative; use python -m p2chll.training --help for\nmore information.\n\nWe create two writers, one each for the training and validation runs. Those writers\nwill be reused for every epoch. When the SummaryWriter class gets initialized, it also\ncreates the log_dir directories as a side effect. These directories show up in Tensor-\nBoard and can clutter the UI with empty runs if the training script crashes before any\ndata gets written, which can be common when you're experimenting with something.\nTo avoid writing too many empty junk runs, we wait to instantiate the SummaryWriter\nobjects until we're ready to write data for the first time. This function is called from\nlogMetrics().\n\nTensorboardwWri\n\nListing 11.21 training.py:127, .\n\ndef initTensorboardwWriters (self):\nif self.trn_writer is None:\nlog_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n\nself.trn_writer = SummaryWriter (\n\nlog_dir=log_dir + '-trn_cls-' + self.cli_args.comment)\nself.val_writer = SummaryWriter (\n\nlog_dir=log_dir + '-val_cls-' + self.cli_args.comment)\n\nIf you recall, the first epoch is kind of a mess, with the early output in the training\nloop being essentially random. When we save the metrics from that first batch, those\nrandom results end up skewing things a bit. Recall from figure 11.11 that Tensor-\nBoard has smoothing to remove noise from the trend lines, which helps somewhat.\nAnother approach could be to skip metrics entirely for the first epoch\u2019s training\ndata, although our model trains quickly enough that it\u2019s still useful to see the first\n\nepoch\u2019s results. Feel free to change this behavior as you see fit; the rest of part 2 will\ncontinue with this pattern of including the first, noisy training epoch.\n\nTIP If you end up doing a lot of experiments that result in exceptions or kill-\ning the training script relatively quickly, you might be left with a number of\njunk runs cluttering up your runs/ directory. Don\u2019t be afraid to clean those\nout!\n\nWRITING SCALARS TO TENSORBOARD\n\nWriting scalars is straightforward. We can take the metrics_dict we've already con-\nstructed and pass in each key/value pair to the writer.add_scalar method. The\ntorch.utils.tensorboard.SummaryWriter class has the add_scalar method (http://\nmng.bz/RAqj) with the following signature.\n\nListing 11.22 PyTorch torch/utils/tensorboard/writer.py:267\n\ndef add_scalar(self, tag, scalar_value, global_step=None, walltime=None) :\n\nThe tag parameter tells TensorBoard which graph we're adding values to, and the\nscalar_value parameter is our data point\u2019s Y-axis value. The global_step parameter\nacts as the X-axis value.\n\nRecall that we updated the totalTrainingSamples_count variable inside the\ndoTraining function. We'll use totalTrainingSamples_count as the X-axis of our\nTensorBoard plots by passing it in as the global_step parameter. Here\u2019s what that\nlooks like in our code.\n\nListing 11.23 training.py:323, LunaTrainingApp.logMetrics\n\nfor key, value in metrics_dict.items():\nwriter.add_scalar(key, value, self.totalTrainingSamples_count)\n\nNote that the slashes in our key names (such as 'loss/all') result in TensorBoard\ngrouping the charts by the substring before the '/'.\n\nThe documentation suggests that we should be passing in the epoch number as the\nglobal_step parameter, but that results in some complications. By using the number\nof training samples presented to the network, we can do things like change the number\nof samples per epoch and still be able to compare those future graphs to the ones we're\ncreating now. Saying that a model trains in half the number of epochs is meaningless if\neach epoch takes four times as long! Keep in mind that this might not be standard prac-\ntice, however; expect to see a variety of values used for the global step.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.22,
                        "section_name": "Why isn\u2019t the model learning to detect nodules?",
                        "section_path": "./screenshots-images-2/chapter_12/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_22/aca742e2-48b7-4587-9cf2-86576053daa7.png",
                            "./screenshots-images-2/chapter_12/section_22/0d4b3438-be72-4589-8a6c-c2aeaf8b36b3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Why isn\u2019t the model learning to detect nodules?\n\nOur model is clearly learning something\u2014the loss trend lines are consistent as epochs\nincrease, and the results are repeatable. There is a disconnect, however, between what\nthe model is learning and what we want it to learn. What\u2019s going on? Let\u2019s use a quick\nmetaphor to illustrate the problem.\n\nImagine that a professor gives students a final exam consisting of 100 True/False\nquestions. The students have access to previous versions of this professor's tests going\nback 30 years, and every time there are only one or two questions with a True answer.\nThe other 98 or 99 are False, every time.\n\nAssuming that the grades aren't on a curve and instead have a typical scale of 90%\ncorrect or better being an A, and so on, it is trivial to get an A+: just mark every ques-\ntion as False! Let\u2019s imagine that this year, there is only one True answer. A student like\nthe one on the left in figure 11.13 who mindlessly marked every answer as False would\nget a 99% on the final but wouldn\u2019t really demonstrate that they had learned anything\n(beyond how to cram from old tests, of course). That\u2019s basically what our model is\ndoing right now.\n\nFigure 11.13 A professor\ngiving two students the same\ngrade, despite different levels\nof knowledge. Question 9 is\nthe only question with an\nanswer of True.\n\nContrast that with a student like the one on the right who also got 99% of the ques-\ntions correct, but did so by answering two questions with True. Intuition tells us that\nthe student on the right in figure 11.13 probably has a much better grasp of the mate-\nrial than the all-False student. Finding the one True question while only getting one\nanswer wrong is pretty difficult! Unfortunately, neither our students\u2019 grades nor our\nmodel\u2019s grading scheme reflect this gut feeling.\n\nWe have a similar situation, where 99.7% of the answers to \u201cIs this candidate a nod-\nule?\u201d are \u201cNope.\u201d Our model is taking the easy way out and answering False on every\nquestion.\n\nStill, if we look back at our model\u2019s numbers more closely, the loss on the training\nand validation sets is decreasing! The fact that we\u2019re getting any traction at all on the\ncancer-detection problem should give us hope. It will be the work of the next chapter\nto realize this potential. We'll start chapter 12 by introducing some new, relevant\n\nterminology, and then we'll come up with a better grading scheme that doesn\u2019t lend\nitself to being gamed quite as easily as what we've done so far.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.23,
                        "section_name": ". Conclusion",
                        "section_path": "./screenshots-images-2/chapter_12/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_23/43f30f40-6ec3-42f0-8270-705fd93bb1e0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\n\nWe've come a long way this chapter\u2014we now have a model and a training loop, and\nare able to consume the data we produced in the last chapter. Our metrics are being\nlogged to the console as well as graphed visually.\n\nWhile our results aren't usable yet, we're actually closer than it might seem. In\nchapter 12, we will improve the metrics we're using to track our progress, and use\nthem to inform the changes we need to make to get our model producing reasonable\nresults.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 13,
                "chapter_name": "Improving training\nwith metrics and\naugmentation",
                "chapter_path": "./screenshots-images-2/chapter_13",
                "sections": [
                    {
                        "section_id": 13.1,
                        "section_name": "Improving training\nwith metrics and\naugmentation",
                        "section_path": "./screenshots-images-2/chapter_13/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_1/853306bf-7c09-4551-9e31-20799ccd5626.png",
                            "./screenshots-images-2/chapter_13/section_1/5a4b74da-b29a-4ea2-96dd-cd578e1f2a1b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The close of the last chapter left us in a predicament. While we were able to get the\nmechanics of our deep learning project in place, none of the results were actually\nuseful; the network simply classified everything as non-nodule! To make matters\nworse, the results seemed great on the surface, since we were looking at the overall\npercent of the training and validation sets that were classified correctly. With our\ndata heavily skewed toward negative samples, blindly calling everything negative is a\n\nquick and easy way for our model to score well. Too bad doing so makes the model\nbasically useless!\n\nThat means we\u2019re still focused on the same part of figure 12.1 as we were in chap-\nter 11. But now we're working on getting our classification model working well instead\nof at all. This chapter is all about how to measure, quantify, express, and then improve\non how well our model is doing its job.\n\nSTEP 4 (CH. WZ):\nCLASSIFICATION\n\nFigure 12.1 Our end-to-end lung cancer detection project, with a focus on this chapter's topic:\nstep 4, classification\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.2,
                        "section_name": "High-level plan for improvement",
                        "section_path": "./screenshots-images-2/chapter_13/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_2/7bfe44f3-904e-4dd4-ab0f-7fa33b2650df.png",
                            "./screenshots-images-2/chapter_13/section_2/b9bbc2f0-b675-4430-8650-781971516f13.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "High-level plan for improvement\nWhile a bit abstract, figure 12.2 shows us how we are going to approach that broad set\nof topics.\n\nLet\u2019s walk through this somewhat abstract map of the chapter in detail. We will be\ndealing with the issues we're facing, like excessive focus on a single, narrow metric and\nthe resulting behavior being useless in the general sense. In order to make some of this\nchapter's concepts a bit more concrete, we'll first employ a metaphor that puts our trou-\nbles in more tangible terms: in figure 12.2, (1) Guard Dogs and (2) Birds and Burglars.\n\nAfter that, we will develop a graphical language to represent some of the core con-\ncepts needed to formally discuss the issues with the implementation from the last\nchapter: (3) Ratios: Recall and Precision. Once we have those concepts solidified,\nwe'll touch on some math using those concepts that will encapsulate a more robust\nway of grading our model's performance and condensing it into a single number: (4)\nNew Mewic: F1 Score. We will implement the formula for those new metrics and look\n\n(. RATIOS RECALL\nAND PRECISION\n\n4. NEW METRIC!\nFi SCORE\n\nFigure 12.2 The metaphors we'll use to modify the metrics measuring our model to make it\nmagnificent\n\nat the how the resulting values change epoch by epoch during training. Finally, we'll\nmake some much-needed changes to our LunaDataset implementation with an aim at\nimproving our training results: (5) Balancing and (6) Augmentation. Then we will see\nif those experimental changes have the expected impact on our performance metrics.\n\nBy the time we\u2019re through with this chapter, our trained model will be performing\nmuch better: (7) Workin\u2019 Great! While it won\u2019t be ready to drop into clinical use just\nyet, it will be capable of producing results that are clearly better than random. This\nwill mean we have a workable implementation of step 4, nodule candidate classifica-\ntion; and once we\u2019re finished, we can begin to think about how to incorporate steps 2\n(segmentation) and 3 (grouping) into the project.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.3,
                        "section_name": "Good dogs vs. bad guys: False positives and false negatives",
                        "section_path": "./screenshots-images-2/chapter_13/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_3/f6975913-50a8-4dd2-bd76-ce79ad156c25.png",
                            "./screenshots-images-2/chapter_13/section_3/df320274-8c95-43ec-a9ad-e43e860c6a7a.png",
                            "./screenshots-images-2/chapter_13/section_3/4ed9192b-cb58-4ab4-947c-fd1064bd5c69.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Good dogs vs. bad guys: False positives and false negatives\n\nInstead of models and tumors, we're going to consider the two guard dogs in figure\n12.3, both fresh out of obedience school. They both want to alert us to burglars\u2014a\nrare but serious situation that requires prompt attention.\n\nUnfortunately, while both dogs are good dogs, neither is a good guard dog. Our\nterrier (Roxie) barks at just about everything, while our old hound dog (Preston)\nbarks almost exclusively at burglars\u2014but only if he happens to be awake when they\narrive.\n\nFigure 12.3 The set of topics for this chapter, with a focus on the framing metaphor\n\nRoxie will alert us to a burglar just about every time. She will also alert us to fire\nengines, thunderstorms, helicopters, birds, the mail carrier, squirrels, passersby, and\nso on. If we follow up on every bark, we'll almost never get robbed (only the sneakiest\nof sneak-thieves can slip past). Perfect! ... Except that being that diligent means we\naren't really saving any work by having a guard dog. Instead, we'll be up every couple\nof hours, flashlight in hand, due to Roxie having smelled a cat, or heard an owl, or\nseen a late bus wander by. Roxie has a problematic number of false positives.\n\nA false positive is an event that is classified as of interest or as a member of the\ndesired class (positive as in \u201cYes, that\u2019s the type of thing I\u2019m interested in knowing\nabout\u201d) but that in truth is not really of interest. For the nodule-detection problem, it\u2019s\nwhen an actually uninteresting candidate is flagged as a nodule and, hence, in need of\na radiologist\u2019s attention. For Roxie, these would be fire engines, thunderstorms, and\nso on. We will use an image of a cat as the canonical false positive in the next section\nand the figures that follow throughout the rest of the chapter.\n\nContrast false positives with true positives: items of interest that are classified cor-\nrectly. These will be represented in the figures by a human burglar.\n\nMeanwhile, if Preston barks, call the police, since that means someone has almost\ncertainly broken in, the house is on fire, or Godzilla is attacking. Preston is a deep\nsleeper, however, and the sound of an in-progress home invasion isn\u2019t likely to rouse\nhim, so we'll still get robbed just about every time someone tries. Again, while it's bet-\nter than nothing, we're not really ending up with the peace of mind that motivated us\nto get a dog in the first place. Preston has a problematic number of false negatives.\n\nA false negative is an event that is classified as not of interest or nota member of the\ndesired class (negative as in \u201cNo, that\u2019s not the type of thing I\u2019m interested in knowing\nabout\u201d) but that in truth is actually of interest. For the nodule-detection problem, it\u2019s\nwhen a nodule (that is, a potential cancer) goes undetected. For Preston, these would\nbe the robberies that he sleeps through. We'll get a bit creative here and use a picture\nof a rodent burglar for false negatives. They\u2019re sneaky!\n\nConwast false negatives with true negatives: uninteresting items that are correctly\nidentified as such. We'll go with a picture of a bird for these.\n\nJust to complete the metaphor, chapter 11's model is basically a cat that refuses to\nmeow at anything that isn\u2019t a can of tuna (while stoically ignoring Roxie). Our focus at\nthe end of the last chapter was on the percent correct for the overall training and vali-\ndation sets. Clearly, that wasn\u2019t a great way to grade ourselves, and as we can see from\neach of our dogs\u2019 myopic focus on a single metric\u2014like the number of true positives or\ntrue negatives\u2014we need a metric with a broader focus to capture our overall perfor-\nmance.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.4,
                        "section_name": "Graphing the positives and negatives",
                        "section_path": "./screenshots-images-2/chapter_13/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_4/23e36981-d466-4833-a7da-b462cf137e88.png",
                            "./screenshots-images-2/chapter_13/section_4/dce4e6ef-0a54-496b-a167-5ecf97a9935f.png",
                            "./screenshots-images-2/chapter_13/section_4/9d777d77-1e40-4295-9312-3e7b65ccdcd8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Graphing the positives and negatives\n\nLet's start developing the visual language we'll use to describe true/false positives/\nnegatives. Please bear with us if our explanation gets repetitive; we want to make sure\nyou develop a solid mental model for the ratios we're going to discuss. Consider figure\n12.4, which shows events that might be of interest to one of our guard dogs.\n\nDOG PREDICTION FALSE\nTRUE POSITIVE\n\nrecent | THRESHOLD\n\neo\n\nCLASSIFICATION\nTHRESHOLD\n\nCl\n\nIGNORE BARK\n\nFigure 12.4 Cats, birds, rodents, and robbers make up our four classification\nquadrants. They are separated by a human label and the dog classification threshold.\n\nWe'll use two thresholds in figure 12.4. The first is the human-decided dividing line that\nseparates burglars from harmless animals. In concrete terms, this is the label thatis given\nfor each training or validation sample. The second is the dog-determined classification\nthreshold that determines whether the dog will bark at something. For a deep learning\nmodel, this is the predicted value that the model produces when considering a sample.\n\nThe combination of these two thresholds divides our events into quadrants:\ntrue/false positives/ negatives. We will shade the events of concern with a darker back-\nground (what with those bad guys sneaking around in the dark all the time).\n\nOf course, reality is far more complicated. There is no Platonic ideal of a burglar,\nand no single point relative to the classification threshold at which all burglars will be\nlocated. Instead, figure 12.5 shows us that some burglars will be particularly sneaky,\nand some birds will be particularly annoying. We will also go ahead and enclose our\ninstances in a graph. Our X-axis will remain the bark-worthiness of each event, as\ndetermined by one of our guard dogs. We're going to have the Y-axis represent some\nvague set of qualities that we as humans are able to perceive, but our dogs cannot.\n\nSince our model produces a binary classification, we can think of the prediction\nthreshold as comparing a single-numerical-value output to our classification threshold\nvalue. This is why we will require that the classification threshold line to be perfectly\nvertical in figure 12.5.\n\nEach possible burglar is different, so our guard dogs will need to evaluate many dif-\nferent situations, and that means more opportunities to make mistakes. We can see\nthe clear diagonal line that separates the birds from the burglars, but Preston and\nRoxie can only perceive the X-axis here: they have a muddled, overlapped set of\n\nBAD\nIGNORE GUYS\n\nFigure 12.5 Each type of event will have many possible instances that our guard\ndogs will need to evaluate.\n\nevents in the middle of our graph. They must pick a vertical bark-worthiness thresh-\nold, which means it\u2019s impossible for either one of them to do so perfectly. Sometimes\nthe person hauling your appliances to their van is the repair person you hired to fix\nyour washing machine, and sometimes burglars show up in a van that says \u201cWashing\nMachine Repair\u201d on the side. Expecting a dog to pick up on those nuances is bound\nto fail.\n\nThe actual input data we\u2019re going to use has high dimensionality\u2014we need to con-\nsider a ton of CT voxel values, along with more abstract things like candidate size,\noverall location in the lungs, and so on. The job of our model is to map each of these\nevents and respective properties into this rectangle in such a way that we can separate\nthose positive and negative events cleanly using a single vertical line (our classification\nthreshold). This is done by the nn.Linear layers at the end of our model. The posi-\ntion of the vertical line corresponds exactly to the classificationThreshold_float\nwe saw in section 11.6.1. There, we chose the hardcoded value 0.5 as our threshold.\n\nNote that in reality, the data presented is not two-dimensional; it goes from very-high-\ndimensional after the second-to-last layer, to one-dimensional (here, our X-axis) at the\noutput\u2014just a single scalar per sample (which is then bisected by the classification\nthreshold). Here, we use the second dimension (the Y-axis) to represent per-sample\nfeatures that our model cannot see or use: things like age or gender of the patient,\nlocation of the nodule candidate in the lung, or even local aspects of the candidate that\nthe model hasn't utilized. It also gives us a convenient way to represent confusion\nbetween non-nodule and nodule samples.\n\nThe quadrant areas in figure 12.5 and the count of samples contained in each will\nbe the values we use to discuss model performance, since we can use the ratios between\nthese values to construct increasingly complex metrics that we can use to objectively\nmeasure how well we are doing. As they say, \u201cthe proof is in the proportions.\u201d! Next,\nwe'll use ratios between these event subsets to start defining better metrics.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.5,
                        "section_name": "Recall is Roxie\u2019s strength",
                        "section_path": "./screenshots-images-2/chapter_13/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_5/98f3f9c0-f142-408d-b25d-60edc8f283e9.png",
                            "./screenshots-images-2/chapter_13/section_5/8ab90c36-39c8-4fc5-b58f-9f1a6fd4f400.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Recall is Roxie\u2019s strength\n\nRecall is basically \u201cMake sure you never miss any interesting events!\u201d Formally, recall is\nthe ratio of the true positives to the union of true positives and false negatives. We can\nsee this depicted in figure 12.6.\n\nNOTE Insome contexts, recall is referred to as sensitivity.\n\nTo improve recall, minimize false negatives. In guard dog terms, that means if you're\nunsure, bark at it, just in case. Don\u2019t let any rodent thieves sneak by on your watch!\n\nRoxie accomplishes having an incredibly high recall by pushing her classification\nthreshold all the way to the left, such that it encompasses nearly all of the positive\nevents in figure 12.7. Note how doing so means her recall value is near 1.0, which\nmeans 99% of robbers are barked at. Since that\u2019s how Roxie defines success, in her\nmind, she\u2019s doing a great job. Never mind the huge expanse of false positives!\n\nRECALL\n\nIS THE RATIO DETERMINED\nBY FALSE NEGATIVES\n\nNS.\n\nFigure 12.6 Recall is the ratio of the true positives to the union of true positives\nand false negatives. High recall minimizes false negatives.\n\n(ea ROXIE BARKS AT EVERYTHING\nCo\n\nLOW BARK THRESHOLD, HIGH RECALL\n\nFigure 12.7 Roxie's choice of threshold prioritizes minimizing false\nnegatives. Every last rat is barked at . . . and cats, and most birds.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.6,
                        "section_name": "Precision is Preston's forte",
                        "section_path": "./screenshots-images-2/chapter_13/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_6/2a652b02-1ea5-4af4-997d-cb1cbac5711d.png",
                            "./screenshots-images-2/chapter_13/section_6/6dc2d26c-5c44-4dd5-92fa-b014d3518c3b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "_ Precision is Preston's forte\n\nPrecision is basically \u201cNever bark unless you're sure.\u201d To improve precision, minimize\nfalse positives. Preston won't bark at something unless he\u2019s certain it\u2019s a burglar. More\nformally, precision is the ratio of the true positives to the union of true positives and\nfalse positives, as shown in figure 12.8.\n\nPRECISION\n\n\\S THE RATIO DETERMINED\nBY FALSE POSITIVES\n\nV8.4\n\nFigure 12.8 Precision is the ratio of the true positives to the union of true positives\nand false positives. High precision minimizes false positives.\n\nPreston accomplishes having an incredibly high precision by pushing his classification\nthreshold all the way to the right, such that it excludes as many uninteresting, negative\nevents as he can manage (see figure 12.9). This is the opposite of Roxie\u2019s approach\nand means Preston has a precision of nearly 1.0: 99% of the things he barks at are rob-\nbers. This also matches his definition of being a good guard dog, even though a large\nnumber of events pass undetected.\n\nWhile neither precision nor recall can be the single metric used to grade our\nmodel, they are both useful numbers to have on hand during training. Let\u2019s calculate\nand display these as part of our training program, and then we'll discuss other metrics\nwe can employ.\n\nHIGH BARK THRESHOLD, HIGH PRECISION\n\nPRESTON MOSTLY SLEEPS\n\nFigure 12.9 Preston's choice of threshold prioritizes minimizing false\npositives. Cats get left alone; only burglars are barked at!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.7,
                        "section_name": "Implementing precision and recall in logMetrics",
                        "section_path": "./screenshots-images-2/chapter_13/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_7/f1a14683-e610-4334-b3b9-a985fbaf8032.png",
                            "./screenshots-images-2/chapter_13/section_7/694e6373-91c6-40c6-ae31-2e15ee20eca3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "| Implementing precision and recall in logMetrics\n\nBoth precision and recall are valuable metrics to be able to track during training,\nsince they provide important insight into how the model is behaving. If either of them\ndrops to zero (as we saw in chapter 11!), it\u2019s likely that our model has started to\nbehave in a degenerate manner. We can use the exact details of the behavior to guide\nwhere to investigate and experiment with getting training back on track. We'd like to\nupdate the logMetrics function to add precision and recall to the output we see for\neach epoch, to complement the loss and correctness metrics we already have.\n\nWe've been defining precision and recall in terms of \u201ctrue positives\u201d and the like\nthus far, so we will continue to do so in the code. It turns out that we are already com-\nputing some of the values we need, though we had named them differently.\n\ning 12.1 training.py:315, LunaTrainingApp.logMetrics\n\nneg_count = int (negLabel_mask.sum())\npos_count = int (posLabel_mask.sum())\n\ntrueNeg_count = neg_correct = int((negLabel_mask & negPred_mask) .sum())\ntruePos_count = pos_correct = int((posLabel_mask & posPred_mask) .sum())\n\nfalsePos_count\nfalseNeg_count\n\nneg_count - neg_correct\npos_count - pos_correct\n\nHere, we can see that neg_correct is the same thing as trueNeg_count! That actually\nmakes sense, since non-nodule is our \u201cnegative\u201d value (as in \u201ca negative diagnosis\u201d),\nand if the classifier gets the prediction correct, then that\u2019s a true negative. Similarly,\ncorrectly labeled nodule samples are true positives.\n\nWe do need to add the variables for our false positive and false negative values.\nThat\u2019s straightforward, since we can take the total number of benign labels and subtract\nthe count of the correct ones. What's left is the count of non-nodule samples misclassi-\nfied as positive. Hence, they are false positives. Again, the false negative calculation is of\nthe same form, but uses nodule counts.\n\nWith those values, we can compute precisionand recall and store them in metrics\n\ndict.\n\nGApp.logMetrics\nprecision = metrics_dict['pr/precision'] = \\\ntruePos_count / np.float32(truePos_count + falsePos_count)\nrecall = metrics_dict['pr/recall'] = \\\n\ntruePos_count / np.float32(truePos_count + falseNeg_count)\n\nNote the double assignment: while having separate precision and recall variables\nisn\u2019t strictly necessary, they improve the readability of the next section. We also extend\nthe logging statement in logMetrics to include the new values, but we skip the imple-\nmentation for now (we'll revisit logging later in the chapter).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.8,
                        "section_name": "Our ultimate performance metric: The F1 score",
                        "section_path": "./screenshots-images-2/chapter_13/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_8/b22cbcb0-b365-4c3c-b00b-7689d67908ff.png",
                            "./screenshots-images-2/chapter_13/section_8/39de826b-934f-4b36-8b01-62365bb991a4.png",
                            "./screenshots-images-2/chapter_13/section_8/f39f0d4a-53f2-4e0d-a29f-5e2799c45f9a.png",
                            "./screenshots-images-2/chapter_13/section_8/f7348b85-d7bd-42db-8b49-79e9bc300935.png",
                            "./screenshots-images-2/chapter_13/section_8/ac798588-5bb1-4f11-b398-85233bb2aaac.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Our ultimate performance metric: The F1 score\n\nWhile useful, neither precision nor recall entirely captures what we need in order to be\nable to evaluate a model. As we've seen with Roxie and Preston, it\u2019s possible to game\neither one individually by manipulating our classification threshold, resulting in a\nmodel that scores well on one or the other but does so at the expense of any real-world\nutility. We need something that combines both of those values in a way that prevents such\ngamesmanship. As we can see in figure 12.10, it\u2019s time to introduce our ultimate metric.\n\nThe generally accepted way of combining precision and recall is by using the F1\nscore (https://en.wikipedia.org/wiki/F1l_score). As with other metrics, the F1 score\nranges between 0 (a classifier with no real-world predictive power) and 1 (a classifier\nthat has perfect predictions). We will update logMetrics to include this as well.\n\nListing 12.3 training.py:338, LunaTrainingApp.logMetrics\n\nmetrics_dict['pr/fl_score'] = \\\n2 * (precision * recall) / (precision + recall)\n\nAt first glance, this might seem more complicated than we need, and it might not be\nimmediately obvious how the F1 score behaves when trading off precision for recall or\n\nFigure 12.10 The set of topics for this chapter, with a focus on the final F1 score metric\n\nvice versa. This formula has a lot of nice properties, however, and it compares favor-\nably to several other, simpler alternatives that we might consider.\n\nOne immediate possibility for a scoring function is to average the values for precision\nand recall together. Unfortunately, this gives both avg(p=1.0, r=0.0) and avg(p=0.5,\nr=0.5) the same score of 0.5, and as we discussed earlier, a classifier with either precision\nor recall of zero is usually worthless. Giving something useless the same nonzero score\nas something useful disqualifies averaging as a meaningful metric immediately.\n\nStill, let\u2019s visually compare averaging and F1 in figure 12.11. A few things stand out.\nFirst, we can see a lack of a curve or elbow in the contour lines for averaging. That\u2019s\nwhat lets our precision or recall skew to one side or the other! There will never be a sit-\nuation where it doesn\u2019t make sense to maximize the score by having 100% recall (the\nRoxie approach) and then eliminate whichever false positives are easy to eliminate.\nThat puts a floor on the addition score of 0.5 right out of the gate! Having a quality\nmetric that is trivial to score at least 50% on doesn\u2019t feel right.\n\nNOTE What we are actually doing here is taking the arithmetic mean\n(https://en.wikipedia.org/wiki/Arithmetic_mean) of the precision and\nrecall, both of which are rates rather than countable scalar values. Taking the\narithmetic mean of rates doesn\u2019t typically give meaningful results. The Fl\nscore is another name for the harmonic mean (https://en.wikipedia.org/wiki/\nHarmonic_mean) of the two rates, which is a more appropriate way of com-\nbining those kinds of values.\n\nANG(P, R)\n\n4o 60 4o Go\nPRECISION PRECISION\n\nFigure 12.11 Computing the final score with avg(p, 1). Lighter values are closer to 1.0.\n\nContrast that with the F1 score: when recall is high but precision is low, trading off a\nlot of recall for even a little precision will move the score closer to that balanced sweet\nspot. There\u2019s a nice, deep elbow that is easy to slide into. That encouragement to have\nbalanced precision and recall is what we want from our grading metric.\n\nLet\u2019s say we still want a simpler metric, but one that doesn\u2019t reward skew at all. In\norder to correct for the weakness of addition, we might take the minimum of preci-\nsion and recall (figure 12.12).\n\nMIN(P, R) FICP, R)\n80- 80\n0 - 0\n4o- a 4o\n20 - 20\no+ \u00b0\n\u00b0 20 4o 60 80 \u00b0 20 4o Go 80\nPRECISION PRECISION\n\nFigure 12.12 Computing the final score with min(p, r)\n\nThis is nice, because if either value is 0, the score is also 0, and the only way to get a\nscore of 1.0 is to have both values be 1.0. However, it still leaves something to be\ndesired, since making a model change that increased the recall from 0.7 to 0.9 while\nleaving precision constant at 0.5 wouldn\u2019t improve the score at all, nor would drop-\nping recall down to 0.6! Although this metric is certainly penalizing having an imbal-\nance between precision and recall, it isn\u2019t capturing a lot of nuance about the two\nvalues. As we have seen, it\u2019s easy to trade one off for the other simply by moving the\nclassification threshold. We'd like our metric to reflect those trades.\n\nWe'll have to acceptat least a bit more complexity to better meet our goals. We could\nmultiply the two values together, as in figure 12.13. This approach keeps the nice prop-\nerty that if either value is 0, the score is 0, and a score of 1.0 means both inputs are per-\nfect. It also favors a balanced trade-off between precision and recall at low values,\nthough when it gets closer to perfect results, it becomes more linear. That\u2019s not great,\nsince we really need to push both up to have a meaningful improvement at that point.\n\nMULT(P, R) FICP, R)\n\nL\n\nRECALL\n\n\u00b0 20 4o Go 80\n\n4o Go\nPRECISION PRECISION\n\nFigure 12.13 Computing the final score with mult (p, xr)\n\nNOTE Here we're taking the geometric mean (https:/ /en.wikipedia.org/wiki/\nGeometric_mean) of two rates, which also doesn\u2019t produce meaningful\nresults.\n\nThere\u2019s also the issue of having almost the entire quadrant from (0, 0) to (0.5, 0.5) be\nvery close to zero. As we'll see, having a metric that\u2019s sensitive to changes in that\nregion is important, especially in the early stages of our model design.\n\nWhile using multiplication as our scoring function is feasible (it doesn\u2019t have any\nimmediate disqualifications the way the previous scoring functions did), we will be\nusing the F1 score to evaluate our classification model's performance going forward.\n\nUPDATING THE LOGGING OUTPUT TO INCLUDE PRECISION, RECALL, AND F1 SCORE\n\nNow that we have our new metrics, adding them to our logging output is pretty\nstraightforward. We'll include precision, recall, and F1 in our main logging statement\nfor each of our training and validation sets.\n\nListing 12.4 training.py:341, LunaTrainingApp.logMetrics\n\nlog. info(\n(\"E{} (:8} (loss/all:.4f\u00a3} loss, *\n+ \"{correct/all:-5.1\u00a3}% correct, *\n+ \"{pr/precision:.4f\u00a3} precision, *\n\n+ \"{pr/recall:.4\u00a3} recall, * Format string\n+ \"{(pr/fl_score:.4f) \u00a31 score* updated\n) . format (\nepoch_ndx,\nmode_str,\n\n**metrics_dict,\n\n)\n\nIn addition, we'll include exact values for the count of correctly identified and the total\nnumber of samples for each of the negative and positive samples.\n\nListing 12.5 training.py:353, LunaTrainingApp.logMetrics\n\nlog.info(\n(\"E{} (:8} (loss/neg:.4f} loss, *\n+ \"{correct/neg:-5.1\u00a3}% correct ({neg_correct:} of {neg_count:})*\n) . format (\nepoch_ndx,\nmode_str + '_neg',\nneg_correct=neg_correct,\nneg_count=neg_count,\n**metrics_dict,\n\nThe new version of the positive logging statement looks much the same.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.9,
                        "section_name": "How does our model perform with our new metrics?",
                        "section_path": "./screenshots-images-2/chapter_13/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_9/18fc8f71-257c-4acf-81c6-8fa12412a0bd.png",
                            "./screenshots-images-2/chapter_13/section_9/09071c11-770b-4c95-a48a-341dc6a093ee.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "How does our model perform with our new metrics?\n\nNow that we\u2019ve implemented our shiny new metrics, let\u2019s take them for a spin; we'll\ndiscuss the results after we show the results of the Bash shell session. You might want\nto read ahead while your system does its number crunching; this could take perhaps\nhalf an hour, depending on your system.* Exactly how long it takes will depend on\nyour system\u2019s CPU, GPU, and disk speeds; our system with an SSD and GTX 1080 Ti\ntook about 20 minutes per full epoch:\n\n$ ../.venv/bin/python -m p2ch12.training\n\nStarting LunaTrainingApp... The count and\n\nEl LunaTrainingApp au eVemning Unce might\n\n.../p2ch12/training.py:274: RuntimeWarning: be different from run to run.\n\nw invalid value encountered in double_scalars\nmetrics_dict['pr/fl_score'] = 2 * (precision * recall) /\n\u2122 (precision + recall) ow \u2014\u2014\u2014I\n\nEl trn 0.0025 loss, 99.8% correct, 0.0000 pre, 0.0000 rel, nan f1\nEl trn_ben 0.0000 loss, 100.0% correct (494735 of 494743)\nEl trn_mal 1.0000 loss, 0.0% correct (0 of 1215)\n\n.../p2ch12/training.py:269: RuntimeWarning:\n\u2122 invalid value encountered in long_scalars\n\nprecision = metrics_dict['pr/precision'] = truePos_count /\nw (truePos_count + falsePos_count)\n\nEl val 0.0025 loss, 99.8% correct, nan pre, 0.0000 rel, nan fi\nEl val_ben 0.0000 loss, 100.0% correct (54971 of 54971)\nEl val_mal 1.0000 loss, 0.0% correct (0 of 136)\n\nBummer. We've got some warnings, and given that some of the values we computed\nwere nan, there\u2019s probably a division by zero happening somewhere. Let\u2019s see what we\ncan figure out.\n\nFirst, since none of the positive samples in the training set are getting classified as\npositive, that means both precision and recall are zero, which results in our F1 score\ncalculation dividing by zero. Second, for our validation set, truePos_count and\nfalsePos_count are both zero due to nothing being flagged as positive. It follows that\nthe denominator of our precision calculation is also zero; that makes sense, as that\u2019s\nwhere we\u2019re seeing another Runt imeWarning.\n\nA handful of negative training samples are classified as positive (494735 of 494743\nare classified as negative, so that leaves 8 samples misclassified). While that might\nseem odd at first, recall that we are collecting our training results throughout the epoch,\nrather than using the model\u2019s end-of-epoch state as we do for the validation results.\nThat means the first batch is literally producing random results. A few of the samples\nfrom that first batch being flagged as positive isn\u2019t surprising.\n\nNOTE Due to both the random initialization of the network weights and the\nrandom ordering of the training samples, individual runs will likely exhibit\nslightly different behavior. Having exactly reproducible behavior can be desir-\nable but is out of scope for what we\u2019re trying to do in part 2 of this book.\n\nWell, that was somewhat painful. Switching to our new metrics resulted in going from\nA+ to \u201cZero, if you\u2019re lucky\u201d\u2014and if we\u2019re not lucky, the score is so bad that it\u2019s not\neven a number. Ouch.\n\nThat said, in the long run, this is good for us. We\u2019ve known that our model\u2019s per-\nformance was garbage since chapter 11. If our metrics told us anything but that, it\nwould point to a fundamental flaw in the metrics!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.1,
                        "section_name": "What does an ideal dataset look like?",
                        "section_path": "./screenshots-images-2/chapter_13/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_10/2b60fa66-24fa-4739-81cd-815a828d40dd.png",
                            "./screenshots-images-2/chapter_13/section_10/6b34f509-3779-4201-84d7-356081cfb485.png",
                            "./screenshots-images-2/chapter_13/section_10/00cda57c-7d58-4ac0-a7f5-f85a2f2b3128.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What does an ideal dataset look like?\n\nBefore we start crying into our cups over the current sorry state of affairs, let\u2019s instead\nthink about what we actually want our model to do. Figure 12.14 says that first we need\nto balance our data so that our model can train properly. Let\u2019s build up the logical\nsteps needed to get us there.\n\nFigure 12.14 The set of topics for this chapter, with a focus on balancing our positive and\nnegative samples\n\nRecall figure 12.5 earlier, and the following discussion of classification thresholds.\nGetting better results by moving the threshold has limited effectiveness\u2014there\u2019s just\ntoo much overlap between the positive and negative classes to work with.\u201d\n\nInstead, we want to see an image like figure 12.15. Here, our label threshold is nearly\nvertical. That's what we want, because it means the label threshold and our classification\nthreshold can line up reasonably well. Similarly, most of the samples are concentrated at\neither end of the diagram. Both of these things require that our data be easily separable\nand that our model have the capacity to perform that separation. Our model currently\nhas enough capacity, so that\u2019s not the issue. Instead, let\u2019s take a look at our data.\n\nRecall that our data is wildly imbalanced. There\u2019s a 400:1 ratio of positive samples\nto negative ones. That\u2019s crushingly imbalanced! Figure 12.16 shows what that looks\nlike. No wonder our \u201cactually nodule\u201d samples are getting lost in the crowd!\n\nFEW INCORRECT\nPREDICTIONS\n\nFigure 12.15 A well-trained\nmodel can cleanly separate\n\nMOST EVENTS IN data, making it easy to pick a\nCLEARLY SEPARATE CLASSES \u2014<\u2014_\nNEGATIVE STILL Too\nSAMPLES MANY FALSE\n\nPOSITIVES\n\nPOSITIVE\nSAMPLES\n\nFigure 12.16 An imbalanced dataset that roughly approximates the imbalance in our LUNA classification data\n\nNow, let's be perfectly clear: when we\u2019re done, our model will be able to handle this\nkind of data imbalance just fine. We could probably even train the model all the way\nthere without changing the balancing, assuming we were willing to wait for a gajillion\nepochs first.* But we're busy people with things to do, so rather than cook our GPU\nuntil the heat death of the universe, let\u2019s try to make our training data look more ideal\nby changing the class balance we are training with.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.11,
                        "section_name": "Making the data look less like the actual and more like the \u201cideal\u201d",
                        "section_path": "./screenshots-images-2/chapter_13/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_11/dc89059a-2a94-485d-85c9-da71599e9bf6.png",
                            "./screenshots-images-2/chapter_13/section_11/67f5363f-2e0b-431c-92bd-f457740e641d.png",
                            "./screenshots-images-2/chapter_13/section_11/1f2ab6b0-2256-4e6f-a1e6-0c26f9826cbf.png",
                            "./screenshots-images-2/chapter_13/section_11/581e6713-eef8-4b48-af7a-04bb77039b2d.png",
                            "./screenshots-images-2/chapter_13/section_11/dc6b2295-5d0c-43eb-82f5-b8ce1a532a27.png",
                            "./screenshots-images-2/chapter_13/section_11/d6f6e971-8303-4839-a3b9-152f2727d60c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Making the data look less like the actual and more like the \u201cideal\u201d\n\nThe best thing to do would be to have relatively more positive samples. During the ini-\ntial epoch of training, when we're going from randomized chaos to something more\norganized, having so few training samples be positive means they get drowned out.\n\nThe method by which this happens is somewhat subtle, however. Recall that since\nour network weights are initially randomized, the per-sample output of the network is\nalso randomized (but clamped to the range [0-1}).\n\nNOTE Our loss function is nn.CrossEntropyLoss, which technically operates\non the raw logits rather than the class probabilities. For our discussion, we'll\nignore that distinction and assume the loss and the label-prediction deltas are\nthe same thing.\n\nThe predictions numerically close to the correct label do not result in much change\nto the weights of the network, while predictions that are significantly different from\nthe correct answer are responsible for a much greater change to the weights. Since\nthe output is random when the model is initialized with random weights, we can\nassume that of our ~500k training samples (495,958, to be exact), we'll have the fol-\nlowing approximate groups:\na 250,000 negative samples will be predicted to be negative (0.0 to 0.5) and result\nin at most a small change to the network weights toward predicting negative.\n2 250,000 negative samples will be predicted to be positive (0.5 to 1.0) and result\nin a large swing toward the network weights predicting negative.\ns 500 positive samples will be predicted to be negative and result in a swing\ntoward the network weights predicting positive.\n4 500 positive samples will be predicted to be positive and result in almost no\nchange to the network weights.\n\nNOTE Keep in mind that the actual predictions are real numbers between 0.0\nand 1.0 inclusive, so these groups won't have strict delineations.\n\nHere\u2019s the kicker, though: groups | and 4 can be any size, and they will continue to\nhave close to zero impact on training. The only thing that matters is that groups 2 and\n3 can counteract each other's pull enough to prevent the network from collapsing to a\ndegenerate \u201conly output one thing\u201d state. Since group 2 is 500 times larger than\n\ngroup 3 and we\u2019re using a batch size of 32, roughly 500/32 = 15 batches will go by\nbefore seeing a single positive sample. That implies that 14 out of 15 training batches\nwill be 100% negative and will only pull all model weights toward predicting negative.\nThat lopsided pull is what produces the degenerate behavior we've been seeing.\nInstead, we'd like to have just as many positive samples as negative ones. For the\nfirst part of training, then, half of both labels will be classified incorrectly, meaning\nthat groups 2 and 3 should be roughly equal in size. We also want to make sure we\npresent batches with a mix of negative and positive samples. Balance would result in\nthe tug-of-war evening out, and the mixture of classes per batch will give the model a\ndecent chance of learning to discriminate between the two classes. Since our LUNA\ndata has only a small, fixed number of positive samples, we'll have to settle for taking\nthe positive samples that we have and presenting them repeatedly during training.\n\nDiscrimination\n\nHere, we define discrimination as \u201cthe ability to separate two classes from each\nother.\u201d Building and training a model that can tell \u201cactually nodule\u201d candidates from\nnormal anatomical structures is the entire point of what we\u2019re doing in part 2.\n\nSome other definitions of discrimination are more problematic. While out of scope for\nthe discussion of our work here, there is a larger issue with models trained from real-\nworld data. If that real-world dataset is collected from sources that have a real-world-\ndiscriminatory bias (for example, racial bias in arrest and conviction rates, or anything\ncollected from social media), and that bias is not corrected for during dataset prepa-\nration or training, then the resulting model will continue to exhibit the same biases\npresent in the training data. Just as in humans, racism is learned.\n\nThis means almost any model trained from internet-at4arge data sources will be com-\npromised in some fashion, unless extreme care is taken to scrub those biases from\nthe model. Note that like our goal in part 2, this is considered an unsolved problem.\n\nRecall our professor from chapter 11 who had a final exam with 99 false answers and 1\ntrue answer. The next semester, after being told \u201cYou should have a more even bal-\nance of true and false answers,\u201d the professor decided to add a midterm with 99 true\nanswers and | false one. \u201cProblem solved!\u201d\n\nClearly, the correct approach is to intermix true and false answers in a way that\ndoesn\u2019t allow the students to exploit the larger structure of the tests to answer things\ncorrectly. Whereas a student would pick up on a pattern like \u201codd questions are true,\neven questions are false,\u201d the batching system used by PyTorch doesn\u2019t allow the\nmodel to \u201cnotice\u201d or utilize that kind of pattern. Our training dataset will need to be\nupdated to alternate between positive and negative samples, as in figure 12.17.\n\nThe unbalanced data is the proverbial needle in the haystack we mentioned at the\nstart of chapter 9. If you had to perform this classification work by hand, you'd proba-\nbly start to empathize with Preston.\n\nUNBALANCED BALANCED\n\nze\nEEE\n\nBATON: 3\n\nFigure 12.17 Batch after batch of imbalanced data will have nothing but negative events long before\nthe first positive event, while balanced data can alternate every other sample.\n\nWe will not be doing any balancing for validation, however. Our model needs to func-\ntion well in the real world, and the real world is imbalanced (after all, that\u2019s where we\ngot the raw data!).\n\nHow should we accomplish this balancing? Let's discuss our choices.\n\nSAMPLERS CAN RESHAPE DATASETS\nOne of the optional arguments to DataLoader is sampler=.. . This allows the data\nloader to override the iteration order native to the dataset passed in and instead\nshape, limit, or reemphasize the underlying data as desired. This can be incredibly\nuseful when working with a dataset that isn\u2019t under your control. Taking a public data-\nset and reshaping it to meet your needs is far less work than reimplementing that data-\nset from scratch.\n\nThe downside is that many of the mutations we could accomplish with samplers\nrequire that we break encapsulation of the underlying dataset. For example, let\u2019s\nassume we have a dataset like CIFAR-10 (www.cs.toronto.edu/~kriz/cifar.html) that\n\nconsists of 10 equally weighted classes, and we want to instead have 1 class (say, \u201cair-\nplane\u201d) now make up 50% of all of the training images. We could decide to use\nWeightedRandomSampler (http://mng.bz/8pIK) and weight each of the \u201cairplane\u201d\nsample indexes higher, but constructing the weights argument requires that we know\nin advance which indexes are airplanes.\n\nAs we discussed, the Dataset API only specifies that subclasses provide __len__\nand __getitem_, but there is nothing direct we can use to ask \u201cWhich samples are\nairplanes?\u201d We'd either have to load up every sample beforehand to inquire about the\nclass of that sample, or we\u2019d have to break encapsulation and hope the information\nwe need is easily obtained from looking at the internal implementation of the Data-\nset subclass.\n\nSince neither of those options is particularly ideal in cases where we have control\nover the dataset directly, the code for part 2 implements any needed data shaping\ninside the Dataset subclasses instead of relying on an external sampler.\n\nIMPLEMENTING CLASS BALANCING IN THE DATASET\nWe are going to directly change our LunaDataset to present a balanced, one-to-one\nratio of positive and negative samples for training. We will keep separate lists of nega-\ntive training samples and positive training samples, and alternate returning samples\nfrom each of those two lists. This will prevent the degenerate behavior of the model\nscoring well by simply answering \u201cfalse\u201d to every sample presented. In addition, the\npositive and negative classes will be intermixed so that the weight updates are forced\nto discriminate between the classes.\n\nLet\u2019s add a ratio_int to LunaDataset that will control the label for the Mh sam-\nple as well as keep track of our samples separated by label.\n\nListing 12.6 dsets.py:217, class LunaDatas\n\nclass LunaDataset (Dataset):\ndef __init (self,\nval_stride=-0,\nisValSet_bool=None,\nratio_int=0,\n\nself.ratio_int = ratio_int\n# ... line 228\nself.negative_list = [\nnt for nt in self.candidateInfo_list if not nt.isNodule_bool\n\nself.pos_list = [\nnt for nt in self.candidateInfo_list if nt.isNodule_bool\n\n]\n# ... line 265 We will call this at the top of each\n\nepoch to randomize the order of\ndef shuffleSamples (self) : Py samples being presented.\n\nif self.ratio_int:\nrandom. shuffle(self.negative_list)\nrandom. shuffle(self.pos_list)\n\nWith this, we now have dedicated lists for each label. Using these lists, it becomes\nmuch easier to return the label we want for a given index into the dataset. In order to\nmake sure we're getting the indexing right, we should sketch out the ordering we\nwant. Let\u2019s assume a ratio_int of 2, meaning a 2:1 ratio of negative to positive sam-\nples. That would mean every third index should be positive:\n\nDS Index 0123456789...\nLabel Si ce a J\nPos Index 0 1 2 3\nNeg Index ol 23 45\n\nThe relationship between the dataset index and the positive index is simple: divide\nthe dataset index by 3 and then round down. The negative index is slightly more com-\nplicated, in that we have to subtract | from the dataset index and then subtract the\nmost recent positive index as well.\n\nImplemented in our LunaDataset class, that looks like the following.\n\nListing 12.7 dsets.py:286, LunaDat\n\nA ratio_int of zero means\nuse the native balance.\n\ndef __getitem__(self, nd@x):\nif self.ratio_int:\n\npos_ndx = ndx // (self.ratio_int + 1) A nonzero remainder\nmeans this should be\n\nif ndx & (self.ratio_int + 1): + a negative sample.\nneg_ndx = ndx - 1 - pos_ndx\nneg_ndx t= len(self.negative_list) 4 1\ncandidateInfo_tup = self.negative_list (neg_ndx] | Overflow results\n\nelse: in wraparound.\npos_ndx t= len(self.pos_list) *t +\ncandidateInfo_tup = self.pos_list[pos_ndx)\n\nelse: Returns the Nth sample\n\ncandidateInfo_tup = self.candidateInfo_list (ndx] <q. lf not balancing classes\n\nThat can geta little hairy, but if you desk-check it out, it will make sense. Keep in mind\nthat with a low ratio, we'll run out of positive samples before exhausting the dataset.\nWe take care of that by taking the modulus of pos_ndx before indexing into\nself.pos_list. While the same kind of index overflow should never happen with\nneg_ndx due to the large number of negative samples, we do the modulus anyway, just\nin case we later decide to make a change that might cause it to overflow.\n\nWe'll also make a change to our dataset\u2019s length. Although this isn\u2019t strictly neces-\nsary, it\u2019s nice to speed up individual epochs. We're going to hardcode our __len__ to\nbe 200,000.\n\nListing 12.8 dsets.py:280, LunaDataset.__len _\n\ndef __len__ (self):\nif self.ratio_int:\nreturn 200000\nelse:\nreturn len(self.candidateInfo_list)\n\nWe're no longer tied to a specific number of samples, and presenting \u201ca full epoch\u201d\ndoesn\u2019t really make sense when we would have to repeat positive samples many, many\ntimes to present a balanced training set. By picking 200,000 samples, we reduce the\ntime between starting a training run and seeing results (faster feedback is always\nnice!), and we give ourselves a nice, clean number of samples per epoch. Feel free to\nadjust the length of an epoch to meet your needs.\n\nFor completeness, we also add a command-line parameter.\n\nListing 12.9 training.py:31, class LunaTrainingApp\n\nclass LunaTrainingApp:\ndef __init (self, sys_argv=None):\n\n# ... line 52\n\nparser .add_argument ('--balanced',\nhelp=\"Balance the training data to half positive, half negative.\",\naction='store_true',\ndefault=False,\n\n)\n\nThen we pass that parameter into the LunaDataset constructor.\n\nListing 12.10 training.py:137, LunaTrainingApp.initTrainDl\n\ndef initTrainDl (self):\ntrain_ds = LunaDataset (\nval_stride=10,\nisValset_bool=False, Here we rely on python\u2019s True\nratio_int=int (self.cli_args.balanced), being convertible to a 1.\n)\n\nWe're all set. Let\u2019s run it!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.12,
                        "section_name": "Contrasting training with a balanced LunaDataset to previous\nruns",
                        "section_path": "./screenshots-images-2/chapter_13/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_12/ad607be1-8950-4afd-b2f5-9b08f68bb2e3.png",
                            "./screenshots-images-2/chapter_13/section_12/05fcc010-9909-4936-928f-37ea92d49a4d.png",
                            "./screenshots-images-2/chapter_13/section_12/24507286-0e22-4881-a1ad-1def9f608d0d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Contrasting training with a balanced LunaDataset to previous\nruns\n\nAs a reminder, our unbalanced training run had results like these:\n\n$ python -m p2chl2.training\n\nEl\nEl\n=\n\nLunaTrainingApp\ntrn 0.0185 loss,\nnan \u00a31 score\n\n99.7% correct, 0.0000 precision,\n\n0.0000 recall,\n\nEl trn_neg 0.0026 loss, 100.0% correct (494717 of 494743)\nEl trn_pos 6.5267 loss, 0.0% correct (0 of 1215)\n\nEl val 0.0173 loss, 99.8% correct, nan precision, 0.0000 recall,\n> nan f1 score\n\nEl val_neg 0.0026 loss, 100.0% correct (54971 of 54971)\n\nEl val_pos 5.9577 loss, 0.0% correct (0 of 136)\n\nBut when we run with --balanced, we see the following:\n\n$ python -m p2chl2.training --balanced\n\nEl LunaTrainingApp\n\nEl trn 0.1734 loss, 92.8% correct, 0.9363 precision, 0.9194 recall,\n0.9277 \u00a31 score\n\nEl trn_neg 0.1770 loss, 93.7% correct (93741 of 100000)\n\nEl trn_pos 0.1698 loss, 91.9% correct (91939 of 100000)\n\nEl val 0.0564 loss, 98.4% correct, 0.1102 precision, 0.7941 recall,\n0.1935 \u00a31 score\n\nEl val_neg 0.0542 loss, 98.4% correct (54099 of 54971)\n\nEl val_pos 0.9549 loss, 79.4% correct (108 of 136)\n\nThis seems much better! We've given up about 5% correct answers on the negative\nsamples to gain 86% correct positive answers. We're back into a solid B range again!\u201d\n\nAs in chapter 11, however, this result is deceptive. Since there are 400 times as\nmany negative samples as positive ones, even getting just 1% wrong means we'd be\nincorrectly classifying negative samples as positive four times more often than there\nare actually positive samples in total!\n\nStill, this is clearly better than the outright wrong behavior from chapter 11 and\nmuch better than a random coin flip. In fact, we\u2019ve even crossed over into being\n(almost) legitimately useful in real-world scenarios. Recall our overworked radiologist\nporing over each and every speck of a CT: well, now we've got something that can doa\nreasonable job of screening out 95% of the false positives. That\u2019s a huge help, since it\ntranslates into about a tenfold increase in productivity for the machine-assisted human.\n\nOf course, there\u2019s still that pesky issue of the 14% of positive samples that were\nmissed, which we should probably deal with. Perhaps some additional epochs of train-\ning would help. Let\u2019s see (and again, expect to spend at least 10 minutes per epoch):\n\n$ python -m p2ch12.training --balanced --epochs 20\n\nE2 LunaTrainingApp\n\nE2 trn 0.0432 loss, 98.7% correct, 0.9866 precision, 0.9879 recall,\n> 0.9873 \u00a31 score\n\nE2 trn_ben 0.0545 loss, 98.7% correct (98663 of 100000)\n\nE2 trnimal 0.0318 loss, 98.8% correct (98790 of 100000)\n\nE2 val 0.0603 loss, 98.5% correct, 0.1271 precision, 0.8456 recall,\nw 0.2209 f1 score\n\nE2 val_ben 0.0584 loss, 98.6% correct (54181 of 54971)\n\nE2 val_mal 0.8471 loss, 84.6% correct (115 of 136)\n\nE5 trn 0.0578 loss, 98.3% correct, 0.9839 precision, 0.9823 recall,\nwe 0.9831 \u00a31 score\n\nE5 trn_ben 0.0665 loss, 98.4% correct (98388 of 100000)\n\nE5 trn_mal 0.0490 loss, 98.2% correct (98227 of 100000)\n\nE5 val 0.0361 loss, 99.2% correct, 0.2129 precision, 0.8235 recall,\nw 0.3384 \u00a31 score\n\nE5 val_ben 0.0336 loss, 99.2% correct (54557 of 54971)\n\nE5 val_mal 1.0515 loss, 82.4% correct (112 of 136)...\n\nE10 trn 0.0212 loss, 99.5% correct, 0.9942 precision, 0.9953 recall,\nw 0.9948 f1 score\n\nE10 trn_ben 0.0281 loss, 99.4% correct (99421 of 100000)\n\nE10 trn_mal 0.0142 loss, 99.5% correct (99530 of 100000)\n\nE10 val 0.0457 loss, 99.3% correct, 0.2171 precision, 0.7647 recall,\nw 0.3382 f1 score\n\nE10 val_ben 0.0407 loss, 99.3% correct (54596 of 54971)\n\nE10 val_mal 2.0594 loss, 76.5% correct (104 of 136)\n\nE20 trn 0.0132 loss, 99.7% correct, 0.9964 precision, 0.9974 recall,\nw 0.9969 \u00a31 score\n\nE20 trn_ben 0.0186 loss, 99.6% correct (99642 of 100000)\n\nE20 trn_mal 0.0079 loss, 99.7% correct (99736 of 100000)\n\nE20 val 0.0200 loss, 99.7% correct, 0.4780 precision, 0.7206 recall,\nw 0.5748 \u00a31 score\n\nE20 val_ben 0.0133 loss, 99.8% correct (54864 of 54971)\n\nE20 val_mal 2.7101 loss, 72.1% correct (98 of 136)\n\nUgh. That\u2019s a lot of text to scroll past to get to the numbers we're interested in. Let's\npower through and focus on the val_mal XX.xX% correct numbers (or skip ahead to\nthe TensorBoard graph in the next section.) After epoch 2, we were at 87.5%; on\nepoch 5, we peaked with 92.6%; and then by epoch 20 we dropped down to 86.8%\u2014\nbelow our second epoch!\n\nNOTE As mentioned earlier, expect each run to have unique behavior due to\nrandom initialization of network weights and random selection and ordering\nof training samples per epoch.\n\nThe training set numbers don\u2019t seem to be having the same problem. Negative train-\ning samples are classified correctly 98.8% of the time, and positive samples are 99.1%\ncorrect. What's going on?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.13,
                        "section_name": "Recognizing the symptoms of overfitting",
                        "section_path": "./screenshots-images-2/chapter_13/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_13/c0b1bff5-4ccc-40fa-be49-8064ad1ad64f.png",
                            "./screenshots-images-2/chapter_13/section_13/4ff41b14-87fc-4695-8e54-2b8b9dbb1119.png",
                            "./screenshots-images-2/chapter_13/section_13/36c25b5f-faeb-4c9a-9dc2-09d6fd30dc68.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Recognizing the symptoms of overfitting\n\nWhat we are seeing are clear signs of overfitting. Let\u2019s take a look at the graph of our\nloss on positive samples, in figure 12.18.\n\ntag: loss/pos\n\n25 VALIDATION\n, LOSS GOES UP\n2\n1.5\n1\n0.5\n9 oe\nTRAINING LOSS\nGOES DOWN TO ZERO\n\nFigure 12.18 Our positive loss showing clear signs of overfitting, as the training loss and\nvalidation loss are trending in different directions\n\nHere, we can see that the training loss for our positive samples is nearly zero\u2014each\npositive training sample gets a nearly perfect prediction. Our validation loss for posi-\ntive samples is increasing, though, and that means our real-world performance is likely\ngetting worse. At this point, it\u2019s often best to stop the training script, since the model\nis no longer improving.\n\nTIP Generally, if your model\u2019s performance is improving on your training set\nwhile getting worse on your validation set, the model has started overfitting.\n\nWe must take care to examine the right metrics, however, since this trend is only hap-\npening on our positive loss. If we take a look at our overall loss, everything seems fine!\nThat\u2019s because our validation set is not balanced, so the overall loss is dominated by\nour negative samples. As shown in figure 12.19, we are not seeing the same divergent\nbehavior for our negative samples. Instead, our negative loss looks great! That\u2019s\nbecause we have 400 times more negative samples, so it\u2019s much, much harder for the\nmodel to remember individual details. Our positive training set has only 1,215 sam-\nples, though. While we repeat those samples multiple times, that doesn\u2019t make them\nharder to memorize. The model is shifting from generalized principles to essentially\nmemorizing quirks of those 1,215 samples and claiming that anything that\u2019s not one\nof those few samples is negative. This includes both negative training samples and\neverything in our validation set (both positive and negative).\n\nClearly, some generalization is still going on, since we are classifying about 70% of\nthe positive validation set correctly. We just need to change how we're training the\nmodel so that our training set and validation set both trend in the right direction.\n\ntag: loss/neg\n\n0.08\nBOTH LOSSES,\n0.06 ARE TRENDING\nDOWN.\n0.04 N\n0.02\n0\n\n500k 1M 1.5M 2M 2.5M 3M 3.5M 4M\n\nFigure 12.19 Our negative loss showing no signs of overfitting\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.14,
                        "section_name": "Revisiting the problem of overfitting",
                        "section_path": "./screenshots-images-2/chapter_13/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_14/7dad8bbd-df8f-4afd-9867-6a16f4eb5fe7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Revisiting the problem of overfitting\n\nWe touched on the concept of overfitting in chapter 5, and now it\u2019s time to take a\ncloser look at how to address this common situation. Our goal with training a model is\nto teach it to recognize the general properties of the classes we are interested in, as\nexpressed in our dataset. Those general properties are present in some or all samples\nof the class and can be generalized and used to predict samples that haven't been\ntrained on. When the model starts to learn specific properties of the waining set, overfit-\nting occurs, and the model starts to lose the ability to generalize. In case that\u2019s a bit\ntoo abstract, let\u2019s use another analogy.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.15,
                        "section_name": "An overfit face-to-age prediction model",
                        "section_path": "./screenshots-images-2/chapter_13/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_15/772504ea-1575-48c4-880e-b35c3273704e.png",
                            "./screenshots-images-2/chapter_13/section_15/59a62e85-127e-4344-9da7-cd7fb6e67d5f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "An overfit face-to-age prediction model\n\nLet\u2019s pretend we have a model that takes an image of a human face as input and out-\nputs a predicted age in years. A good model would pick up on age signifiers like wrin-\nkles, gray hair, hairstyle, clothing choices, and similar, and use those to build a general\nmodel of what different ages look like. When presented with a new picture, it would\nconsider things like \u201cconservative haircut\u201d and \u201creading glasses\u201d and \u201cwrinkles\u201d to\nconclude \u201caround 65 years old.\u201d\n\nAn overfit model, by contrast, instead remembers specific people by remembering\nidentifying details. \u201cThat haircut and those glasses mean it\u2019s Frank. He\u2019s 62.8 years\nold\u201d; \u201cOh, that scar means it\u2019s Harry. He\u2019s 39.3\u201d; and so on. When shown a new per-\nson, the model won't recognize the person and will have absolutely no idea what age\nto predict.\n\nEven worse, if shown a picture of Frank Jr. (the spittin\u2019 image of his dad, at least\nwhen he\u2019s wearing his glasses!), the model will say, \u201cI think that\u2019s Frank. He\u2019s 62.8\nyears old.\u201d Never mind that Junior is 25 years younger!\n\nOverfitting is usually due to having too few training samples when compared to the\nability of the model to just memorize the answers. The median human can memorize\nthe birthdays of their immediate family but would have to resort to generalizations\nwhen predicting the ages of any group larger than a small village.\n\nOur face-to-age model has the capacity to simply memorize the photos of anyone\nwho doesn\u2019t look exactly their age. As we discussed in part 1, model capacity is a some-\nwhat abstract concept, but is roughly a function of the number of parameters of the\nmodel times how efficiently those parameters are used. When a model has a high\ncapacity relative to the amount of data needed to memorize the hard samples from\nthe training set, it\u2019s likely that the model will begin to overfit on those more difficult\ntraining samples.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.16,
                        "section_name": "Preventing overfitting with data augmentation",
                        "section_path": "./screenshots-images-2/chapter_13/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_16/2cc6428a-e022-472e-8dce-28c3976a48ae.png",
                            "./screenshots-images-2/chapter_13/section_16/71133f8e-476b-4898-92fa-5b1690b371c3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Preventing overfitting with data augmentation\n\nIt\u2019s time to take our model training from good to great. We need to cover one last step\nin figure 12.20.\n\nFigure 12.20 The set of topics for this chapter, with a focus on data augmentation\n\nWe augment a dataset by applying synthetic alterations to individual samples, resulting\nin a new dataset with an effective size that is larger than the original. The typical goal\nis for the alterations to result in a synthetic sample that remains representative of the\nsame general class as the source sample, but that cannot be trivially memorized along-\nside the original. When done properly, this augmentation can increase the training set\n\nsize beyond what the model is capable of memorizing, resulting in the model being\nforced to increasingly rely on generalization, which is exactly what we want. Doing so\nis especially useful when dealing with limited data, as we saw in section 12.4.1.\n\nOf course, not all augmentations are equally useful. Going back to our example of\na face-to-age prediction model, we could trivially change the red channel of the four\ncorner pixels of each image to a random value 0-255, which would result in a dataset\n4 billion times larger the original. Of course, this wouldn't be particularly useful, since\nthe model can pretty trivially learn to ignore the red dots in the image corners, and\nthe rest of the image remains as easy to memorize as the single, unaugmented original\nimage. Contrast that approach with flipping the image left to right. Doing so would\nonly result in a dataset twice as large as the original, but each image would be quite a\nbit more useful for training purposes. The general properties of aging are not cor-\nrelated left to right, so a mirrored image remains representative. Similarly, it\u2019s rare for\nfacial pictures to be perfectly symmetrical, so a mirrored version is unlikely to be trivi-\nally memorized alongside the original.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.17,
                        "section_name": "Specific data augmentation techniques",
                        "section_path": "./screenshots-images-2/chapter_13/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_17/3b82d0f5-2ef6-4a6b-8aca-120e6c9d3c58.png",
                            "./screenshots-images-2/chapter_13/section_17/7ad0989c-f5de-4304-8e19-4ec6ea6c0817.png",
                            "./screenshots-images-2/chapter_13/section_17/0eaa3188-d23f-4f07-b020-f4e0094ad32a.png",
                            "./screenshots-images-2/chapter_13/section_17/dbfc21af-d906-4642-a7ed-abb927aaa1e3.png",
                            "./screenshots-images-2/chapter_13/section_17/a464220e-0dd0-4e7c-9db1-9be25ee27017.png",
                            "./screenshots-images-2/chapter_13/section_17/7639aee5-5352-4a94-83f9-493751ea8c37.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": ". Specific data augmentation techniques\nWe are going to implement five specific types of data augmentation. Our implementa-\ntion will allow us to experiment with any or all of them, individually or in aggregate.\nThe five techniques are as follows:\n\n= Mirroring the image up-down, left-right, and/or front-back\n\nShifting the image around by a few voxels\n\nScaling the image up or down\n\nRotating the image around the head-foot axis\n\n= Adding noise to the image\n\nFor each technique, we want to make sure our approach maintains the training sam-\nple\u2019s representative nature, while being different enough that the sample is useful to\ntrain with.\n\nWe'll define a function getCtAugmentedCandidate that is responsible for taking our\nstandard chunk-of-CT-with-candidate-inside and modifying it. Our main approach will\ndefine an affine wansformation matrix (http://mng.bz/Edxq) and use it with the\nPyTorch affine_grid (https://pytorch.org/docs/stable/nn.htmlfaffine-grid) and grid\n_sample (https://pytorch.org/docs/stable/nn.html#torch.nn.functional.grid_sample)\nfunctions to resample our candidate.\n\nListing 12.11 dsets.py:149, def getCtAugmentedCandidate\n\ndef getCtAugmentedCandidate (\naugmentation_dict,\nseries_uid, center_xyz, width_irc,\nuse_cache=True):\nif use_cache:\net_chunk, center_ire = \\\n\ngetCtRawCandidate(series_uid, center_xyz, width_irc)\nelse:\net = getCt(series_uid)\net_chunk, center_ire = ct.getRawCandidate(center_xyz, width_irc)\n\net_t = torch.tensor(ct_chunk) .unsqueeze (0) .unsqueeze (0) .to(torch. float32)\n\nWe first obtain ct_chunk, either from the cache or directly by loading the CT (some-\nthing that will come in handy once we are creating our own candidate centers), and\nthen convert it to a tensor. Next is the affine grid and sampling code.\n\nListing 12.12 dset: 62, def getCtAugmentedCandid:\n\ntransform_t = torch.eye(4)\n\n#... a Modifications to\n\n# ... line 195 transform_tensor will go here.\naffine_t = F.affine_grid( ~\n\ntransform_t[:3] .unsqueeze(0) .to(torch. float32),\net_t.size(),\nalign_corners=False,\n\n)\n\naugmented_chunk = F.grid_sample(\nett,\naffine_t,\npadding_mode='border',\nalign_corners=False,\n).to('epu')\n# ... line 214\nreturn augmented_chunk[0], center_irc\n\nWithout anything additional, this function won't do much. Let\u2019s see what it takes to\nadd in some actual transforms.\n\nNOTE It\u2019s important to structure your data pipeline such that your caching\nsteps happen before augmentation! Doing otherwise will result in your data\nbeing augmented once and then persisted in that state, which defeats the\npurpose.\n\nMIRRORING\n\nWhen mirroring a sample, we keep the pixel values exactly the same and only change\nthe orientation of the image. Since there\u2019s no strong correlation between tumor\ngrowth and left-right or front-back, we should be able to flip those without changing\nthe representative nature of the sample. The index-axis (referred to as Z in patient\ncoordinates) corresponds to the direction of gravity in an upright human, however, so\nthere\u2019s a possibility of a difference in the top and bottom of a tumor. We are going to\nassume it\u2019s fine, since quick visual investigation doesn\u2019t show any gross bias. Were we\nworking toward a clinically relevant project, we\u2019d need to confirm that assumption\nwith an expert.\n\nListing 12.13 dsets.py:165, def getCtAugmentedCandidate\n\nfor i in range(3):\nif \u2018flip\u2019 in augmentation_dict:\nif random.random() > 0.5:\ntransform_t[i,i]) *= -1\n\nThe grid_sample function maps the range [-1, 1] to the extents of both the old and\nnew tensors (the rescaling happens implicitly if the sizes are different). This range\nmapping means that to mirror the data, all we need to do is multiply the relevant ele-\nment of the transformation matrix by -1.\n\nSHIFTING BY A RANDOM OFFSET\n\nShifting the nodule candidate around shouldn't make a huge difference, since convo-\nlutions are translation independent, though this will make our model more robust to\nimperfectly centered nodules. What will make a more significant difference is that the\noffset might not be an integer number of voxels; instead, the data will be resampled\nusing trilinear interpolation, which can introduce some slight blurring. Voxels at the\nedge of the sample will be repeated, which can be seen as a smeared, streaky section\nalong the border.\n\nListing 12.14 dsets.py:165, def getCtAugmentedCandidate\n\nfor i in range(3):\n# ... line 170\nif \u2018offset\" in augmentation_dict:\noffset_float = augmentation_dict['offset']\nrandom_float = (random.random() * 2 - 1)\ntransform_t[i,3] = offset_float * random_float\n\nNote that our 'offset' parameter is the maximum offset expressed in the same scale\nas the [-1, 1] range the grid sample function expects.\n\nSCALING\n\nScaling the image slightly is very similar to mirroring and shifting. Doing so can also\nresult in the same repeated edge voxels we just mentioned when discussing shifting\nthe sample.\n\nListing 12.15 dsets.py:165, def getCtAugmentedCandidate\n\nfor i in range(3):\n# ... line 175\nif 'seale' in augmentation_dict:\nscale_float = augmentation_dict['scale']\nrandom_float = (random.random() * 2 - 1)\ntransform_t[i,i] *= 1.0 + seale_float * random_float\n\nSince random_float is converted to be in the range [-1, 1], it doesn\u2019t actually matter\nif we add scale_float * random_float to or subtract it from 1.0.\n\nROTATING\n\nRotation is the first augmentation technique we\u2019re going to use where we have to care-\nfully consider our data to ensure that we don\u2019t break our sample with a conversion that\ncauses it to no longer be representative. Recall that our CT slices have uniform spacing\nalong the rows and columns (X- and Y-axes), but in the index (or Z) direction, the vox-\nels are non-cubic. That means we can\u2019t treat those axes as interchangeable.\n\nOne option is to resample our data so that our resolution along the index-axis is\nthe same as along the other two, but that\u2019s not a true solution because the data along\nthat axis would be very blurry and smeared. Even if we interpolate more voxels, the\nfidelity of the data would remain poor. Instead, we'll treat that axis as special and con-\nfine our rotations to the X-Y plane.\n\nListing 12.16 dsets.py:181, def CtAugmentedCandi\n\nif \u2018rotate' in augmentation_dict:\nangle_rad = random.random() * math.pi * 2\ns = math.sin(angle_rad)\n\u00a2 = math.cos(angle_rad)\n\nrotation_t = torch.tensor([\n\n{c, -s, 0, 0],\n{s, \u00a2, 0, 0),\n(0, 0, 1, 0),\n(0, 0, 0, 1),\n\n1)\n\ntransform_t @= rotation_t\n\nNOISE\n\nOur final augmentation technique is different from the others in that it is actively\ndestructive to our sample in a way that flipping or rotating the sample is not. If we add\ntoo much noise to the sample, it will swamp the real data and make it effectively\nimpossible to classify. While shifting and scaling the sample would do something simi-\nlar if we used extreme input values, we've chosen values that will only impact the edge\nof the sample. Noise will have an impact on the entire image.\n\nListing 12.17 dsets.py:208, def getCtAugmentedCandidate\n\nif 'noise' in augmentation_dict:\nnoise_t = torch.randn_like(augmented_chunk)\nnoise_t *= augmentation_dict['noise')\n\naugmented_chunk += noise_t\n\nThe other augmentation types have increased the effective size of our dataset. Noise\nmakes our model's job harder. We'll revisit this once we see some training results.\n\nEXAMINING AUGMENTED CANDIDATES\n\nWe can see the result of our efforts in figure 12.21. The upper-left image shows an un-\n\naugmented positive candidate, and the next five show the effect of each augmentation\n\ntype in isolation. Finally, the bottom row shows the combined result three times.\nSince each __getitem__ call to the augmenting dataset reapplies the augmenta-\n\ntions randomly, each image on the bottom row looks different. This also means it\u2019s\n\nnearly impossible to generate an image exactly like this again! It\u2019s also important to\n\nNONE FLIP OFFSET\n\n\u00b0 io 20 30 4o \u00b0 7] 20 30 4o \u00b0 i)\n\nFigure 12.21 Various augmentation types performed on a positive nodule sample\n\nremember that sometimes the \u2018flip\u2019 augmentation will result in no flip. Returning\nalways-flipped images is just as limiting as not flipping in the first place. Now let\u2019s see if\nany of this makes a difference.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.18,
                        "section_name": "Seeing the improvement from data augmentation",
                        "section_path": "./screenshots-images-2/chapter_13/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_18/0cd94ad4-eeb4-493c-9e87-e7bf478b5a11.png",
                            "./screenshots-images-2/chapter_13/section_18/7a1a5335-e1e7-4fa2-8527-ee1db704ed7f.png",
                            "./screenshots-images-2/chapter_13/section_18/09f3704b-2333-4fdd-920b-49fba6cd6d8b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Seeing the improvement from data augmentation\n\nWe are going to train additional models, one per augmentation type discussed in the\nlast section, with an additional model training run that combines all of the augmenta-\ntion types. Once they\u2019re finished, we'll take a look at our numbers in TensorBoard.\n\nIn order to be able to turn our new augmentation types on and off, we need to\nexpose the construction of augmentation_dict to our command-line interface. Argu-\nments to our program will be added by parser .add_argument calls (not shown, but\nsimilar to the ones our program already has), which will then be fed into code that\nactually constructs augmentation_dict.\n\nListing 12.18 training.py:105, LunaTraining:\n\nself.augmentation_dict = {}\n\nif self.cli_args.augmented or self.cli_args.augment_flip:\nself.augmentation_dict['flip'] = True\n\nif self.cli_args.augmented or self.cli_args.augment_offset:\n\nself.augmentation_dict['offset') = 0.1\nome oo \u2018 : ~) These values were\n\nif self.cli_args.augmented or self.cli_args.augment_scale:\nself.augmentation_dict['scale'] = 0.2 . F Y\n\n. . to have a reasonable\n\nif self.cli_args.augmented or self.cli_args.augment_rotate: im but better\nself.augmentation_dict['rotate') = True values probably\n\nif self.cli_args.augmented or self.cli_args.augment_noise: exist.\nself.augmentation_dict['noise'] = 25.0 a\n\nNow that we have those command-line arguments ready, you can either run the fol-\nlowing commands or revisit p2_run_everything.ipynb and run cells 8 through 16.\nEither way you run it, expect these to take a significant time to finish:\n\nYou only need to prep the\n$ .venv/bin/python -m p2ch12.prepeache cache once per chapter.\n$ .venv/bin/python -m p2ch12.training --epochs 20 \\\n--balanced sanity-bal +\u2014 You might have this run\n$ .venv/bin/python -m p2ch12.training --epochs 10 \\ from earlier in the chapter;\n--balanced --augment-flip sanity-bal-flip ito it!\n\n$ .venv/bin/python -m p2ch12.training --epochs 10 \\\n~-balanced --augment-shift sanity-bal-shift\n\n$ .venv/bin/python -m p2ch12.training --epochs 10 \\\n~-balanced --augment-scale sanity-bal-scale\n\n$ .venv/bin/python -m p2ch12.training --epochs 10 \\\n~-balanced --augment-rotate sanity-bal-rotate\n\n$ .venv/bin/python -m p2chl2.training --epochs 10 \\\n\n--balanced --augment-noise sanity-bal-noise\n\n$ .venv/bin/python -m p2chi2.training --epochs 20 \\\n--balanced --augmented sanity-bal-aug\n\nWhile that\u2019s running, we can start TensorBoard. Let's direct it to only show these runs\nby changing the logdir parameter like so: ../path/to/tensorboard ~--logdir\nruns/p2chi2.\n\nDepending on the hardware you have at your disposal, the training might take a\nlong time. Feel free to skip the flip, shift, and scale training jobs and reduce the\nfirst and last runs to 11 epochs if you need to move things along more quickly. We\nchose 20 runs because that helps them stand out from the other runs, but 11 should\nwork as well.\n\nIf you let everything run to completion, your TensorBoard should have data like\nthat shown in figure 12.22. We're going to deselect everything except the validation\ndata, to reduce clutter. When you're looking at your data live, you can also change the\nsmoothing value, which can help clarify the tend lines. Take a quick look at the fig-\nure, and then we'll go over it in some detail.\n\nNOIS\u00e9 IS\nWORSE THAN\nei UNAUGMENTED\n. UNAUGMENTED\ntag: correct/all tag: correct/neg tag: correct/pos\n- . ==\nINDIVIDUAL cs os\n0 |\noi wtf *\n/ AUGMENTED FULLY\nsoe un UGA ae a ad SAT cece 12M aM Ea aM 2% aM me pat om 33M a\ntag: loss/all tag: loss/neg tag: loss/pos AUGMENTED\nore ot \\ as s\nate\n2 . i\nom oe : poe\nots\nop ns '\no|\u2014\n7 on =\u2014=_=SSt\u2014__.\nan\nSch Ne 1290 GM 2 3M SM SoD ot LM But EU san 2.30 an em Ww va aM ram oe aM a\ntag: pr/fi_score tag: pr/precision tag: pr/recall\n\nFULLY AUGMENTED EXCEPT FOR HOW UNAUGMENTED\n'S WORSE THAN UNAUGMENTED... IS OVERFITTING ON POSITIVE SAMPLES\n\nFigure 12.22 Percent correctly classified, loss, F1 score, precision, and recall for the validation set from\nnetworks trained with a variety of augmentation schemes\n\nThe first thing to notice in the upper-left graph (\u201ctag: correct/all\u201d) is that the individ-\nual augmentation types are something of a jumble. Our unaugmented and fully aug-\nmented runs are on opposite sides of that jumble. That means when combined, our\naugmentation is more than the sum of its parts. Also of interest is that our fully aug-\nmented run gets many more wrong answers. While that\u2019s bad generally, if we look at\nthe right column of images (which focus on the positive candidate samples we actually\ncare about\u2014the ones that are really nodules), we see that our fully augmented model\nis much better at finding the positive candidate samples. The recall for the fully aug-\nmented model is great! It\u2019s also much better at not overfitting. As we saw earlier, our\nunaugmented model gets worse over time.\n\nOne interesting thing to note is that the noise-augmented model is worse at identi-\nfying nodules than the unaugmented model. This makes sense if we remember that\nwe said noise makes the model's job harder.\n\nAnother interesting thing to see in the live data (it\u2019s somewhat lost in the jumble\nhere) is that the rotation-augmented model is nearly as good as the fully augmented\nmodel when it comes to recall, and it has much better precision. Since our F1 score is\nprecision limited (due to the higher number of negative samples), the rotation-\naugmented model also has a better F1 score.\n\nWe'll stick with the fully augmented model going forward, since our use case\nrequires high recall. The F1 score will still be used to determine which epoch to save\nas the best. In a real-world project, we might want to devote extra time to investigating\nwhether a different combination of augmentation types and parameter values could\nyield better results.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.19,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_13/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_19/27e1cbdf-7d00-4390-b04d-2dcb9933e0ef.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\n\nWe spent a lot of time and energy in this chapter reformulating how we think about\nour model's performance. It\u2019s easy to be misled by poor methods of evaluation, and\nit\u2019s crucial to have a strong intuitive understanding of the factors that feed into evalu-\nating a model well. Once those fundamentals are internalized, it\u2019s much easier to spot\nwhen we\u2019re being led astray.\n\nWe've also learned about how to deal with data sources that aren\u2019t sufficiently pop-\nulated. Being able to synthesize representative training samples is incredibly useful.\nSituations where we have too much training data are rare indeed!\n\nNow that we have a classifier that is performing reasonably, we'll turn our attention\nto automatically finding candidate nodules to classify. Chapter 13 will start there;\nthen, in chapter 14, we will feed those candidates back into the classifier we developed\nhere and venture into building one more classifier to tell malignant nodules from\nbenign ones.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 14,
                "chapter_name": "Using segmentation\nto find suspected nodules",
                "chapter_path": "./screenshots-images-2/chapter_14",
                "sections": [
                    {
                        "section_id": 14.1,
                        "section_name": "Using segmentation\nto find suspected nodules",
                        "section_path": "./screenshots-images-2/chapter_14/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_1/8fd1b7fc-bf3d-4bc5-b412-b2087ec506b4.png",
                            "./screenshots-images-2/chapter_14/section_1/2aceec02-5cc6-4c5f-89e8-a4e732790c85.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the last four chapters, we have accomplished a lot. We've learned about CT scans\nand lung tumors, datasets and data loaders, and metrics and monitoring. We have\nalso applied many of the things we learned in part 1, and we have a working classi-\nfier. We are still operating in a somewhat artificial environment, however, since we\nrequire hand-annotated nodule candidate information to load into our classifier.\nWe don\u2019t have a good way to create that input automatically. Just feeding the entire\nCT into our model\u2014that is, plugging in overlapping 32 x 32 x 32 patches of data\u2014\nwould result in 31 x 31 x 7 = 6,727 patches per CT, or about 10 times the number of\nannotated samples we have. We'd need to overlap the edges; our classifier expects\nthe nodule candidate to be centered, and even then the inconsistent positioning\nwould probably present issues.\n\nAs we explained in chapter 9, our project uses multiple steps to solve the problem\nof locating possible nodules, identifying them, with an indication of their possible\nmalignancy. This is a common approach among practitioners, while in deep learning\nresearch there is a tendency to demonstrate the ability of individual models to solve\ncomplex problems in an end-to-end fashion. The multistage project design we use in\nthis book gives us a good excuse to introduce new concepts step by step.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.2,
                        "section_name": "Adding a second model to our project",
                        "section_path": "./screenshots-images-2/chapter_14/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_2/69e855bb-3cd3-44ce-b7c2-7e58a2d5ce22.png",
                            "./screenshots-images-2/chapter_14/section_2/374b44da-36d4-409e-afb7-1ddcf0a46add.png",
                            "./screenshots-images-2/chapter_14/section_2/4bacdb5c-b4ad-437d-9557-94a9fbc70217.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adding a second model to our project\n\nIn the previous two chapters, we worked on step 4 of our plan shown in figure 13.1:\nclassification. In this chapter, we'll go back not just one but two steps. We need to find\na way to tell our classifier where to look. To do this, we are going to take raw CT scans\nand find everything that might be a nodule.\u2019 This is the highlighted step 2 in the fig-\nure. To find these possible nodules, we have to flag voxels that look like they might be\npart of a nodule, a process known as segmentation. Then, in chapter 14, we will deal\nwith step 3 and provide the bridge by transforming the segmentation masks from this\nimage into location annotations.\n\nBy the time we're finished with this chapter, we'll have created a new model with\nan architecture that can perform per-pixel labeling, or segmentation. The code that\n\n\u2018STEP 2 (CH. 13):\nSEGMENTATION\n\nFigure 13.1 Our end-to-end lung cancer detection project, with a focus on this chapter's\ntopic: step 2, segmentation\n\nwill accomplish this will be very similar to the code from the last chapter, especially if\nwe focus on the larger structure. All of the changes we're going to make will be\nsmaller and targeted. As we see in figure 13.2, we need to make updates to our model\n(step 2A in the figure), dataset (2B), and training loop (2C) to account for the new\nmodel's inputs, outputs, and other requirements. (Don\u2019t worry if you don\u2019t recognize\neach component in each of these steps in step 2 on the right side of the diagram. We'll\ngo through the details when we get to each step.) Finally, we'll examine the results we\nget when running our new model (step 3 in the figure).\n\n\\. SEGMENTATION 2. UPDATE:\nUNET 2A. MODEL\n|) Se Sew\n\u2014 28. DATASET\n3. RESULTS 20. TRAINING\n\niateas\n\nFigure 13.2 The new model architecture for segmentation, along with the model, dataset,\nand training loop updates we will implement\n\nBreaking down figure 13.2 into steps, our plan for this chapter is as follows:\n\n1 Segmentation. First we will learn how segmentation works with a U-Net model,\nincluding what the new model components are and what happens to them as\nwe go through the segmentation process. This is step 1 in figure 13.2.\n\n2 Update. To implement segmentation, we need to change our existing code base\nin three main places, shown in the substeps on the right side of figure 13.2.The\ncode will be structurally very similar to what we developed for classification, but\nwill differ in detail:\n\na Update the model (step 2A). We will integrate a preexisting U-Net into our seg-\nmentation model. Our model in chapter 12 output a simple true/false classi-\nfication; our model in this chapter will instead output an entire image.\n\n\u00bb Change the dataset (step 2B). We need to change our dataset to not only\ndeliver bits of the CT but also provide masks for the nodules. The classifica-\ntion dataset consisted of 3D crops around nodule candidates, but we'll\nneed to collect both full CT slices and 2D crops for segmentation training\nand validation.\n\ne Adapt the training loop (step 2C). We need to adapt the training loop so we\nbring in a new loss to optimize. Because we want to display images of our seg-\nmentation results in TensorBoard, we'll also do things like saving our model\nweights to disk.\n\n3 Results. Finally, we'll see the fruits of our efforts when we look at the quantitative\nsegmentation results.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.3,
                        "section_name": "Various types of segmentation",
                        "section_path": "./screenshots-images-2/chapter_14/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_3/397f7eca-dba4-4449-ad12-ae5a33f5f032.png",
                            "./screenshots-images-2/chapter_14/section_3/164eb4f4-1deb-4c3b-bc31-4ecb441af75b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Various types of segmentation\n\nTo get started, we need to talk about different flavors of segmentation. For this project,\nwe will be using semantic segmentation, which is the act of classifying individual pixels\nin an image using labels just like those we've seen for our classification tasks, for\nexample, \u201cbear,\u201d \u201ccat,\u201d \u201cdog,\u201d and so on. If done properly, this will result in distinct\nchunks or regions that signify things like \u201call of these pixels are part of a cat.\u201d This takes\nthe form of a label mask or heatmap that identifies areas of interest. We will have a\nsimple binary label: true values will correspond to nodule candidates, and false values\nmean uninteresting healthy tissue. This partially meets our need to find nodule\ncandidates that we will later feed into our classification network.\n\nBefore we get into the details, we should briefly discuss other approaches we could\ntake to finding our nodule candidates. For example, instance segmentation labels indi-\nvidual objects of interest with distinct labels. So whereas semantic segmentation would\nlabel a picture of two people shaking hands with two labels (\u201cperson\u201d and \u201cback-\nground\u201d), instance segmentation would have three labels (\u201cperson1,\u201d \u201cperson2,\u201d and\n\u201cbackground\u201d) with a boundary somewhere around the clasped hands. While this\ncould be useful for us to distinguish \u201cnodulel\u201d from \u201cnodule2,\u201d we will instead use\ngrouping to identify individual nodules. That approach will work well for us since\nnodules are unlikely to touch or overlap.\n\nAnother approach to these kinds of tasks is object detection, which locates an item of\ninterest in an image and puts a bounding box around the item. While both instance\nsegmentation and object detection could be great for our uses, their implementations\nare somewhat complex, and we don\u2019t feel they are the best things for you to learn\nnext. Also, training object-detection models typically requires much more computa-\ntional resources than our approach requires. If you\u2019re feeling up to the challenge, the\nYOLOv3 paper is a more entertaining read than most deep learning research papers.\u201d\nFor us, though, semantic segmentation it is.\n\nNOTE As we go through the code examples in this chapter, we're going to\nrely on you checking the code from GitHub for much of the larger context.\nWe'll be omitting code that\u2019s uninteresting or similar to what\u2019s come before\nin earlier chapters, so that we can focus on the crux of the issue at hand.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.4,
                        "section_name": "Semantic segmentation: Per-pixel classification",
                        "section_path": "./screenshots-images-2/chapter_14/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_4/7c6ace7c-e476-4f61-ae33-6cee2cfcb95c.png",
                            "./screenshots-images-2/chapter_14/section_4/e9fd226b-6556-4fcd-8633-7e68662fbea1.png",
                            "./screenshots-images-2/chapter_14/section_4/4f7fa0cb-7bfc-417a-ae94-55d273ac1bad.png",
                            "./screenshots-images-2/chapter_14/section_4/b6ba2761-d60c-4d53-a5ce-adf4db0fbacd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Semantic segmentation: Per-pixel classification\n\nOften, segmentation is used to answer questions of the form \u201cWhere is a cat in this pic-\nture?\u201d Obviously, most pictures of a cat, like figure 13.3, have a lot of non-cat in them;\nthere\u2019s the table or wall in the background, the keyboard the cat is sitting on, that kind\nof thing. Being able to say \u201cThis pixel is part of the cat, and this other pixel is part of the\nwall\u201d requires fundamentally different model output and a different internal structure\nfrom the classification models we've worked with thus far. Classification can tell us\nwhether a cat is present, while segmentation will tell us where we can find it.\n\nCLASSIFICATION VS. SEGMENTATION\n\nCAT: YES CAT: HERE\n\nFigure 13.3 Classification results in one or more binary flags, while segmentation produces\na mask or heatmap.\n\nIf your project requires differentiating between a near cat and a far cat, or a cat on the\nleft versus a cat on the right, then segmentation is probably the right approach. The\nimage-consuming classification models that we've implemented so far can be thought\nof as funnels or magnifying glasses that take a large bunch of pixels and focus them\ndown into a single \u201cpoint\u201d (or, more accurately, a single set of class predictions), as\nshown in figure 13.4. Classification models provide answers of the form \u201cYes, this huge\npile of pixels has a cat in it, somewhere,\u201d or \u201cNo, no cats here.\u201d This is great when you\ndon\u2019t care where the cat is, just that there is (or isn\u2019t) one in the image.\n\nRepeated layers of convolution and downsampling mean the model starts by con-\nsuming raw pixels to produce specific, detailed detectors for things like texture and\ncolor, and then builds up higher-level conceptual feature detectors for parts like eyes\n\nSN \u2018iowa\n\n2ST CAT YES\nA =e\n(\\_L\\ \\\n\n= ~ wat\n\n\u2014\nPIXELS\nSN \u2014\n\nsHapesS \u2014\n\nCLASSES\n\nFigure 13.4 The magnifying glass model structure for classification\n\nand ears and mouth and nose\u2019 that finally result in \u201ccat\u201d versus \u201cdog.\u201d Due to the increas-\ning receptive field of the convolutions after each downsampling layer, those higher-level\ndetectors can use information from an increasingly large area of the input image.\n\nUnfortunately, since segmentation needs to produce an image-like output, ending\nup ata single classification-like list of binary-ish flags won\u2019t work. As we recall from sec-\ntion 11.4, downsampling is key to increasing the receptive fields of the convolutional\nlayers, and is what helps reduce the array of pixels that make up an image to a single\nlist of classes. Notice figure 13.5, which repeats figure 11.6.\n\nIn the figure, our inputs flow from the left to right in the top row and are continued\nin the bottom row. In order to work out the receptive field\u2014the area influencing the\nsingle pixel at bottom right\u2014we can go backward. The max-pool operation has 2 x 2\ninputs producing each final output pixel. The 3 x 3 cony in the middle of the bottom\nrow looks at one adjacent pixel (including diagonally) in each direction, so the total\nreceptive field of the convolutions that result in the 2 x 2 output is 4x 4 (with the right\n\u201cx\u201d characters). The 3 x 3 convolution in the top row then adds an additional pixel of\ncontext in each direction, so the receptive field of the single output pixel at bottom right\nis a 6 x 6 field in the input at top left. With the downsampling from the max pool, the\nreceptive field of the next block of convolutions will have double the width, and each\nadditional downsampling will double it again, while shrinking the size of the output.\n\nWe'll need a different model architecture if we want our output to be the same size\nas our input. One simple model to use for segmentation would have repeated convo-\nlutional layers without any downsampling. Given appropriate padding, that would\nresult in output the same size as the input (good), but a very limited receptive field\n\n4x4 OUTPUT\n\n2K2 2x2\nOUTPUT MAX POOL\n\n{a\n\nikl OUTPUT\n\nFigure 13.5 The convolutional architecture of a LunaModel block, consisting of two 3 x 3\nconvolutions followed by a max pool. The final pixel has a 6 x 6 receptive field.\n\n(bad) due to the limited reach based on how much overlap multiple layers of small\nconvolutions will have. The classification model uses each downsampling layer to dou-\nble the effective reach of the following convolutions; and without that increase in\neffective field size, each segmented pixel will only be able to consider a very local\nneighborhood.\n\nNOTE Assuming 3 x 3 convolutions, the receptive field size for a simple\nmodel of stacked convolutions is 2 * L + 1, with ZL being the number of convo-\nlutional layers.\n\nFour layers of 3 x 3 convolutions will have a receptive field of 9 x 9 per output pixel. By\ninserting a 2 x 2 max pool between the second and third convolutions, and another at\nthe end, we increase the receptive field to ...\n\nNOTE See if you can figure out the math yourself; when you're done, check\nback here.\n\n.. 16 x 16. The final series of conv-cony-pool has a receptive field of 6 x 6, but that\nhappens after the first max pool, which makes the final effective receptive field 12 x 12\nin the original input resolution. The first two cony layers add a total border of 2 pixels\naround the 12 x 12, fora total of 16 x 16.\n\nSo the question remains: how can we improve the receptive field of an output pixel\nwhile maintaining a 1:1 ratio of input pixels to output pixels? One common answer is\n\nto use a technique called upsampling, which takes an image of a given resolution and\nproduces an image of a higher resolution. Upsampling at its simplest just means\nreplacing each pixel with an N x N block of pixels, each with the same value as the\noriginal input pixel. The possibilities only get more complex from there, with options\nlike linear interpolation and learned deconvolution.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.5,
                        "section_name": "The U-Net architecture",
                        "section_path": "./screenshots-images-2/chapter_14/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_5/ec357027-255a-4125-a482-9bdeb446d0bf.png",
                            "./screenshots-images-2/chapter_14/section_5/68c79ab1-fe54-46f2-8f20-d539fe440fdc.png",
                            "./screenshots-images-2/chapter_14/section_5/ad9eae40-0e8d-4a98-a3b8-e166b68f12d5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "L The U-Net architecture\n\nBefore we end up diving down a rabbit hole of possible upsampling algorithms, let\u2019s\nget back to our goal for the chapter. Per figure 13.6, step 1 is to get familiar with a\nfoundational segmentation algorithm called U-Net.\n\nThe U-Net architecture is a design for a neural network that can produce pixel-\nwise output and that was invented for segmentation. As you can see from the highlight\nin figure 13.6, a diagram of the U-Net architecture looks a bit like the letter U, which\nexplains the origins of the name. We also immediately see that it is quite a bit more\ncomplicated than the mostly sequential structure of the classifiers we are familiar with.\nWe'll see a more detailed version of the U-Net architecture shortly, in figure 13.7, and\nlearn exactly what each of those components is doing. Once we understand the model\narchitecture, we can work on training one to solve our segmentation task.\n\nFigure 13.6 The new model architecture for segmentation, that we will be working with\n\nThe U-Net architecture shown in figure 13.7 was an early breakthrough for image seg-\nmentation. Let\u2019s take a look and then walk through the architecture.\n\nIn this diagram, the boxes represent intermediate results and the arrows represent\noperations between them. The U-shape of the architecture comes from the multiple\n\nUNET ARCHITECTURE\n\nSKIP CONNECTIONS\n\noutput\nsegmentation\n\"| map\n\nUPSAMPLING\n\n=> conv 3x3, ReLU\n= copy and crop\n# max pool 2x2\n4 up-cony 2x2\n=> conv 1x1\n\nCLASSIFICATION\nMAGIFYING\n\nGLASS\nCOULD BE FED INTO LINEAR LAYER\n\nFigure 13.7 From the U-Net paper, with annotations. Source: The base of this figure is courtesy Olaf Ronneberger\net al., from the paper \u201cU-Net: Convolutional Networks for Biomedical Image Segmentation,\u201d which can be found at\nhttps://arxiv.org/abs/1505.04597 and https: //Imb.informatik.uni-freiburg.de/ people /ronneber/u-net.\n\nresolutions at which the network operates. In the top row is the full resolution (512 x\n512 for us), the row below has half that, and so on. The data flows from top left to bot-\ntom center through a series of convolutions and downscaling, as we saw in the classifi-\ners and looked at in detail in chapter 8. Then we go up again, using upscaling\nconyolutions to get back to the full resolution. Unlike the original U-Net, we will be\npadding things so we don\u2019t lose pixels off the edges, so our resolution is the same on\nthe left and on the right.\n\nEarlier network designs already had this U-shape, which people attempted to use\nto address the limited receptive field size of fully convolutional networks. To address\nthis limited field size, they used a design that copied, inverted, and appended the\nfocusing portions of an image-classification network to create a symmetrical model\nthat goes from fine detail to wide receptive field and back to fine detail.\n\nThose earlier network designs had problems converging, however, most likely due\nto the loss of spatial information during downsampling. Once information reaches a\nlarge number of very downscaled images, the exact location of object boundaries gets\n\nharder to encode and therefore reconstruct. To address this, the U-Net authors added\nthe skip connections we see at the center of the figure. We first touched on skip con-\nnections in chapter 8, although they are employed differently here than in the ResNet\narchitecture. In U-Net, skip connections short-circuit inputs along the downsampling\npath into the corresponding layers in the upsampling path. These layers receive as input\nboth the upsampled results of the wide receptive field layers from lower in the U as well\nas the output of the earlier fine detail layers via the \u201ccopy and crop\u201d bridge connections.\nThis is the key innovation behind U-Net (which, interestingly, predated ResNet).\n\nAll of this means those final detail layers are operating with the best of both worlds.\nThey've got both information about the larger context surrounding the immediate\narea and fine detail data from the first set of full-resolution layers.\n\nThe \u201ccony 1x1\u201d layer at far right, in the head of the network, changes the number\nof channels from 64 to 2 (the original paper had 2 output channels; we have 1 in our\ncase). This is somewhat akin to the fully connected layer we used in our classification\nnetwork, but per-pixel, channel-wise: it\u2019s a way to convert from the number of filters\nused in the last upsampling step to the number of output classes needed.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.6,
                        "section_name": "Updating the model for segmentation",
                        "section_path": "./screenshots-images-2/chapter_14/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_6/b9cf8c07-0d41-419d-989d-78118053edb6.png",
                            "./screenshots-images-2/chapter_14/section_6/5c7ee828-ac9e-47ce-83f9-c1afaea48215.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Updating the model for segmentation\n\nIt\u2019s time to move through step 2A in figure 13.8. We've had enough theory about\nsegmentation and history about U-Net; now we want to update our code, starting with\nthe model. Instead of just outputting a binary classification that gives us a single output\nof true or false, we integrate a U-Net to get to a model that\u2019s capable of outputting a\n\nFigure 13.8 The outline of this chapter, with a focus on the changes needed for our\nsegmentation model\n\nprobability for every pixel: that is, performing segmentation. Rather than imple-\nmenting a custom U-Net segmentation model from scratch, we're going to appropriate\nan existing implementation from an open source repository on GitHub.\n\nThe U-Net implementation at https://github.com/jvanvugt/pytorch-unet seems\nto meet our needs well.\u2018 It\u2019s MIT licensed (copyright 2018 Joris), it\u2019s contained in a\nsingle file, and it has a number of parameter options for us to tweak. The file is\nincluded in our code repository at util/unet.py, along with a link to the original repos-\nitory and the full text of the license used.\n\nNOTE While it\u2019s less of an issue for personal projects, it\u2019s important to be\naware of the license terms attached to open source software you use for a proj-\nect. The MIT license is one of the most permissive open source licenses, and\nit still places requirements on users of MIT licensed code! Also be aware that\nauthors retain copyright even if they publish their work in a public forum\n(yes, even on GitHub), and if they do not include a license, that does not\nmean the work is in the public domain. Quite the opposite! It means you\ndon\u2019t have any license to use the code, any more than you'd have the right to\nwholesale copy a book you borrowed from the library.\n\nWe suggest taking some time to inspect the code and, based on the knowledge you have\nbuilt up until this point, identify the building blocks of the architecture as they are\nreflected in the code. Can you spot skip connections? A particularly worthy exercise for\nyou is to draw a diagram that shows how the model is laid out, just by looking at the code.\n\nNow that we have found a U-Net implementation that fits the bill, we need to\nadapt it so that it works well for our needs. In general, it\u2019s a good idea to keep an eye\nout for situations where we can use something off the shelf. It\u2019s important to have a\nsense of what models exist, how they\u2019re implemented and trained, and whether any\nparts can be scavenged and applied to the project we're working on at any given\nmoment. While that broader knowledge is something that comes with time and expe-\nrience, it\u2019s a good idea to start building that toolbox now.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.7,
                        "section_name": "Adapting an off-the-shelf model to our project",
                        "section_path": "./screenshots-images-2/chapter_14/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_7/9dfccbfd-dcfc-451c-aebd-447c186d0411.png",
                            "./screenshots-images-2/chapter_14/section_7/a12bbe10-1ec9-4788-993b-8f7e74bc4994.png",
                            "./screenshots-images-2/chapter_14/section_7/e1920cf0-8608-4edd-a9af-96ca4d6ca930.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adapting an off-the-shelf model to our project\n\nWe will now make some changes to the classic U-Net, justifying them along the way. A\nuseful exercise for you will be to compare results between the vanilla model and the\none after the tweaks, preferably removing one at a time to see the effect of each\nchange (this is also called an ablation study in research circles).\n\nFirst, we're going to pass the input through batch normalization. This way, we\nwon't have to normalize the data ourselves in the dataset; and, more importantly, we\nwill get normalization statistics (read mean and standard deviation) estimated over\nindividual batches. This means when a batch is dull for some reason\u2014that is, when\nthere is nothing to see in all the CT crops fed into the network\u2014it will be scaled more\n\nstrongly. The fact that samples in batches are picked randomly at every epoch will\nminimize the chances of a dull sample ending up in an all-dull batch, and hence those\ndull samples getting overemphasized.\n\nSecond, since the output values are unconstrained, we are going to pass the output\nthrough an nn.Sigmoid layer to restrict the output to the range [0, 1]. Third, we will\nreduce the total depth and number of filters we allow our model to use. While this is\njumping ahead of ourselves a bit, the capacity of the model using the standard param-\neters far outstrips our dataset size. This means we're unlikely to find a pretrained model\nthat matches our exact needs. Finally, although this is not a modification, it\u2019s important\nto note that our output is a single channel, with each pixel of output representing the\nmodel's estimate of the probability that the pixel in question is part of a nodule.\n\nThis wrapping of U-Net can be done rather simply by implementing a model with\nthree attributes: one each for the two features we want to add, and one for the U-Net\nitself/\u2014which we can treat just like any prebuilt module here. We will also pass any key-\nword arguments we receive into the U-Net constructor.\n\nListing 13.1 model.py:17, class UNetWrapper\n\nkwarg is a dictionary containing all keyword\n\narguments passed to the constructor. BatchNorm2d wants us to\n. specify the number of input\ncies UNetWrapper (nn. Module) : r ls, which we take from\nlef __init__(self, **kwargs): the ke rd\nThe U-llat: super().__init__() \u2018yword argument.\na small thing self.input_batchnorm = nn.BatchNorm2d(kwargs['in_channels'))\nto include elf.unet = UNet (**kwargs)\nhere, butit\u2019s| o\u00a2 \u00a2inal = nn. Sigmoid() Just as for the classifier in chapter 11, we use\nrealy doing our custom weight initialization. The function is\nal work. self._init_weights() copied over, so we will not show the code again.\n\nThe forward method is a similarly straightforward sequence. We could use an\ninstance of nn.Sequential as we saw in chapter 8, but we'll be explicit here for both\nclarity of code and clarity of stack traces.\u00b0\n\nListing 13.2 model.py:50, UNetWrapper . forward\n\ndef forward(self, input_batch):\nbn_output = self.input_batchnorm(input_batch)\nun_output self.unet (bn_output)\nfn_output self.final (un_output)\nreturn fn_output\n\nNote that we\u2019re using nn. BatchNorm2d here. This is because U-Net is fundamentally a\ntwo-dimensional segmentation model. We could adapt the implementation to use 3D\n\n\u00ae In the unlikely event our code throws any exceptions\u2014which it clearly won't, will it?\n\nconvolutions, in order to use information across slices. The memory usage of a straight-\nforward implementation would be considerably greater: that is, we would have to chop\nup the CT scan. Also, the fact that pixel spacing in the Z direction is much larger than\nin-plane makes a nodule less likely to be present across many slices. These consider-\nations make a fully 3D approach less attractive for our purposes. Instead, we'll adapt\nour 3D data to be segmented a slice at a time, providing adjacent slices for context (for\nexample, detecting that a bright lump is indeed a blood vessel gets much easier along-\nside neighboring slices). Since we\u2019re sticking with presenting the data in 2D, we'll use\nchannels to represent the adjacent slices. Our treatment of the third dimension is sim-\nilar to how we applied a fully connected model to images in chapter 7: the model will\nhave to relearn the adjacency relationships we're throwing away along the axial direc-\ntion, but that\u2019s not difficult for the model to accomplish, especially with the limited\nnumber of slices given for context owing to the small size of the target structures.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.8,
                        "section_name": "Updating the dataset for segmentation",
                        "section_path": "./screenshots-images-2/chapter_14/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_8/2cb0709f-e5f7-402c-99ca-93e077797089.png",
                            "./screenshots-images-2/chapter_14/section_8/95878665-5698-4f3a-a6dd-94c066b99e64.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Updating the dataset for segmentation\nOur source data for this chapter remains unchanged: we\u2019re consuming CT scans and\nannotation data about them. But our model expects input and will produce output of\na different form than we had previously. As we hint at in step 2B of figure 13.9, our\nprevious dataset produced 3D data, but we need to produce 2D data now.\n\nThe original U-Net implementation did not use padded convolutions, which\nmeans while the output segmentation map was smaller than the input, every pixel of\nthat output had a fully populated receptive field. None of the input pixels that fed\n\n28. DATASET\n\nFigure 13.9 The outline of this chapter, with a focus on the changes needed for our\nsegmentation dataset\n\ninto the determination of that output pixel were padded, fabricated, or otherwise\nincomplete. Thus the output of the original U-Net will tile perfectly, so it can be used\nwith images of any size (except at the edges of the input image, where some context\nwill be missing by definition).\n\nThere are two problems with us taking the same pixel-perfect approach for our\nproblem. The first is related to the interaction between convolution and downsam-\npling, and the second is related to the nature of our data being three-dimensional.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.9,
                        "section_name": "U-Net has very specific input size requirements",
                        "section_path": "./screenshots-images-2/chapter_14/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_9/f256f6d0-2f21-4d30-8958-39b5660fea9d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "U-Net has very specific input size requirements\nThe first issue is that the sizes of the input and output patches for U-Netare very specific.\nIn order to have the two-pixel loss per convolution line up evenly before and after\ndownsampling (especially when considering the further convolutional shrinkage at that\nlower resolution), only certain input sizes will work. The U-Net paper used 572 x 572\nimage patches, which resulted in 388 x 388 output maps. The input images are bigger\nthan our 512 x 512 CT slices, and the outputis quite a bit smaller! That would mean any\nnodules near the edge of the CT scan slice wouldn't be segmented at all. Although this\nsetup works well when dealing with very large images, it\u2019s not ideal for our use case.\nWe will address this issue by setting the padding flag of the U-Net constructor to\nTrue. This will mean we can use input images of any size, and we will get output of the\nsame size. We may lose some fidelity near the edges of the image, since the receptive\nfield of pixels located there will include regions that have been artificially padded, but\nthat\u2019s a compromise we decide to live with.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.1,
                        "section_name": "2 U-Net trade-offs for 3D vs. 2D data",
                        "section_path": "./screenshots-images-2/chapter_14/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_10/92809ba7-aaf0-4d53-93d6-ad722e2ee72e.png",
                            "./screenshots-images-2/chapter_14/section_10/61675953-969c-4416-81a4-044b0a2e097d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "U-Net trade-offs for 3D vs. 2D data\n\nThe second issue is that our 3D data doesn\u2019t line up exactly with U-Net\u2019s 2D expected\ninput. Simply taking our 512 x 512 x 128 image and feeding it into a converted-to-3D\nU-Net class won't work, because we'll exhaust our GPU memory. Each image is 2\u00b0 by 2\u00b0\nby 2\u201d, with 2\u201d bytes per voxel. The first layer of U-Net is 64 channels, or 2\u00b0. That\u2019s an expo-\nnent of 9+ 9 +7 +2 +6=33, or 8 GB just for the first convolutional layer. There are two con-\nvolutional layers (16 GB); and then each downsampling halves the resolution but\ndoubles the channels, which is another 2 GB for each layer after the first downsample\n(remember, halving the resolution results in one-eighth the data, since we\u2019re working\nwith 3D data). So we've hit 20 GB before we even get to the second downsample, much\nless anything on the upsample side of the model or anything dealing with autograd.\n\nNOTE There are a number of clever and innovative ways to get around these\nproblems, and we in no way suggest that this is the only approach that will\never work.\u00ae We do feel that this approach is one of the simplest that gets the\njob done to the level we need for our project in this book. We'd rather keep\nthings simple so that we can focus on the fundamental concepts; the clever\nstuff can come later, once you've mastered the basics.\n\nAs anticipated, instead of trying to do things in 3D, we're going to treat each slice as a\n2D segmentation problem and cheat our way around the issue of context in the third\ndimension by providing neighboring slices as separate channels. Instead of the tradi-\ntional \u201cred,\u201d \u201cgreen,\u201d and \u201cblue\u201d channels that we're familiar with from photographic\nimages, our main channels will be \u201ctwo slices above,\u201d \u201cone slice above,\u201d \u201c\nactually segmenting,\u201d \u201cone slice below,\u201d and so on.\n\nThis approach isn\u2019t without trade-offs, however. We lose the direct spatial relation-\nship between slices when represented as channels, as all channels will be linearly com-\nbined by the convolution kernels with no notion of them being one or two slices away,\nabove or below. We also lose the wider receptive field in the depth dimension that\nwould come from a true 3D segmentation with downsampling. Since CT slices are\noften thicker than the resolution in rows and columns, we do get a somewhat wider\nview than it seems at first, and this should be enough, considering that nodules typi-\ncally span a limited number of slices.\n\nAnother aspect to consider, that is relevant for both the current and fully 3D\napproaches, is that we are now ignoring the exact slice thickness. This is something\nour model will eventually have to learn to be robust against, by being presented with\ndata with different slice spacings.\n\nIn general, there isn\u2019t an easy flowchart or rule of thumb that can give canned\nanswers to questions about which trade-offs to make, or whether a given set of com-\npromises compromise too much. Careful experimentation is key, however, and system-\natically testing hypothesis after hypothesis can help narrow down which changes and\napproaches are working well for the problem at hand. Although it\u2019s tempting to make\na flurry of changes while waiting for the last set of results to compute, resist that impulse.\n\nThat\u2019s important enough to repeat: do not test multiple modifications at the same time.\nThere is far too high a chance that one of the changes will interact poorly with the\nother, and you'll be left without solid evidence that either one is worth investigating\nfurther. With that said, let\u2019s start building out our segmentation dataset.\n\nthe slice we're\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.11,
                        "section_name": "Building the ground truth data",
                        "section_path": "./screenshots-images-2/chapter_14/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_11/4da6c0cd-c7f3-43e0-bc5b-a4dda225c0b0.png",
                            "./screenshots-images-2/chapter_14/section_11/7253cf81-2a85-4a7b-99ac-bee71ef298c5.png",
                            "./screenshots-images-2/chapter_14/section_11/4f23e16f-2148-4548-a9ce-132b00cbf09c.png",
                            "./screenshots-images-2/chapter_14/section_11/2e5cbe41-5579-4448-aae0-1d3c245315ae.png",
                            "./screenshots-images-2/chapter_14/section_11/074444a2-7410-4034-9e29-c7da019b1ce4.png",
                            "./screenshots-images-2/chapter_14/section_11/dc6fbab0-b3b0-4345-a196-de1123e5def3.png",
                            "./screenshots-images-2/chapter_14/section_11/45958221-597e-4c71-9977-1e9fe3b160b6.png",
                            "./screenshots-images-2/chapter_14/section_11/5994c4b4-fa8c-4838-a99e-3ddb8e271082.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "} Building the ground truth data\nThe first thing we need to address is that we have a mismatch between our human-\nlabeled training data and the actual output we want to get from our model. We have\nannotated points, but we want a per-voxel mask that indicates whether any given voxel\nis part of a nodule. We'll have to build that mask ourselves from the data we have and\nthen do some manual checking to make sure the routine that builds the mask is per-\nforming well.\n\nValidating these manually constructed heuristics at scale can be difficult. We aren\u2019t\ngoing to attempt to do anything comprehensive when it comes to making sure each\nand every nodule is properly handled by our heuristics. If we had more resources,\napproaches like \u201ccollaborate with (or pay) someone to create and/or verify everything\nby hand\u201d might be an option, but since this isn\u2019t a well-funded endeavor, we'll rely on\nchecking a handful of samples and using a very simple \u201cdoes the output look reason-\nable?\u201d approach.\n\nTo that end, we'll design our approaches and our APIs to make it easy to investi-\ngate the intermediate steps that our algorithms are going through. While this might\nresult in slightly clunky function calls returning huge tuples of intermediate values,\nbeing able to easily grab results and plot them in a notebook makes the clunk worth it.\n\nBOUNDING BOXES\n\nWe are going to begin by converting the nodule locations that we have into bounding\nboxes that cover the entire nodule (note that we'll only do this for actual nodules). If\nwe assume that the nodule locations are roughly centered in the mass, we can trace\noutward from that point in all three dimensions until we hit low-density voxels, indi-\ncating that we've reached normal lung tissue (which is mostly filled with air). Let\u2019s fol-\nlow this algorithm in figure 13.10.\n\nCOL STEP |\n\nCOL STEP 2 COL FINISHED\n\nCOL_RADIVS=2\n\nFINAL 8.80%\n\nCOL_RADIUS=1 COL_RADIUS=2\n\nROW FINISHED\n\nROW START\n\nSLICE(-2,42),\nSLICe(-3,43)\n\nROW_RADIUS=1 ROW_RADIUS=3\n\nFigure 13.10 An algorithm for finding a bounding box around a lung nodule\n\nWe start the origin of our search (O in the figure) at the voxel at the annotated center\nof our nodule. We then examine the density of the voxels adjacent to our origin on\nthe column axis, marked with a question mark (?). Since both of the examined voxels\ncontain dense tissue, shown here in lighter colors, we continue our search. After\n\nincrementing our column search distance to 2, we find that the left voxel has a density\nbelow our threshold, and so we stop our search at 2.\n\nNext, we perform the same search in the row direction. Again, we start at the ori-\ngin, and this time we search up and down. After our search distance becomes 3, we\nencounter a low-density voxel in both the upper and lower search locations. We only\nneed one to stop our search!\n\nWe'll skip showing the search in the third dimension. Our final bounding box is\nfive voxels wide and seven voxels tall. Here\u2019s what that looks like in code, for the index\ndirection.\n\nListing 13.3 dsets.py:131, c\n\ncenter_ire = xyz2ire(\ncandidateInfo_tup.center_xyz, > candidatelnfo tup here is the same as\nself.origin_xyz, we've seen previously: as returned by\n\nself .vxSize_xyz, getCandidatelnfoList.\nself.direction_a,\n\n)\n\nei\ner\nec\n\nint (center_irc. index) > Gets the center voxel\n\nint (center_irc.row) indices, our starting point\nint (center_irec.col)\n\nindex_radius = 2\n\ntry: The search\nwhile self.hu_a[ci + index_radius, cr, cc] > threshold_hu and \\ described\nself.hu_a(ci - index_radius, cr, cc) > threshold_hu: <a! previously\nindex_radius += 1\nexcept IndexError: The safety net for indexing\nbeyond the size of the tensor\n\nindex_radius -= 1\n\nWe first grab the center data and then do the search in a while loop. As a slight com-\nplication, our search might fall off the boundary of our tensor. We are not terribly\nconcerned about that case and are lazy, so we just catch the index exception.\u2019\n\nNote that we stop incrementing the very approximate radius values after the density\ndrops below threshold, so our bounding box should contain a one-voxel border of low-\ndensity tissue (at least on one side; since nodules can be adjacent to regions like the lung\nwall, we have to stop searching in both directions when we hit air on either side). Since\nwe check both center_index + index_radius and center_index - index_radius\nagainst that threshold, that one-voxel boundary will only exist on the edge closest to our\nnodule location. This is why we need those locations to be relatively centered. Since\nsome nodules are adjacent to the boundary between the lung and denser tissue like mus-\ncle or bone, we can\u2019t trace each direction independently, as some edges would end up\nincredibly far away from the actual nodule.\n\nWe then repeat the same radius-expansion process with row_radius and col\n_radius (this code is omitted for brevity). Once that\u2019s done, we can set a box in our\nbounding-box mask array to True (we'll see the definition of boundingBox_ary in just\na moment; it\u2019s not surprising).\n\nOK, let\u2019s wrap all this up in a function. We loop over all nodules. For each nodule,\nwe perform the search shown earlier (which we elide from listing 13.4). Then, in a\nBoolean tensor boundingBox_a, we mark the bounding box we found.\n\nAfter the loop, we do a bit of cleanup by taking the intersection between the\nbounding-box mask and the tissue that\u2019s denser than our threshold of -700 HU (or 0.3\ng/cc). That\u2019s going to clip off the corners of our boxes (at least, the ones not embed-\nded in the lung wall), and make it conform to the contours of the nodule a bit better.\n\n13.4 dsets.py:127, ct. 1dAnnotationMask\n\nStarts with an all-False tensor\nof the same size as the CT\ndef buildAnnotationMask(self, positiveInfo_list, threshold_hu = -700):\na boundingBox_a = np.zeros_like(self.hu_a, dtype=np.bool)\n\nfor candidateInfo_tup in positiveInfo_list: | Loops over the nodules. As a reminder\n\n. e.. - line 169 that we are only looking at nodules,\nRestricts boundingBox_a[ we call the variable positivelnfo_list.\nthe mask to ci - index_radius: ci + index_radius + 1,\nvoxels above er - row_radius: cr + row_radius + 1,\nour density ce - col_radius: ce + col_radius + 1) = True After we get the nodule\nthreshold radius (the search itself\n\n> mask_a = boundingBox_a & (self.hu_a > threshold_hu)\n\nis left out), we mark\nthe bounding box.\n\nreturn mask_a\n\nLet's take a look at figure 13.11 to see what these masks look like in practice. Addi-\ntional images in full color can be found in the p2ch13_explore_data.ipynb notebook.\n\nThe bottom-right nodule mask demonstrates a limitation of our rectangular\nbounding-box approach by including a portion of the lung wall. It\u2019s certainly something\n\nPOSITIVE MASK\n\n\\oo\n\n200\n\n300\n\n400\n\nFigure 13.11 Three nodules from\n500 ct .positive_mask, highlighted\n\n() 100 -=\u2014 200 800. 400.\u2014\u2014s\u00abCS!CDD_~_inwhiite\n\n\nwe could fix, butsince we're not yet convinced that\u2019s the best use of our time and attention,\nwe'll let it remain as is for now.\" Next, we'll go about adding this mask to our CT class.\n\nCALLING MASK CREATION DURING CT INITIALIZATION\n\nNow that we can take a list of nodule information tuples and turn them into at CT-\nshaped binary \u201cIs this a nodule?\u201d mask, let\u2019s embed those masks into our CT object.\nFirst, we'll filter our candidates into a list containing only nodules, and then we'll use\nthat list to build the annotation mask. Finally, we'll collect the set of unique array\nindexes that have at least one voxel of the nodule mask. We'll use this to shape the\ndata we use for validation.\n\nListing 13.5 dsets. 9,Cct.. init\n\ndef __init__ (self, series_uid):\n# ... line 116\ncandidateInfo_list = getCandidateInfoDict() [self.series_uid]\n\nself.positiveInfo_list = [\n\ncandidate_tup Gives us a 1D vector (over the\nFilters for for candidate_tup in candidateInfo_list slices) with the number of voxels\nnodules |_.. if candidate_tup.isNodule_bool flagged in the mask in each slice\n\n]\nself.positive_mask = self.buildAnnotationMask(self.positiveInfo_list)\nself.positive_indexes = (self.positive_mask.sum(axis=(1,2)) oh\n\n-nonzero()[0].tolist())\nTakes indices of the mask slices that have a\nnonzero count, which we make into a list\n\nKeen eyes might have noticed the getCandidateInfoDict function. The definition isn\u2019t\nsurprising; it\u2019s just a reformulation of the same information as in the getCandidate-\nInfoList function, but pregrouped by series_uid.\n\nListing 13.6 dsets.\n\nThis can be useful to\nTakes the list of candidates for the series UID\nkeep Ct init from being a from the dict, defaulting to a fresh, empty list\nperformance bottleneck. if we cannot find it. Then appends the\n@functools.1lru_cache(1) present candidatelnfo_tup to it.\n\ndef getCandidateInfoDict (requireOnDisk_bool=True) :\ncandidateInfo_list getCandidateInfoList (requireOnDisk_bool)\ncandidateInfo_dict {}\n\nfor candidateInfo_tup in candidateInfo_list:\ncandidateInfo_dict.setdefault (candidateInfo_tup.series_uid,\n[]) .append (candidateInfo_tup)\n\nreturn candidateInfo_dict\n\nCACHING CHUNKS OF THE MASK IN ADDITION TO THE CT\n\nIn earlier chapters, we cached chunks of CT centered around nodule candidates,\nsince we didn\u2019t want to have to read and parse all of a CT\u2019s data every time we wanted\na small chunk of the CT. We'll want to do the same thing with our new positive\n_mask, so we need to also return it from our Ct.getRawCandidate function. This\nworks out to an additional line of code and an edit to the return statement.\n\nListing 13.7 dsets.py:178, Ct .getRawCandidate\n\ndef getRawCandidate(self, center_xyz, width_irc):\ncenter_irc = xyz2ire(center_xyz, self.origin_xyz, self.vxSize_xyz,\nself.direction_a)\n\nslice_list = []\n\n# ... line 203\n\net_chunk = self.hu_a[tuple(slice_list)]\n\npos_chunk = self.positive_mask[(tuple(slice_list)] -\u2014\u2014 Newly added\nreturn ct_chunk, pos_chunk, center_ire \u00abt\u2014\u2014 New value returned here\n\nThis will, in turn, be cached to disk by the getCtRawCandidate function, which opens\nthe CT, gets the specified raw candidate including the nodule mask, and clips the CT\nvalues before returning the CT chunk, mask, and center information.\n\nListing 13.8 dsets.py:212\n\n@raw_cache.memoize(typed=True)\ndef getCtRawCandidate(series_uid, center_xyz, width_irc):\net = getCt(series_uid)\net_chunk, pos_chunk, center_ire = ct.getRawCandidate(center_xyz,\nwidth_irc)\net_chunk.clip(-1000, 1000, ct_chunk)\nreturn ct_chunk, pos_chunk, center_ire\n\nThe prepcache script precomputes and saves all these values for us, helping keep\ntraining quick.\nCLEANING UP OUR ANNOTATION DATA\nAnother thing we're going to take care of in this chapter is doing some better screen-\ning on our annotation data. It turns out that several of the candidates listed in candi-\ndates.csy are present multiple times. To make it even more interesting, those entries\nare not exact duplicates of one another. Instead, it seems that the original human\nannotations weren't sufficiently cleaned before being entered in the file. They might\nbe annotations on the same nodule on different slices, which might even have been\nbeneficial for our classifier.\n\nWe'll do a bit of a hand wave here and provide a cleaned up annotation.csy file. In\norder to fully walk through the provenance of this cleaned file, you'll need to know that\nthe LUNA dataset is derived from another dataset called the Lung Image Database\n\nConsortium image collection (LIDC-IDRI)\u00ae and includes detailed annotation\ninformation from multiple radiologists. We've already done the legwork to get the\noriginal LIDC annotations, pull out the nodules, dedupe them, and save them to the\nfile /data/part2/luna/annotations_with_malignancy.csv.\n\nWith that file, we can update our getCandidateInfoList function to pull our nod-\nules from our new annotations file. First, we loop over the new annotations for the\nactual nodules. Using the CSV reader,'\u00ae we need to convert the data to the appropri-\nate types before we stick them into our CandidateInfoTuple data structure.\n\nListing 13.9 dsets.py:43, def getCandidateInfoList\n\ncandidateInfo_list = []\nwith open('data/part2/luna/annotations_with_malignancy.csv', \"r\") as \u00a3:\n\nfor row in list(csv.reader(f))[1:]: For each line in\n\nseries_uid = row[0) the annotations\nannotationCenter_xyz = tuple([float(x) for x in row[1:4]]) file that\nannotationDiameter_mm = float(row[4]) represents one\nisMal_bool = ('False': False, 'True': True) [row[5]] nodule, ...\n\ncandidateInfo_list.append( <\u2014\u2014 ... we add a record to our list.\n\nCandidateInfoTuple(\nTrue, <+\u2014\u2014 isNodule_bool\nTrue, \u00ab\nisMal_bool, hasAnnotation_bool\n\nannotationDiameter_mm,\nseries_uid,\nannotationCenter_xyz,\n)\n)\n\nSimilarly, we loop over candidates from candidates.csv as before, but this time we only\nuse the non-nodules. As these are not nodules, the nodule-specific information will\njust be filled with False and 0.\n\nListing 13.10 dsets.py:62, def getCandidateInfoList\n\nwith open('data/part2/luna/candidates.csv', \"r\") as \u00a3: | Foreach line inthe\n\nfor row in list (esv.reader(f))[1:): \u00ab candidates file ...\nseries_uid = row[0)\n# ... line 72 .-. but only the non-nodules (we\nif not isNodule_bool: cS have the others from earlier) ...\n\ncandidateInfo_list.append( 7\nCandidateInfoTuple( ... we add a candidate record.\n\n* Samuel G. Armato 3rd et al., 2011, \u201cThe Lung Image Database Consortium (LIDC) and Image Database\nResource Initiative (IDRI): A Completed Reference Database of Lung Nodules on CT Scans,\u201d Medical Physics\n38, no. 2 (2011): 915-31, https: //pubmed.ncbi.nlm.nih.gov/21452728/. See also Bruce Vendt, LIDC-IDRI,\nCancer Imaging Archive, http://mng.bz/mBO4.\n\n'\u00ae Ifyou do this a lot, the pandas library that just released 1.0 in 2020 is a great tool to make this faster. We stick\nwith the CSV reader included in the standard Python distribution here.\n\nFalse, <\u2014\u2014 isNodule_bool\nFalse, +\nisMal_bool \u2014\u2014> False, | hasAnnotation_bool\n0.0,\nseries_uid,\ncandidateCenter_xyz,\n)\n)\n\nOther than the addition of the hasAnnotation_bool and isMal_bool flags (which we\nwon't use in this chapter), the new annotations will slot in and be usable just like the\nold ones.\n\nNOTE You might be wondering why we haven't discussed the LIDC before\nnow. As it turns out, the LIDC has a large amount of tooling that\u2019s already\nbeen constructed around the underlying dataset, which is specific to the\nLIDC. You could even get ready-made masks from PyLIDC. That tooling pres-\nents a somewhat unrealistic picture of what sort of support a given dataset\nmight have, since the LIDC is anomalously well supported. What we\u2019ve done\nwith the LUNA data is much more typical and provides for better learning,\nsince we\u2019re spending our time manipulating the raw data rather than learn-\ning an API that someone else cooked up.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.12,
                        "section_name": "Implementing Luna2dSegmentationDataset",
                        "section_path": "./screenshots-images-2/chapter_14/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_12/37f1cb57-24f6-4f45-b4a8-f13e59af679e.png",
                            "./screenshots-images-2/chapter_14/section_12/27216177-1d3c-4dd4-8c14-82ef2303b749.png",
                            "./screenshots-images-2/chapter_14/section_12/f562cd55-b1ff-4973-a0b7-21781a677e59.png",
                            "./screenshots-images-2/chapter_14/section_12/5e78c3e6-b530-4d79-85bd-c2299878d1ea.png",
                            "./screenshots-images-2/chapter_14/section_12/3a6487ba-045d-47fd-9eb7-353d0051a615.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Implementing Luna2dSegmentationDataset\n\nCompared to previous chapters, we are going to take a different approach to the train-\ning and validation split in this chapter. We will have two classes: one acting as a general\nbase class suitable for validation data, and one subclassing the base for the training\nset, with randomization and a cropped sample.\n\nWhile this approach is somewhat more complicated in some ways (the classes\naren't perfectly encapsulated, for example), it actually simplifies the logic of selecting\nrandomized training samples and the like. It also becomes extremely clear which code\npaths impact both training and validation, and which are isolated to training only.\nWithout this, we found that some of the logic can become nested or intertwined in\nways that make it hard to follow. This is important because our training data will look\nsignificantly different from our validation data!\n\nNOTE Other class arrangements are also viable; we considered having two\nentirely separate Dataset subclasses, for example. Standard software engi-\nneering design principles apply, so try to keep your structure relatively sim-\nple, and try to not copy and paste code, but don\u2019t invent complicated\nframeworks to prevent having to duplicate three lines of code.\n\nThe data that we produce will be two-dimensional CT slices with multiple channels.\nThe extra channels will hold adjacent slices of CT. Recall figure 4.2, shown here as\nfigure 13.12; we can see that each slice of CT scan can be thought of as a 2D grayscale\nimage.\n\nToP MIDDLE BOTTOM\n\nFigure 13.12 Each slice of a CT scan represents a different position in space.\n\nHow we combine those slices is up to us. For the input to our classification model, we\ntreated those slices as a 3D array of data and used 3D convolutions to process each\nsample. For our segmentation model, we are going to instead treat each slice as a sin-\ngle channel, and produce a multichannel 2D image. Doing so will mean that we are\ntreating each slice of CT scan as if it was a color channel of an RGB image, like we saw\nin figure 4.1, repeated here as figure 13.13. Each input slice of the CT will get stacked\ntogether and consumed just like any other 2D image. The channels of our stacked CT\nimage won't correspond to colors, but nothing about 2D convolutions requires the\ninput channels to be colors, so it works out fine.\n\nFor validation, we'll need to produce one sample per slice of CT that has an entry\nin the positive mask, for each validation CT we have. Since different CT scans can\nhave different slice counts,'! we're going to introduce a new function that caches the\n\nRED GREEN BLUE\n\nFigure 13.13 Each channel of a photographic image represents a different color.\n\nsize of each CT scan and its positive mask to disk. We need this to be able to quickly\nconstruct the full size of a validation set without having to load each CT at Dataset ini-\ntialization. We'll continue to use the same caching decorator as before. Populating\nthis data will also take place during the prepcache.py script, which we must run once\nbefore we start any model training.\n\nListing 13.11 dsets.py:220\n\n@raw_cache.memoize(typed=True)\ndef getCtSampleSize(series_uid):\net = Ct(series_uid)\nreturn int(ct.hu_a.shape[0]), ct.positive_indexes\n\nThe majority of the Luna2dSegmentationDataset.__init__ method is similar to what\nwe've seen before. We have a new contextSlices_count parameter, as well as an\naugmentation_dict similar to what we introduced in chapter 12.\n\nThe handling for the flag indicating whether this is meant to be a training or vali-\ndation set needs to change somewhat. Since we\u2019re no longer training on individual\nnodules, we will have to partition the list of series, taken as a whole, into training and\nvalidation sets. This means an entire CT scan, along with all nodule candidates it con-\ntains, will be in either the training set or the validation set.\n\nListing 13.12 dsets.py:242,.__ init _\n\nif isValSet_bool:\nassert val_stride > 0, val_stride\n\nself.series_list = self.series_list[::val_stride] \u00ab Starting with a series list\nassert self.series_list containing all our series, we\nelif val_stride > 0: keep only every val_stride-th\ndel self.series_list[::val_stride] element, starting with 0.\nassert self.series_list\nIf we are training, we delete every\n\nval_stride-th element instead.\n\nSpeaking of validation, we\u2019re going to have two different modes we can validate our\ntraining with. First, when ful1Ct_bool is True, we will use every slice in the CT for our\ndataset. This will be useful when we're evaluating end-to-end performance, since we\nneed to pretend that we're starting off with no prior information about the CT. We'll\nuse the second mode for validation during training, which is when we're limiting our-\nselves to only the CT slices that have a positive mask present.\n\nAs we now only want certain CT series to be considered, we loop over the series\nUIDs we want and get the total number of slices and the list of interesting ones.\n\nself.sample_list = []\nfor series_uid in self.series_list:\n\nindex_count, positive_indexes = getCtSampleSize(series_uid)\n\nif self.fullct_bool: Here we extend sample_list\nself.sample_list += [(series_uid, slice_ndx) <q\u2014 with every slice of the CT by\nfor slice_ndx in range (index_count) } using range ...\nelse:\nself.sample_list += ((series_uid, slice_ndx) + ... While here we take\nfor slice_ndx in positive_indexes] ~ only the interesting slices.\n\nDoing it this way will keep our validation relatively quick and ensure that we're getting\ncomplete stats for true positives and false negatives, but we\u2019re making the assumption\nthat other slices will have false positive and true negative stats relatively similar to the\nones we evaluate during validation.\n\nOnce we have the set of series_uid values we'll be using, we can filter our candi-\ndateInfo_list to contain only nodule candidates with a series_uid that is included\nin that set of series. Additionally, we'll create another list that has only the positive can-\ndidates so that during training, we can use those as our training samples.\n\nting 13.14 dsets. 61, . =\nself.candidateInfo_list = getCandidateInfoList () <t-\u2014 This is cached.\nseries_set = set(self.series_list) a Makes a set for faster lookup\nself.candidateInfo_list = [cit for cit in self.candidateInfo_list\nif cit.series_uid in series_set)\n\nFilters out the candidates\n\nself.pos_list = [nt for nt in self.candidateInfo_list from series not in our set\nif nt.isNodule_bool] <\u2014-\u2014\nFor the data balancing yet to come,\n\nwe want a list of actual nodules.\n\nOur __getitem__ implementation will also be a bit fancier by delegating a lot of the\nlogic to a function that makes it easier to retrieve a specific sample. At the core of it,\nwe'd like to retrieve our data in three different forms. First, we have the full slice of\nthe CT, as specified by a series_uid and ct_ndx. Second, we have a cropped area\naround a nodule, which we'll use for training data (we'll explain in a bit why we're not\nusing full slices). Finally, the DataLoader is going to ask for samples via an integer ndx,\nand the dataset will need to return the appropriate type based on whether it\u2019s training\nor validation.\n\nThe base class or subclass __get item__ functions will convert from the integer ndx\nto either the full slice or training crop, as appropriate. As mentioned, our validation\nset\u2019s __getitem__just calls another function to do the real work. Before that, it wraps\nthe index around into the sample list in order to decouple the epoch size (given by\nthe length of the dataset) from the actual number of samples.\n\nListing 13.15 dsets.py:281, .__getitem__\n\nvee \u2014etiton tee,\n\ndef __getitem__(self, ndx):\nseries_uid, slice_ndx = self.sample_list([ndx % len(self.sample_list)]\nreturn self.getitem_fullSlice(series_uid, slice_ndx)\n\nThat was easy, but we still need to implement the interesting functionality from the\ngetItem_fullSlice method.\n\nListing 13.16 dsets.py:285, .getitem_fullslice\n\ndef getitem_fullSlice(self, series_uid, slice_ndx): Preallocates the output\net = getCt(series_uid)\net_t = torch.zeros((self.contextSlices_count * 2 + 1, 512, 512))\nstart_ndx = slice_ndx - self.contextSlices_count When we reach\nend_ndx = slice_ndx + self.contextSlices_count + 1 beyond the bounds of\n\nfor i, context_ndx in enumerate(range(start_ndx, end_ndx)): the ct_a, we\ncontext_ndx = max(context_ndx, 0) the first or last slice.\ncontext_ndx = min(context_ndx, ct.hu_a.shape[0] - 1)\net_t[i] = torch. from_numpy(ct.hu_a[context_ndx] .astype(np.float32))\net_t.clamp_(-1000, 1000)\n\npos_t = torch. from_numpy(ct.positive_mask[(slice_ndx]) .unsqueeze (0)\n\nreturn ct_t, pos_t, ct.series_uid, slice_ndx\n\nSplitting the functions like this means we can always ask a dataset for a specific slice\n(or cropped training chunk, which we'll see in the next section) indexed by series\nUID and position. Only for the integer indexing do we go through __getitem_,\nwhich then gets a sample from the (shuffled) list.\n\nAside from ct_t and pos_t, the rest of the tuple we return is all information that\nwe include for debugging and display. We don\u2019t need any of it for training.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.13,
                        "section_name": "Designing our training and validation data",
                        "section_path": "./screenshots-images-2/chapter_14/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_13/dea0e924-e9c4-4619-9dc3-d1fe6a5f0bb2.png",
                            "./screenshots-images-2/chapter_14/section_13/332655c5-8c52-4722-93c8-792a994642b7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Designing our training and validation data\n\nBefore we get into the implementation for our training dataset, we need to explain\nwhy our training data will look different from our validation data. Instead of the full\nCT slices, we're going to train on 64 x 64 crops around our positive candidates (the\nactually-a-nodule candidates). These 64 x 64 patches will be taken randomly from a 96\nx 96 crop centered on the nodule. We will also include three slices of context in both\ndirections as additional \u201cchannels\u201d to our 2D segmentation.\n\nWe're doing this to make training more stable, and to converge more quickly. The\nonly reason we know to do this is because we tried to train on whole CT slices, but we\nfound the results unsatisfactory. After some experimentation, we found that the 64 x\n64 semirandom crop approach worked well, so we decided to use that for the book.\n\nWhen you work on your own projects, you'll need to do that kind of experimentation\nfor yourself!\n\nWe believe the whole-slice training was unstable essentially due to a class-balancing\nissue. Since each nodule is so small compared to the whole CT slice, we were right\nback in a needle-in-a-haystack situation similar to the one we got out of in the last\nchapter, where our positive samples were swamped by the negatives. In this case, we're\ntalking about pixels rather than nodules, but the concept is the same. By training on\ncrops, we're keeping the number of positive pixels the same and reducing the nega-\ntive pixel count by several orders of magnitude.\n\nBecause our segmentation model is pixel-to-pixel and takes images of arbitrary\nsize, we can get away with training and validating on samples with different dimen-\nsions. Validation uses the same convolutions with the same weights, just applied to a\nlarger set of pixels (and so with fewer border pixels to fill in with edge data).\n\nOne caveat to this approach is that since our validation set contains orders of mag-\nnitude more negative pixels, our model will have a huge false positive rate during vali-\ndation. There are many more opportunities for our segmentation model to get\ntricked! It doesn\u2019t help that we're going to be pushing for high recall as well. We'll dis-\ncuss that more in section 13.6.3.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.14,
                        "section_name": "Implementing TrainingLuna2dSegmentationDataset",
                        "section_path": "./screenshots-images-2/chapter_14/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_14/8b98d7f9-7d20-4ebb-ab83-1a9af4a779f0.png",
                            "./screenshots-images-2/chapter_14/section_14/69ad64cf-36a0-4181-bd6f-e17aac0da29e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Implementing TrainingLuna2dSegmentationDataset\n\nWith that out of the way, let\u2019s get back to the code. Here\u2019s the training set\u2019s_getitem_.\nItlooks just like the one for the validation set, except that we now sample from pos_list\nand call get Item_trainingCrop with the candidate info tuple, since we need the series\nand the exact center location, not just the slice.\n\nListing 13.17 dsets.py:320, .__ getitem __\n\ndef __getitem_(self, ndx):\ncandidateInfo_tup = self.pos_list[ndx % len(self.pos_list)]\nreturn self.getitem_trainingCrop(candidateInfo_tup)\n\nTo implement getItem_trainingCrop, we will use a getCtRawCandidate function\nsimilar to the one we used during classification training. Here, we're passing in a dif-\nferent size crop, but the function is unchanged except for now returning an addi-\ntional array with a crop of the ct .positive_mask as well.\n\nWe limit our pos_a to the center slice that we're actually segmenting, and then con-\nstruct our 64 x 64 random crops of the 96 x 96 we were given by getCtRawCandidate.\nOnce we have those, we return a tuple with the same items as our validation dataset.\n\nListing 13.18 dsets.py:324, .getitem_trainingCrop\n\ndef getitem_trainingCrop(self, candidateInfo_tup):\nct_a, pos_a, center_ire = getCtRawCandidate( Gets the candidate with a\ncandidateInfo_tup.series_uid, bit of extra surrounding\ncandidateInfo_tup.center_xyz,\n\n) (7, 96, 96), Taking a one-element slice keeps\n_ [3:4] the third dimension, which will be With two random\npos\u2014a = Pos_a\\ss the (single) output channel. numbers between 0\nand 31, we crop\nrow_offset = random.randrange (0,32) <\u2014 bot .\ncol_offset = random.randrange (0,32) CT and mask.\net_t = torch.from_numpy(ct_a[:, row_offset:row_offset+64,\n\ncol_offset:col_offset+64] ) .to(torch. float32)\npos_t = torch.from_numpy(pos_a[:, row_offset:row_offset+64,\ncol_offset:col_offset+64))}.to(torch. long)\n\nslice_ndx = center_irc. index\nreturn ct_t, pos_t, candidateInfo_tup.series_uid, slice_ndx\nYou might have noticed that data augmentation is missing from our dataset imple-\n\nmentation. We're going to handle that a little differently this time around: we'll aug-\nment our data on the GPU.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.15,
                        "section_name": "Augmenting on the GPU",
                        "section_path": "./screenshots-images-2/chapter_14/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_15/4c43b967-6671-43a1-9512-ee3bb1baf525.png",
                            "./screenshots-images-2/chapter_14/section_15/90e2c03e-db8a-4277-b4ef-0f3cce5768e0.png",
                            "./screenshots-images-2/chapter_14/section_15/8126ed0f-7dca-4eee-973e-3a7fff06591b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "\u2019 Augmenting on the GPU\nOne of the key concerns when it comes to training a deep learning model is avoiding\nbottlenecks in your training pipeline. Well, that\u2019s not quite ttue\u2014there will always be a\nbottleneck.'? The trick is to make sure the bottleneck is at the resource that\u2019s the most\nexpensive or difficult to upgrade, and that your usage of that resource isn\u2019t wasteful.\nSome common places to see bottlenecks are as follows:\n\n= In the data-loading pipeline, either in raw I/O or in decompressing data once\nit\u2019s in RAM. We addressed this with our diskcache library usage.\n\n= In CPU preprocessing of the loaded data. This is often data normalization or\naugmentation.\n\n= In the training loop on the GPU. This is typically where we want our bottleneck\nto be, since total deep learning system costs for GPUs are usually higher than\nfor storage or CPU.\n\n= Less commonly, the bottleneck can sometimes be the memory bandwidth between\nCPU and GPU. This implies that the GPU isn\u2019t doing much work compared to\nthe data size that\u2019s being sent in.\n\nSince GPUs can be 50 times faster than CPUs when working on tasks that fit GPUs\nwell, it often makes sense to move those tasks to the GPU from the CPU in cases where\nCPU usage is becoming high. This is especially true if the data gets expanded during\nthis processing; by moving the smaller input to the GPU first, the expanded data is\nkept local to the GPU, and less memory bandwidth is used.\n\nIn our case, we're going to move data augmentation to the GPU. This will keep\nour CPU usage light, and the GPU will easily be able to accommodate the additional\nworkload. Far better to have the GPU busy with a small bit of extra work than idle\nwaiting for the CPU to struggle through the augmentation process.\n\nWe'll accomplish this by using a second model, similar to all the other subclasses of\nnn.Module we've seen so far in this book. The main difference is that we're not inter-\nested in backpropagating gradients through the model, and the forward method will\nbe doing decidedly different things. There will be some slight modifications to the\nactual augmentation routines since we\u2019re working with 2D data for this chapter, but\notherwise, the augmentation will be very similar to what we saw in chapter 12. The\nmodel will consume tensors and produce different tensors, just like the other models\nwe've implemented.\n\nOur model\u2019s __init__ takes the same data augmentation arguments\u2014flip,\noffset, and so on\u2014that we used in the last chapter, and assigns them to self.\n\nListing 13.19 model.py:56, class SegmentationAugmentation\n\nclass SegmentationAugmentation(nn.Module) :\ndef _init (\nself, flip=None, offset=None, scale=None, rotate=None, noise=None\n):\nsuper().__init__()\n\nself.flip = flip\nself.offset = offset\n# ... line 64\n\nOur augmentation forward method takes the input and the label, and calls out to\nbuild the transform_t tensor that will then drive our affine_grid and grid_sample\ncalls. Those calls should feel very familiar from chapter 12.\n\nListing 13.20 model.py:68, SegmentationAugmentation. forward\n\ndef forward(self, input_g, label_g): Note that we're augmenting\ntransform_t = self._build2dTransformMatrix() 2D data.\n\ntransform_t = transform_t.expand(input_g.shape[0), -1, -1)\ntransform_t = transform_t.to(input_g.device, torch. float32)\n\naffine_t = F.affine_grid(transform_t[:,:2], <J\ninput_g.size(), align_corners=False) The first dimension of the\ntransformation is the batch,\naugmented_input_g = F.grid_sample(input_g, but we only want the first two\n\naffine_t, padding_mode='border', rows of the 3 x 3 matrices per\nalign_corners=False) batch item.\naugmented_label_g = F.grid_sample(label_g.to(torch.float32),\n\naffine_t, padding_mode='border',\nalign_corners=False) We need the same transformation applied to CT and\n\nmask, so we use the same grid. Because grid_sample\nif self.noise: only works with floats, we convert here.\n\nnoise_t = torch.randn_like(augmented_input_g)\n\nnoise_t *= self.noise Just before returning, we convert the mask back to\n\nBooleans by comparing to 0.5. The interpolation\nthat grid_sample results in fractional values.\n\nreturn augmented_input_g, augmented_label_g > 0.5 +\n\naugmented_input_g += noise_t\n\nNow that we know what we need to do with transform_t to get our data out, let\u2019s take\na look at the _build2dTransformMatrix function that actually creates the transforma-\ntion matrix we use.\n\nListing 13.21 model.py:90, ._build2dTransformMatrix\n\ndef _build2dTransformMatrix(self): Creates a3 x 3 matrix, but we\ntransform_t = torch.eye(3) a will drop the last row later.\n\nfor i in range(2): Again, we're augmenting\n\nif self.flip: 2D data here.\nif random.random() > 0.5:\n\ntransform_t[i,i) *= -1\n# ... line 108\nif self.rotate:\n\nangle_rad = random.random() * math.pi * 2 @ } Takes a random angle in radians,\n\ns = math.sin(angle_rad) so in the range 0 .. 2{pi}\n\u00a2 = math.cos(angle_rad)\n\nrotation_t = torch.tensor ([ Rotation matrix for the 2D rotation by the\ni ree random angle in the first two dimensions\ns, \u00a2, 0),\n{o, 0, 1)))\n\ntransform_t @= rotation_t + Applies the rotation to the transformation matrix\nusing the Python matrix multiplication operator\nreturn transform_t\nOther than the slight differences to deal with 2D data, our GPU augmentation code\nlooks very similar to our CPU augmentation code. That's great, because it means\nwe\u2019re able to write code that doesn\u2019t have to care very much about where it runs. The\nprimary difference isn\u2019t in the core implementation: it\u2019s how we wrapped that imple-\nmentation into a nn.Module subclass. While we\u2019ve been thinking about models as\nexclusively a deep learning tool, this shows us that with PyTorch, tensors can be used\nquite a bit more generally. Keep this in mind when you start your next project\u2014the\nrange of things you can accomplish with a GPU-accelerated tensor is pretty large!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.16,
                        "section_name": "Updating the training script for segmentation",
                        "section_path": "./screenshots-images-2/chapter_14/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_16/98390e95-2861-467a-b84b-ba28e31ac8cc.png",
                            "./screenshots-images-2/chapter_14/section_16/a6c5a0d4-382b-4e41-9b70-a28af987682b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Updating the training script for segmentation\nWe have a model. We have data. We need to use them, and you won't be surprised\nwhen step 2C of figure 13.14 suggests we should train our new model with the new\n\ndata.\nTo be more precise about the process of training our model, we will update three\nthings affecting the outcome from the training code we got in chapter 12:\n\n= We need to instantiate the new model (unsurprisingly).\n\n= We will introduce a new loss: the Dice loss.\n\n= We will also look at an optimizer other than the venerable SGD we've used so\nfar. We'll stick with a popular one and use Adam.\n\n2C. TRAINING\n\nFigure 13.14 The outline of this chapter, with a focus on the changes needed for our\ntraining loop\n\nBut we will also step up our bookkeeping, by\n= Logging images for visual inspection of the segmentation to TensorBoard\n\n= Performing more metrics logging in TensorBoard\n= Saving our best model based on the validation\n\nOverall, the training script p2ch13/training.py is even more similar to what we used\nfor classification training in chapter 12 than the adapted code we've seen so far. Any\nsignificant changes will be covered here in the text, but be aware that some of the\nminor tweaks are skipped. For the full story, check the source.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.17,
                        "section_name": "Initializing our segmentation and augmentation models",
                        "section_path": "./screenshots-images-2/chapter_14/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_17/b34d91fb-9f4a-438f-bc4c-a32e4f425814.png",
                            "./screenshots-images-2/chapter_14/section_17/50be5fb9-eec4-4e78-816f-0bd5d3c02239.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Initializing our segmentation and augmentation models\n\nOur initModel method is very unsurprising. We are using the UNetWrapper class and\ngiving it our configuration parameters\u2014which we will look at in detail shortly. Also, we\nnow have a second model for augmentation. Just like before, we can move the model\nto the GPU if desired and possibly set up multi-GPU training using DataParallel. We\nskip these administrative tasks here.\n\nListing 13.22 training.py:133, .initModel\n\ndef initModel (self):\nsegmentation_model = UNetWrapper (\nin_channels=7,\n\nn_classes=1,\ndepth=3,\n\nwfi=4,\npadding=True,\nbatch_norm=True,\nup_mode='upconv',\n\n)\naugmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n\n# ... line 154\nreturn segmentation_model, augmentation_model\n\nFor input into UNet, we've got seven input channels: 3 + 3 context slices, and 1 slice\nthat is the focus for what we're actually segmenting. We have one output class indicat-\ning whether this voxel is part of a nodule. The depth parameter controls how deep the\nU goes; each downsampling operation adds 1 to the depth. Using wf=5 means the first\nlayer will have 2**wf == 32 filters, which doubles with each downsampling. We want\nthe convyolutions to be padded so that we get an output image the same size as our\ninput. We also want batch normalization inside the network after each activation func-\ntion, and our upsampling function should be an upconvolution layer, as implemented\nby nn. ConvTranspose2d (see util/unet.py, line 123).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.18,
                        "section_name": "Using the Adam optimizer",
                        "section_path": "./screenshots-images-2/chapter_14/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_18/efd4f136-f139-4fad-acec-1cca39cc68b5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using the Adam optimizer\n\nThe Adam optimizer (https://arxiv.org/abs/1412.6980) is an alternative to using\n\nSGD when training our models. Adam maintains a separate learning rate for each\n\nparameter and automatically updates that learning rate as training progresses. Due to\n\nthese automatic updates, we typically won\u2019t need to specify a non-default learning rate\n\nwhen using Adam, since it will quickly determine a reasonable learning rate by itself.\nHere\u2019s how we instantiate Adam in code.\n\nListing 13.23 training.py:156, .initOptimizer\n\ndef initOptimizer(self):\nreturn Adam(self.segmentation_model .parameters ())\n\nIt\u2019s generally accepted that Adam is a reasonable optimizer to start most projects\nwith.'* There is often a configuration of stochastic gradient descent with Nesterov\nmomentum that will outperform Adam, but finding the correct hyperparameters to\nuse when initializing SGD for a given project can be difficult and time consuming.\n\nThere have been a large number of variations on Adam\u2014AdaMax, RAdam,\nRanger, and so on\u2014that each have strengths and weaknesses. Delving into the details\nof those is outside the scope of this book, but we think that it\u2019s important to know that\nthose alternatives exist. We'll use Adam in chapter 13.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.19,
                        "section_name": "Dice loss",
                        "section_path": "./screenshots-images-2/chapter_14/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_19/70330206-28d0-4120-ba78-1f465f96f276.png",
                            "./screenshots-images-2/chapter_14/section_19/2a1e0aaf-f4f1-408d-99fb-8ef3996d2450.png",
                            "./screenshots-images-2/chapter_14/section_19/ba1214e3-eb4a-44f2-841e-f203293ac27f.png",
                            "./screenshots-images-2/chapter_14/section_19/bfc09d75-160b-4c9b-b644-45cd4432c53d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Dice loss\n\nThe Sgrensen-Dice coefficient (https://en.wikipedia.org/wiki/S%C3%B8rensen %E2\n%80%93Dice_coefficient), also known as the Dice loss, is a common loss metric for seg-\nmentation tasks. One advantage of using Dice loss over a per-pixel cross-entropy loss is\nthat Dice handles the case where only a small portion of the overall image is flagged as\npositive. As we recall from chapter 11 in section 11.10, unbalanced training data can be\nproblematic when using cross-entropy loss. That\u2019s exactly the situation we have here\u2014\nmost of a CT scan isn\u2019ta nodule. Luckily, with Dice, thatwon\u2019t pose as much ofa problem.\nThe Sgrensen-Dice coefficient is based on the ratio of correctly segmented pixels\nto the sum of the predicted and actual pixels. Those ratios are laid out in figure 13.15.\nOn the left, we see an illustration of the Dice score. It is twice the joint area (true posi-\ntives, swiped) divided by the sum of the entire predicted area and the entire ground-\ntruth marked area (the overlap being counted twice). On the right are two prototypi-\ncal examples of high agreement/high Dice score and low agreement/low Dice score.\n\nPREDICTED ACTUAL\n\n0.4\n\n8\n\n= DICE\n\nFigure 13.15 The ratios that make up the Dice score\n\nThat might sound familiar; it\u2019s the same ratio that we saw in chapter 12. We're basi-\ncally going to be using a per-pixel F1 score!\n\nNOTE This is a per-pixel Fl score where the \u201cpopulation\u201d is one image\u2019s pixels. Since\nthe population is entirely contained within one training sample, we can use it\nfor training directly. In the classification case, the F1 score is not calculable\nover a single minibatch, and, hence, we cannot use it for training directly.\n\nSince our label_g is effectively a Boolean mask, we can multiply it with our predic-\ntions to get our true positives. Note that we aren\u2019t treating prediction_devtensor asa\nBoolean here. A loss defined with it wouldn't be differentiable. Instead, we're replac-\ning the number of true positives with the sum of the predicted values for the pixels\nwhere the ground truth is 1. This converges to the same thing as the predicted values\napproach 1, but sometimes the predicted values will be uncertain predictions in the\n0.4 to 0.6 range. Those undecided values will contribute roughly the same amount to\nour gradient updates, no matter which side of 0.5 they happen to fall on. A Dice coef-\nficient utilizing continuous predictions is sometimes referred to as soft Dice.\n\nThere\u2019s one tiny complication. Since we're wanting a loss to minimize, we're going\nto take our ratio and subtract it from 1. Doing so will invert the slope of our loss func-\ntion so that in the high-overlap case, our loss is low; and in the low-overlap case, it\u2019s\nhigh. Here\u2019s what that looks like in code.\n\nListing 13.24 training.py:315, .diceLoss\n\nSums over everything except the batch dimension to The Dice ratio. To\nget the positively labeled, (softly) positively detected, avoid problems when we\nand (softly) correct positives per batch item accidentally have neither\n. . . predictions nor labels, we\n\ndef diceLoss(self, prediction_g, label_g, epsilon=1): \u2018add 1 to both numerator\n\n> diceLabel_g = label_g.sum(dim=[1,2,3)) and denominator.\n\ndicePrediction_g = prediction_g.sum(dim=[1,2,3])}\ndiceCorrect_g = (prediction_g * label_g) .sum(dim=[1,2,3])\n\ndiceRatio_g = (2 * diceCorrect_g + epsilon) \\\n/ (dicePrediction_g + diceLabel_g + epsilon) r+\n\n- . To make it a loss, we take 1 \u2014 Dice\nreturn 1 - diceRatio_g ratio, so lower loss is better.\n\nWe're going to update our computeBatchLoss function to call self.diceLoss. Twice.\nWe'll compute the normal Dice loss for the training sample, as well as for only the pixels\nincluded in label_g. By multiplying our predictions (which, remember, are floating-point\nvalues) times the label (which are effectively Booleans), we'll get pseudo-predictions that\ngot every negative pixel \u201cexactly right\u201d (since all the values for those pixels are multiplied\nby the false-is-zero values from label_g). The only pixels that will generate loss are the\nfalse negative pixels (everything that should have been predicted true, but wasn\u2019t). This\nwill be helpful, since recall is incredibly important for our overall project; after all, we can\u2019t\nclassify tumors properly if we don\u2019t detect them in the first place!\n\nListing 13.25 training.py:282, .computeBatchLoss\n\ndef computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g,\nclassificationThreshold=0.5):\ninput_t, label_t, series_list, _slice_ndx_list = batch_tup\nTransfers\n\ninput_g input_t.to(self.device, non_blocking=True) > to GPU\n\nlabel_g = label_t.to(self.device, non_blocking=True)\n\nAugments as needed if we are training.\nIn validation, we would skip this.\nif self.segmentation_model.training and self.augmentation_dict: <t-\u2014_\ninput_g, label_g = self.augmentation_model(input_g, label_g)\n\n7 Runs the segmentation\nprediction_g = self.segmentation_model (input_g) \u2014 model ...\ndiceLoss_g = self.diceLoss(prediction_g, label_g) 7\u2014 and applies\nfnLoss_g = self.diceLoss(prediction_g * label_g, label_g) our fine Dice loss\n# ... line 313\nreturn diceLoss_g.mean() + fnLoss_g.mean() * 8 t Oops. What is this?\n\nLet\u2019s talk a bit about what we\u2019re doing with our return statement of diceLoss_g\n-mean() + fnLoss_g.mean() * 8.\n\nLOSS WEIGHTING\n\nIn chapter 12, we discussed shaping our dataset so that our classes were not wildly\nimbalanced. That helped training converge, since the positive and negative samples\npresent in each batch were able to counteract the general pull of the other, and the\nmodel had to learn to discriminate between them to improve. We're approximating\nthat same balance here by cropping down our training samples to include fewer non-\npositive pixels; but it\u2019s incredibly important to have high recall, and we need to make\nsure that as we train, we're providing a loss that reflects that fact.\n\nWe are going to have a weighted loss that favors one class over the other. What we're\nsaying by multiplying fnLoss_g by 8 is that getting the entire population of our posi-\ntive pixels right is eight times more important than getting the entire population of\nnegative pixels right (nine, if you count the one in diceLoss_g). Since the area cov-\nered by the positive mask is much, much smaller than the whole 64 x 64 crop, that also\nmeans each individual positive pixel wields that much more influence when it comes\nto backpropagation.\n\nWe're willing to trade away many correctly predicted negative pixels in the general\nDice loss to gain one correct pixel in the false negative loss. Since the general Dice\nloss is a strict superset of the false negative loss, the only correct pixels available to\nmake that trade are ones that start as true negatives (all of the true positive pixels are\nalready included in the false negative loss, so there\u2019s no trade to be made).\n\nSince we're willing to sacrifice huge swaths of true negative pixels in the pursuit of\nhaving better recall, we should expect a large number of false positives in general.'*\nWe're doing this because recall is very, very important to our use case, and we\u2019d much\nrather have some false positives than even a single false negative.\n\nWe should note that this approach only works when using the Adam optimizer.\nWhen using SGD, the push to overpredict would lead to every pixel coming back as\npositive. Adam's ability to fine-tune the learning rate means stressing the false nega-\ntive loss doesn\u2019t become overpowering.\n\nCOLLECTING METRICS\n\nSince we\u2019re going to purposefully skew our numbers for better recall, let\u2019s see just how\ntilted things will be. In our classification computeBatchLoss, we compute various per-\nsample values that we used for metrics and the like. We also compute similar values for\nthe overall segmentation results. These true positive and other metrics were previ-\nously computed in logMetrics, but due to the size of the result data (recall that each\nsingle CT slice from the validation set is a quarter-million pixels!), we need to com-\npute these summary stats live in the computeBatchLoss function.\n\nListing 13.26 training.py:297, . comput: tchLoss\n\n_ * We threshold the Computing true\nstart_ndx = batch_ndx * batch_size 0 . puting\nend_ndx = start_ndx + input_t.size(0) prediction to get \u201chard positives, false\n\nDice but convert to float for | _ positives, and false\n\nwith torch.no_grad(): the later multiplication. | negatives is similar\n\npredictionBool_g = (prediction_g[:, 0:1] i we did\n> classificationThreshold) .to(torch.float32) < \u201c\nthe Dice loss.\ntp = ( predictionBool_g * label_g).sum(dim=[1,2,3]) Eas\nfn = ((1 - predictionBool_g) * label_g).sum(dim=[1,2,3])\nfp = ( predictionBool_g * (~label_g)) .sum(dim=[1,2,3])\nmetrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g \u201c3\n\nmetrics_g[METRICS_TP_NDX, start_ndx:end_ndx] tp We store cur metrics te a h\nmetrics_g[METRICS_FN_NDX, start_ndx:end_ndx] fn tensor for future reference. Tas\nmetrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp is per batch item rather than\n\naveraged over the batch.\n\nAs we discussed at the beginning of this section, we can compute our true positives and\nso on by multiplying our prediction (or its negation) and our label (or its negation)\ntogether. Since we\u2019re not as worried about the exact values of our predictions here (it\ndoesn\u2019t really matter if we flag a pixel as 0.6 or 0.9\u2014as long as it\u2019s over the threshold,\nwe'll call it part of a nodule candidate), we are going to create predictionBool_g by\ncomparing it to our threshold of 0.5.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.2,
                        "section_name": "Getting images into TensorBoard",
                        "section_path": "./screenshots-images-2/chapter_14/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_20/0c688b9e-2210-42d4-96be-bfdaac318405.png",
                            "./screenshots-images-2/chapter_14/section_20/37ed9425-3bc3-4f56-bbb9-25b168f20efb.png",
                            "./screenshots-images-2/chapter_14/section_20/e902eba3-2dc1-4322-9e3e-a21d6106815a.png",
                            "./screenshots-images-2/chapter_14/section_20/014bf3da-31a8-4a41-9784-05c8bf057132.png",
                            "./screenshots-images-2/chapter_14/section_20/8a2042aa-1b5b-4be0-ab63-99d4dab2a15a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Getting images into TensorBoard\n\nOne of the nice things about working on segmentation tasks is that the output is easily\nrepresented visually. Being able to eyeball our results can be a huge help for determin-\ning whether a model is progressing well (but perhaps needs more training), or if it has\ngone off the rails (so we need to stop wasting our time with further training). There\nare many ways we could package up our results as images, and many ways we could dis-\nplay them. TensorBoard has great support for this kind of data, and we already have\nTensorBoard SummaryWriter instances integrated with our training runs, so we're\ngoing to use TensorBoard. Let\u2019s see what it takes to get everything hooked up.\n\nWe'll add a logImages function to our main application class and call it with both\nour training and validation data loaders. While we are at it, we will make another\n\nTrains\nfor one\n\nepoch\n\nchange to our training loop: we\u2019re only going to perform validation and image log-\nging on the first and then every fifth epoch. We do this by checking the epoch num-\nber against a new constant, validation_cadence.\n\nWhen training, we\u2019re trying to balance a few things:\n\n= Getting a rough idea of how our model is training without having to wait very\n\nlong\n= Spending the bulk of our GPU cycles training, rather than validating\n= Making sure we are still performing well on the validation set\n\nThe first point means we need to have relatively short epochs so that we get to call\nlogMetrics more often. The second, however, means we want to train for a relatively\nlong time before calling doValidation. The third means we need to call doValidation\nregularly, rather than once at the end of training or something unworkable like that. By\nonly doing validation on the first and then every fifth epoch, we can meet all of those\ngoals. We get an early signal of training progress, spend the bulk of our time training,\nand have periodic check-ins with the validation set as we go along.\n\nListing 13.27  training.py:210, SegmentationTrainingApp.main\n\ndef main(self): Our outermost loop,\n# ... line 217 over the epochs\nself.validation_cadence = 5\n\nfor epoch_ndx in range(1, self.cli_args.epochs + 1): <}\u2014~\n# ... line 228 Logs the (scalar)\n\nrt trnMetrics_t = self.doTraining(epoch_ndx, train_dl) metrics (rom training\nself.logMetrics(epoch_ndx, \u2018trn', trnMetrics_t) po\n\nif epoch_ndx == 1 or epoch_ndx % self.validation_cadence == 0: <q\u2014\u2014~\n\n# line 239\n-7 Only every validatior\n> self.logImages(epoch_ndx, \u2018trn', train_dl) \u2018 ye th iota f] \"\n\nself.logImages(epoch_ndx, \u2018'val', val_dl)\n... we validate the model and log images.\n\nThere isn\u2019t a single right way to structure our image logging. We are going to grab a\nhandful of CTs from both the training and validation sets. For each CT, we will select 6\nevenly spaced slices, end to end, and show both the ground truth and our model\u2019s\noutput. We chose 6 slices only because TensorBoard will show 12 images at a time, and\nwe can arrange the browser window to have a row of label images over the model out-\nput. Arranging things this way makes it easy to visually compare the two, as we can see\nin figure 13.16.\n\nAlso note the small slider-dot on the prediction images. That slider will allow us to\nview previous versions of the images with the same label (such as val/0_prediction_3,\nbut at an earlier epoch). Being able to see how our segmentation output changes over\ntime can be useful when we're trying to debug something or make tweaks to achieve a\nspecific result. As training progresses, TensorBoard will limit the number of images\n\nPOSITIVE\nLABEL\n\nNo LABEL\n\nEPOCH\nSLIDER\n\n8 a\n\nFALSE\nPOSITIVES\n\nPOSITWE\nPREDICTION\n\nFigure 13.16 Top row: label data for training. Bottom row: output from the segmentation\n\nviewable from the slider to 10, probably to avoid overwhelming the browser with a huge\nnumber of images.\n\nThe code that produces this output starts by getting 12 series from the pertinent\ndata loader and 6 images from each series.\n\n28 training.py:326, . logImages\n\ndef logImages(self, epoch_ndx, mode_str, dl):\nodel.eval() + Sets the model to eval\n\nself.segmentati\n\nimages = sorted(dl.dataset.series_ list) [:12]\n\nTakes (the same) 12 CTs by\nbypassing the data loader and using\nthe dataset directly. The series list\nmight be shuffled, so we sort.\n\nries_ndx, series_uid in enumerate(images) :\n\net = getCt (series_uid)\n\nfor slice_ndx in range (6):\nct_ndx = slice_ndx * (ct.hu_a.shape[0] - 1) // 5\nsample_tup = dl.dataset.getitem_fullslice(series_uid, ct_ndx)\nct_t, label_t, series_uid, ct_ndx = sample_tup Selects six equidistant\n\nslices throughout the CT\n\nAfter that, we feed ct_t it into the model. This looks very much like what we see in\ncomputeBatchLoss; see p2ch13/training. py for details if desired.\n\nOnce we have prediction_a, we need to build an image_a that will hold RGB values\nto display. We\u2019re using np.float32 values, which need to be in a range from 0 to 1.\n\nOur approach will cheat a little by adding together various images and masks to get\ndata in the range 0 to 2, and then multiplying the entire array by 0.5 to get it back into\nthe right range.\n\nListing 13.29 training.py:346, . logImages\n\nCT intensity is assigned to all RGB channels\nto provide a grayscale base image.\n\net_t(:-1,:,:] /= 2000\netlt(:-1,:,:] += 0.5\n\netSlice_a = ct_t[(dl.dataset.contextSlices_count] .numpy()\n\nimage_a = np.zeros((512, 512, 3), dtype=np.float32) False positives are flagged as\nimage_a[:,:,:] = ctSlice_a.reshape((512,512,1)) red and overlaid on the image.\nimage_a[:,:,0] += prediction_a & (1 - label_a)\n\nimage_a[:,:,0] += (1 - prediction_a) & label_a + False negatives\nimage_a[:,:,1] += ((1 - prediction_a) & label_a) * 0.5 are orange.\nimage_a[:,:,1] += prediction_a & label_a True positives\n\nimage_a *= 0.5\n\nare green.\nimage_a.clip(0, 1, image_a)\n\nOur goal is to have a grayscale CT at half intensity, overlaid with predicted-nodule (or,\nmore correctly, nodule-candidate) pixels in various colors. We're going to use red for\nall pixels that are incorrect (false positives and false negatives). This will mostly be\nfalse positives, which we don\u2019t care about too much (since we\u2019re focused on recall).\n1 - label_a inverts the label, and that multiplied by the prediction_a gives us only\nthe predicted pixels that aren\u2019t in a candidate nodule. False negatives get a half-\nstrength mask added to green, which means they will show up as orange (1.0 red and\n0.5 green renders as orange in RGB). Every correctly predicted pixel inside a nodule\nis set to green; since we got those pixels right, no red will be added, and so they will\nrender as pure green.\n\nAfter that, we renormalize our data to the 0...1 range and clamp it (in case we start\ndisplaying augmented data here, which would cause speckles when the noise was out-\nside our expected CT range). All that remains is to save the data to TensorBoard.\n\nListing 13.30 training.py:361, . logImage\n\nwriter = getattr(self, mode_str + '_writer')\n\nwriter .add_image(\n\u00a3'{mode_str)} /{series_ndx)_prediction_{slice_ndx}',\nimage_a,\nself.totalTrainingSamples_count,\ndataformats='HWC',\n\nThis looks very similar to the writer .add_scalar calls we've seen before. The data-\nformats='HWC' argument tells TensorBoard that the order of axes in our image has\nour RGB channels as the third axis. Recall that our network layers often specify out-\nputs that are Bx Cx Hx W, and we could put that data directly into TensorBoard as\nwell if we specified 'CHw'.\n\nWe also want to save the ground truth that we\u2019re using to train, which will form the\ntop row of our TensorBoard CT slices we saw earlier in figure 13.16. The code for that\n\nis similar enough to what we just saw that we'll skip it. Again, check p2ch13/training.py\nif you want the details.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.21,
                        "section_name": "Updating our metrics logging",
                        "section_path": "./screenshots-images-2/chapter_14/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_21/74e52907-9d29-4fc4-a7d8-e75dc7caee7c.png",
                            "./screenshots-images-2/chapter_14/section_21/42446dac-56b7-4536-af8a-fc378b789a42.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "_ Updating our metrics logging\nTo give us an idea how we are doing, we compute per-epoch metrics: in particular,\ntrue positives, false negatives, and false positives. This is what the following listing\ndoes. Nothing here will be particularly surprising.\n\nListing 13.31 training.py:400, .logMetrics\n\nsum_a = metrics_a.sum(axis=1)\nallLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n\nmetrics_dict['percent_all/tp'] = \\ Can be larger than 100%\nsum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100 since we're comparing to\n\nmetrics_dict['percent_all/fn'] = \\ the total number of pixels\nsum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100 labeled as candidate\n\nmetrics_dict['percent_all/fp'] = \\ nodules, which is a tiny\nsum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100 fraction of each image\n\nWe are going to start scoring our models as a way to determine whether a particular\ntraining run is the best we've seen so far. In chapter 12, we said we'd be using the F1\nscore for our model ranking, but our goals are different here. We need to make sure\nour recall is as high as possible, since we can\u2019t classify a potential nodule if we don\u2019t\nfind it in the first place!\n\nWe will use our recall to determine the \u201cbest\u201d model. As long as the F1 score is rea-\nsonable for that epoch,'\u00ae we just want to get recall as high as possible. Screening out\nany false positives will be the responsibility of the classification model.\n\nListing 13.32 training.py:393, .logMetrics\n\ndef logMetrics(self, epoch_ndx, mode_str, metrics_t):\n# ... line 453\nscore = metrics_dict['pr/recall']\n\nreturn score\n\nWhen we add similar code to our classification training loop in the next chapter, we'll\nuse the FI score.\n\nBack in the main training loop, we'll keep track of the best_score we've seen so\nfar in this training run. When we save our model, we'll include a flag that indicates\nwhether this is the best score we've seen so far. Recall from section 13.6.4 that we\u2019re\nonly calling the doValidation function for the first and then every fifth epochs. That\nmeans we\u2019re only going to check for a best score on those epochs. That shouldn't be a\nproblem, but it\u2019s something to keep in mind if you need to debug something happen-\ning on epoch 7. We do this checking just before we save the images.\n\nListing 13.33  training.py:21) gmentationTrainingApp.ma\n\ndef main(self): The epoch-loop\n\nbest_score = 0.0 we already saw Computes th\nfor epoch_ndt in range(1, self.cli_args.epochs + 1): score. bo we emt\n# if validation is wanted earlier, we take\n# ... line 233 the recall.\nvalMetrics_t = self.doValidation(epoch_ndx, val_dl)\nscore = self.logMetrics(epoch_ndx, \u2018val', valMetrics_t)\nbest_score = max(score, best_score)\n\nself.saveModel('seg', epoch_ndx, score == best_score) a\n\nNow we only need to write saveModel. The third parameter\nis whether we want to save it as best model, too.\n\nLet\u2019s take a look at how we persist our model to disk.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.22,
                        "section_name": "Saving our model",
                        "section_path": "./screenshots-images-2/chapter_14/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_22/fc525afb-3ce9-48cf-aabd-ae60f3bae22e.png",
                            "./screenshots-images-2/chapter_14/section_22/c8032e3d-d311-4e38-a3f5-54cb5daa0bdf.png",
                            "./screenshots-images-2/chapter_14/section_22/e3a8d1f2-643c-40dc-9652-a704e4c28192.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "| Saving our model\n\nPyTorch makes it pretty easy to save our model to disk. Under the hood, torch. save\nuses the standard Python pickle library, which means we could pass our model\ninstance in directly, and it would save properly. That's not considered the ideal way to\npersist our model, however, since we lose some flexibility.\n\nInstead, we will save only the parameters of our model. Doing this allows us to load\nthose parameters into any model that expects parameters of the same shape, even if\nthe class doesn\u2019t match the model those parameters were saved under. The save-\nparameters-only approach allows us to reuse and remix our models in more ways than\nsaving the entire model.\n\nWe can get at our model\u2019s parameters using the model.state_dict() function.\n\nListing 13.34 training.py:480, . saveModel\n\ndef saveModel(self, type_str, epoch_ndx, isBest=False):\n\n# ... line 496\nmodel = self.segmentation_model\nif isinstance(model, torch.nn.DataParallel): Gets rid of the DataParallel\n\nmodel = model.module wrapper, if it exists\n\nstate = {\n\u2018syS_argv': sys.argv,\n\u2018time\u2019: str(datetime.datetime.now()),\n*model_state': model.state_dict(), <+\u2014\u2014 The important part\n*model_name': type(model).__name__,\n\u2018optimizer_state' : self.optimizer.state_dict(), Preserves momentum,\n\u2018optimizer_name': type(self.optimizer).__name__, and so on\n\u2018epoch': epoch_ndx,\n*totalTrainingSamples_count': self.totalTrainingSamples_count,\n}\ntorch.save(state, file path)\n\nWe set file_path to something like data-unversioned/part2/models/p2ch13/\nseg_2019-07-10_02.17.22_ch12.50000.state. The .50000. part is the number of\ntraining samples we've presented to the model so far, while the other parts of the path\nare obvious.\n\nTIP By saving the optimizer state as well, we could resume training seamlessly.\nWhile we don\u2019t provide an implementation of this, it could be useful ifyour access\nto computing resources is likely to be interrupted. Details on loading a model and\noptimizer to restart training can be found in the official documentation\n(https: // pytorch.org/tutorials/beginner/saving_loading_models.hunl).\n\nIf the current model has the best score we've seen so far, we save a second copy of\nstate with a .best.state filename. This might get overwritten later by another, higher-\nscore version of the model. By focusing only on this best file, we can divorce custom-\ners of our trained model from the details of how each epoch of training went (assum-\ning, of course, that our score metric is of high quality).\n\nListing 13.35 training.py:514, . Model\nif isBest:\nbest_path = os.path.join(\n\u2018data~unversioned', \u2018part2', 'models',\n\nself.cli_args.tb_prefix,\n\u00a3'{type_str}_{self.time_str}_{self.cli_args.comment)}.best.state')\nshutil.copyfile(file_ path, best_path)\n\nlog.info(\"Saved model params to ({}\".format (best_path) )\n\nwith open(file_path, 'rb') as f:\nlog.info(\"SHA1: \" + hashlib.shal(f.read()) .hexdigest())\n\nWe also output the SHAI of the model we just saved. Similar to sys.argv and the\ntimestamp we put into the state dictionary, this can help us debug exactly what model\nwe\u2019re working with if things become confused later (for example, if a file gets\nrenamed incorrectly).\n\nWe will update our classification training script in the next chapter with a similar\nroutine for saving the classification model. In order to diagnose a CT, we'll need to\nhave both models.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.23,
                        "section_name": "Results",
                        "section_path": "./screenshots-images-2/chapter_14/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_23/f3bfcbc0-46a4-4019-a554-05717ca5c5c8.png",
                            "./screenshots-images-2/chapter_14/section_23/c2d65376-42d2-4491-bff1-b0e1535ed43c.png",
                            "./screenshots-images-2/chapter_14/section_23/104b0266-c4b3-41a0-9366-58f9f242b6cf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Results\n\nNow that we've made all of our code changes, we've hit the last section in step 3 of fig-\nure 13.17. It\u2019s time to run python -m p2ch13.training --epochs 20 --augmented\nfinal_seg. Let\u2019s see what our results look like!\n\n\\. SEGMENTATION 2. UPDATE:\nUNET 5 9 2A. MODEL 4\n| So Seo\nre S O\u00b0 1/F 3 e oO\n\nim ym 28. DATASET\n[ em |\n\n= o\u00ae of\n\n3. RESULTS 2C. TRAINING\n\nFigure 13.17 The outline of this chapter, with a focus on the results we see from training\n\nHere is what our training metrics look like if we limit ourselves to the epochs we have\nvalidation metrics for (we'll be looking at those metrics next, so this will keep it an\napples-to-apples comparison):\n\nIn these rows, we are particularly interested TPs are trending up, too. Great! And\nin the F1 score\u2014it is trending up. Good! FNs and FPs are trending down.\n\nEl trn 0.5235 loss, 0.2276 precision, 0.9381 recall, 0.3663 \u00a31 score\n\nEl trniall 0.5235 loss, 93.8% tp, 6.2% fn, 318.4% fp a\n& ES trn 0.2537 loss, 0.5652 precision, 0.9377 recall, 0.7053 \u00a31 score\n\nE5 trn_all 0.2537 loss, 93.8% tp, 6.2% fn, 72.1% fp \u201c\nm E10 trn 0.2335 loss, 0.6011 precision, 0.9459 recall, 0.7351 f1 score\n\nE10 trn_all 0.2335 loss, 94.6% tp, 5.4% fn, 62.8% fp a\n\ne\n\nE15 trn 0.2226 loss, 0.6234 precision, 0.9536 recall, 0.7540 \u00a31 score\n\nE15 trn_all 0.2226 loss, 95.4% tp, <2> 4.6% fn, 57.6% fp\nte E20 trn 0.2149 loss, 0.6368 precision, 0.9584 recall, 0.7652 \u00a31 score\nE20 trn_all 0.2149 loss, 95.8% tp, <2> 4.2% fn, 54.7% fp +\nIn these rows, we are particularly interested TPs are trending up, too. Great! And\nin the F1 score\u2014it is trending up. Good! FNs and FPs are trending down.\n\nOverall, it looks pretty good. True positives and the F1 score are trending up, false\npositives and negatives are trending down. That\u2019s what we want to see! The validation\nmetrics will tell us whether these results are legitimate. Keep in mind that since we're\ntraining on 64 x 64 crops, but validating on whole 512 x 512 CT slices, we are almost\ncertainly going to have drastically different TP:FN:FP ratios. Let\u2019s see:\n\nThe highest TP rate (great). Note that the TP rate is the same\nas recall. But FPs are 4495%\u2014that sounds like a lot.\n\nEl val 0.9441 loss, 0.0219 precision, 0.8131 recall, 0.0426 \u00a31 score\nEl val_all 0.9441 loss, 81.3% tp, 18.7% fn, 3637.5% fp\n\nE5 val 0.9009 loss, 0.0332 precision, 0.8397 recall, 0.0639 \u00a31 score\nE5 val_all 0.9009 loss, 84.0% tp, 16.0% fn, 2443.0% fp\n\nE10 val 0.9518 loss, 0.0184 precision, 0.8423 recall, 0.0360 \u00a31 score\nE10 val_all 0.9518 loss, 84.2% tp, 15.8% fn, 4495.0% fp\n\nE15 val 0.8100 loss, 0.0610 precision, 0.7792 recall, 0.1132 \u00a31 score\nE15 val_all 0.8100 loss, 77.9% tp, 22.1% fn, 1198.7% fp\n\nE20 val 0.8602 loss, 0.0427 precision, 0.7691 recall, 0.0809 \u00a31 score\nE20 val_all 0.8602 loss, 76.9% tp, 23.1% fn, 1723.9% fp\n\nOuch\u2014false positive rates over 4,000%? Yes, actually, that\u2019s expected. Our validation\nslice area is 2'* pixels (512 is 2\u00b0), while our training crop is only 2'*. That means we're\nvalidating on a slice surface that\u2019s 2\u00b0 = 64 times bigger! Having a false positive count\nthat\u2019s also 64 times bigger makes sense. Remember that our true positive rate won't\nhave changed meaningfully, since it would all have been included in the 64 x 64 sam-\nple we trained on in the first place. This situation also results in very low precision,\nand, hence, a low F1 score. That's a natural result of how we've structured the training\nand validation, so it\u2019s not a cause for alarm.\n\nWhat\u2019s problematic, however, is our recall (and, hence, our true positive rate). Our\nrecall plateaus between epochs 5 and 10 and then starts to drop. It\u2019s pretty obvious that\nwe begin overfitting very quickly, and we can see further evidence of that in figure\n13.18\u2014while the training recall keeps trending upward, the validation recall decreases\nafter 3 million samples. This is how we identified overfitting in chapter 5, in particular\nfigure 5.14.\n\nrecall\n\ntag: seg_pr/recall TRAINING eT\n\noe ere\n\n0.92\n\n088 PLATEAU\n\n0.84\n\nOVERFITTING\n\nFigure 13.18 The validation\nset recall, showing signs of\n\n0.8\n\n500K 15M 25M 3.5M 4.5M 55M overfitting when recall goes\nVALIDATION DATASET eee nen 40 Ganition\n\nNOTE Always keep in mind that TensorBoard will smooth your data lines by\ndefault. The lighter ghost line behind the solid color shows the raw values.\n\nThe U-Net architecture has a lot of capacity, and even with our reduced filter and\ndepth counts, it\u2019s able to memorize our training set pretty quickly. One upside is that\nwe don\u2019t end up needing to train the model for very long!\n\nRecall is our top priority for segmentation, since we'll let issues with precision be\nhandled downstream by the classification models. Reducing those false positives is the\nentire reason we have those classification models! This skewed situation does mean it\nis more difficult than we'd like to evaluate our model. We could instead use the F2\nscore, which weights recall more heavily (or F5, or F10 ...), but we\u2019d have to pick an N\nhigh enough to almost completely discount precision. We'll skip the intermediates\nand just score our model by recall, and use our human judgment to make sure a given\ntraining run isn\u2019t being pathological about it. Since we\u2019re training on the Dice loss,\nrather than directly on recall, it should work out.\n\nThis is one of the situations where we are cheating a little, because we (the\nauthors) have already done the training and evaluation for chapter 14, and we know\nhow all of this is going to turn out. There isn\u2019t any good way to look at this situation\nand know that the results we're seeing will work. Educated guesses are helpful, but\nthey are no substitute for actually running experiments until something clicks.\n\nAs it stands, our results are good enough to use going forward, even if our metrics\nhave some pretty extreme values. We're one step closer to finishing our end-to-end\nproject!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 14.24,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_14/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_14/section_24/3c2ccbf3-e909-4041-9a04-0cfa88328ffb.png",
                            "./screenshots-images-2/chapter_14/section_24/ec954d95-1883-4a06-b0a0-580329bfd44f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\n\nIn this chapter, we've discussed a new way of structuring models for pixel-to-pixel seg-\nmentation; introduced U-Net, an off-the-shelf, proven model architecture for those\nkinds of tasks; and adapted an implementation for our own use. We've also changed\nour dataset to provide data for our new model\u2019s training needs, including small crops\n\nfor training and a limited set of slices for validation. Our training loop now has the\nability to save images to TensorBoard, and we have moved augmentation from the\ndataset into a separate model that can operate on the GPU. Finally, we looked at our\ntraining results and discussed how even though the false positive rate (in particular)\nlooks different from what we might hope, our results will be acceptable given our\nrequirements for them from the larger project. In chapter 14, we will pull together\nthe various models we've written into a cohesive, end-to-end whole.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 15,
                "chapter_name": "End-to-end\nnodule analysis,\nand where to go next",
                "chapter_path": "./screenshots-images-2/chapter_15",
                "sections": [
                    {
                        "section_id": 15.1,
                        "section_name": "End-to-end\nnodule analysis,\nand where to go next",
                        "section_path": "./screenshots-images-2/chapter_15/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_1/5cd9fb53-87b5-4ec1-9151-fdb49b6ac322.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Over the past several chapters, we have built a decent number of systems that are\nimportant components of our project. We started loading our data, built and\nimproved classifiers for nodule candidates, trained segmentation models to find\nthose candidates, handled the support infrastructure needed to train and evaluate\nthose models, and started saving the results of our training to disk. Now it\u2019s time to\nunify the components we have into a cohesive whole, so that we may realize the full\ngoal of our project: it\u2019s time to automatically detect cancer.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.2,
                        "section_name": "Towards the finish line",
                        "section_path": "./screenshots-images-2/chapter_15/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_2/5dc30267-d523-47f3-beb8-2a506950b8ac.png",
                            "./screenshots-images-2/chapter_15/section_2/4fa05cd5-d39f-4fe9-b905-7fb2014e4815.png",
                            "./screenshots-images-2/chapter_15/section_2/a740dda9-0517-4152-be0a-5b2d7b3ac58f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Towards the finish line\n\nWe can get a hint of the work remaining by looking at figure 14.1. In step 3 (grouping)\nwe see that we still need to build the bridge between the segmentation model from\nchapter 13 and the classifier from chapter 12 that will tell us whether what the segmen-\ntation network found is, indeed, a nodule. On the right is step 5 (nodule analysis and\ndiagnosis), the last step to the overall goal: seeing whether a nodule is cancer. This is\nanother classification task; but to learn something in the process, we'll take a fresh\nangle at how to approach it by building on the nodule classifier we already have.\n\nSTEP 3 (Ch. 14):\nGROUPING\n\nSTEP 5 (CH. i):\nNODULE ANALYSIS.\nAND DIAGNOSIS.\n\nFigure 14.1 Our end-to-end lung cancer detection project, with a focus on this chapter's\ntopics: steps 3 and 5, grouping and nodule analysis\n\nOf course, these brief descriptions and their simplified depiction in figure 14.1 leave\nout a lot of detail. Let's zoom in a little with figure 14.2 and see what we've got left to\naccomplish.\n\nAs you can see, three important tasks remain. Each item in the following list corre-\nsponds to a major line item from figure 14.2:\n\na Generate nodule candidates. This is step 3 in the overall project. Three tasks go\ninto this step:\n\na Segmentation\u2014The segmentation model from chapter 13 will predict if a\ngiven pixel is of interest: if we suspect it is part of a nodule. This will be done\nper 2D slice, and every 2D result will be stacked to form a 3D array of voxels\ncontaining nodule candidate predictions.\n\n|. NODULE CANDIDATE GENERATION\n\nIA. SEGMENTATION 1B. GROUPING \\C. SAMPLE TUPLES\n\u2122 So i _& w+ TRC),\n>. at a -\ndenies (... IRC))\n\n2. NODULE AND MALIGNANCY CLASSIFICATION\n\n2A, NODULE CLASSIFICATION 28. ROC/AUC METRICS  2C. FINE-TUNING MALIGNANCY MODEL\n\n3. END-TO-END perecion\n\na \u2122\nBA. (.. IRC) 38. ISNODULEP 3\u00a2.1S viccnant?\nYN 7 Nr\n\nFigure 14.2 A detailed look at the work remaining for our end-to-end project\n\n\u00bb Grouping\u2014We will group the voxels into nodule candidates by applying a thresh-\nold to the predictions, and then grouping connected regions of flagged voxels.\n\ne Constructing sample tuples\u2014Each identified nodule candidate will be used to\nconstruct a sample tuple for classification. In particular, we need to produce\nthe coordinates (index, row, column) of that nodule\u2019s center.\n\nOnce this is achieved, we will have an application that takes a raw CT scan from a\npatient and produces a list of detected nodule candidates. Producing such a list is the\ntask in the LUNA challenge. If this project were to be used clinically (and we\nreemphasize that our project should not be!), this nodule list would be suitable for\ncloser inspection by a doctor.\n\n2 Classify nodules and malignancy. We'll take the nodule candidates we just pro-\nduced and pass them to the candidate classification step we implemented in\nchapter 12, and then perform malignancy detection on the candidates flagged\nas nodules:\n\na Nodule classification\u2014Each nodule candidate from segmentation and group-\ning will be classified as either nodule or non-nodule. Doing so will allow us to\nscreen out the many normal anatomical structures flagged by our segmenta-\n\ntion process.\n\nROC/AUC metrics\u2014Before we can start our last classification step, we'll define\nsome new metrics for examining the performance of classification models, as\nwell as establish a baseline metric against which to compare our malignancy\nclassifiers.\n\nFine-tuning the malignancy model\u2014Once our new metrics are in place, we will\ndefine a model specifically for classifying benign and malignant nodules,\ntrain it, and see how it performs. We will do the training by fine-tuning: a\nprocess that cuts out some of the weights of an existing model and replaces\nthem with fresh values that we then adapt to our new task.\n\nAt that point we will be within arm\u2019s reach of our ultimate goal: to classify nodules into\nbenign and malignant classes and then derive a diagnosis from the CT. Again, diag-\n\nnosing lung cancer in the real world involves much more than staring at a CT scan, so\nour performing this diagnosis is more an experiment to see how far we can get using\ndeep learning and imaging data alone.\n\n3 End-to-end detection. Finally, we will put all of this together to get to the finish\nline, combining the components into an end-to-end solution that can look at a\nCT and answer the question \u201cAre there malignant nodules present in the\nlungs?\u201d\n\nIRC\u2014We will segment our CT to get nodule candidate samples to classify.\nDetermine the nodules\u2014We will perform nodule classification on the candidate\nto determine whether it should be fed into the malignancy classifier.\nDetermine malignancy\u2014We will perform malignancy classification on the nod-\nules that pass through the nodule classifier to determine whether the patient\nhas cancer.\n\nWe've got a lot to do. To the finish line!\n\nNOTE As in the previous chapter, we will discuss the key concepts in detail in\nthe text and leave out the code for repetitive, tedious, or obvious parts. Full\ndetails can be found in the book\u2019s code repository.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.3,
                        "section_name": "Independence of the validation set",
                        "section_path": "./screenshots-images-2/chapter_15/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_3/7fdc0546-7601-40dd-aa04-bbb0b688b57c.png",
                            "./screenshots-images-2/chapter_15/section_3/bc72c42e-21a6-44f9-92c2-5fd5fd258bc2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Independence of the validation set\n\nWe are in danger of making a subtle but critical mistake, which we need to discuss and\navoid: we have a potential leak from the training set to the validation set! For each of\nthe segmentation and classification models, we took care of splitting the data into a\ntraining set and an independent validation set by taking every tenth example for vali-\ndation and the remainder for training.\n\nHowever, the split for the classification model was done on the list of nodules, and\nthe split for the segmentation model was done on the list of CT scans. This means we\nlikely have nodules from the segmentation validation set in the training set of the clas-\nsification model and vice versa. We must avoid that! If left unfixed, this situation could\nlead to performance figures that would be artificially higher compared to what we\n\nwould obtain on an independent dataset. This is called a leak, and it would invalidate\nour validation.\n\nTo rectify this potential data leak, we need to rework the classification dataset to also\nwork at the CT scan level, just as we did for the segmentation task in chapter 13. Then\nwe need to retrain the classification model with this new dataset. On the bright side, we\ndidn\u2019t save our classification model earlier, so we would have to retrain anyway.\n\nYour takeaway from this should be to keep an eye on the end-to-end process when\ndefining the validation set. Probably the easiest way to do this (and the way it is done\nfor most important datasets) is to make the validation split as explicit as possible\u2014for\nexample, by having two separate directories for training and validation\u2014and then\nstick to this split for your entire project. When you need to redo the split (for exam-\nple, when you need to add stratification of the dataset split by some criterion), you\nneed to retrain all of your models with the newly split dataset.\n\nSo what we did for you was to take LunaDataset from chapters 10-12 and copy\nover getting the candidate list and splitting it into test and validation datasets from\nLuna2dSegmentationDataset in chapter 13. As this is very mechanical, and there is\nnot much to learn from the details (you are a dataset pro by now), we won't show the\ncode in detail.\n\nWe'll retrain our classification model by rerunning the training for the classifier:!\n\n$ python3 -m p2chl4.training --num-workers=4 --epochs 100 nodule-nonnodule\n\nAfter 100 epochs, we achieve about 95% accuracy for positive samples and 99% for\nnegative ones. As the validation loss isn\u2019t seen to be trending upward again, we could\ntrain the model longer to see if things continued to improve.\n\nAfter 90 epochs, we reach the maximal F1 score and have 99.2% validation accu-\nracy, albeit only 92.8% on the actual nodules. We'll take this model, even though we\nmight also try to trade a bit of overall accuracy for better accuracy on the malignant\nnodules (in between, the model got 95.4% accuracy on actual nodules for 98.9% total\naccuracy). This will be good enough for us, and we are ready to bridge the models.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.4,
                        "section_name": "Bridging CT segmentation and nodule candidate classification",
                        "section_path": "./screenshots-images-2/chapter_15/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_4/48e25dc2-0fc3-411d-92f0-82c33bd02999.png",
                            "./screenshots-images-2/chapter_15/section_4/abb7e29c-1806-47da-b3bb-b776810e7ad5.png",
                            "./screenshots-images-2/chapter_15/section_4/10ee5e7a-0e50-4034-963b-977d84db992f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Bridging CT segmentation and nodule candidate classification\n\nNow that we have a segmentation model saved from chapter 13 and a classification\nmodel we just trained in the previous section, figure 14.3, steps la, 1b, and 1c show\nthat we\u2019re ready to work on writing the code that will convert our segmentation out-\nput into sample tuples. We are doing the grouping: finding the dashed outline around\nthe highlight of step 1b in figure 14.3. Our input is the segmentation: the voxels flagged\nby the segmentation model in la. We want to find Ic, the coordinates of the center of\nmass of each \u201clump\u201d of flagged voxels: the index, row, and column of the 1b plus mark\nis what we need to provide in the list of sample tuples as output.\n\n(COs TRO),\n\n\u2014\n\n(1. TRC))\n\n2. NODULE AND MALIGNANCY CLASSIFICATION\n\n2A. NODULE CLASSIFICATION 28. ROC/AUC METRICS 2C. FINE-TUNING MALIGNANCY MODEL\n\nfo\nAP \u00b0\nyo _\n: TAN\n\n3. END-TO-END DETECTION\n\n3A.(.., IRC) 38. ISNODULEP 3\u00a2. IS MALIGNANT? |\n\nFigure 14.3 Our plan for this chapter, with a focus on grouping segmented voxels into nodule candidates\n\nRunning the models will naturally look very similar to how we handled them during\ntraining and validation (validation in particular). The difference here is the loop over\nthe CTs. For each CT, we segment every slice and then take all the segmented output as\nthe input to grouping. The output from grouping will be fed into a nodule classifier,\nand the nodules that survive that classification will be fed into a malignancy classifier.\n\nThis is accomplished by the following outer loop over the CTs, which for each CT\nsegments, groups, classifies candidates, and provides the classifications for further\nprocessing.\n\nListing 14.1 nodule_analysis.py:324, NoduleAnalysisApp.main\n\nLoops over the series UIDs Gets the CT (step 1 in the big picture)\n\u00a9 for _, series_uid in series_iter: Runs our segmentation\nct = getCt (series_uid) model on it (step 2)\n\nmask_a = self.segmentCt(ct, series_uid)\nGroups the flagged voxels\ncandidateInfo_list = self.groupSegmentationOutput( <\u00ab in the output (step 3)\nseries_uid, ct, mask_a)\nclassifications_list = self.classifyCandidates( ~\u2014. Runs our nodule classifier\net, candidateInfo_list) on them (step 4)\n\nWe'll break down the segmentCt, groupSegmentationOutput, and classifyCandi-\ndates methods in the following sections.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.5,
                        "section_name": "Segmentation",
                        "section_path": "./screenshots-images-2/chapter_15/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_5/618c782a-d9d6-4968-9b09-76752055b2f6.png",
                            "./screenshots-images-2/chapter_15/section_5/501a5bd9-0ed5-4264-828a-93e66688d73e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "14.3.1 Segmentation\n\nFirst up, we are going to perform segmentation on every slice of the entire CT scan.\nAs we need to feed a given patient's CT slice by slice, we build a Dataset that loads a\nCT with a single series_uid and returns each slice, one per ___getitem__ call.\n\nNOTE The segmentation step in particular can take quite a while when exe-\ncuted on the CPU. Even though we gloss over it here, the code will use the\nGPU if available.\n\nOther than the more expansive input, the main difference is what we do with the out-\nput. Recall that the output is an array of per-pixel probabilities (that is, in the range\n0...1) that the given pixel is part of a nodule. While iterating over the slices, we collect\nthe slice-wise predictions in a mask array that has the same shape as our CT input.\nAfterward, we threshold the predictions to get a binary array. We will use a threshold\nof 0.5, but if we wanted to, we could experiment with thresholding to trade getting\nmore true positives for an increase in false positives.\n\nWe also include a small cleanup step using the erosion operation from\nscipy.ndimage.morphology. It deletes one layer of boundary voxels and only keeps\nthe inner ones\u2014those for which all eight neighboring voxels in the axis direction are\nalso flagged. This makes the flagged area smaller and causes very small components\n(smaller than 3 x 3 x 3 voxels) to vanish. Put together with the loop over the data\nloader, which we instruct to feed us all slices from a single CT, we have the following.\n\nListing 14.2 nodule_analysis.py:384, .segmentCct\n\nWe do not need gradients here, This array will hold\nso we don\u2019t build the graph. ur vot oral a saiey\ndef segmentCt (self, ct, series_uid): annotations. We get a data\nwith torch.no_grad(): loader that lets\noutput_a = np.zeros_like(ct.hu_a, dtype=np.float32) <\u00ab\u2014 us loop over our\nseg_dl = self.initSegmentationDl(series_uid) # + CT in batches.\nfor input_t, _, _, slice_ndx_list in seg_dl:\nAfter moving the\n... we run the input_g = input_t.to(self.device) a | input to the GPU ...\nsegmentation prediction_g = self.seg_model (input_g)\nmodel ... +... and copy each\nfor i, slice _ndx in enumerate(slice_ndx_list): element to the\noutput_a[slice_ndx] = prediction_g[i].cpu().numpy() | output array.\nmask_a = output_a > 0.5\nmask_a = morphology.binary_erosion(mask_a, iterations=1)\nreturn mask_a Thresholds the probability outputs\nto get a binary output, and then\n\napplies binary erosion as cleanup\n\nThis was easy enough, but now we need to invent the grouping.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.6,
                        "section_name": "Grouping voxels into nodule candidates",
                        "section_path": "./screenshots-images-2/chapter_15/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_6/0d488ff7-76a7-471e-bd1b-278dfb807972.png",
                            "./screenshots-images-2/chapter_15/section_6/1f6976ee-94f6-49b2-b7e9-9538ec7d75bb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "14.3.2 Grouping voxels into nodule candidates\n\nWe are going to use a simple connected-components algorithm for grouping our sus-\npected nodule voxels into chunks to feed into classification. This grouping approach\nlabels connected components, which we will accomplish using scipy.ndimage\n-measurements. label. The label function will take all nonzero pixels that share an\nedge with another nonzero pixel and mark them as belonging to the same group.\nSince our output from the segmentation model has mostly blobs of highly adjacent\npixels, this approach matches our data well.\n\nListing 14.3 nodule_analysis.py:401\n\nAssigns each voxel the label\nof the group it belongs to\ndef groupSegmentationOutput (self, series_uid, ct, clean_a):\nus, candidateLabel_a, candidate_count = measurements. label (clean_a)\ncenterIrc_list = measurements.center_of_mass (\n\net.hu_a.clip(-1000, 1000) + 1001, Gets the center of mass for\nlabels=candidateLabel_a, each group as index, row,\ncolumn coordinates\n\nindex=np.arange(1, candidate_count+1),\n)\n\nThe output array candidateLabel_a is the same shape as clean_a, which we used for\ninput, but it has 0 where the background voxels are, and increasing integer labels 1, 2,\n..., with one number for each of the connected blobs of voxels making up a nodule can-\ndidate. Note that the labels here are not the same as labels in a classification sense! These\nare just saying \u201cThis blob of voxels is blob 1, this blob over here is blob 2, and so on.\u201d\n\nSciPy also sports a function to get the centers of mass of the nodule candidates:\nscipy.ndimage.measurements.center_of_mass. It takes an array with per-voxel den-\nsities, the integer labels from the label function we just called, and a list of which of\nthose labels need to have a center calculated. To match the function's expectation\nthat the mass is non-negative, we offset the (clipped) ct .hu_a by 1,001. Note that this\nleads to all flagged voxels carrying some weight, since we clamped the lowest air value\nto -1,000 HU in the native CT units.\n\nListing 14.4 nodule_analysis.py:409\n\ncandidateInfo_list = [])\nfor i, center_ire in enumerate(centerIrc_list):\ncenter_xyz = irc2xyz( <\n\ncenter_irc, Converts the voxel\nct.origin_xyz, coordinates to real\nct.vxSize_xyz, patient coordinates\n\ncet.direction_a,\n\ncandidateInfo_tup = \\\nCandidateInfoTuple(False, False, False, 0.0, series_uid, center_xyz) <<\ncandidateInfo_list.append(candidateInfo_tup)\n~ PP \u2014eUp Builds our candidate info tuple and\n\nreturn candidateInfo_list appends it to the list of jons\n\nAs output, we get a list of three arrays (one each for the index, row, and column) the\nsame length as our candidate_count. We can use this data to populate a list of\ncandidateInfo_tup instances; we have grown attached to this little data structure, so\nwe stick our results into the same kind of list we\u2019ve been using since chapter 10. As we\ndon\u2019t really have suitable data for the first four values (isNodule_bool,\nhasAnnotation_bool, isMal_bool, and diameter_mm), we insert placeholder values of\na suitable type. We then convert our coordinates from voxels to physical coordinates in\na loop, creating the list. It might seem a bit silly to move our coordinates away from our\narray-based index, row, and column, but all of the code that consumes\ncandidateInfo_tup instances expects center_xyz, not center_irc. We'd get wildly\nwrong results if we tried to swap one for the other!\n\nYay\u2014we\u2019ve conquered step 3, getting nodule locations from the voxel-wise detec-\ntions! We can now crop out the suspected nodules and feed them to our classifier to\nweed out some more false positives.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.7,
                        "section_name": "Did we find a nodule? Classification to reduce false positives",
                        "section_path": "./screenshots-images-2/chapter_15/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_7/8bfaaca4-8cd5-403f-840e-e6e0fb0a7437.png",
                            "./screenshots-images-2/chapter_15/section_7/5b430f5b-fdf0-44b8-a49f-c50b502c6fa4.png",
                            "./screenshots-images-2/chapter_15/section_7/793be1a5-dd48-4778-8868-a1fb5ead3c63.png",
                            "./screenshots-images-2/chapter_15/section_7/8c39b36f-7e58-4f59-9675-942494efa2cd.png",
                            "./screenshots-images-2/chapter_15/section_7/529d6c28-20ff-4ef9-aeac-70ca5d1bb794.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "14.3.3 Did we find a nodule? Classification to reduce false positives\n\nAs we started part 2 of this book, we described the job of a radiologist looking through\nCT scans for signs of cancer thus:\n\nCurrently, the work of reviewing the data must be performed by highly trained specialists,\n\nrequires painstaking attention to detail, and it is dominated by cases where no cancer exists.\n\nDoing that job well is akin to being placed in front of 100 haystacks and being told,\n\n\u201cDetermine which of these, if any, contain a needle.\u201d\n\nWe've spent time and energy discussing the proverbial needles; let\u2019s discuss the hay\nfor a moment by looking at figure 14.4. Our job, so to speak, is to fork away as much\nhay as we can from in front of our glassy-eyed radiologist, so that they can refocus\ntheir highly trained attention where it can do the most good.\n\nLet\u2019s look at how much we are discarding at each step while we perform our end-\nto-end diagnosis. The arrows in figure 14.4 show the data as it flows from the raw CT\nvoxels through our project to our final malignancy determination. Each arrow that\nends with an X indicates a swath of data discarded by the previous step; the arrow\npointing to the next step represents the data that survived the culling. Note that the\nnumbers here are very approximate.\n\n~2\u00b025 VOXELS \u201c210\nCANDIDATES\n\n3. NODULE\nCLASSIFICATION\n\\. SEGMENTATION\n\u201c210 \u201cFe\nmA Me 20 4. MALIGNANT\nCLASSIFICATION\n\nA 1\n++ } 2. GRoveinc L \\\ncor a2B 2\"\n\n\\\n\nFigure 14.4 The steps of our end-to-end detection project, and the rough order of magnitude of data removed at\neach step\n\nLet\u2019s go through the steps in figure 14.4 in more detail:\n\n1 Segmentation\u2014Segmentation starts with the entire CT: hundreds of slices, or\nabout 33 million (2?\u00b0) voxels (give or take quite a lot). About 2\u00b0 voxels are\nflagged as being of interest; this is orders of magnitude smaller than the total\ninput, which means we're throwing out 97% of the voxels (that\u2019s the 2\u201d on the\nleft leading to the X).\n\n2 Grouping. While grouping doesn\u2019t remove anything explicitly, it does reduce the\nnumber of items we're considering, since we consolidate voxels into nodule\ncandidates. The grouping produces about 1,000 candidates (2'\u00b0) from 1 million\nvoxels. A nodule of 16 x 16 x 2 voxels would have a total of 2'\u00b0 voxels.\n\n3 Nodule classification. This process throws away the majority of the remaining ~2'\u00b0\nitems. From our thousands of nodule candidates, we're left with tens of nod-\nules: about 2\u00b0.\n\n4 Malignant classification. Finally, the malignancy classifier takes tens of nodules\n(2\u00b0) and finds the one or two (2!) that are cancer.\n\n\u00ae The size of any given nodule is highly variable, obviously.\n\nEach step along the way allows us to discard a huge amount of data that our model is\nconfident is irrelevant to our cancer-detection goal. We went from millions of data\n\npo\n\nints to a handful of tumors.\n\nFully automated vs. assistive systems\n\nThere is a difference between a fully automated system and one that is designed to\naugment a human's abilities. For our automated system, once a piece of data is\nflagged as irrelevant, it is gone forever. When presenting data for a human to con-\nsume, however, we should allow them to peel back some of the layers and look at\nthe near misses, as well as annotate our findings with a degree of confidence. Were\nwe designing a system for clinical use, we\u2019d need to carefully consider our exact\nintended use and make sure our system design supported those use cases well.\nSince our project is fully automated, we can move forward without having to consider\nhow best to surface the near misses and the unsure answers.\n\nNow that we have identified regions in the image that our segmentation model con-\nsiders probable candidates, we need to crop these candidates from the CT and feed\nthem into the classification module. Happily, we have candidateInfo_list from the\nprevious section, so all we need to do is make a DataSet from it, put it into a Data-\nLoader, and iterate over it. Column 1 of the probability predictions is the predicted\nprobability that this is a nodule and is what we want to keep. Just as before, we collect\nthe output from the entire loop.\n\n-ClassifyCandidates\n\nAgain, we get a data loader to loop over,\n\nthis time\n\nbased on our candidate list.\n\ndef classifyCandidates(self, ct, candidateInfo_list):\n> cls_dl = self.initClassificationDl (candidateInfo_list)\nclassifications_list = []\nfor batch_ndx, batch_tup in enumerate(cls_dl):\n\ninput_t, _, _, series_list, center_list = batch_tup Runs the inputs\nthrough the nodule\n\u2122 input _g = input_t.to(self.device) Ys. non-nodule\nwith torch.no_grad(): network\n.. probability_nodule_g = self.cls_model(input_g) a\nSends the if self.malignancy_model is not None: If we have a\ninputs to _, probability_mal_g = self.malignancy_model(input_g) | malignancy model,\nthedevice = aye: we run that, too.\n\nprobability_mal_g = torch. zeros_like(probability_nodule_g)\n\nzip_iter = zip(center_list,\nprobability_nodule_g[:,1].tolist(),\nprobability_mal_g[:,1].tolist())\nfor center_irc, prob_nodule, prob_mal in zip_iter: \u00ab+ Does our bookkeeping,\ncenter_xyz = irc2xyz(center_irc, constructing a list of our\ndirection_a=ct.direction_a, results\n\norigin_xyz=ct.origin_xyz,\nvxSize_xyz=ct.vxSize_xyz,\n)\nels_tup = (prob_nodule, prob_mal, center_xyz, center_irc)\nclassifications_list.append(cls_tup)\nreturn classifications_list\n\nThis is great! We can now threshold the output probabilities to get a list of things our\nmodel thinks are actual nodules. In a practical setting, we would probably want to out-\nput them for a radiologist to inspect. Again, we might want to adjust the threshold to\nerr a bit on the safe side: that is, if our threshold was 0.3 instead of 0.5, we would pres-\nent a few more candidates that turn out not to be nodules, while reducing the risk of\nmissing actual nodules.\n\nting 14.6 nodule_analysis.py:333, NoduleAnalysisApp.main\n\nIf we don\u2019t pass run_validation, we\nprint individual information ...\nif not self.cli_args.run_validation:\nprint (f\"found nodule candidates in {series_uid):\")\nfor prob, prob_mal, center_xyz, center_ire in classifications_list:\n\nif prob > 0.5: t 7\n\ns = \u00a3\"nodule prob (prob:.3f}, \" \u00ab+. for all candidates found by\nif self.malignancy_model: the segmentation where the\n\ns += \u00a3*malignancy prob {prob_mal:.3\u00a3), \" __ \u201classifier assigned a nodule\ns += f\"center xyz (center_xyz)\" probability of 50% or more.\nprint (s)\n\nIf we have the ground truth data, we\nif series_uid in candidateInfo_dict: compute and print the confusion matrix and\n\none_confusion = match_and_score( also add the current results to the total.\nclassifications_list, candidateInfo_dict[series_uid]\n\n)\n\nall_confusion += one_confusion\n\nprint_confusion(\n\nseries_uid, one_confusion, self.malignancy_model is not None\n)\n\nprint_confusion(\n\n\"Total\", all_confusion, self.malignancy_model is not None\n)\n\nLet\u2019s run this for a given CT from the validation set:*\n\n$ python3.6 -m p2chl4.nodule_analysis 1.3.6.1.4.1.14519.5.2.1.6279.6001\nwe .592821488053137951302246128864\n\nfound nodule candidates in 1.3.6.1.4.1.14519.5.2.1.6279.6001.5928214880\nwe 53137951302246128864:\n\nThis candidate is assigned a 53% probability of being malignant, so it barely makes the probability\nthreshold of 50%. The malignancy classification assigns a very low (3%) probability.\n\n\\. nodule prob 0.533, malignancy prob 0.030, center xyz XyzTuple\nwe (x=-128.857421875, y=-80.349609375, z=-31.300007820129395)\nnodule prob 0.754, malignancy prob 0.446, center xyz XyzTuple\nwe (x=-116.396484375, y=-168.142578125, z=-238.30000233650208)\n\n\u00a2\u00bb nodule prob 0.974, malignancy prob 0.427, center xyz XyzTuple\n> (x=121.494140625, y=-45.798828125, z=-211.3000030517578)\nnodule prob 0.700, malignancy prob 0.310, center xyz XyzTuple\nw (x=123.759765625, y=-44.666015625, z=-211.3000030517578)\n\nDetected as a nodule with very high confidence\nand assigned a 42% probability of malignancy\n\nThe script found 16 nodule candidates in total. Since we're using our validation set, we\nhave a full set of annotations and malignancy information for each CT, which we can\nuse to create a confusion matrix with our results. The rows are the truth (as defined by\nthe annotations), and the columns show how our project handled each case:\n\nPrognosis: Complete Miss means the segmentation\ndidn\u2019t find a nodule, Filtered Out is the classifier\u2019s work,\n\nScan ID and Predicted Nodules are those it marked as nodules.\n1.3.6.1.4.1.14519.5.2.1.6279.6001.592821488053137951302246128864\n| Complete Miss | Filtered Out | Pred. Nodule\nrt Non-Nodules | | 1088 | 15\nBenign | 1 | o | 0\nMalignant | o | o | 1\nThe rows contain the ground truth.\n\nThe Complete Miss column is when our segmenter did not flag a nodule at all. Since\nthe segmenter was not trying to flag non-nodules, we leave that cell blank. Our seg-\nmenter was trained to have high recall, so there are a large number of non-nodules,\nbut our nodule classifier is well equipped to screen those out.\n\nSo we found the | malignant nodule in this scan, but missed a 17th benign one. In\naddition, 15 false positive non-nodules made it through the nodule classifier. The\nfiltering by the classifier brought the false positives down from over 1,000! As we saw\nearlier, 1,088 is about oc\"), so that lines up with what we expect. Similarly, 15 is\nabout O(2\u00b0), which isn\u2019t far from the O(2\u00b0) we ballparked.\n\nCool! But what's the larger picture?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.8,
                        "section_name": "Quantitative validation",
                        "section_path": "./screenshots-images-2/chapter_15/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_8/121202b3-8223-4fdf-a9df-289fc82fae69.png",
                            "./screenshots-images-2/chapter_15/section_8/a83901be-8e73-4b5a-8392-676cb0195e7c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Quantitative validation\n\nNow that we have anecdotal evidence that the thing we built might be working on one\ncase, let\u2019s take a look at the performance of our model on the entire validation set.\nDoing so is simple: we run our validation set through the previous prediction and\ncheck how many nodules we get, how many we miss, and how many candidates are\nerroneously identified as nodules.\n\nWe run the following, which should take half an hour to an hour when run on the\nGPU. After coffee (or a full-blown nap), here is what we get:\n\n$ python3 -m p2chi4.nodule_analysis --run-validation\n\nTotal\n\n| Complete Miss | Filtered Out | Pred. Nodule\n\nNon-Nodules | | 164893 | 2156\nBenign | 12 | 3 | 87\nMalignant | 1 | 6 | 45\n\nWe detected 132 of the 154 nodules, or 85%. Of the 22 we missed, 13 were not consid-\nered candidates by the segmentation, so this would be the obvious starting point for\nimprovements.\n\nAbout 95% of the detected nodules are false positives. This is of course not great; on\nthe other hand, it\u2019s a lot less critical\u2014having to look at 20 nodule candidates to find one\nnodule will be much easier than looking at the entire CT. We will go into this in more detail\nin section 14.7.2, but we want to stress that rather than treating these mistakes as a black\nbox, it\u2019s a good idea to investigate the misclassified cases and see if they have commonal-\nities. Are there characteristics that differentiate them from the samples that were correctly\nclassified? Can we find anything that could be used to improve our performance?\n\nFor now, we\u2019re going to accept our numbers as is: not bad, but not perfect. The\nexact numbers may differ when you run your self-trained model. Toward the end of\nthis chapter, we will provide some pointers to papers and techniques that can help\nimprove these numbers. With inspiration and some experimentation, we are confi-\ndent that you can achieve better scores than we show here.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.9,
                        "section_name": "Predicting malignancy",
                        "section_path": "./screenshots-images-2/chapter_15/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_9/0267acb8-54d2-4189-8111-9be79a613de9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Predicting malignancy\n\nNow that we have implemented the nodule-detection task of the LUNA challenge and\ncan produce our own nodule predictions, we ask ourselves the logical next question:\ncan we distinguish malignant nodules from benign ones? We should say that even with\na good system, diagnosing malignancy would probably take a more holistic view of the\npatient, additional non-CT context, and eventually a biopsy, rather than just looking\nat single nodules in isolation on a CT scan. As such, this seems to be a task that is likely\nto be performed by a doctor for some time to come.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.1,
                        "section_name": "Getting malignancy information",
                        "section_path": "./screenshots-images-2/chapter_15/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_10/3d7ca1d1-c945-43a6-9634-fff991b3de44.png",
                            "./screenshots-images-2/chapter_15/section_10/eb363f37-6e83-455f-a370-6c3a5145ee9a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Getting malignancy information\n\nThe LUNA challenge focuses on nodule detection and does not come with malig-\nnancy information. The LIDC-IDRI dataset (http://mng.bz/4A4R) has a superset of\nthe CT scans used for the LUNA dataset and includes additional information about\nthe degree of malignancy of the identified tumors. Conveniently, there is a PyLIDC\nlibrary that can be installed easily, as follows:\n\n$ pip3 install pylide\n\nThe pylicd library gives us ready access to the additional malignancy information we\nwant. Just like matching the annotations with the candidates by location as we did in\nchapter 10, we need to associate the annotation information from LIDC with the coor-\ndinates of the LUNA candidates.\n\nIn the LIDC annotations, the malignancy information is encoded per nodule and\ndiagnosing radiologist (up to four looked at the same nodule) using an ordinal five-value\nscale from 1 (highly unlikely) through moderately unlikely, indeterminate, and moder-\nately suspicious, and ending with 5 (highly suspicious).* These annotations are based on\nthe image alone and subject to assumptions about the patient. To convert the list of num-\nbers to a single Boolean yes/no, we will consider nodules to be malignant when at least\ntwo radiologists rated that nodule as \u201cmoderately suspicious\u201d or greater. Note that this cri-\nterion is somewhat arbitrary; indeed, the literature has many different ways of dealing\nwith this data, including predicting the five steps, using averages, or removing nodules\nfrom the dataset where the rating radiologists were uncertain or disagreed.\n\nThe technical aspects of combining the data are the same as in chapter 10, so we skip\nshowing the code here (it is in the code repository for this chapter) and will use the\nextended CSV file. We will use the dataset in a way very similar to what we did for the nod-\nule classifier, except that we now only need to process actual nodules and use whether\na given nodule is malignant or not as the label to predict. This is structurally very similar\nto the balancing we used in chapter 12, but instead of sampling from pos_list and\nneg_list, we sample from mal_list and ben_list. Just as we did for the nodule classi-\nfier, we want to keep the training data balanced. We put this into the MalignancyLuna-\nDataset class, which subclasses the LunaDataset but is otherwise very similar.\n\nFor convenience, we create a dataset command-line argument in training.py and\ndynamically use the dataset class specified on the command line. We do this by using\nPython\u2019s getattr function. For example, if self.cli_args.dataset is the string\nMalignancyLunaDataset, it will get p2chl4.dsets.MalignancyLunaDataset and\nassign this type to ds_cls, as we can see here.\n\nListing 14.7 training.py:154, .initTrainDl\n\nds_cls = getattr(p2chl4.dsets, self.cli_args.dataset) <)\u2014 Dynamie class-name\n\ntrain_ds = ds_els( lookup\n\nval_stride=10,\nisValSet_bool=False, Recall that this is the one-to-one balancing of the\nratio_int=1, <\u2014 training data, here between benign and malignant.\n\n)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.11,
                        "section_name": "An area under the curve baseline: Classifying by diameter",
                        "section_path": "./screenshots-images-2/chapter_15/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_11/f4592090-7602-4c64-ad77-512bcf25ba87.png",
                            "./screenshots-images-2/chapter_15/section_11/ccb766c5-35d0-4fb2-9ff6-0023a0ac6cdf.png",
                            "./screenshots-images-2/chapter_15/section_11/177bf049-e03c-4664-8313-9f3f462179f4.png",
                            "./screenshots-images-2/chapter_15/section_11/1c0e0044-f189-414b-b615-2ad430c82e78.png",
                            "./screenshots-images-2/chapter_15/section_11/d14a036b-ca1c-41e5-89cb-a9b945b71499.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "An area under the curve baseline: Classifying by diameter\n\nIt is always good to have a baseline to see what performance is better than nothing. We\ncould go for better than random, but here we can use the diameter as a predictor for\nmalignancy\u2014larger nodules are more likely to be malignant. Step 2b of figure 14.5\nhints at a new metric we can use to compare classifiers.\n\n|. NODULE CANDIDATE GENERATION\na\n\n1A. SEGMENTATION 1B. GROUPING iC. SAMPLE TUPLES\nwey (Ge TROD,\nhed (21 TRE))\n\n2. NODULE AND MA ANCY CLASSIFICATION\n\n2A. NODULE CLASSIFICATIO! 2C. FINE-TUNING MALIGNANCY MODEL\n\n=\n\n3A. |... IRC) 38. IS NODULE? 3C. IS MALIGNANT?\n\nFigure 14.5 The end-to-end project we are implementing in this chapter, with a focus on the ROC graph\n\nWe could use the nodule diameter as the sole input to a hypothetical classifier predict-\ning whether a nodule is malignant. It wouldn't be a very good classifier, but it turns\nout that saying \u201cEverything bigger than this threshold X is malignant\u201d is a better pre-\ndictor of malignancy than we might expect. Of course, picking the right threshold is\nkey\u2014there\u2019s a sweet spot that gets all the huge tumors and none of the tiny specks,\nand roughly splits the uncertain area that\u2019s a jumble of larger benign nodules and\n\nsmaller malignant ones.\n\nAs we might recall from chapter 12, our true positive, false positive, true negative,\nand false negative counts change based on what threshold value we choose. As we\ndecrease the threshold over which we predict that a nodule is malignant, we will\nincrease the number of true positives, but also the number of false positives. The false\npositive rate (FPR) is FP / (FP + TN), while the true positive rate (TPR) is TP / (TP +\n\nFN), which you might also remember from chapter 12 as the recall.\n\nLet\u2019s set a range for our threshold. The lower bound will be the value at which all\nof our samples are classified as positive, and the upper bound will be the opposite,\nwhere all samples are classified as negative. At one extreme, our FPR and TPR will\nboth be zero, since there won't be any positives; and at the other, both will be one,\n\nsince TNs and FNs won\u2019t exist (everything is positive!).\n\nNo one true way to measure false positives: Precision vs. false positive rate\nThe FPR here and the precision from chapter 12 are rates (between O and 1) that\nmeasure things that are not quite opposites. As we discussed, precision is TP /\n(TP + FP) and measures how many of the samples predicted to be positive will actu-\nally be positive. The FPR is FP / (FP + TN) and measures how many of the actually\nnegative samples are predicted to be positive. For heavily imbalanced datasets (like\nthe nodule versus non-nodule classification), our model might achieve a very good\nFPR (which is closely related to the cross-entropy criterion as a loss) while the preci-\nsion\u2014and thus the F1 score\u2014is still very poor. A low FPR means we're weeding out\na lot of what we're not interested in, but if we are looking for that proverbial needle,\nwe still have mostly hay.\n\nFor our nodule data, that\u2019s from 3.25 mm (the smallest nodule) to 22.78 mm (the\nlargest). If we pick a threshold value somewhere between those two values, we can\nthen compute FPR(threshold) and TPR(threshold). If we set the FPR value to X and\nTPR to Y, we can plot a point that represents that threshold; and if we instead plot the\nFPR versus TPR for every possible threshold, we get a diagram called the receiver operat-\ning characteristic (ROC) shown in figure 14.6. The shaded area is the area under the\n(ROC) curve, or AUC. It is between 0 and 1, and higher is better.\u00ae\n\nROC DIAMETER BASELINE, AUC=0.901\n\nLo\ne 08\nWOW 0.6\nE\n2 on\n8 5.42 MM\n& 0.2 THRESHOLD\n\n0.0\n\n0.0 02 on 0.6 08 \\.0\nFALSE POSITIVE RATE\n\nFigure 14.6 Receiver operating characteristic (ROC) curve for our baseline\n\nHere, we also call out two specific threshold values: diameters of 5.42 mm and 10.55\nmm. We chose those two values because they give us somewhat reasonable endpoints\nfor the range of thresholds we might consider, were we to need to pick a single thresh-\nold. Anything smaller than 5.42 mm, and we'd only be dropping our TPR. Larger\nthan 10.55 mm, and we'd just be flagging malignant nodules as benign for no gain.\nThe best threshold for this classifier will probably be in the middle somewhere.\n\nHow do we actually compute the values shown here? We first grab the candidate\ninfo list, filter out the annotated nodules, and get the malignancy label and diameter.\nFor convenience, we also get the number of benign and malignant nodules.\n\nListing 14.8 p2chi4_malben_baseline.ipynb\n\nTakes the regular dataset and in particular\nthe list of benign and malignant nodules\n\n# In[2]:\n& ds = p2chl4.dsets.MalignantLunaDataset (val_stride=10, isValSet_bool=True)\nnodules = ds.ben_list + ds.mal_list\n\nis_mal = torch.tensor([n.isMal_bool for n in nodules]) a Gets lists of\ndiam = torch. tensor ((n.diameter_mm for n in nodules]) malignancy status\nnum_mal = is_mal.sum() 2 and diameter\nnum_ben = len(is_mal) - num_mal | Fornormalization of the TPR and\n\nFPR , we take the number of\n\nmalignant and benign nodules.\n\nTo compute the ROC curve, we need an array of the possible thresholds. We get this\nfrom torch. 1linspace, which takes the two boundary elements. We wish to start at\nzero predicted positives, so we go from maximal threshold to minimal. This is the 3.25\nto 22.78 we already mentioned:\n\n# In[3):\nthreshold = torch.linspace(diam.max(), diam.min())\n\nWe then build a two-dimensional tensor in which the rows are per threshold, the col-\numns are per-sample information, and the value is whether this sample is predicted as\npositive. This Boolean tensor is then filtered by whether the label of the sample is\nmalignant or benign. We sum the rows to count the number of True entries. Dividing\nby the number of malignant or benign nodules gives us the TPR and FPR\u2014the two\n\ncoordinates for the ROC curve:\nIndexing by None adds a dimension of size 1, just like\n-unsqueeze(ndx). This gets us a 2D tensor of whether a\ngiven nodule (in a column) is classified as malignant for\n\n# In[4]: a given diameter (in the row).\npredictions = (diam[None] >= threshold[:, None]) 4\ntp_diam = (predictions & is_mal(None]).sum(1).float() / num_mal\nfp_diam = (predictions & ~is_mal[None]).sum(1).float() / num_ben\nWith the predictions matrix, we can\ncompute the TPRs and FPRs for each\n\ndiameter by summing over the columns.\n\nTo compute the area under this curve, we use numeric integration by the trapezoidal\nrule (https://en.wikipedia.org/wiki/Trapezoidal_rule), where we multiply the aver-\nage TPRs (on the Y-axis) between two points by the difference of the two FPRs (on the\nX-axis) \u2014the area of trapezoids between two points of the graph. Then we sum the\narea of the trapezoids:\n\n# In(5]:\n\nfp_diam diff = fp_diam[i:] - fp_diam[:-1]\ntp_diam avg = (tp_diam[1i:] + tp_diam[:-1])/2\nauc_diam = (fp_diam diff * tp_diam_avg) .sum()\n\nNow, if we run pyplot.plot(fp_diam, tp_diam, label=f\"diameter baseline,\nAUC=(auc_diam:.3f\u00a3}\") (along with the appropriate figure setup we see in cell 8), we\nget the plot we saw in figure 14.6.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.12,
                        "section_name": "| Reusing preexisting weights: Fine-tuning",
                        "section_path": "./screenshots-images-2/chapter_15/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_12/d97c9f22-c5a7-44f4-ae7c-3c30ea354ace.png",
                            "./screenshots-images-2/chapter_15/section_12/2628f291-57d1-47c1-8e27-c076e98b0d8c.png",
                            "./screenshots-images-2/chapter_15/section_12/443b3e27-179e-4ed0-82c6-7a2202e3345e.png",
                            "./screenshots-images-2/chapter_15/section_12/8b93aa3a-9d26-4304-8d83-86d42feae816.png",
                            "./screenshots-images-2/chapter_15/section_12/940cf883-29bb-4080-af83-e10eec41dd58.png",
                            "./screenshots-images-2/chapter_15/section_12/e8e2e1b4-6759-4ab5-871c-a74c11805cb1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "14.5.3 Reusing preexisting weights: Fine-tuning\nOne way to quickly get results (and often also get by with much less data) is to start not\nfrom random initializations but from a network trained on some task with related data.\nThis is called transfer learning or, when training only the last few layers, fine-tuning. Look-\ning at the highlighted part in figure 14.7, we see that in step 2c, we're going to cut out\nthe last bit of the model and replace it with something new.\n\nFigure 14.7 The end-to-end project we're implementing in this chapter, with a focus on fine-tuning\n\nRecall from chapter 8 that we could interpret the intermediate values as features\nextracted from the image\u2014features could be edges or corners that the model detects\nor indications of any pattern. Before deep learning, it was very common to use hand-\ncrafted features similar to what we briefly experimented with when starting with con-\nvolutions. Deep learning has the network derive features useful for the task at hand,\nsuch as discrimination between classes, from the data. Now, fine-tuning has us mix the\nancient ways (almost a decade ago!) of using preexisting features and the new way of\nusing learned features. We treat some (often large) part of the network as a fixed fea-\nture extractor and only train a relatively small part on top of it.\n\nThis generally works very well. Pretrained networks trained on ImageNet as we saw\nin chapter 2 are very useful as feature extractors for many tasks dealing with natural\nimages\u2014sometimes they also work amazingly for completely different inputs, from\npaintings or imitations thereof in style transfer to audio spectrograms. There are cases\nwhen this strategy works less well. For example, one of the common data augmentation\nstrategies in training models on ImageNet is randomly flipping the images\u2014a dog\nlooking right is the same class as one looking left. As a result, the features between\nflipped images are very similar. But if we now try to use the pretrained model for a task\nwhere left or right matters, we will likely encounter accuracy problems. If we want to\nidentify traffic signs, turn left here is quite different than turn right here, but a network\nbuilding on ImageNet-based features will probably make lots of wrong assignments\nbetween the two classes.\u00ae\n\nIn our case, we have a network that has been trained on similar data: the nodule\nclassification network. Let's try using that.\n\nFor the sake of exposition, we will stay very basic in our fine-tuning approach. In\nthe model architecture in figure 14.8, the two bits of particular interest are high-\nlighted: the last convolutional block and the head_linear module. The simplest fine-\ntuning is to cut out the head_linear part\u2014in truth, we are just keeping the random\ninitialization. After we try that, we will also explore a variant where we retrain both\nhead_linear and the last convolutional block.\n\nWe need to do the following:\n\n= Load the weights of the model we wish to start with, except for the last linear\n\nlayer, where we want to keep the initialization.\n\n= Disable gradients for the parameters we do not want to train (everything except\nparameters with names starting with head).\n\nWhen we do fine-tuning training on more than head_linear, we still only reset head\n_linear to random, because we believe the previous feature-extraction layers might\n\nfa LUNA MODEL ARCHITECTURE\n\n--FINETUNE-DEPTH=l\n\nFigure 14.8 The model architecture from chapter 11, with the depth-1 and depth-2 weights highlighted\n\nnot be ideal for our problem, but we expect them to be a reasonable starting point.\nThis is easy: we add some loading code into our model setup.\n\nListing 14.9 training.py:124, .initModel\n\nFilters out top-level modules\n\nthat have parameters (as\n\nopposed to the final activation)\nd = torch. load(self.cli_args.finetune, map_location='cpu')\nmodel_blocks = [\n\nn for n, subm in model.named_children() Takes the last finetune_depth blocks.\nt if len(list(subm.parameters())) > 0 The default (if fine-tuning) is 1.\n]\nfinetune_blocks = model_blocks[-self.cli_args.finetune_depth: ] +\nmodel. load_state_dict( Filters out the last block (the final\n{ linear part) and does not load it.\nk: v for k,v in d['model_state'].items() Starting from a fully initialized model\nif k.split('.') [0] not in model_blocks[-1] \u00ab+ would have us begin with (almost) all\n}, nodules labeled as malignant,\nbecause that output means \u201cnodule\u201d\n\nin the classifier we start from.\n\nstrict=False, Passing strict=False lets us load only some\n\n) module tered\nfor n, p in model.named_parameters(): of the (with the i ones missing).\n\nif n.split('.')(0] not in finetune_blocks: \u201c For all but finetune_blocks,\np.requires_grad_ (False) we do not want gradients.\n\nWe're set! We can train only the head by running this:\n\npython3 -m p2chl4.training \\\n\n--malignant \\\n\n--dataset MalignantLunaDataset \\\n\n--finetune data/part2/models/cls_2020-02-06_14.16.55_final-nodule-\n> nonnodule.best.state \\\n\n--epochs 40 \\\n\nmalben-finetune\n\nLet\u2019s run our model on the validation set and get the ROC curve, shown in figure\n14.9. It\u2019s a lot better than random, but given that we're not outperforming the base-\nline, we need to see what is holding us back.\n\n~~ DIAMETER BASELINE, AUC = 0.901\n\u2014\u2014 FINETUNED-| MODEL, AUC = 0.886\n\n0.0 0.2 o4 0.6 08 \\.0\n\nFigure 14.9 ROC curve for our fine-tuned model with a retrained final linear\nlayer. Not too bad, but not quite as good as the baseline.\n\nFigure 14.10 shows the TensorBoard graphs for our training. Looking at the validation\nloss, we see that while the AUC slowly increases and the loss decreases, even the training\nloss seems to plateau at a somewhat high level (say, 0.3) instead of trending toward zero.\nWe could run a longer training to check whether it is just very slow; but comparing this\nto the loss progression discussed in chapter 5\u2014in particular, figure 5.14\u2014we can see\nour loss value has not flatlined as badly as case A in the figure, but our problem with\n\nauc\n\nlosses stagnating is qualitatively similar. Back then, case A indicated that we did not have\nenough capacity, so we should consider the following three possible causes:\n\n= Features (the output of the last convolution) obtained by training the network\non nodule versus non-nodule classification are not useful for malignancy detec-\ntion.\n\n= The capacity of the head\u2014the only part we are training\u2014is not large enough.\n\n= The network might have too little capacity overall.\n\n0.96 |\n\n0.92 |\n\n0.88 |\n\nloss/all\nDEPTH | TRAININ os\n: (VALIDATION\n04\n. DEPTH I TRAINING\n[VALIDATION 0.2\n|\n500k 1M 1.5M 2M 25M 3M 35M 4M S00k 1M 15M 2M 25M 3M 3.5M 4M\n\na) n=\n\nFigure 14.10 AUC (left) and loss (right) for the fine-tuning of the last linear layer\n\nIf training only the fully connected part in fine-tuning is not enough, the next thing to\nty is to include the last convolutional block in the fine-tuning training. Happily, we\nintroduced a parameter for that, so we can include the block4 part into our training:\n\npython3 -m p2chl4.training \\\n\n--malignant \\\n\n~-dataset MalignantLunaDataset \\\n\n--finetune data/part2/models/cls_2020-02-06_14.16.55_final-nodule-\n> nonnodule.best.state \\\n\n~-finetune-depth 2 \\ <\u2014 This CLI\n\n--epochs 10 \\ parameter is new.\nmalben-finetune-twolayer\n\nOnce done, we can check our new best model against the baseline. Figure 14.11 looks\nmore reasonable! We flag about 75% of the malignant nodules with almost no false\npositives. This is clearly better than the 65% the diameter baseline can give us. Trying\nto push beyond 75%, our model\u2019s performance falls back to the baseline. When we go\nback to the classification problem, we will want to pick a point on the ROC curve to\nbalance true positives versus false positives.\n\nWe are roughly on par with the baseline, and we will be content with that. In sec-\ntion 14.7, we hint at the many things that you can explore to improve these results,\nbut that didn\u2019t fit in this book.\n\nDIAMETER BASELINE, AUC = 0.901\n\u2014\u2014 FINETUNED-ILR MODEL, AUC = 0.888 Figure 14.11 ROC\n\u2014\u2014 FINETUNED-2LR MODEL, AUC = 0.4L curve for our modified\nmodel. Now we're\n\n0.0 0.2 o4 0.6 08 lo \u2014_ realty close to\n\nLooking at the loss curves in figure 14.12, we see that our model is now overfitting\nvery early; thus the next step would be to check into further regularization methods.\nWe will leave that for you.\n\nThere are more refined methods of fine-tuning. Some advocate gradually unfreeze\nthe layers, starting from the top. Others propose to train the later layers with the usual\nlearning rate and use a smaller rate for the lower layers. PyTorch does natively support\nusing different optimization parameters like learning rates, weight decay, and\nmomentum for different parameters by separating them in several parameter groups\nthat are just that: lists of parameters with separate hyperparameters (https://pytorc\n-org/docs/stable/optim.html#per-parameter-options).\n\nauc loss/all\n\n:\n\n0.2\n\n\u00b0\n\n500k 1M 1.5M 2M 2.5M 3M 3.5M 4M 500k 1M 1.5M 2M 2.5M 3M 3.5M 4M\n3260) 120\n\nFigure 14.12 AUC (left) and loss (right) for the fine-tuning of the last convolutional block and the fully connected\nlayer\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.13,
                        "section_name": "More output in TensorBoard",
                        "section_path": "./screenshots-images-2/chapter_15/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_13/a87b089e-b559-4af1-a1b4-8cff4d5a003e.png",
                            "./screenshots-images-2/chapter_15/section_13/cc958cae-db96-4dc4-af98-82b43aef26ba.png",
                            "./screenshots-images-2/chapter_15/section_13/96be78a3-5d4c-43a3-ba34-9cabcf0d6f81.png",
                            "./screenshots-images-2/chapter_15/section_13/e1e8cace-3fb4-4878-ae6f-25cb31ee0e3f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "_ More output in TensorBoard\n\nWhile we are retraining the model, it might be worth looking at a few more outputs\nwe could add to TensorBoard to see how we are doing. For histograms, TensorBoard\nhas a premade recording function. For ROC curves, it does not, so we have an oppor-\ntunity to meet the Matplotlib interface.\n\nHisTOGRAMS\n\nWe can take the predicted probabilities for malignancy and make a histogram of\nthem. Actually, we make two: one for (according to the ground truth) benign and one\nfor malignant nodules. These histograms give us a fine-grained view into the outputs\nof the model and let us see if there are large clusters of output probabilities that are\ncompletely wrong.\n\nNOTE In general, shaping the data you display is an important part of getting\nquality information from the data. If you have many extremely confident cor-\nrect classifications, you might want to exclude the leftmost bin. Getting the\nright things onscreen will typically require some iteration of careful thought\nand experimentation. Don\u2019t hesitate to tweak what you're showing, but also\ntake care to remember if you change the definition of a particular metric with-\nout changing the name. It can be easy to compare apples to oranges unless\nyou're disciplined about naming schemes or removing now-invalid runs of data.\n\nWe first create some space in the tensor metrics_t holding our data. Recall that we\ndefined the indices somewhere near the top.\n\nListing 14.10 training.py:31\n\nMETRICS_LABEL_NDX=0\nMETRICS_PRED_NDX=1\nMETRICS_PRED_P_NDX=2 \u201c Our new index, carrying the prediction probabilities\n\nMETRICS_LOSS_NDX=3 (rather than prethresholded predictions)\nMETRICS_SIZE = 4\n\nOnce that\u2019s done, we can call writer.add_histogram with a label, the data, and the\nglobal_step counter set to our number of training samples presented; this is similar\nto the scalar call earlier. We also pass in bins set to a fixed scale.\n\nListing 14.11 training.py:496, . logMetrics\n\nbins = np.linspace(0, 1)\n\nwriter.add_histogram(\n\u2018label_neg',\nmetrics_t(METRICS_PRED_P_NDX, negLabel_mask],\nself.totalTrainingSamples_count,\nbins=bins\n)\nwriter .add_histogram(\n\u2018label_pos',\n\nmetrics_t(METRICS_PRED_P_NDX, posLabel_mask),\nself.totalTrainingSamples_count,\nbins=bins\n\nNow we can take a look at our prediction distribution for benign samples and how it\nevolves over each epoch. We want to examine two main features of the histograms in\nfigure 14.13. As we would expect if our network is learning anything, in the top row of\nbenign samples and non-nodules, there is a mountain on the left where the network is\nvery confident that what it sees is not malignant. Similarly, there is a mountain on the\nright for the malignant samples.\n\nBut looking closer, we see the capacity problem of fine-tuning only one layer.\nFocusing on the top-left series of histograms, we see the mass to the left is somewhat\nspread out and does not seem to reduce much. There is even a small peak round 1.0,\nand quite a bit of probability mass is spread out across the entire range. This reflects\nthe loss that didn\u2019t want to decrease below 0.3.\n\nlabel_neg\n\n2020-02-08_01.19.40-tm_cis-finetune-head 19.40-vi\n\nlabel_neg label_neg\n\nea ea\nta ta\nlabel_pos\n\n2020-02-08_01.19.40-tmn_cls-finetune-head\n\nlabel_pos\n\nFigure 14.13 TensorBoard histogram display for fine-tuning the head only\n\nGiven this observation on the training loss, we would not have to look further, but let\u2019s\npretend for a moment that we do. In the validation results on the right side, it appears\nthat the probability mass away from the \u201ccorrect\u201d side is larger for the non-malignant\nsamples in the top-right diagram than for the malignant ones in the bottom-right dia-\ngram. So the network gets non-malignant samples wrong more often than malignant\nones. This might have us look into rebalancing the data to show more non-malignant\nsamples. But again, this is when we pretend there was nothing wrong with the training\non the left side. We typically want to fix training first!\n\nFor comparison, let\u2019s take a look at the same graph for our depth 2 fine-tuning\n(figure 14.14). On the training side (the left two diagrams), we have very sharp peaks\nat the correct answer and not much else. This reflects that taining works well.\n\nOn the validation side, we now see that the most pronounced artifact is the litle\npeak at 0 predicted probability for malignancy in the bottom-right histogram. So our\nsystematic problem is that we\u2019re misclassifying malignant samples as non-malignant.\n(This is the reverse of what we had earlier!) This is the overfitting we saw with two-layer\n\nlabel_neg\n\nry ry\nta ta\n\nlabel_pos\n\nlabel_pos 2020-02-08 _00.19.45-trn_cls-finetune-depth2 label_pos\u2019\nry oe)\nta ta\n\nFigure 14.14 TensorBoard histogram display for fine-tuning with depth 2\n\nfine-tuning. It probably would be good to pull up a few images of that type to see\nwhat's happening.\n\nROC AND OTHER CURVES IN TENSORBOARD\n\nAs mentioned earlier, TensorBoard does not natively support drawing ROC curves. We\ncan, however, use the ability to export any graph from Matplotlib. The data prepara-\ntion looks just like in section 14.5.2: we use the data that we also plotted in the histo-\ngram to compute the TPR and FPR\u2014tpr and fpr, respectively. We again plot our\ndata, but this time we keep track of pyplot . figure and pass it to the SummarywWriter\nmethod add_figure.\n\nListing 14.12 training.py:482, .logMetrics\n\nSets up a new Matplotlib figure. We usually don\u2019t need it\nbecause it is implicitly done in Matplotlib, but here we do. _ | U8\u00a2* @*bitrary pyplot functions\n\n\u2014S\n\nfig = pyplot.figure() Adds our figure to TensorBoard\npyplot.plot(fpr, tpr) re\nwriter.add_figure('roc', fig, self.totalTrainingSamples_count) 4\n\nBecause this is given to TensorBoard as an image, it appears under that heading. We\ndidn\u2019t draw the comparison curve or anything else, so as not to distract you from the\nactual function call, but we could use any Matplotlib facilities here. In figure 14.15, we\nsee again that the depth-2 fine-tuning (left) overfits, while the head-only fine-tuning\n(right) does not.\n\nroc\n\nroc 202 roc | 2020-02-08_01.19.40-en_ste-finetune heed |\nstep 4,000,000 step 4,000,000\n\nSat Feb 08 2020 01:19:39 GMT+0100 (Central European Standard Sat Feb 08 2020 02:16:47 GMT+0100 (Central European Standard\nTime) Time)\n\nee |\n\nwm rT) \u2014\u2014\u2014\n\no oe {\n\n02\now\n\nry\nae a as ae on ip ae ae a ae on fr)\n\nFigure 14.15 Training ROC curves in TensorBoard. A slider lets us go through the iterations.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.14,
                        "section_name": "What we see when we diagnose",
                        "section_path": "./screenshots-images-2/chapter_15/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_14/d67b1297-c0f9-4be6-ad93-1a8f781c8299.png",
                            "./screenshots-images-2/chapter_15/section_14/a2337737-e7b9-46f7-8fa2-ff23b0d9cd5b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "14.6 What we see when we diagnose\nFollowing along with steps 3a, 3b, and 3c in figure 14.16, we now need to run the full\npipeline from the step 3a segmentation on the left to the step 3c malignancy model on\nthe right. The good news is that almost all of our code is in place already! We just need\nto stitch it together: the moment has come to actually write and run our end-to-end\n\ndiagnosis script.\n\n3. END-TO-END DETECTIO\na \u2122\n\nBA.(.., IRC) 38. ISNODULEP 3C. IS MALIGNANT?\nV7\n\nFigure 14.16 The end-to-end project we are implementing in this chapter, with a focus on end-to-end detection\n\nWe saw our first hints at handling the malignancy model back in the code in section\n14.3.3. If we pass an argument --malignancy-path to the nodule_analysis call, it\nruns the malignancy model found at this path and outputs the information. This\nworks for both a single scan and the --run-validation variant.\n\nBe warned that the script will probably take a while to finish; even just the 89 CTs\nin the validation set took about 25 minutes.\u201d\n\nLet\u2019s see what we get:\n\nTotal\n| Complete Miss | Filtered Out | Pred. Benign | Pred. Malignant\n\nNon-Nodules | | 164893 | 1593 | 563\nBenign | 12 | 3 | 70 | 17\nMalignant | i | 6 | 9 | 36\n\nNot too bad! We detect about 85% of the nodules and correctly flag about 70% of the\nmalignant ones, end to end.* While we have a lot of false positives, it would seem that\nhaving 16 of them per true nodule reduces what needs to be looked at (well, if it were\nnot for the 30% false negatives). As we already warned in chapter 9, this isn\u2019t at the\nlevel where you could collect millions of funding for your medical AI startup,\u201d but it\u2019s\na pretty reasonable starting point. In general, we should be pretty happy that we\u2019re\ngetting results that are clearly meaningful; and of course our real goal has been to\nstudy deep learning along the way.\n\nWe might next choose to look at the nodules that are actually misclassified. Keep\nin mind that for our task at hand, even the radiologists who annotated the dataset dif-\nfered in opinion. We might stratify our validation set by how clearly they identified a\nnodule as malignant.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.15,
                        "section_name": "| Training, validation, and test sets",
                        "section_path": "./screenshots-images-2/chapter_15/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_15/f11d7bc1-ad6b-4784-a061-dc3cba7257b1.png",
                            "./screenshots-images-2/chapter_15/section_15/3c2c8cb4-b90f-4443-9901-b2a3dca81dd1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "14.6.1 Training, validation, and test sets\nThere is one caveat that we must mention. While we didn\u2019t explicitly ain our model on\nthe validation set, although we ran this risk at the beginning of the chapter, we did choose\nthe epoch of training to use based on the model\u2019s performance on the validation set.\nThat\u2019s a bit of a data leak, too. In fact, we should expect our real-world performance to be\nslightly worse than this, as it\u2019s unlikely that whatever model performs best on our valida-\ntion set will perform equally well on every other unseen set of data (on average, at least).\nFor this reason, practitioners often split data into three sets:\n\n= A training set, exactly as we've done here\n\n= A validation set, used to determine which epoch of evolution of the model to\nconsider \u201cbest\u201d\n\n= A test set, used to actually predict performance for the model (as chosen by the\nvalidation set) on unseen, real-world data\n\nAdding a third set would have led us to pull another nontrivial chunk of our training\ndata, which would have been somewhat painful, given how badly we had to fight over-\nfitting already. It would also have complicated the presentation, so we purposely left it\nout. Were this a project with the resources to get more data and an imperative to build\nthe best possible system to use in the wild, we'd have to make a different decision here\nand actively seek more data to use as an independent test set.\n\nThe general message is that there are subtle ways for bias to creep into our models.\nWe should use extra care to control information leakage at every step of the way and\nverify its absence using independent data as much as possible. The price to pay for tak-\ning shortcuts is failing egregiously at a later stage, at the worst possible time: when\nwe\u2019re closer to production.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.16,
                        "section_name": "What next? Additional sources of inspiration (and data)",
                        "section_path": "./screenshots-images-2/chapter_15/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_16/d20f134a-e8e5-478d-8593-25c2de97f580.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What next? Additional sources of inspiration (and data)\n\nFurther improvements will be difficult to measure at this point. Our classification vali-\ndation set contains 154 nodules, and our nodule classification model is typically get-\nting at least 150 of them right, with most of the variance coming from epoch-by-epoch\ntraining changes. Even if we were to make a significant improvement to our model, we\ndon\u2019t have enough fidelity in our validation set to tell whether that change is an\nimprovement for certain! This is also very pronounced in the benign versus malignant\nclassification, where the validation loss zigzags a lot. If we reduced our validation\nstride from 10 to 5, the size of our validation set would double, at the cost of one-ninth\nof our training data. That might be worth it if we wanted to try other improvements.\nOf course, we would also need to address the question of a test set, which would take\naway from our already limited training data.\n\nWe would also want to take a good look at the cases where the network does not\nperform as well as we'd like, to see if we can identify any pattern. But beyond that, let\u2019s\ntalk briefly about some general ways we could improve our project. In a way, this sec-\ntion is like section 8.5 in chapter 8. We will endeavor to fill you with ideas to try; don\u2019t\nworry if you don\u2019t understand each in detail.'\u201d\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.17,
                        "section_name": "_ Preventing overfitting: Better regularization",
                        "section_path": "./screenshots-images-2/chapter_15/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_17/c511e866-a347-48f6-8cb0-6f6124f67d2b.png",
                            "./screenshots-images-2/chapter_15/section_17/a0a8f9be-ecf8-4adf-b28c-8e1f29aaba69.png",
                            "./screenshots-images-2/chapter_15/section_17/932d09df-44d6-4e3b-a305-4689761cea46.png",
                            "./screenshots-images-2/chapter_15/section_17/592eba2b-a6a4-4236-a199-d96ce413bc81.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "14.7.1 Preventing overfitting: Better regularization\n\nReflecting on what we did throughout part 2, in each of the three problems\u2014the clas-\nsifiers in chapter 11 and section 14.5, as well as the segmentation in chapter 13\u2014we\nhad overfitting models. Overfitting in the first case was catastrophic; we dealt with it\nby balancing the data and augmentation in chapter 12. This balancing of the data to\nprevent overfitting has also been the main motivation to train the U-Net on crops\naround nodules and candidates rather than full slices. For the remaining overfitting,\nwe bailed out, stopping training early when the overfitting started to affect our valida-\ntion results. This means preventing or reducing overfitting would be a great way to\nimprove our results.\n\nThis pattern\u2014get a model that overfits, and then work to reduce that overfitting\u2014\ncan really can be seen as a recipe.'' So this two-step process should be used when we\nwant to improve on the state we have achieved now.\n\nCLASSIC REGULARIZATION AND AUGMENTATION\nYou might have noticed that we did not even use all the regularization techniques\nfrom chapter 8. For example, dropout would be an easy thing to try.\n\nWhile we have some augmentation in place, we could go further. One relatively pow-\nerful augmentation method we did not attempt to employ is elastic deformations,\nwhere we put \u201cdigital crumples\u201d into the inputs.\u2019 This makes for much more variability\nthan rotation and flipping alone and would seem to be applicable to our tasks as well.\n\nMORE ABSTRACT AUGMENTATION\n\nSo far, our augmentation has been geometrically inspired\u2014we transformed our input\nto more or less look like something plausible we might see. It turns out that we need\nnot limit ourselves to that type of augmentation.\n\nRecall from chapter 8 that mathematically, the cross-entropy loss we have been using\nis a measure of the discrepancy between two probability distributions\u2014that of the pre-\ndictions and the distribution that puts all probability mass on the label and can be rep-\nresented by the one-hot vector for the label. If overconfidence is a problem for our\nnetwork, one simple thing we could try is not using the one-hot distribution but rather\nputting a small probability mass on the \u201cwrong\u201d classes.' This is called label smoothing.\n\nWe can also mess with inputs and labels at the same time. A very general and also\neasy-to-apply augmentation technique for doing this has been proposed under the\nname of mixup:'* the authors propose to randomly interpolate both inputs and labels.\nInterestingly, with a linearity assumption for the loss (which is satisfied by binary cross\nentropy), this is equivalent to just manipulating the inputs with a weight drawn from\nan appropriately adapted distribution.'\u00ae Clearly, we don\u2019t expect blended inputs to\noccur when working on real data, but it seems that this mixing encourages stability of\nthe predictions and is very effective.\n\nBEYOND A SINGLE BEST MODEL: ENSEMBLING\n\nOne perspective we could have on the problem of overfitting is that our model is\ncapable of working the way we want if we knew the right parameters, but we don\u2019t\nactually know them.'\u00ae If we followed this intuition, we might try to come up with sev-\neral sets of parameters (that is, several models), hoping that the weaknesses of each\nmight compensate for the other. This technique of evaluating several models and\ncombining the output is called ensembling. Simply put, we train several models and\nthen, in order to predict, run all of them and average the predictions. When each\nindividual model overfits (or we have taken a snapshot of the model just before we\nstarted to see the overfitting), it seems plausible that the models might start to make\nbad predictions on different inputs, rather than always overfit the same sample first.\n\nIn ensembling, we typically use completely separate training runs or even varying\nmodel structures. But if we were to make it particularly simple, we could take several\nsnapshots of the model from a single training run\u2014preferably shortly before the end\nor before we start to observe overfitting. We might try to build an ensemble of these\nsnapshots, but as they will still be somewhat close to each other, we could instead aver-\nage them. This is the core idea of stochastic weight averaging.'\u201d We need to exercise\nsome care when doing so: for example, when our models use batch normalization, we\nmight want to adjust the statistics, but we can likely get a small accuracy boost even\nwithout that.\n\nGENERALIZING WHAT WE ASK THE NETWORK TO LEARN\n\nWe could also look at multitask learning, where we require a model to learn additional\noutputs beyond the ones we will then evaluate,'* which has a proven track record of\nimproving results. We could try to train on nodule versus non-nodule and benign ver-\nsus malignant at the same time. Actually, the data source for the malignancy data pro-\nvides additional labeling we could use as additional tasks; see the next section. This\nidea is closely related to the transfer-learning concept we looked at earlier, but here\nwe would typically train both tasks in parallel rather than first doing one and then try-\ning to move to the next.\n\nIf we do not have additional tasks but rather have a stash of additional unlabeled\ndata, we can look into semi-supervised learning. An approach that was recently proposed\nand looks very effective is unsupervised data augmentation.\" Here we train our model\nas usual on the data. On the unlabeled data, we make a prediction on an unaug-\nmented sample. We then take that prediction as the target for this sample and train\nthe model to predict that target on the augmented sample as well. In other words, we\ndon\u2019t know if the prediction is correct, but we ask the network to produce consistent\noutputs whether we augment or not.\n\nWhen we run out of tasks of genuine interest but do not have additional data, we\nmay look at making things up. Making up data is somewhat difficult (although people\nsometimes use GANSs similar to the ones we briefly saw in chapter 2, with some suc-\ncess), so we instead make up tasks. This is when we enter the realm of self-supervised\nlearning, the tasks are often called pretext tasks. A very popular crop of pretext tasks\napply some sort of corruption to some of the inputs. Then we can train a network to\nreconstruct the original (for example, using a U-Net-like architecture) or train a clas-\nsifier to detect real from corrupted data while sharing large parts of the model (such\nas the convolutional layers) .\n\nThis is still dependent on us coming up with a way to corrupt our inputs. If we\ndon\u2019t have such a method in mind and aren\u2019t getting the results we want, there are\n\nother ways to do self-supervised learning. A very generic task would be if the features\nthe model learns are good enough to let the model discriminate between different\nsamples of our dataset. This is called contrastive learning.\n\nTo make things more concrete, consider the following: we take the extracted features\nfrom the current image and a largish number K of other images. This is our key set of\nfeatures. Now we set up a classification pretext task as follows: given the features of the\ncurrent image, the query, to which of the K+ 1 key features does it belong? This might\nseem trivial at first, but even if there is perfect agreement between the query features\nand the key features for the correct class, training on this task encourages the feature\nof the query to be maximally dissimilar from those of the K other images (in terms of\nbeing assigned low probability in the classifier output). Of course, there are many details\nto fill in; we recommend (somewhat arbitrarily) looking at momentum contrast.\u201d\u2019\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.18,
                        "section_name": "Refined training data",
                        "section_path": "./screenshots-images-2/chapter_15/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_18/6efa56d0-287e-4e8c-abd7-09a285b6035f.png",
                            "./screenshots-images-2/chapter_15/section_18/106640a2-c703-4f78-b1d5-86a9a6a9db23.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Refined training data\n\nWe could improve our training data in a few ways. We mentioned earlier that the malig-\nnancy classification is actually based on a more nuanced categorization by several\nradiologists. An easy way to use the data we discarded by making it into the dichotomy\n\u201cmalignant or not?\u201d would be to use the five classes. The radiologists\u2019 assessments could\nthen be used as a smoothed label: we could one-hot-encode each one and then average\nover the assessments of a given nodule. So if four radiologists look at a nodule and two\ncall it \u201cindeterminate,\u201d one calls that same nodule \u201cmoderately suspicious,\u201d and the\nfourth labels it \u201chighly suspicious,\u201d we would train on the cross entropy between the\nmodel output and the target probability distribution given by the vector 0 0 0.5 0.25\n0.25. This would be similar to the label smoothing we mentioned earlier, but in a\nsmarter, problem-specific way. We would, however, have to find a new way of evaluating\nthese models, as we lose the simple accuracy, ROC, and AUC notions we have in\nbinary classification.\n\nAnother way to use multiple assessments would be to train a number of models\ninstead of one, each trained on the annotations given by an individual radiologist. At\ninference we would then ensemble the models by, for example, averaging their output\nprobabilities.\n\nIn the direction of multiple tasks mentioned earlier, we could again go back to the\nPyLIDC-provided annotation data, where other classifications are provided for each\nannotation (subtlety, internal structure, calcification, sphericity, margin definedness,\nlobulation, spiculation, and texture (https://pylide.github.io/annotation.html). We\nmight have to learn a lot more about nodules, first, though.\n\nIn the segmentation, we could try to see whether the masks provided by PyLIDC\nwork better than those we generated ourselves. Since the LIDC data has annotations\nfrom multiple radiologists, it would be possible to group nodules into \u201chigh agree-\nment\u201d and \u201clow agreement\u201d groups. It might be interesting to see if that corresponds\n\nto \u201ceasy\u201d and \u201chard\u201d to classify nodules in terms of seeing whether our classifier gets\nalmost all easy ones right and only has trouble on the ones that were more ambiguous\nto the human experts. Or we could approach the problem from the other side, by\ndefining how difficult nodules are to detect in terms of our model performance:\n\u201ceasy\u201d (correctly classified after an epoch or two of training), \u201cmedium\u201d (eventually\ngotten right), and \u201chard\u201d (persistently misclassified) buckets.\n\nBeyond readily available data, one thing that would probably make sense is to fur-\nther partition the nodules by malignancy type. Getting a professional to examine our\ntraining data in more detail and flag each nodule with a cancer type, and then forcing\nthe model to report that type, could result in more efficient training. The cost to con-\ntract out that work is prohibitive for hobby projects, but paying might make sense in\ncommercial contexts.\n\nEspecially difficult cases could also be subject to a limited repeat review by human\nexperts to check for errors. Again, that would require a budget but is certainly within\nreason for serious endeavors.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.19,
                        "section_name": "3. Competition results and research papers",
                        "section_path": "./screenshots-images-2/chapter_15/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_19/eb968629-98c6-48e1-95da-2a80f691435d.png",
                            "./screenshots-images-2/chapter_15/section_19/ba11fcd0-bbb3-4f52-939a-20e5b54e5cb0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Competition results and research papers\n\nOur goal in part 2 was to present a self-contained path from problem to solution, and\nwe did that. But the particular problem of finding and classifying lung nodules has\nbeen worked on before; so if you want to dig deeper, you can also see what other peo-\nple have done.\n\nData SciENCcE Bow. 2017\nWhile we have limited the scope of part 2 to the CT scans in the LUNA dataset, there\nis also a wealth of information available from Data Science Bowl 2017 (www.kaggle\n.com/c/data-science-bowl-2017), hosted by Kaggle (www.kaggle.com). The data itself\nis no longer available, but there are many accounts of people describing what worked\nfor them and what did not. For example, some of the Data Science Bowl (DSB) final-\nists reported that the detailed malignancy level (1...5) information from LIDC was\nuseful during training.\n\nTwo highlights you could look at are these:*!\n\n= Second-place solution write-up by Daniel Hammack and Julian de Wit: http://\n\nmng.bz/Md48\n\n= Ninth-place solution write-up by Team Deep Breath: http://mng.bz/aRAX\n\nNOTE Many of the newer techniques we hinted at previously were not yet\navailable to the DSB participants. The three years between the 2017 DSB and\nthis book going to print are an eternity in deep learning!\n\nOne idea for a more legitimate test set would be to use the DSB dataset instead of\nreusing our validation set. Unfortunately, the DSB stopped sharing the raw data, so\nunless you happen to have access to an old copy, you would need another data source.\n\nLUNA PAPERS\n\nThe LUNA Grand Challenge has collected several results (https: //lunal6.grand-chal-\nlenge.org/Results) that show quite a bit of promise. While not all of the papers pro-\nvided include enough detail to reproduce the results, many do contain enough\ninformation to improve our project. You could review some of the papers and attempt\nto replicate approaches that seem interesting.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 15.2,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_15/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_15/section_20/df0a035b-a83b-4498-857f-c17d661e96b5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\n\nThis chapter concludes part 2 and delivers on the promise we made back in chapter 9:\nwe now have a working, end-to-end system that attempts to diagnose lung cancer from\nCT scans. Looking back at where we started, we\u2019ve come a long way and, hopefully,\nlearned a lot. We trained a model to do something interesting and difficult using pub-\nlicly available data. The key question is, \u201cWill this be good for anything in the real\nworld?\u201d with the follow-up question, \u201cIs this ready for production?\u201d The definition of\nproduction critically depends on the intended use, so if we\u2019re wondering whether our\nalgorithm can replace an expert radiologist, this is definitely not the case. We'd argue\nthat this can represent version 0.1 of a tool that could in the future support a radiolo-\ngist during clinical routine: for instance, by providing a second opinion about some-\nthing that could have gone unnoticed.\n\nSuch a tool would require clearance by regulatory bodies of competence (like the\nFood and Drug Administration in the United States) in order for it to be employed\noutside of research contexts. Something we would certainly be missing is an extensive,\ncurated dataset to further train and, even more importantly, validate our work. Indi-\nvidual cases would need to be evaluated by multiple experts in the context of a\nresearch protocol; and a proper representation of a wide spectrum of situations, from\ncommon presentations to corner cases, would be mandatory.\n\nAll these cases, from pure research use to clinical validation to clinical use, would\nrequire us to execute our model in an environment amenable to be scaled up. Need-\nless to say, this comes with its own set of challenges, both technical and in terms of\nprocess. We'll discuss some of the technical challenges in chapter 15.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 16,
                "chapter_name": "Deploying to production",
                "chapter_path": "./screenshots-images-2/chapter_16",
                "sections": [
                    {
                        "section_id": 16.1,
                        "section_name": "Deploying to production",
                        "section_path": "./screenshots-images-2/chapter_16/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_1/bb0552c8-5023-4b8a-be64-96c1dea5b3c2.png",
                            "./screenshots-images-2/chapter_16/section_1/7a21e276-2006-4dfd-9097-2a21eafbcd70.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In part 1 of this book, we learned a lot about models; and part 2 left us with a\ndetailed path for creating good models for a particular problem. Now that we have\nthese great models, we need to take them where they can be useful. Maintaining\ninfrastructure for executing inference of deep learning models at scale can be\nimpactful from an architectural as well as cost standpoint. While PyTorch started\noff as a framework focused on research, beginning with the 1.0 release, a set of\nproduction-oriented features were added that today make PyTorch an ideal end-to-\nend platform from research to large-scale production.\n\nWhat deploying to production means will vary with the use case:\n\n= Perhaps the most natural deployment for the models we developed in part 2\nwould be to set up a network service providing access to our models. We'll do\nthis in two versions using lightweight Python web frameworks: Flask (http://\nflask.pocoo.org) and Sanic (https://sanicframework.org). The first is arguably\none of the most popular of these frameworks, and the latter is similar in spirit\nbut takes advantage of Python\u2019s new async/await support for asynchronous\noperations for efficiency.\n\n= We can export our model to a well-standardized format that allows us to ship it\nusing optimized model processors, specialized hardware, or cloud services. For\nPyTorch models, the Open Neural Network Exchange (ONNX) format fills this\nrole.\n\n= We may wish to integrate our models into larger applications. For this it would\nbe handy if we were not limited to Python. Thus we will explore using PyTorch\nmodels from C++ with the idea that this also is a stepping-stone to any language.\n\n= Finally, for some things like the image zebraification we saw in chapter 2, it may\nbe nice to run our model on mobile devices. While it is unlikely that you will have\na CT module for your mobile, other medical applications like do-it-yourself skin\nscreenings may be more natural, and the user might prefer running on the\ndevice versus having their skin sent to a cloud service. Luckily for us, PyTorch has\ngained mobile support recently, and we will explore that.\n\nAs we learn how to implement these use cases, we will use the classifier from chapter\n14 as our first example for serving, and then switch to the zebraification model for the\nother bits of deployment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.2,
                        "section_name": "Serving PyTorch models",
                        "section_path": "./screenshots-images-2/chapter_16/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_2/23ff6225-8b82-483a-90ef-ccad7d06afda.png",
                            "./screenshots-images-2/chapter_16/section_2/0bea2ccd-13b1-4938-82bf-9748e88d0ec7.png",
                            "./screenshots-images-2/chapter_16/section_2/7e39665a-e189-43e5-a7b8-13c52235e2da.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "15.1\n\n15.11\n\nServing PyTorch models\n\nWe'll begin with what it takes to put our model on a server. Staying true to our hands-\non approach, we'll start with the simplest possible server. Once we have something\nbasic that works, we'll take look at its shortfalls and take a stab at resolving them.\n\nFinally, we'll look at what is, at the time of writing, the future. Let\u2019s get something that\nlistens on the network.'\n\nOur model behind a Flask server\nFlask is one of the most widely used Python modules. It can be installed using pip:*\n\npip install Flask\n\nThe API can be created by decorating functions.\n\nListing 15.1 fiask_hello_world.py:1\n\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/hello\")\ndef hello():\nreturn \"Hello World!\"\n\nif _ name == '__main__':\napp.run(host='0.0.0.0', port=8000)\n\nWhen started, the application will run at port 8000 and expose one route, /hello, that\nreturns the \u201cHello World\u201d string. At this point, we can augment our Flask server by\nloading a previously saved model and exposing it through a POST route. We will use\nthe nodule classifier from chapter 14 as an example.\n\nWe'll use Flask\u2019s (somewhat curiously imported) request to get our data. More pre-\ncisely, request.files contains a dictionary of file objects indexed by field names. We'll use\nJSON to parse the input, and we'll return a JSON string using flask\u2019s jsonify helper.\n\nInstead of /hello, we will now expose a /predict route that takes a binary blob (the\npixel content of the series) and the related metadata (a JSON object containing a dic-\ntionary with shape as a key) as input files provided with a POST request and returns a\nJSON response with the predicted diagnosis. More precisely, our server takes one sam-\nple (rather than a batch) and returns the probability that it is malignant.\n\nIn order to get to the data, we first need to decode the JSON to binary, which we\ncan then decode into a one-dimensional array with numpy. frombuffer. We'll convert\nthis to a tensor with torch. from_numpy and view its actual shape.\n\nThe actual handling of the model is just like in chapter 14: we'll instantiate Luna-\nModel from chapter 14, load the weights we got from our training, and put the model\nin eval mode. As we are not training anything, we'll tell PyTorch that we will not want\ngradients when running the model by running in a with torch.no_grad() block.\n\nListing 15.2 flask_server.py:1\n\nimport numpy as np\n\nimport sys\n\nimport os\n\nimport torch\n\nfrom flask import Flask, request, jsonify\nimport json\n\nfrom p2ch13.model_cls import LunaModel\n\napp = Flask(__name_) Sets up our model, loads\nthe weights, and moves\n\nmodel = LunaModel () <t to evaluation mode\n\nmodel. load_state_dict (torch. load(sys.argv[1),\nmap_location='cpu\") ['model_state'])\nmodel.eval()\n\ndef run_inference(in_tensor) :\nwith torch.no_grad(): \u00ab\u2014_l Mo autograd for us.\n# LunaModel takes a batch and outputs a tuple (scores, probs)\nout_tensor = model (in_tensor.unsqueeze(0)) [1] .squeeze(0)\nprobs = out_tensor.tolist()\nout = {'prob_malignant': probs[1)}\nreturn out We expect a form submission\n(HTTP POST) at the \u201c/predict\u201d\n@app.route(\"/predict\", methods=[\"POST\"])}) < endpoint.\ndef predict ():\n\nmeta = json.load(request.files['meta')) \u201c Our request will have\nblob = request.files['blob'].read() one file called meta.\nin_tensor = torch. from_numpy (np. frombuf fer (\n\nblob, dtype=np.float32)) *+\u2014 Converts our data from\nin_tensor = in_tensor.view(*meta['shape']) binary blob to torch\nout = run_inference(in_tensor)\nreturn jsonify(out) + Encodes our response\n\ncontent as JSON\n\nif __name__ \u2018main o':\napp.run(host='0.0.0.0', port=8000)\n\nprint (sys.argv[1])\n\nRun the server as follows:\n\npython3 -m p3ch15.flask_server\n data/part2/models/cls_2019-10-19_15.48.24 final _cls.best.state\n\nWe prepared a trivial client at cls_client.py that sends a single example. From the code\ndirectory, you can run it as\n\npython3 p3chi5/cls_client.py\n\nIt should tell you that the nodule is very unlikely to be malignant. Clearly, our server\ntakes inputs, runs them through our model, and returns the outputs. So are we done?\nNot quite. Let\u2019s look at what could he hetter in the next section.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.3,
                        "section_name": "What we want from deployment",
                        "section_path": "./screenshots-images-2/chapter_16/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_3/188d3dd5-5dd6-4a19-8dc0-ba7c043f3ac3.png",
                            "./screenshots-images-2/chapter_16/section_3/d6625c72-e20e-41e8-b320-4b2f3c9b37b3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What we want from deployment\n\nLet\u2019s collect some things we desire for serving models.\u2019 First, we want to support mod-\nern protocols and their features. Old-school HTTP is deeply serial, which means when a\nclient wants to send several requests in the same connection, the next requests will\nonly be sent after the previous request has been answered. Not very efficient if you\nwant to send a batch of things. We will partially deliver here\u2014our upgrade to Sanic\ncertainly moves us to a framework that has the ambition to be very efficient.\n\n\nWhen using GPUs, it is often much more efficient to batch requests than to process\nthem one by one or fire them in parallel. So next, we have the task of collecting requests\nfrom several connections, assembling them into a batch to run on the GPU, and then\ngetting the results back to the respective requesters. This sounds elaborate and (again,\nwhen we write this) seems not to be done very often in simple tutorials. That is reason\nenough for us to do it properly here! Note, though, that until latency induced by the\nduration of a model run is an issue (in that waiting for our own run is OK; but waiting\nfor the batch that\u2019s running when the request arrives to finish, and then waiting for our\nrun to give results, is prohibitive), there is little reason to run multiple batches on one\nGPU ata given time. Increasing the maximum batch size will generally be more efficient.\n\nWe want to serve several things in parallel. Even with asynchronous serving, we\nneed our model to run efficiently on a second thread\u2014this means we want to escape\nthe (in) famous Python global interpreter lock (GIL) with our model.\n\nWe also want to do as little copying as possible. Both from a memory-consumption\nand a time perspective, copying things over and over is bad. Many HTTP things are\nencoded in Base64 (a format restricted to 6 bits per byte to encode binary in more or\nless alphanumeric strings), and\u2014say, for images\u2014decoding that to binary and then\nagain to a tensor and then to the batch is clearly relatively expensive. We will partially\ndeliver on this\u2014we\u2019ll use streaming PUT requests to not allocate Base64 strings and to\navoid growing strings by successively appending to them (which is terrible for perfor-\nmance for strings as much as tensors). We say we do not deliver completely because we\nare not truly minimizing the copying, though.\n\nThe last desirable thing for serving is safety. Ideally, we would have safe decoding.\nWe want to guard against both overflows and resource exhaustion. Once we have a\nfixed-size input tensor, we should be mostly good, as it is hard to crash PyTorch start-\ning from fixed-sized inputs. The stretch to get there, decoding images and the like, is\nlikely more of a headache, and we make no guarantees. Internet security is a large\nenough field that we will not cover it at all. We should note that neural networks are\nknown to be susceptible to manipulation of the inputs to generate desired but wrong\nor unforeseen outputs (known as adversarial examples), but this isn\u2019t extremely perti-\nnent to our application, so we'll skip it here.\n\nEnough talk. Let\u2019s improve on our server.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.4,
                        "section_name": "Request batching",
                        "section_path": "./screenshots-images-2/chapter_16/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_4/63ada83b-b046-4ea3-b9d8-d69799ffd799.png",
                            "./screenshots-images-2/chapter_16/section_4/f9c6e882-8027-4f64-b0b6-226531256ba7.png",
                            "./screenshots-images-2/chapter_16/section_4/a34b76d0-9949-458f-9a6e-891360c4d5ac.png",
                            "./screenshots-images-2/chapter_16/section_4/773e14d5-da41-4a5c-be84-25b2347ec045.png",
                            "./screenshots-images-2/chapter_16/section_4/3941e62c-34c4-4d1d-91d2-a44626f56ddb.png",
                            "./screenshots-images-2/chapter_16/section_4/cca7964b-d393-479c-8368-a023f670ab85.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Request batching\nOur second example server will use the Sanic framework (installed via the Python\npackage of the same name). This will give us the ability to serve many requests in par-\nallel using asynchronous processing, so we'll tick that off our list. While we are at it, we\nwill also implement request batching.\n\nAsynchronous programming can sound scary, and it usually comes with lots of ter-\nminology. But what we are doing here is just allowing functions to non-blockingly wait\nfor results of computations or events.*\n\nREQUEST\nPROCESSOR\n(SoTTOM HALF)\n\nNEEDS_PROCESSING\n(BASED ON QUEUE SIZE\nOR TIMER)\n\nFigure 15.1 Dataflow with request batching\n\nIn order to do request batching, we have to decouple the request handling from run-\nning the model. Figure 15.1 shows the flow of the data.\n\nAt the top of figure 15.1 are the clients, making requests. One by one, these go\nthrough the top half of the request processor. They cause work items to be enqueued\nwith the request information. When a full batch has been queued or the oldest\nrequest has waited for a specified maximum time, a model runner takes a batch from\nthe queue, processes it, and attaches the result to the work items. These are then pro-\ncessed one by one by the bottom half of the request processor.\n\nIMPLEMENTATION\n\nWe implement this by writing two functions. The model runner function starts at the\nbeginning and runs forever. Whenever we need to run the model, it assembles a batch\nof inputs, runs the model in a second thread (so other things can happen), and\nreturns the result.\n\nThe request processor then decodes the request, enqueues inputs, waits for the\nprocessing to be completed, and returns the output with the results. In order to\nappreciate what asynchronous means here, think of the model runner as a wastepaper\nbasket. All the figures we scribble for this chapter can be quickly disposed of to the\nright of the desk. But every once in a while\u2014either because the basket is full or when\nit is time to clean up in the evening\u2014we need to take all the collected paper out to the\ntrash can. Similarly, we enqueue new requests, trigger processing if needed, and wait\nfor the results before sending them out as the answer to the request. Figure 15.2 shows\nour two functions in the blocks we execute uninterrupted before handing back to the\nevent loop.\n\nA slight complication relative to this picture is that we have two occasions when we\nneed to process events: if we have accumulated a full batch, we start right away; and\nwhen the oldest request reaches the maximum wait time, we also want to run. We\nsolve this by setting a timer for the latter.\u00b0\n\nD = GET_REQUEST_DATAL) WHILE TRUE:\nIM = DECODE_IMAGE_TO_TENSOR(D)\n\nWORK_ITEM['INPUT'] = IMAGE\n\nADD_TO_QUEUE( WORK_ITEM) BATCH = GET_SATCH_FROM_GUEUE()\nSCHEDULE_NEXT_PROCESSOR_RUN() IF MORB_WORK_LEFT:\n\nWAIT_FOR_READY( WORK ITEM) \u2018SCHEDULE_NEXT_PROCESSOR_RUN()\n\nRESULT = LAUNCH MODEL. IN_OTHERTHREAD( BATCH)\n\nIM_OUT = WORK_ITEM[RESULT']\nRETURN ENCODE_IN_RESPONSE(IM_OUT)\n\nMODEL EXECUTION (LAUNCHED BY MODEL RUNNER)\nRUNS IN OTHER THREAD To NOT BLOCK\nRUN-MODELINWIT() # NO GIL ONCE IN UIT\nSIGNAL TO EVENT LOOP (AND THUS MODEL RUNNER)\n\nFigure 15.2 Our asynchronous server consists of three blocks: request processor, model runner, and model\nexecution. These blocks are a bit like functions, but the first two will yield to the event loop in between.\n\n\u00ae An alternative might be to forgo the timer and just run whenever the queue is not empty. This would\npotentially run smaller \u201cfirst\u201d batches, but the overall performance impact might not be so large for most\napplications.\n\nAll our interesting code is in a ModelRunner class, as shown in the following listing.\n\nmodel. cot,\n\nListi .3 request_batching_server.py:32, ModelRunner\nclass ModelRunner: Loads and instantiates the\ndef _init__(self, model_name) : model. This is the (only) thing\nself.model_name = model_name we will need to change for\nThis will self.queue = [] 4 The queue switching to the JIT. For now,\nbecome we import the CycleGAN (with\nour lock. le. lock = N the slight modification of\n0 BREE ERIE OCS BONS standardizing to 0..1 input\n- i and output) from\nself.model = get_pretrained_model (self.model_name,\nOur signal map_location=device) y p3ch15/cyclegan.py.\nrun the\n\nneeds_processing = None\n\nself.needs_processing_timer = None 7 Finally, the timer\n\nModelRunner first loads our model and takes care of some administrative things. In\naddition to the model, we also need a few other ingredients. We enter our requests\ninto a queue. This is a just a Python list in which we add work items at the back and\nremove them in the front.\n\nWhen we modify the queue, we want to prevent other tasks from changing the\nqueue out from under us. To this effect, we introduce a queue_lock that will be an\nasyncio.Lock provided by the asyncio module. As all asyncio objects we use here\nneed to know the event loop, which is only available after we initialize the application,\nwe temporarily set it to None in the instantiation. While locking like this may not be\nstrictly necessary because our methods do not hand back to the event loop while hold-\ning the lock, and operations on the queue are atomic thanks to the GIL, it does explic-\nitly encode our underlying assumption. If we had multiple workers, we would need to\nlook at locking. One caveat: Python\u2019s async locks are not threadsafe. (Sigh.)\n\nModelRunner waits when it has nothing to do. We need to signal it from Request-\nProcessor that it should stop slacking off and get to work. This is done via an\nasyncio.Event called needs_processing. ModelRunner uses the wait() method to\nwait for the needs_processing event. The RequestProcessor then uses set () to sig-\nnal, and ModelRunner wakes up and clear ()s the event.\n\nFinally, we need a timer to guarantee a maximal wait time. This timer is created\nwhen we need it by using app.loop.call_at. It sets the needs_processing event; we\njust reserve a slot now. So actually, sometimes the event will be set directly because a\nbatch is complete or when the timer goes off. When we process a batch before the\ntimer goes off, we will clear it so we don't do too much work.\n\nFROM REQUEST TO QUEUE\n\nNext we need to be able to enqueue requests, the core of the first part of Request-\nProcessor in figure 15.2 (without the decoding and reencoding). We do this in our\nfirst async method, process_input.\n\nListing 15.4 request_batching_server.\n\nasync def process_input (self, input):\nour_task = {\"done_event\": asyncio.Event(loop=app.loop), <\u2014 gee. up the\n\n\u201cinput\": input, task\n\"time\": app.loop.time() } With the lock, we data\nasyne with self.queue_lock: \u201c add our task and ...\nif len(self.queue) >= MAX_QUEUE_SIZE:\nraise HandlingError(*I'm too busy\", code=503) ... schedule processing.\nself .queue.append(our_task) Processing will set\nself.schedule_processing_if_needed() <q-\u2014\u2014. needs_processing if we have a\nfull batch. If we don\u2019t and no\nawait our_task[\"done_event\"] .wait() timer is set, it will set one to\n\nreturn our_task[\"output\") when the max wait time is up.\n\nWaits (and hands back to the loop using\nawait) for the processing to finish\n\nWe set up a little Python dictionary to hold our task\u2019s information: the input of\ncourse, the time it was queued, and a done_event to be set when the task has been\nprocessed. The processing adds an output.\n\nHolding the queue lock (conveniently done in an async with block), we add our\ntask to the queue and schedule processing if needed. As a precaution, we error out if\nthe queue has become too large. Then all we have to do is wait for our task to be pro-\ncessed, and return it.\n\nNOTE It is important to use the loop time (typically a monotonic clock),\nwhich may be different from the time.time(). Otherwise, we might end up\nwith events scheduled for processing before they have been queued, or no\nprocessing at all.\n\nThis is all we need for the request processing (except decoding and encoding).\n\nRUNNING BATCHES FROM THE QUEUE\nNext, let's look at the model_runner function on the right side of figure 15.2, which\ndoes the model invocation.\n\nListing 15.5 request_batching_server.py:71, .run_model\n\nasync def model_runner (self):\nself.queue_lock = asyncio.Lock(loop=app. loop)\nself.needs_processing = asyncio.Event (loop=app.loop)\nwhile True:\n\nawait self.needs_processing.wait() | <\u2014\u2014 Waits until there is something to do\nself .needs_processing.clear()\nif self.needs_processing_timer is not None: <+\u2014\u2014 Cancels the timer if it is set\n\nself.needs_processing_timer.cancel ()\n\nself.needs_processing_timer = None\nasync with self.queue_lock: Grabs a batch and schedules\n# ... line 87 the running of the next batch,\nto_process = self.queue[:MAX_BATCH_SIZE) if needed\ndel self.queue[:len(to_process) }\n\nself.schedule_processing_if_needed()\nbatch = torch.stack((t[\"input\"] for t in to_process], dim=0)\n# we could delete inputs here...\n\nRuns the model in a separate\n\nresult = await app.loop.run_in_executor ( thread, moving data to the\nNone, functools.partial(self.run_model, batch) device and then handing over\n\n) to the model. We continue\n\nfor t, r in zip(to_process, result): \u00ab\u2014\\ processing after it is done.\nt[\"output\"] =r\n\nAdds the results to the work-\n\nt[{\"done_event\"] .set() item and sets the ready\n\ndel to_process\n\nAs indicated in figure 15.2, model_runner does some setup and then infinitely loops\n(but yields to the event loop in between). It is invoked when the app is instantiated, so\nit can set up queue_lock and the needs_processing event we discussed earlier. Then\nit goes into the loop, await-ing the needs_processing event.\n\nWhen an event comes, first we check whether a time is set and, if so, clear it,\nbecause we'll be processing things now. Then model_runner grabs a batch from the\nqueue and, if needed, schedules the processing of the next batch. It assembles the\nbatch from the individual tasks and launches a new thread that evaluates the model\nusing asyncio's app. loop. run_in_executor. Finally, it adds the outputs to the tasks\nand sets done_event.\n\nAnd that\u2019s basically it. The web framework\u2014roughly looking like Flask with async\nand await sprinkled in\u2014needs a little wrapper. And we need to start the model_runner\nfunction on the event loop. As mentioned earlier, locking the queue really is not nec-\nessary if we do not have multiple runners taking from the queue and potentially inter-\nrupting each other, but knowing our code will be adapted to other projects, we stay on\nthe safe side of losing requests.\n\nWe start our server with\n\npython3 -m p3ch15.request_batching_server data/plch2/horse2zebra_0.4.0.pth\n\nNow we can test by uploading the image data/plch2/horse.jpg and saving the result:\n\ncurl -T data/pich2/horse. jpg\nw http://localhost:8000/image --output /tmp/res.jpg\n\nNote that this server does get a few things right\u2014it batches requests for the GPU and\nruns asynchronously\u2014but we still use the Python mode, so the GIL hampers running\nour model in parallel to the request serving in the main thread. It will not be safe for\npotentially hostile environments like the internet. In particular, the decoding of\nrequest data seems neither optimal in speed nor completely safe.\n\nIn general, it would be nicer if we could have decoding where we pass the request\nstream to a function along with a preallocated memory chunk, and the function\ndecodes the image from the stream to us. But we do not know of a library that does\nthings this way.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.5,
                        "section_name": "Exporting models",
                        "section_path": "./screenshots-images-2/chapter_16/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_5/e5e7ce84-5d99-49cd-95c7-e214781a89db.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Exporting models\n\nSo far, we have used PyTorch from the Python interpreter. But this is not always desir-\nable: the GIL is still potentially blocking our improved web server. Or we might want\nto run on embedded systems where Python is too expensive or unavailable. This is\nwhen we export our model. There are several ways in which we can play this. We might\ngo away from PyTorch entirely and move to more specialized frameworks. Or we\nmight stay within the PyTorch ecosystem and use the JIT, a just in time compiler for a\nPyTorch-centric subset of Python. Even when we then run the JITed model in Python,\nwe might be after two of its advantages: sometimes the JIT enables nifty optimizations,\nor\u2014as in the case of our web server\u2014we just want to escape the GIL, which JITed\nmodels do. Finally (but we take some time to get there), we might run our model\nunder libtorch, the C++ library PyTorch offers, or with the derived Torch Mobile.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.6,
                        "section_name": ". Interoperability beyond PyTorch with ONNX",
                        "section_path": "./screenshots-images-2/chapter_16/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_6/a528ca31-4cac-41d8-9a2b-cd6d1c9841ad.png",
                            "./screenshots-images-2/chapter_16/section_6/188b01b3-1159-4046-8d6e-ef8ffc729f81.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "_ Interoperability beyond PyTorch with ONNX\n\nSometimes we want to leave the PyTorch ecosystem with our model in hand\u2014for\nexample, to run on embedded hardware with a specialized model deployment pipe-\nline. For this purpose, Open Neural Network Exchange provides an interoperational\nformat for neural networks and machine learning models (https://onnx.ai). Once\nexported, the model can be executed using any ONNX-compatible runtime, such as\nONNX Runtime,\" provided that the operations in use in our model are supported by\nthe ONNX standard and the target runtime. It is, for example, quite a bit faster on the\nRaspberry Pi than running PyTorch directly. Beyond traditional hardware, a lot of spe-\ncialized AI accelerator hardware supports ONNX (https://onnx.ai/supported-tools\n-html#deployModel).\n\nIn a way, a deep learning model is a program with a very specific instruction set,\nmade of granular operations like matrix multiplication, convolution, relu, tanh, and\nso on. As such, if we can serialize the computation, we can reexecute it in another run-\ntime that understands its low-level operations. ONNX is a standardization of a format\ndescribing those operations and their parameters.\n\nMost of the modern deep learning frameworks support serialization of their com-\nputations to ONNX, and some of them can load an ONNX file and execute it\n(although this is not the case for PyTorch). Some low-footprint (\u201cedge\u201d) devices\naccept an ONNX files as input and generate low-level instructions for the specific\ndevice. And some cloud computing providers now make it possible to upload an\nONNX file and see it exposed through a REST endpoint.\n\nIn order to export a model to ONNX, we need to run a model with a dummy\ninput: the values of the input tensors don\u2019t really matter; what matters is that they are\nthe correct shape and type. By invoking the torch.onnx.export function, PyTorch\n\nwill trace the computations performed by the model and serialize them into an ONNX\nfile with the provided name:\n\ntorch.onnx.export(seg_model, dummy_input, \"seg_model.onnx\")\n\nThe resulting ONNX file can now be run in a runtime, compiled to an edge device, or\nuploaded to a cloud service. It can be used from Python after installing onnxruntime\nor onnxruntime-gpu and getting a batch as a NumPy array.\n\nListing 15.6 onnx_example.py\n\nimport onnxruntime The ONNX runtime API uses\nsessions to define models and\n\nsess = onnxruntime. InferenceSession(\"seg_model.onnx\") then calls the run method\n\ninput_name = sess.get_inputs() [0] .name with a set of named inputs.\n\npred_onnx, = sess.run(None, {input_name: batch}) eur when dealing ich\ncomputations defined in\nstatic graphs.\n\nNot all TorchScript operators can be represented as standardized ONNX operators. If\nwe export operations foreign to ONNX, we will get errors about unknown aten opera-\ntors when we try to use the runtime.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.7,
                        "section_name": "PyTorch\u2019s own export: Tracing",
                        "section_path": "./screenshots-images-2/chapter_16/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_7/5f84692f-13e3-4f85-89d0-ec42a745dc5b.png",
                            "./screenshots-images-2/chapter_16/section_7/8366289a-3b6a-4011-9750-acfe7b0195b6.png",
                            "./screenshots-images-2/chapter_16/section_7/726e871a-5c6c-4877-9241-6ecf64202734.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "PyTorch\u2019s own export: Tracing\n\nWhen interoperability is not the key, but we need to escape the Python GIL or other-\nwise export our network, we can use PyTorch\u2019s own representation, called the Torch-\nScript graph. We will see what that is and how the JIT that generates it works in the next\nsection. But let\u2019s give ita spin right here and now.\n\nThe simplest way to make a TorchScript model is to trace it. This looks exactly like\nONNX exporting. This isn\u2019t surprising, because that is what the ONNX model uses\nunder the hood, too. Here we just feed dummy inputs into the model using the\ntorch.jit.trace function. We import UNetWrapper from chapter 13, load the\ntrained parameters, and put the model into evaluation mode.\n\nBefore we trace the model, there is one additional caveat: none of the parameters\nshould require gradients, because using the torch.no_grad() context manager is\nstrictly a runtime switch. Even if we trace the model within no_grad but then run it\noutside, PyTorch will record gradients. If we take a peek ahead at figure 15.4, we see\nwhy: after the model has been traced, we ask PyTorch to execute it. But the traced\nmodel will have parameters requiring gradients when executing the recorded opera-\ntions, and they will make everything require gradients. To escape that, we would have\nto run the traced model in a torch.no_grad context. To spare us this\u2014from experi-\nence, it is easy to forget and then be surprised by the lack of performance\u2014we loop\nthrough the model parameters and set all of them to not require gradients.\n\nBut then all we need to do is call torch. jit.trace.?\n\nsting 15.7 trace_example.py\n\nimport torch\nfrom p2ch13.model_segq import UNetWrapper\n\nseg_dict = torch. load(\u2018data-unversioned/part2/models/p2ch13/seg_2019-10-20_15\nw\u2122 .57.21_none.best.state\", map_location='cpu')\n\nseg_model = UNetWrapper(in_channels=8, n_classes=1, depth=4, wf=3,\n\nw padding=True, batch_norm=True, up_mode='upconv')\n\nseg_model. load_state_dict (seg_dict['model_state'])\n\nseg_model.eval()\n\nfor p in seg_model.parameters(): S Sets the parameters to\np.requires_grad_ (False) not require gradients\n\ndummy_input = torch.randn({1, 8, 512, 512)\ntraced_seg_model = torch.jit.trace(seg_model, dummy_input) <\u2014 The tracing\n\nThe tracing gives us a warning:\n\nTracerWarning: Converting a tensor to a Python index might cause the trace\nto be incorrect. We can't record the data flow of Python values, so this\nvalue will be treated as a constant in the future. This means the trace\nmight not generalize to other inputs!\n\nreturn layer[:, :, diff_y: (diff_y + target_size[0]), diff_x: (diff_x +\nwe target_size[1i]})]\n\nThis stems from the cropping we do in U-Net, but as long as we only ever plan to feed\nimages of size 512 x 512 into the model, we will be OK. In the next section, we'll take\na closer look at what causes the warning and how to get around the limitation it high-\nlights if we need to. It will also be important when we want to convert models that are\nmore complex than convolutional networks and U-Nets to TorchScript.\n\nWe can save the traced model\n\ntorch. jit.save(traced_seg_model, 'traced_seg_model.pt')\n\nand load it back without needed anything but the saved file, and then we can call it:\n\nloaded_model = torch. jit.load('traced_seg_model.pt')\nprediction = loaded_model (batch)\n\nThe PyTorch JIT will keep the model's state from when we saved it: that we had put it\ninto evaluation mode and that our parameters do not require gradients. If we had not\ntaken care of it beforehand, we would need to use with torch.no_grad(): in the\nexecution.\n\nTIP You can run the JITed and exported PyTorch model without keeping the\nsource. However, we always want to establish a workflow where we automati-\ncally go from source model to installed JITed model for deployment. If we do\nnot, we will find ourselves in a situation where we would like to tweak some-\nthing with the model but have lost the ability to modify and regenerate.\nAlways keep the source, Luke!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.8,
                        "section_name": "Our server with a traced model",
                        "section_path": "./screenshots-images-2/chapter_16/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_8/e61b6910-4449-48ad-908c-1d1cdba987d4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Our server with a traced model\n\nNow is a good time to iterate our web server to what is, in this case, our final version.\nWe can export the traced CycleGAN model as follows:\n\npython3 p3chl5/cyclegan.py data/plch2/horse2zebra_0.4.0.pth\nw data/p3ch15/traced_zebra_model.pt\n\nNow we just need to replace the call to get_pretrained_model with torch. jit.load\nin our server (and drop the now-unnecessary import of get_pretrained_model). This\nalso means our model runs independent of the GIL\u2014and this is what we wanted our\nserver to achieve here. For your convenience, we have put the small modifications in\nrequest_batching_jit_server.py. We can run it with the traced model file path as a\ncommand-line argument.\n\nNow that we have had a taste of what the JIT can do for us, let\u2019s dive into the\ndetails!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.9,
                        "section_name": "Interacting with the PyTorch JIT",
                        "section_path": "./screenshots-images-2/chapter_16/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_9/5cd8ed44-41a1-4c3e-a358-3734ae325865.png",
                            "./screenshots-images-2/chapter_16/section_9/383a036e-9316-4671-9094-f0ddb3e423dc.png",
                            "./screenshots-images-2/chapter_16/section_9/e9803f3f-2944-4f8d-8318-8b1c3a75746c.png",
                            "./screenshots-images-2/chapter_16/section_9/d5311d9f-0b0c-43bb-905f-c297e969fe38.png",
                            "./screenshots-images-2/chapter_16/section_9/1d9370f5-2005-4d7d-8e51-157e6739e664.png",
                            "./screenshots-images-2/chapter_16/section_9/dfd3deb6-d407-4d22-83de-08eea9c67f74.png",
                            "./screenshots-images-2/chapter_16/section_9/c2160c31-0526-43d6-a365-aeffe635b3b8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "15.3\n\n15.3.1\n\nInteracting with the PyTorch JIT\n\nDebuting in PyTorch 1.0, the PyTorch JIT is at the center of quite a few recent innova-\ntions around PyTorch, not least of which is providing a rich set of deployment options.\n\nWhat to expect from moving beyond classic Python/PyTorch\n\nQuite often, Python is said to lack speed. While there is some truth to this, the tensor\noperations we use in PyTorch usually are in themselves large enough that the Python\nslowness between them is not a large issue. For small devices like smartphones, the\nmemory overhead that Python brings might be more important. So keep in mind that\nfrequently, the speedup gained by taking Python out of the computation is 10% or less.\n\nAnother immediate speedup from not running the model in Python only appears\nin multithreaded environments, but then it can be significant: because the intermedi-\nates are not Python objects, the computation is not affected by the menace of all\nPython parallelization, the GIL. This is what we had in mind earlier and realized when\nwe used a traced model in our server.\n\nMoving from the classic PyTorch way of executing one operation before looking at\nthe next does give PyTorch a holistic view of the calculation: that is, it can consider the\ncalculation in its entirety. This opens the door to crucial optimizations and higher-\nlevel transformations. Some of those apply mostly to inference, while others can also\nprovide a significant speedup in training.\n\nLet\u2019s use a quick example to give you a taste of why looking at several operations at\nonce can be beneficial. When PyTorch runs a sequence of operations on the GPU, it\ncalls a subprogram (kernel, in CUDA parlance) for each of them. Every kernel reads\nthe input from GPU memory, computes the result, and then stores the result. Thus\nmost of the time is typically spent not computing things, but reading from and writing\nto memory. This can be improved on by reading only once, computing several opera-\ntions, and then writing at the very end. This is precisely what the PyTorch JIT fuser\ndoes. To give you an idea of how this works, figure 15.3 shows the pointwise computa-\ntion taking place in long short-term memory (LSTM; https://en.wikipedia.org/wiki/\nLong_short-term_memory) cell, a popular building block for recurrent networks.\n\nThe details of figure 15.3 are not important to us here, but there are 5 inputs at\nthe top, 2 outputs at the bottom, and 7 intermediate results represented as rounded\nindices. By computing all of this in one go in a single CUDA function and keeping the\nintermediates in registers, the JIT reduces the number of memory reads from 12 to 5\nand the number of writes from 9 to 2. These are the large gains the JIT gets us; it can\nreduce the time to train an LSTM network by a factor of four. This seemingly simple\n\nCsIGMoID SC tank DC siemoi> > SIGMOID\n\nl\n\nake\n;\n\n! !\nome NN\nit\n\nte\na\n\nFigure 15.3 LSTM cell pointwise operations. From five inputs at the top, this block computes\ntwo outputs at the bottom. The boxes in between are intermediate results that vanilla PyTorch\nwill store in memory but the JIT fuser will just keep in registers.\n\ntrick allows PyTorch to significantly narrow the gap between the speed of LSTM and\ngeneralized LSTM cells flexibly defined in PyTorch and the rigid but highly optimized\nLSTM implementation provided by libraries like cuaDNN.\n\nIn summary, the speedup from using the JIT to escape Python is more modest than\nwe might naively expect when we have been told that Python is awfully slow, but avoid-\ning the GIL is a significant win for multithreaded applications. The large speedups in\nJiTed models come from special optimizations that the JIT enables but that are more\nelaborate than just avoiding Python overhead.\n\nThe dual nature of PyTorch as interface and backend\n\nTo understand how moving beyond Python works, it is beneficial to mentally separate\nPyTorch into several parts. We saw a first glimpse of this in section 1.4. Our PyTorch\ntorch.nn modules\u2014which we first saw in chapter 6 and which have been our main\ntool for modeling ever since\u2014hold the parameters of our network and are\nimplemented using the functional interface: functions taking and returning tensors.\nThese are implemented as a C++ extension, handed over to the C++-level autograd-\nenabled layer. (This then hands the actual computation to an internal library called\nATen, performing the computation or relying on backends to do so, but this is\nnot important.)\n\nGiven that the C++ functions are already there, the PyTorch developers made them\ninto an official API. This is the nucleus of LibTorch, which allows us to write C++ ten-\nsor operations that look almost like their Python counterparts. As the torch.nn mod-\nules are Python-only by nature, the C++ API mirrors them in a namespace torch: :nn\nthat is designed to look a lot like the Python part but is independent.\n\nThis would allow us to redo in C++ what we did in Python. But that is not what we\nwant: we want to export the model. Happily, there is another interface to the same\nfunctions provided by PyTorch: the PyTorch JIT. The PyTorch JIT provides a \u201csym-\nbolic\u201d representation of the computation. This representation is the TorchScript inter-\nmediate representation (TorchScript IR, or sometimes just TorchScript). We mentioned\nTorchScript in section 15.2.2 when discussing delayed computation. In the following\nsections, we will see how to get this representation of our Python models and how they\ncan be saved, loaded, and executed. Similar to what we discussed for the regular\nPyTorch API, the PyTorch JIT functions to load, inspect, and execute TorchScript\nmodules can also be accessed both from Python and from C++.\n\nIn summary, we have four ways of calling PyTorch functions, illustrated in figure\n15.4: from both C++ and Python, we can either call functions directly or have the JIT\nas an intermediary. All of these eventually call the C++ LibTorch functions and from\nthere ATen and the computational backend.\n\nC++ LIBTORCH\n(INCLUDES AUTOGRAD)\n\nCUSTOM OPS\n\nATEN (TEN\nCrncoes) (ATT EXTENSIONS)\n\nATEN \"NATIVE\" KERNELS\nBACKENDS: CUDNN NNPACK oe\n\nFigure 15.4 Many ways of calling into PyTorch\n\n15.3.3 TorchScript\n\nTorchScript is at the center of the deployment options envisioned by PyTorch. As\nsuch, it is worth taking a close look at how it works.\n\nThere are two straightforward ways to create a TorchScript model: tracing and\nscripting. We will look at each of them in the following sections. At a very high level,\nthe two work as follows:\n\n= In tracing, which we used in in section 15.2.2, we execute our usual PyTorch\nmodel using sample (random) inputs. The PyTorch JIT has hooks (in the C++\nautograd interface) for every function that allows it to record the computation.\nIn a way, it is like saying \u201cWatch how I compute the outputs\u2014now you can do\nthe same.\u201d Given that the JIT only comes into play when PyTorch functions\n(and also nn.Modules) are called, you can run any Python code while tracing,\nbut the JIT will only notice those bits (and notably be ignorant of control flow).\nWhen we use tensor shapes\u2014usually a tuple of integers\u2014the JIT tries to follow\nwhat's going on but may have to give up. This is what gave us the warning when\ntracing the U-Net.\n\n= In scripting, the PyTorch JIT looks at the actual Python code of our computation\nand compiles it into the TorchScript IR. This means that, while we can be sure\nthat every aspect of our program is captured by the JIT, we are restricted to\nthose parts understood by the compiler. This is like saying \u201cI am telling you how\nto do it\u2014now you do the same.\u201d Sounds like programming, really.\n\nWe are not here for theory, so let\u2019s try tracing and scripting with a very simple function\nthat adds inefficiently over the first dimension:\n\n# In(2]:\ndef myfn(x):\ny = x[0]\n\nfor i in range(1, x.size(0)):\ny=y + xfi)\nreturn y\n\nWe can trace it:\n\n# In(3]:\n\ninp = torch. randn(5,5)\n\ntraced_fn = torch.jit.trace(myfn, inp)\nprint (traced_fn.code)\n\n# oue (3): Indexing in the first\n\ndef myfn(x: Tensor) -> Tensor: line af cur kmetion Our loop\u2014but completely\ny = torch.select(x, 0, 0) + unrolled and fixed to 1...4\nyO = torch.add(y, torch.select(x, 0, 1), alpha=1) << regardless of the size of x\n\nyl torch.add(y0, torch.select(x, 0, 2), alpha=1)\ny2 torch.add(yl, torch.select(x, 0, 3), alpha=1)\n0 = torch.add(y2, torch.select(x, 0, 4), alpha=1)\nreturn _0\n\nScary, but so true!\n\nTracerWarning: Converting a tensor to a Python index might cause the trace\nto be incorrect. We can't record the data flow of Python values, so this\nvalue will be treated as a constant in the future. This means the\n\ntrace might not generalize to other inputs!\n\nWe see the big warning\u2014and indeed, the code has fixed indexing and additions for\nfive rows, and it would not deal as intended with four or six rows.\nThis is where scripting helps:\n\n# In[4]:\nseripted_fn = torch. jit.script (myfn)\nprint (scripted_fn.code)\n\njee awe ve: Tensor) -> Tensor: PyTorch constructs the\ny = torch.select(x, 0, 0) range length from the\n0 = torch.__range_length(1, torch.size(x, 0), 1) .\nyory tal\nfor _1 in range(_0): | Our for loop\u2014even if we have to take the\n\ni = torch.__derive_index(_1, 1, 1) funny-looking next line to get our index i\n\nyO = torch.add(y0, torch.select(x, 0, i), alpha=1)\nreturn yO\n\nOur loop body, which is\njust a tad more verbose\n\nWe can also print the scripted graph, which is closer to the internal representation of\nTorchScript:\n\n# In[5):\n\nxprint (scripted _fn.graph)\n\n# end: :cell_5_code[]\n\n# tag::cell_5 output[]\n\n# Out(5):\ngraph(tx.1 : Tensor): Seems a lot more\n#10 : bool = prim::Constant [value=1) () <\u00a2-_| Verbose than we need\n#2 : int = prim::Constant[value=0) ()\n#5 : int = prim::Constant(value=1] () The first\nty.1 : Tensor aten::select(tx.1, 2, %2) <-\u2014 assignment of y\n%7 : int = aten::size(%x.1, %2)\n$9 : int = ate: __range_length(%5, %7, %5) <\u2014 Constructing the range is\n& ty : Tensor = prim::Loop(%9, 10, %y.1) recognizable after we see\nblockO(%11 : int, ty.6 : Tensor): the code.\nOur for loop %i.l : int = :t__derive_index(%11, %5, %5)\nreturns the #18 : Tensor = select ($x.1, %2, %i.1) oe Body of the for loop:\nvalue (y) it ty.3 : Tensor $18, %5) selects a slice, and\ncalculates. > ($10, ty.3) adds to y\n\nreturn (ty)\n\nIn practice, you would most often use torch. jit.script in the form of a decorator:\n\n@torch.jit.script\ndef myfn(x):\n\nYou could also do this with a custom trace decorator taking care of the inputs, but\nthis has not caught on.\n\nAlthough TorchScript (the language) looks like a subset of Python, there are fun-\ndamental differences. If we look very closely, we see that PyTorch has added type spec-\nifications to the code. This hints at an important difference: TorchScript is statically\ntyped\u2014every value (variable) in the program has one and only one type. Also, the\ntypes are limited to those for which the TorchScript IR has a representation. Within\nthe program, the JIT will usually infer the type automatically, but we need to annotate\nany non-tensor arguments of scripted functions with their types. This is in stark con-\ntrast to Python, where we can assign anything to any variable.\n\nSo far, we've traced functions to get scripted functions. But we graduated from just\nusing functions in chapter 5 to using modules a long time ago. Sure enough, we can\nalso trace or script models. These will then behave roughly like the modules we know\nand love. For both tracing and scripting, we pass an instance of Module to\ntorch.jit.trace (with sample inputs) or torch.jit.script (without sample\ninputs), respectively. This will give us the forward method we are used to. If we want\nto expose other methods (this only works in scripting) to be called from the outside,\nwe decorate them with @torch.jit.export in the class definition.\n\nWhen we said that the JITed modules work like they did in Python, this includes\nthe fact that we can use them for training, too. On the flip side, this means we need to\nset them up for inference (for example, using the torch.no_grad() context) just like\nour traditional models, to make them do the right thing.\n\nWith algorithmically relatively simple models\u2014like the CycleGAN, classification\nmodels and U-Net-based segmentation\u2014we can just trace the model as we did earlier.\nFor more complex models, a nifty property is that we can use scripted or traced func-\ntions from other scripted or traced code, and that we can use scripted or traced sub-\nmodules when constructing and tracing or scripting a module. We can also trace\nfunctions by calling nn.Models, but then we need to set all parameters to not require\ngradients, as the parameters will be constants for the traced model.\n\nAs we have seen tracing already, let\u2019s look at a practical example of scripting in\nmore detail.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.1,
                        "section_name": "| Scripting the gaps of traceability",
                        "section_path": "./screenshots-images-2/chapter_16/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_10/a5031251-42a8-4435-bc94-99a715c9387e.png",
                            "./screenshots-images-2/chapter_16/section_10/64565a12-bf2a-4e5f-80e1-ab5255be3caa.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "15.3.4 Scripting the gaps of traceability\n\nIn more complex models, such as those from the Fast R-CNN family for detection or\nrecurrent networks used in natural language processing, the bits with control flow like\nfor loops need to be scripted. Similarly, if we needed the flexibility, we would find the\ncode bit the tracer warned about.\n\nListing 15.8 From utils/unet.py\n\nclass UNetUpBlock(nn.Module) :\n\ndef center_crop(self, layer, target_size):\n\u2014_, ., layer_height, layer_width = layer.size()\ndagiff_y = (layer_height - target_size[0]) // 2\ndiff_x = (layer_width - target_size[1]) // 2\nreturn layer[:, :, diff_y:(diff_y + target_size[0]),\nwe diff_x: (diff_x + target_size[1])] J\nThe tracer warns here.\n\ndef forward(self, x, bridge):\n\neropl = self.center_crop(bridge, up.shape[2:])\n\nWhat happens is that the JIT magically replaces the shape tuple up.shape with a 1D\ninteger tensor with the same information. Now the slicing [2:] and the calculation of\ndiff_x and diff_y are all traceable tensor operations. However, that does not save us,\nbecause the slicing then wants Python ints; and there, the reach of the JIT ends, giv-\ning us the warning.\n\nBut we can solve this issue in a straightforward way: we script center_crop. We\nslightly change the cut between caller and callee by passing up to the scripted center\n_crop and extracting the sizes there. Other than that, all we need is to add the\n@torch.jit.script decorator. The result is the following code, which makes the\nU-Net model traceable without warnings.\n\nListing 15.9 Rewritten excerpt from utils/unet.py\n\n@torch.jit.seript fe the sign f *\n\ndef center_crop(layer, target): i i\n\u2014, ., layer_height, layer_width = layer.size() [3 of B _size\n\n_, ., target_height, target_width = target.size()\n\ndiff_y = (layer_height - target_height) // 2 Gets the sizes within\ndiff_x = (layer_width - target_width]) // 2 the scripted part\nreturn layer[:, :, diff_y:(diff_y + target_height),\nwe diff_x:(diff_x + target_width)]\nThe indexing uses the\nclass UNetUpBlock(nn.Module) : size values we got.\ndef forward(self, x, bridge):\nwee We adapt our call to pass\ncropl = center_crop(bridge, up) \u201cJ up rather than the size.\n\nAnother option we could choose\u2014but that we will not use here\u2014would be to move\nunscriptable things into custom operators implemented in C++. The TorchVision\nlibrary does that for some specialty operations in Mask R-CNN models.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.11,
                        "section_name": "LibTorch: PyTorch in C++",
                        "section_path": "./screenshots-images-2/chapter_16/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_11/113c381c-213b-4ceb-9774-f5a14161d79c.png",
                            "./screenshots-images-2/chapter_16/section_11/fadc16e3-db10-4369-93fe-7b204b52f081.png",
                            "./screenshots-images-2/chapter_16/section_11/8f5955f7-1fab-49fa-b4f0-5b1eccc5e4d3.png",
                            "./screenshots-images-2/chapter_16/section_11/c48d7921-cba2-46f2-aeb5-57cbd72aa2d1.png",
                            "./screenshots-images-2/chapter_16/section_11/83b0b9b3-7fac-4bf5-9441-c19dc1ee9aaa.png",
                            "./screenshots-images-2/chapter_16/section_11/62f78adf-fa27-4396-bd49-901c8ea87ef7.png",
                            "./screenshots-images-2/chapter_16/section_11/c95168ba-041e-46e3-90f5-9b8f72c7c208.png",
                            "./screenshots-images-2/chapter_16/section_11/6f462935-56f1-44d6-a8d3-8a2a0e39d493.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "15.4\n\n15.4.1\n\nLibTorch: PyTorch in C++\n\nWe have seen various way to export our models, but so far, we have used Python. We'll\nnow look at how we can forgo Python and work with C++ directly.\n\nLet\u2019s go back to the horse-to-zebra CycleGAN example. We will now take the JITed\nmodel from section 15.2.3 and run it from a C++ program.\n\nRunning JITed models from C++\n\nThe hardest part about deploying PyTorch vision models in C++ is choosing an image\nlibrary to choose the data.\" Here, we go with the very lightweight library Clmg\n(http://cimg.eu). If you are very familiar with OpenCV, you can adapt the code to use\nthat instead; we just felt that CImg is easiest for our exposition.\n\nRunning a JITed model is very simple. We'll first show the image handling; it is not\nreally what we are after, so we will do this very quickly.\u201d\n\nListing 15.10 cyclegan_jit.cpp\n\n#include \"torch/script.h\" $ Includes the PyTorch script header\n\nfdefine cimg_use_jpeg and Clmg with native JPEG support\n#include \"CImg.h* me !\n\nusing namespace cimg_library;\nint main(int arge, char **argv) { Loads and decodes the\nCImg<float> image(argv[2]); | image into a float array\n\nimage = image.resize(227, 227); <\u2014 Resizes to a smaller size\n\n// ...here we need to produce an output tensor from input\n\nCImg<float> out_img(output.data_ptr<float>(), output.size(2), +t\noutput.size(3), 1, output.size(1));\n\nouk img -save(aravi2})i The method data_ptr<float>() gives us a pointer\nveruen Nt Saves the ima, to the tensor storage. With it and the shape\n) Be information, we can construct the output image.\n\nFor the PyTorch side, we include a C++ header torch/script.h. Then we need to set up\nand include the CIng library. In the main function, we load an image from a file given\non the command line and resize it (in CImg). So we now have a 227 x 227 image in\nthe CImg<float> variable image. At the end of the program, we'll create an out_img of\nthe same type from our (1, 3, 277, 277)-shaped tensor and save it.\n\nDon\u2019t worry about these bits. They are not the PyTorch C++ we want to learn, so we\ncan just take them as is.\n\nThe actual computation is straightforward, too. We need to make an input tensor\nfrom the image, load our model, and run the input tensor through it.\n\nListin, 11 cyclegan_jit.cpp\n\nPuts the image data into a tensor\nauto input_ = torch::tensor( Reshapes and rescales\ntorch: :ArrayRef<float>(image.data(), image.size())); to move from Clmg\nauto input = input_.reshape({1, 3, image.height(), conventions to\nimage.width()}) .div_ (255); \u201c PyTorch\u2019s\nauto module = torch::jit::load(argv[1)); Load the JITed mi jel\nstd: :vector<torch::jit::IValue> inputs; \u00a2\u2014___,_ | oF function from a file\ninputs .push_back (input); Packs the input into a (one-\nr\u2014 auto output_ = module. forward(inputs) .toTensor(); element) vector of [Values\nauto output = output_.contiguous().mul_(255); <\u00ab\u2014\nCalls the module and extracts the result tensor. For Makes sure our result\nefficiency, the ownership is moved, so if we held on is contiguous\n\nto the IValue, it would be empty afterward.\n\nRecall from chapter 3 that PyTorch keeps the values of a tensor in a large chunk of\nmemory in a particular order. So does CImg, and we can get a pointer to this memory\nchunk (as a float array) using image.data() and the number of elements using\nimage.size(). With these two, we can create a somewhat smarter reference: a\ntorch: :ArrayRef (which is just shorthand for pointer plus size; PyTorch uses those at\nthe C++ level for data but also for returning sizes without copying). Then we can just\nparse that into the torch: : tensor constructor, just as we would with a list.\n\nTIP Sometimes you might want to use the similar-working torch: : from_blob\ninstead of torch: : tensor. The difference is that tensor will copy the data. If\nyou do not want copying, you can use from_blob, but then you need to take care\nthat the underpinning memory is available during the lifetime of the tensor.\n\nOur tensor is only 1D, so we need to reshape it. Conveniently, CImg uses the same\nordering as PyTorch (channel, rows, columns). If not, we would need to adapt the\nreshaping and permute the axes as we did in chapter 4. As CImg uses a range of\n0...255 and we made our model to use 0...1, we divide here and multiply later.\nThis could, of course, be absorbed into the model, but we wanted to reuse our\ntraced model.\n\nA common pitfall to avoid: pre- and postprocessing\n\nWhen switching from one library to another, it is easy to forget to check that the con-\nversion steps are compatible. They are non-obvious unless we look up the memory\nlayout and scaling convention of PyTorch and the image processing library we use. If\nwe forget, we will be disappointed by not getting the results we anticipate.\n\nHere, the model would go wild because it gets extremely large inputs. However, in\nthe end, the output convention of our model is to give RGB values in the 0..1 range.\nIf we used this directly with Clmg, the result would look all black.\n\nOther frameworks have other conventions: for example OpenCV likes to store images\nas BGR instead of RGB, requiring us to flip the channel dimension. We always want\nto make sure the input we feed to the model in the deployment is the same as what\nwe fed into it in Python.\n\nLoading the traced model is very straightforward using torch: : jit: :load. Next, we\nhave to deal with an abstraction PyTorch introduces to bridge between Python and\nC++: we need to wrap our input in an IValue (or several Ivalues), the generic data type\nfor any value. A function in the JIT is passed a vector of Ivalues, so we declare that\nand then push_back our input tensor. This will automatically wrap our tensor into an\nIValue. We feed this vector of IValues to the forward and get a single one back. We\ncan then unpack the tensor in the resulting Ivalue with .toTensor.\n\nHere we see a bit about I'Values: they have a type (here, Tensor), but they could also\nbe holding int64_ts or doubles or a list of tensors. For example, if we had multiple\noutputs, we would get an IValue holding a list of tensors, which ultimately stems from\nthe Python calling conventions. When we unpack a tensor from an IValue using\n. toTensor, the IValue transfers ownership (becomes invalid). But let\u2019s not worry about\nit; we gota tensor back. Because sometimes the model may return non-contiguous data\n(with gaps in the storage from chapter 3), but CImg reasonably requires us to provide\nit with a contiguous block, we call contiguous. It is important that we assign this\ncontiguous tensor to a variable that is in scope until we are done working with the\nunderlying memory. Just like in Python, PyTorch will free memory if it sees that no\ntensors are using it anymore.\n\nSo let\u2019s compile this! On Debian or Ubuntu, you need to install cimg-dev,\nlibjpeg-dev, and 1ibx11-dev to use CImg.\n\nYou can download a C++ library of PyTorch from the PyTorch page. But given that\nwe already have PyTorch installed,'\u201d we might as well use that; it comes with all we\nneed for C++. We need to know where our PyTorch installation lives, so open Python\nand check torch.__file__, which may say /usr/local/lib/python3.7/dist-packages/\ntorch/__init_.py. This means the CMake files we need are in /usr/local/lib/\npython3.7/dist-packages/torch/share/cmake/.\n\nWhile using CMake seems like overkill for a single source file project, linking to\nPyTorch is a bit complex; so we just use the following as a boilerplate CMake file.''\n\nListing 15.12 CMakeLists.txt\n\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR) | Project name. Replace it with your\n\nproject (cyclegan-jit) <;}\u2014_! own here and on the other lines.\nneed Torch.\nfind_package (Torch REQUIRED) ~~ We ore \u2018We want to compile\nset (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS) ${TORCH_CXX_FLAGS)\") | an executable named\ncyclegan-jit from the\nadd_executable(cyclegan-jit cyclegan_jit.cpp) <q\u2014_\u2014. \u201cyclegan_jit.cpp\ntarget_link_libraries(cyclegan-jit pthread jpeg X11) + source file.\ntarget_link_libraries(cyclegan-jit \"${TORCH_LIBRARIES}\") Links to the bits ired\nset_property (TARGET cyclegan-jit PROPERTY CXX_STANDARD 14) for Clmg Cimg i i is all-\ninclude, so it does not\nappear here.\n\nIt is best to make a build directory as a subdirectory of where the source code resides\nand then in it run CMake as!\u00ae CMAKE_PREFIX_PATH=/usr/local/1lib/python3.7/\ndist-packages/torch/share/cmake/ cmake .. and finally make. This will build the\ncyclegan-jit program, which we can then run as follows:\n\n./eyclegan-jit ../traced_zebra_model.pt ../../data/plch2/horse.jpg /tmp/z.jpg\n\nWe just ran our PyTorch model without Python. Awesome! If you want to ship your\napplication, you likely want to copy the libraries from /usr/local/lib/python3.7/ dist-\npackages/torch/lib into where your executable is, so that they will always be found.\n\nC++ from the start: The C++ API\n\nThe C++ modular API is intended to feel a lot like the Python one. To get a taste, we\nwill translate the CycleGAN generator into a model natively defined in C++, but with-\nout the JIT. We do, however, need the pretrained weights, so we'll save a traced version\nof the model (and here it is important to trace not a function but the model).\n\n\u2018\u00a9 We hope you have not been slacking off about trying out things you read.\n\n\"! The code directory has a bit longer version to work around Windows issues.\n\n* You might have to replace the path with where your PyTorch or LibTorch installation is located. Note that\nthe C++ library can be more picky than the Python one in terms of compatibility: If you are using a CUDA-\nenabled library, you need to have the matching CUDA headers installed. If you get cryptic error messages\nabout \u201cCaffe2 using CUDA,\u201d you need to install a CPU-only version of the library, but CMake found a CUDA-\nenabled one.\n\nWe'll start with some administrative details: includes and namespaces.\n\nListing 15.13 cyclegan_cpp_api.cpp\n\n#include <torch/torch.h> in the on\n\n#define cimg_use_jpeg torch/torch.h header and Clmg\n\n#include <CImg.h>\n\nusing torch::Tensor; am Spelling out torch::Tensor can be tedious, so\nwe import the name into the main namespace.\n\nWhen we look at the source code in the file, we find that ConvTransposed2d is ad hoc\ndefined, when ideally it should be taken from the standard library. The issue here is\nthat the C++ modular API is still under development; and with PyTorch 1.4, the pre-\nmade ConvTranspose2d module cannot be used in Sequential because it takes an\noptional second argument.!* Usually we could just leave Sequential\u2014as we did for\nPython\u2014but we want our model to have the same structure as the Python CycleGAN\ngenerator from chapter 2.\nNext, let\u2019s look at the residual block.\n\nListing 15.14 Residual block in cyclegan_cpp_api.cpp\n\nstruct ResNetBlock : torch::nn::Module {\ntorch: :nn::Sequential conv_block;\nResNetBlock(int64_t dim) Initializes Sequential,\n: conv_block( including its submodules\ntorch: :\ntorch: :\n\nnn::ReflectionPad2d(1),\nnn\ntorch: :nn\nnn\nnn\n\nlonv2d (torch: :nn::Conv2dO0ptions(dim, dim, 3)),\nnstanceNorm2d (\nnstanceNorm2dOptions (dim) ),\nReLU(/*inplace=*/true),\neflectionPad2d(1),\n:Conv2d(torch: :nn::Conv2d0ptions(dim, dim, 3))},\n:inn::InstanceNorm2d(\n:imn::InstanceNorm2dOptions(dim))) {\nregister_module(\"conv_block\", conv_block) ; Always remember to register\n} the modules you assign, or\nbad things will happen!\n\nTensor forward(const Tensor &inp) {\n\nreturn inp + conv_block->forward(inp); <a\u2014 As might be expected, our forward\n,\n) ) function is pretty simple.\n\nJust as we would in Python, we register a subclass of torch: :nn: :Module. Our residual\nblock has a sequential conv_block submodule.\n\nAnd just as we did in Python, we need to initialize our submodules, notably\nSequential. We do so using the C++ initialization statement. This is similar to how we\n\nconstruct submodules in Python in the __init__ constructor. Unlike Python, C++\ndoes not have the introspection and hooking capabilities that enable redirection of\n__setattr__ to combine assignment to a member and registration.\n\nSince the lack of keyword arguments makes the parameter specification awkward\nwith default arguments, modules (like tensor factory functions) typically take an\noptions argument. Optional keyword arguments in Python correspond to methods of\nthe options object that we can chain. For example, the Python module\nnn.Conv2d(in_channels, out_channels, kernel_size, stride=2, padding=1) that\nwe need to convert translates to torch: :nn::Conv2d(torch::nn::Conv2d0ptions\n(in_channels, out_channels, kernel_size) .stride(2) .padding(1)). This is a bit\nmore tedious, but you're reading this because you love C++ and aren't deterred by the\nhoops it makes you jump through.\n\nWe should always take care that registration and assignment to members is in sync,\nor things will not work as expected: for example, loading and updating parameters\nduring training will happen to the registered module, but the actual module being\ncalled is a member. This synchronization was done behind the scenes by the Python\nnn.Module class, but it is not automatic in C++. Failing to do so will cause us many\nheadaches.\n\nIn contrast to what we did (and should!) in Python, we need to call m-> forward (...)\nfor our modules. Some modules can also be called directly, but for Sequential, this is\nnot currently the case.\n\nA final comment on calling conventions is in order: depending on whether you\nmodify tensors provided to functions,'* tensor arguments should always be passed as\nconst Tensor\u00e9 for tensors that are left unchanged or Tensor if they are changed. Ten-\nsors should be returned as Tensor. Wrong argument types like non-const references\n(Tensor&) will lead to unparsable compiler errors.\n\nIn the main generator class, we'll follow a typical pattern in the C++ API more\nclosely by naming our class ResNetGeneratorImp1 and promoting it to a torch module\nResNetGenerator using the TORCH_MODULE macro. The background is that we want to\nmostly handle modules as references or shared pointers. The wrapped class\naccomplishes this.\n\nsNetGenerator in cyclegan_cpp_api.cpp\n\nAdds modules to the Sequential container in the\nconstructor. This allows us to add a variable\nstruct ResNetGeneratorImpl : torch::nn::Module { number of modules in a for loop.\ntorch: :nn::Sequential model;\nResNetGeneratorImpl(int64_t input_ne = 3, int64_t output_ne = 3,\nint6\u00e94_t ngf = 64, int64_t n_blocks = 9) {\nTORCH_CHECK(n_blocks >= 0);\nmodel->push_back (torch: :nn: :ReflectionPad2d (3) ); -\u2014\u2014\u2014\u2014\u2014\n\n\nmodel->push_back (torch: :nn: :Conv2d(\n\nSpares us from torch: :nn::Conv2dO0ptions(ngf * mult, ngf * mult * 2, 3)\nreproducing some stride (2)\ntedious things -padding(1))); | <\u2014\u2014 An example of Options in action\n\nregister_module(\"model\", model);\n}\nTensor forward(const Tensor &inp) { return model->forward(inp); }\n\nai Creates a wrapper ResNetGenerator around our\n: ResNetGeneratorimp! class. As archaic as it seems,\nTORCH_MODULE (ResNetG tor); \u201d\n~ (ResNetGenerator) the matching names are important here.\n\nThat's it\u2014we\u2019ve defined the perfect C++ analogue of the Python ResNetGenerator\nmodel. Now we only need a main function to load parameters and run our model.\nLoading the image with Clmg and converting from image to tensor and tensor back\nto image are the same as in the previous section. To include some variation, we'll dis-\nplay the image instead of writing it to disk.\n\nListing 15.16 cyclegan_cpp_api.cpp main\n\nResNetGenerator model; <\u2014\u2014 Instantiates our model Declaring a guard variable\nLee Loads th is the equivalent of the\ntorch: : load (model, (1); seme torch.no_grad() context.\nnore OHNE SS parameters You can put it ina { ... }\ncimg_library: :CImg<float> image(argv[2]); block if you need to limit\n\nimage.resize(400, 400); how long you turn off\nauto input_ = gradients. .\ntorch: : tensor (torch: :ArrayRef<float>(image.data(), image.size()));\nauto input = input_.reshape({1, 3, image.height(), image.width()});\ntorch: :NoGradGuard no_grad;\n\nAs in Python, eval mode is turned on (for our\nmodel, it would not be strictly relevant).\n\nauto output = model->forward(input); SE | Again, we call\n\ncimg_library::CImg<float> out_img(output.data_ptr<float>(), than the model.\noutput.size(3), output.size(2),\n1, output.size(1));\n\ncimg_library::CImgDisplay disp(out_img, \"See a C++ API zebra!\"); <\n\nwhile (!disp.is_closed()) {\n\ndisp.wait (); Displaying the image, we need to wait for a key\n) rather than immediately exiting our program.\n\nmodel->eval ();\n\nThe interesting changes are in how we create and run the model. Just as expected, we\ninstantiate the model by declaring a variable of the model type. We load the model\nusing torch: : load (here it is important that we wrapped the model). While this looks\nvery familiar to PyTorch practitioners, note that it will work on JIT-saved files rather\nthan Python-serialized state dictionaries.\n\nWhen running the model, we need the equivalent of with torch.no_grad() :. This\nis provided by instantiating a variable of type NoGradGuard and keeping it in scope for\n\nas long as we do not want gradients. Just like in Python, we set the model into evaluation\nmode calling model ->eval () . This time around, we call model->forward with our input\ntensor and geta tensor as a result\u2014no JIT is involved, so we do not need Ivalue packing\nand unpacking.\n\nPhew. Writing this in C++ was a lot of work for the Python fans that we are. We are\nglad that we only promised to do inference here, but of course LibTorch also offers\noptimizers, data loaders, and much more. The main reason to use the API is, of\ncourse, when you want to create models and neither the JIT nor Python is a good fit.\n\nFor your convenience, CMakeLists.txt contains also the instructions for building\ncyclegan-cpp-api, so building is just like in the previous section.\n\nWe can run the program as\n\n./eyclegan_cpp_api ../traced_zebra_model.pt ../../data/plch2/horse.jpg\n\nBut we knew what the model would be doing, didn\u2019t we?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.12,
                        "section_name": "Going mobile",
                        "section_path": "./screenshots-images-2/chapter_16/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_12/553be7d5-8449-4920-896c-2cd465b888d0.png",
                            "./screenshots-images-2/chapter_16/section_12/8cd81844-4cc1-4ac2-a93e-5bf33bd41990.png",
                            "./screenshots-images-2/chapter_16/section_12/b91dcba4-35f6-4c58-8618-b01ae5410cb0.png",
                            "./screenshots-images-2/chapter_16/section_12/129bfe34-fad6-488a-9364-d17d0c496b2e.png",
                            "./screenshots-images-2/chapter_16/section_12/455cdf54-3c8c-44cf-b26e-5deead5e85a6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Going mobile\n\nAs the last variant of deploying a model, we will consider deployment to mobile\ndevices. When we want to bring our models to mobile, we are typically looking at\nAndroid and/or iOS. Here, we'll focus on Android.\n\nThe C++ parts of PyTorch\u2014LibTorch\u2014can be compiled for Android, and we\ncould access that from an app written in Java using the Android Java Native Interface\n(JNI). But we really only need a handful of functions from PyTorch\u2014loading a JITed\nmodel, making inputs into tensors and IValues, running them through the model,\nand getting results back. To save us the trouble of using the JNI, the PyTorch develop-\ners wrapped these functions into a small library called PyTorch Mobile.\n\nThe stock way of developing apps in Android is to use the Android Studio IDE,\nand we will be using it, too. But this means there are a few dozen files of administra-\ntiva\u2014which also happen to change from one Android version to the next. As such, we\nfocus on the bits that turn one of the Android Studio templates (Java App with Empty\nActivity) into an app that takes a picture, runs it through our zebra-CycleGAN, and\ndisplays the result. Sticking with the theme of the book, we will be efficient with the\nAndroid bits (and they can be painful compared with writing PyTorch code) in the\nexample app.\n\nTo infuse life into the template, we need to do three things. First, we need to define\na UI. To keep things as simple as we can, we have two elements: a Text View named head-\nline that we can click to take and transform a picture; and an ImageView to show our\npicture, which we call image_view. We will leave the picture-taking to the camera app\n(which you would likely avoid doing in an app for a smoother user experience) , because\ndealing with the camera directly would blur our focus on deploying PyTorch models.\u201d\n\nThen, we need to include PyTorch as a dependency. This is done by editing our app\u2019s\nbuild.gradle file and adding pytorch_android and pytorch_android_torchvision.\n\nListing 15.17 Additions to bulld.gradle\n\ndependencies { The dependencies section is very likely The pytorch_android\nwee already there. If not, add it at the bottom. library gets the core things\nimplementation \u2018org.pytorch:pytorch_android:1.4.0\u00b0 mentioned in the text.\n\nimplementation \u2018org.pytorch:pytorch_android_torchvision:1.4.0' \u00ab\n\n) The helper library pytorch_android_torchvision\u2014perhaps a bit immodestly named\nwhen compared to its larger TorchVision sibling contains a few utilities to convert\n\nbitmap objects to tensors, but at the time of writing not much more.\n\nWe need to add our traced model as an asset.\n\nFinally, we can get to the meat of our shiny app: the Java class derived from activity\nthat contains our main code. We'll just discuss an excerpt here. It starts with imports\nand model setup.\n\nListing 15.18 MainActivity.java part 1\n\nimport org.pytorch.Ivalue; <+\u2014\u2014 Don\u2019t you love imports?\nimport org.pytorch.Module;\n\nimport org.pytorch.Tensor;\n\nimport org.pytorch.torchvision.TensorImageUtils;\n\npublic class MainActivity extends AppCompatActivity {\nprivate org.pytorch.Module model; <\u2014\u2014 Holds our JiTed model\n\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\n\na_! In Java we have to catch the exceptions.\n\ntry {\n\nmodel = Module.load(assetFilePath(this, \u201ctraced_zebra_model.pt\")); \u00aba\n\n} catch (IOException e) { \u00bb|\nLog.e(*Zebraify*, \"Error reading assets\", e); Loads the module from a file\nfinish();\n\n}\n\n)\n\nWe need some imports from the org.pytorch namespace. In the typical style that is a\nhallmark of Java, we import IValue, Module, and Tensor, which do what we might\nexpect; and the class org. pytorch. torchvision.TensorImageUtils, which holds util-\nity functions to convert between tensors and images.\n\nFirst, of course, we need to declare a variable holding our model. Then, when our\napp is started\u2014in onCreate of our activity\u2014we\u2019ll load the module using the Model . load\n\nmethod from the location given as an argument. There is a slight complication though:\napps\u2019 data is provided by the supplier as assets that are not easily accessible from the\nfilesystem. For this reason, a utility method called assetFilePath (taken from the\nPyTorch Android examples) copies the asset to a location in the filesystem. Finally, in\nJava, we need to catch exceptions that our code throws, unless we want to (and are able\nto) declare the method we are coding as throwing them in turn.\n\nWhen we get an image from the camera app using Android\u2019s Intent mechanism, we\nneed to run it through our model and display it. This happens in the onActivityResult\nevent handler.\n\nListi: .19 MainActivityjava, part 2\nPerforms normalization, but the default is images in Gets a tensor from a bitmap, combining\nthe range of 0...1 so we do not need to transform: ( steps Uke TorehVision\u2019s Jorensor\n. converting to a float tensor with entries\nthat * have eae and a scaling divisor of 1 0 and 1) and ee\nOverride\n\nprotected void onActivityResult(int requestCode, int resultCode,\nIntent data) ( This is executed when the\nif (requestCode == REQUEST_IMAGE_CAPTURE && camera app takes a picture.\nresultCode == RESULT_OK) {\nBitmap bitmap = (Bitmap) data.getExtras().get(\"data\");\n\nfinal float[] means = {0.0f, 0.0f, 0.0\u00a3);\nfinal float[] stds = {1.0f, 1.0f\u00a3, 1.0\u00a3};\n\nfinal Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor ( +\n\nbitmap, means, stds); This looks like\nfinal Tensor outputTensor = model. forward( <\u2014. what we did in C+ +.\nIValue.from(inputTensor) ) .toTensor();\nBitmap output_bitmap = tensorToBitmap(outputTensor, means, stds,\nBitmap .Config.RGB_565); $\nimage_view.setImageBitmap (output_bitmap) ; tensorToBitmap is\n} our own invention.\n\nConverting the bitmap we get from Android to a tensor is handled by the Tensor-\nImageUtils.bitmapToFloat32Tensor function (static method), which takes two float\narrays, means and stds, in addition to bitmap. Here we specify the mean and standard\ndeviation of our input data(set), which will then be mapped to have zero mean and unit\nstandard deviation just like TorchVision\u2019s Normalize transform. Android already gives\nus the images in the 0..1 range that we need to feed into our model, so we specify mean\n0 and standard deviation 1 to prevent the normalization from changing our image.\nAround the actual call to model . forward, we then do the same IValue wrapping and\nunwrapping dance that we did when using the JIT in C++, except that our forward takes\na single [Value rather than a vector of them. Finally, we need to get back to a bitmap.\nHere PyTorch will not help us, so we need to define our own tensorToBitmap (and\nsubmit the pull request to PyTorch). We spare you the details here, as they are tedious\n\n15.5.1\n\nand full of copying (from the tensor to a float []\narray to a int[] array containing ARGB values to\nthe bitmap), but it is as itis. Itis designed to be the\ninverse of bitmapToFloat32Tensor.\n\nAnd that\u2019s all we need to do to get PyTorch\ninto Android. Using the minimal additions to the\ncode we left out here to request a picture, we have\na Zebraify Android app that looks like in what\nwe see in figure 15.5. Well done!!\u00ae\n\nWe should note that we end up with a full ver-\nsion of PyTorch with all ops on Android. This will,\nin general, also include operations you will not\nneed for a given task, leading to the question of\nwhether we could save some space by leaving\nthem out. It turns out that starting with PyTorch\n1.4, you can build a customized version of\nthe PyTorch library that includes only the opera-\ntions you need (see https://pytorch.org/mobile/\nandroid/#custom-build).\n\nImproving efficiency: Model design and\nquantization\n\nIf we want to explore mobile in more detail, our\nnext step is to try to make our models faster. Figure 15.5 Our CycleGAN zebra app\nWhen we wish to reduce the memory and compute footprint of our models, the first\nthing to look at is streamlining the model itself: that is, computing the same or very\nsimilar mappings from inputs to outputs with fewer parameters and operations. This\nis often called distillation. The details of distillation vary\u2014sometimes we try to shrink\neach weight by eliminating small or irrelevant weights;'\u2019 in other examples, we com-\nbine several layers of a net into one (DistiIBERT) or even train a fully different, sim-\npler model to reproduce the larger model\u2019s outputs (OpenNMT\u2019s original\nCTranslate). We mention this because these modifications are likely to be the first step\nin getting models to run faster.\n\nAnother approach to is to reduce the footprint of each parameter and operation:\ninstead of expending the usual 32-bit per parameter in the form of a float, we convert\nour model to work with integers (a typical choice is 8-bit). This is guantization.'*\n\n\u2018\u00a9 At the time of writing, PyTorch Mobile is still relatively young, and you may hit rough edges. On Pytorch 1.3,\nthe colors were off on an actual 32-bit ARM phone while working in the emulator. The reason is likely a bug\nin one of the computational backend functions that are only used on ARM. With PyTorch 1.4 and a newer\nphone (64-bit ARM), it seemed to work better.\n\n'\u201d Examples include the Lottery Ticket Hypothesis and WaveRNN.\n\n'* In contrast to quantization, (partially) moving to 16-bit floating-point for training is usually called reduced or\n(if some bits stay 32-bit) mixed-precision taining.\n\nPyTorch does offer quantized tensors for this purpose. They are exposed as a set\nof scalar types similar to torch. float, torch.double, and torch. long (compare sec-\ntion 3.5). The most common quantized tensor scalar types are torch.quint8 and\ntorch.gint8, representing numbers as unsigned and signed 8-bit integers, respec-\ntively. PyTorch uses a separate scalar type here in order to use the dispatch mecha-\nnism we briefly looked at in section 3.11.\n\nIt might seem surprising that using 8-bit integers instead of 32-bit floating-points\nworks at all; and typically there is a slight degradation in results, but not much. Two\nthings seem to contribute: if we consider rounding errors as essentially random, and\nconvolutions and linear layers as weighted averages, we may expect rounding errors to\ntypically cancel.'* This allows reducing the relative precision from more than 20 bits in\n32-bit floating-points to the 7 bits that signed integers offer. The other thing quantiza-\ntion does (in contrast to training with 16-bit floating-points) is move from floating-\npoint to fixed precision (per tensor or channel). This means the largest values are\nresolved to 7-bit precision, and values that are one-eighth of the largest values to only\n7-3 = 4 bits. But if things like L1 regularization (briefly mentioned in chapter 8)\nwork, we might hope similar effects allow us to afford less precision to the smaller val-\nues in our weights when quantizing. In many cases, they do.\n\nQuantization debuted with PyTorch 1.3 and is still a bit rough in terms of sup-\nported operations in PyTorch 1.4. It is rapidly maturing, though, and we recommend\nchecking it out if you are serious about computationally efficient deployment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.13,
                        "section_name": "Emerging technology: Enterprise serving of PyTorch models",
                        "section_path": "./screenshots-images-2/chapter_16/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_13/55ac40a4-135e-4f02-ad5b-1059e9b6ae95.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Emerging technology: Enterprise serving of PyTorch models\n\nWe may ask ourselves whether all the deployment aspects discussed so far should\ninvolve as much coding as they do. Sure, it is common enough for someone to code\nall that. As of early 2020, while we are busy with the finishing touches to the book, we\nhave great expectations for the near future; but at the same time, we feel that the\ndeployment landscape will significantly change by the summer.\n\nCurrently, RedisAI (https://github.com/RedisAI/redisai-py), which one of the\nauthors is involved with, is waiting to apply Redis goodness to our models. PyTorch has just\nexperimentally released TorchServe (after this book is finalized, see https: / / pytorch.org/\nblog/pytorch-library-updates-new-model-serving-library/#torchserve-experimental).\n\nSimilarly, MLflow (https://mlflow.org) is building out more and more support, and\nCortex (https: / /cortex.dev) wants us to use it to deploy models. For the more specific task\nof information retrieval, there also is EuclidesDB (https: / /euclidesdb.readthedocs.io/\nen/latest) to do Al-based feature databases.\n\nExciting times, but unfortunately, they do not sync with our writing schedule. We\nhope to have more to tell in the second edition (or a second book)!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 16.14,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_16/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_16/section_14/455b44a0-6d98-448f-b97c-2271d0a0b6d1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conclusion\nThis concludes our short tour of how to get our models out to where we want to apply\nthem. While the ready-made Torch serving is not quite there yet as we write this, when\nit arrives you will likely want to export your models through the JIT\u2014so you'll be glad\nwe went through it here. In the meantime, you now know how to deploy your model\nto a network service, in a C++ application, or on mobile. We look forward to seeing\nwhat you will build!\n\nHopefully we've also delivered on the promise of this book: a working knowledge\nof deep learning basics, and a level of comfort with the PyTorch library. We hope\nyou've enjoyed reading as much as we've enjoyed writing.\u201d\u201d\n",
                        "extracted-code": ""
                    }
                ]
            }
        ]
    }
}