{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "Introduction to\ndeep reinforcement learning",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "Introduction to\ndeep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/52837933-6f28-4644-b137-c0dd55c36a7d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Humans naturally pursue feelings of happiness. From picking out our meals to advancing\nour careers, every action we choose is derived from our drive to experience rewarding\nmoments in life. Whether these moments are self-centered pleasures or the more generous of\ngoals, whether they bring us immediate gratification or long-term success, they\u2019re still our\nperception of how important and valuable they are. And to some extent, these moments are\nthe reason for our existence.\n\nOur ability to achieve these precious moments seems to be correlated with intelligence;\n\u201cintelligence\u201d is defined as the ability to acquire and apply knowledge and skills. People who\nare deemed by society as intelligent are capable of trading not only immediate satisfaction for\nlong-term goals, but also a good, certain future for a possibly better, yet uncertain, one. Goals\nthat take longer to materialize and that have unknown long-term value are usually the hard-\nest to achieve, and those who can withstand the challenges along the way are the exception,\nthe leaders, the intellectuals of society.\n\nIn this book, you learn about an approach, known as deep reinforcement learning, involved\nwith creating computer programs that can achieve goals that require intelligence. In this\nchapter, I introduce deep reinforcement learning and give suggestions to get the most out of\nthis book.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.2,
                        "section_name": "What is deep reinforcement learning?",
                        "section_path": "./screenshots-images-2/chapter_1/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_2/35fce756-133d-4b71-81eb-993c16e6671b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is deep reinforcement learning?\n\nDeep reinforcement learning (DRL) is a machine learning approach to artificial intelligence\nconcerned with creating computer programs that can solve problems requiring intelligence.\nThe distinct property of DRL programs is learning through trial and error from feedback\nthat\u2019s simultaneously sequential, evaluative, and sampled by leveraging powerful non-linear\nfunction approximation.\n\nI want to unpack this definition for you one bit at a time. But, don\u2019t get too caught up with\nthe details because it'll take me the whole book to get you grokking deep reinforcement\nlearning. The following is the introduction to what you learn about in this book. As such, it\u2019s\nrepeated and explained in detail in the chapters ahead.\n\nIf I succeed with my goal for this book, after you complete it, you should understand this\ndefinition precisely. You should be able to tell why I used the words that I used, and why I\ndidn\u2019t use more or fewer words. But, for this chapter, simply sit back and plow through it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.3,
                        "section_name": "Deep reinforcement learning is a machine learning approach\nto artificial intelligence",
                        "section_path": "./screenshots-images-2/chapter_1/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_3/e5165275-bfa3-4ddc-994a-fead30421f96.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning is a machine learning approach\nto artificial intelligence\n\nArtificial intelligence (AI) is a branch of computer science involved in the creation of com-\nputer programs capable of demonstrating intelligence. Traditionally, any piece of software\nthat displays cognitive abilities such as perception, search, planning, and learning is consid-\nered part of AI. Several examples of functionality produced by AI software are\n\n+ The pages returned by a search engine\n\n+ The route produced by a GPS app\n\n+ The voice recognition and the synthetic voice of smart-assistant software\n+ The products recommended by e-commerce sites\n\n+ The follow-me feature in drones\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.4,
                        "section_name": "Subfields of artificial intelligence",
                        "section_path": "./screenshots-images-2/chapter_1/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_4/c4123fb7-bcf3-4df9-afd4-030004e04687.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Subfields of artificial intelligence\n\nArtificial intelligence\n\n@ Some of the most\ni areas of s!\nunder the field of artificial\n\nPerception\nMachine learning\n\nExpert systems Planning\n\nNatural language processing\n\nComputer vision\nRobotics\n\nSearch\n\nAll computer programs that display intelligence are considered AI, but not all examples of AI\ncan learn. Machine learning (ML) is the area of AI concerned with creating computer pro-\ngrams that can solve problems requiring intelligence by learning from data. There are three\nmain branches of ML: supervised, unsupervised, and reinforcement learning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.5,
                        "section_name": "Main branches of machine learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_5/f08acd72-2287-4dbb-a663-5cf8a60e3092.png",
                            "./screenshots-images-2/chapter_1/section_5/380daaa8-0fc8-4d11-a00f-efe84b911ed9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Main branches of machine learning\n\n@ These types of machine\nlearning tasks are all\nimportant, and they aren't\nmutually exclusive.\n\nMachine learning\n\nSupervised learning (SL) is the task of learning from labeled data. In SL, a human decides\nwhich data to collect and how to label it. The goal in SL is to generalize. A classic example of\nSL is a handwritten-digit-recognition application: a human gathers images with handwritten\ndigits, labels those images, and trains a model to recognize and classify digits in images cor-\nrectly. The trained model is expected to generalize and correctly classify handwritten digits in\nnew images.\n\nUnsupervised learning (UL) is the task of learning from unlabeled data. Even though data\nno longer needs labeling, the methods used by the computer to gather data still need to be\ndesigned by a human. The goal in UL is to compress. A classic example of UL is a customer\nsegmentation application; a human collects customer data and trains a model to group cus-\ntomers into clusters. These clusters compress the information, uncovering underlying rela-\ntionships in customers.\n\nReinforcement learning (RL) is the task of learning through trial and error. In this type of\ntask, no human labels data, and no human collects or explicitly designs the collection of data.\nThe goal in RL is to act. A classic example of RL is a Pong-playing agent; the agent repeatedly\ninteracts with a Pong emulator and learns by taking actions and observing their effects. The\ntrained agent is expected to act in such a way that it successfully plays Pong.\n\nA powerful recent approach to ML, called deep learning (DL), involves using multi-layered\nnon-linear function approximation, typically neural networks. DL isn\u2019t a separate branch of\nML, so it\u2019s not a different task than those described previously. DL is a collection of tech-\nniques and methods for using neural networks to solve ML tasks, whether SL, UL, or RL. DRL\nis simply the use of DL to solve RL tasks.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.6,
                        "section_name": "Deep learning is a powerful toolbox",
                        "section_path": "./screenshots-images-2/chapter_1/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_6/eadd804f-60c0-40fa-bf30-6d335425c53b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep learning is a powerful toolbox\n\n@ The important thing here is deep learning is a\ntoolbox, and any advancement in the Field of\ndeep learning is Felt in all oF machine learning,\n\n@) deep reinforcement learning is the intersection\nof reinforcement learning and deep learning.\n\nThe bottom line is that DRL is an approach to a problem. The field of AI defines the problem:\ncreating intelligent machines. One of the approaches to solving that problem is DRL.\nThroughout the book, will you find comparisons between RL and other ML approaches, but\nonly in this chapter will you find definitions and a historical overview of AI in general. It\u2019s\nimportant to note that the field of RL includes the field of DRL, so although I make a distinc-\ntion when necessary, when I refer to RL, remember that DRL is included.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.7,
                        "section_name": "Deep reinforcement learning is concerned\nwith creating computer programs",
                        "section_path": "./screenshots-images-2/chapter_1/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_7/e0125b43-6b92-494d-af57-993b28b858f8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning is concerned\n\nwith creating computer programs\n\nAt its core, DRL is about complex sequential decision-making problems under uncertainty.\nBut, this is a topic of interest in many fields; for instance, control theory (CT) studies ways to\ncontrol complex known dynamic systems. In CT, the dynamics of the systems we try to con-\ntrol are usually known in advance. Operations research (OR), another instance, also studies\ndecision-making under uncertainty, but problems in this field often have much larger action\nspaces than those commonly seen in DRL. Psychology studies human behavior, which is partly\nthe same \u201ccomplex sequential decision-making under uncertainty\u201d problem.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.8,
                        "section_name": "The synergy between similar fields",
                        "section_path": "./screenshots-images-2/chapter_1/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_8/8af1c170-ee61-4cc9-89ae-6836343eeacb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The synergy between similar fields\n\n@ All of these Fields (and\nmany more) study complex\n\n@ As a result, there\u2019s a\nbetween these Fields. For instance,\nreinforcement learning and\n\ncontrol both contribute to the research\nof model-based methods.\n\n@) Or reinforcement learning and\noperations research both contribute\nto the study of problems with large\naction spaces.\n@) The downside is an inconsistency in notation,\ndefinitions, and so on, that makes it hard for\nnewcomers to Find their way around.\n\nReinforcement\nlearning\n\nThe bottom line is that you have come to a field that\u2019s influenced by a variety of others.\nAlthough this is a good thing, it also brings inconsistencies in terminologies, notations, and\nso on. My take is the computer science approach to this problem, so this book is about build-\ning computer programs that solve complex decision-making problems under uncertainty,\nand as such, you can find code examples throughout the book.\n\nIn DRL, these computer programs are called agents. An agent is a decision maker only and\nnothing else. That means if you\u2019re training a robot to pick up objects, the robot arm isn\u2019t part\nof the agent. Only the code that makes decisions is referred to as the agent.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.9,
                        "section_name": "Deep reinforcement learning agents\ncan solve problems that require intelligence",
                        "section_path": "./screenshots-images-2/chapter_1/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_9/1e8b435f-0415-47d3-81ab-8549cd16f94c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents\ncan solve problems that require intelligence\n\nOn the other side of the agent is the environment. The environment is everything outside the\nagent; everything the agent has no total control over. Again, imagine you're training a robot\nto pick up objects. The objects to be picked up, the tray where the objects lay, the wind, and\neverything outside the decision maker are part of the environment. That means the robot\narm is also part of the environment because it isn\u2019t part of the agent. And even though the\nagent can decide to move the arm, the actual arm movement is noisy, and thus the arm is part\nof the environment.\n\nThis strict boundary between the agent and the environment is counterintuitive at first,\nbut the decision maker, the agent, can only have a single role: making decisions. Everything\nthat comes after the decision gets bundled into the environment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.1,
                        "section_name": "Boundary between agent and environment",
                        "section_path": "./screenshots-images-2/chapter_1/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_10/d6fb5dbe-bd6f-4611-87b8-4d80f3f9df81.png",
                            "./screenshots-images-2/chapter_1/section_10/eeafcfef-207e-4d14-a652-3ed7c908b80c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Boundary between agent and environment\n\n@ An agent is the\ndecision-making,\nportion of the code.\n\n@) The environment is everything outside the\nagent. In this case that includes network\nlatencies, the motor's noise, the camera noise,\nand so on. This may seem counterintuitive ot\nFirst, but it helps in understanding the algorithms.\n\nEnvironment\n\nChapter 2 provides an in-depth survey of all the components of DRL. The following is a pre-\nview of what you'll learn in chapter 2.\n\nThe environment is represented by a set of variables related to the problem. For instance,\nin the robotic arm example, the location and velocities of the arm would be part of the vari-\nables that make up the environment. This set of variables and all the possible values that they\ncan take are referred to as the state space. A state is an instantiation of the state space, a set of\nvalues the variables take.\n\nInterestingly, often, agents don\u2019t have access to the actual full state of the environment.\nThe part ofa state that the agent can observe is called an observation. Observations depend on\nstates but are what the agent can see. For instance, in the robotic arm example, the agent may\nonly have access to camera images. While an exact location of each object exists, the agent\ndoesn\u2019t have access to this specific state. Instead, the observations the agent perceives are\nderived from the states. You'll often see in the literature observations and states being used\ninterchangeably, including in this book. I apologize in advance for the inconsistencies. Simply\nknow the differences and be aware of the lingo; that\u2019s what matters.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.11,
                        "section_name": "States vs. observations",
                        "section_path": "./screenshots-images-2/chapter_1/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_11/e640345f-74a3-4054-bb6b-f7eab7f24865.png",
                            "./screenshots-images-2/chapter_1/section_11/69478b1a-3a39-4c3d-b8d1-587311a4f807.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "States vs. observations\n\nState:\ntrue locations\n\n@ Stotes are the perfect and complete\ninformation related to the task at hand.\n\nObservation:\njust an image\n\n(@ while observations are the information\nthe agent receives, this could be noisy or\nincomplete information.\n\nL_\n\nAteach state, the environment makes available a set of actions the agent can choose from. The\nagent influences the environment through these actions. The environment may change states\nas a response to the agent\u2019s action. The function that\u2019s responsible for this mapping is called\nthe transition function. The environment may also provide a reward signal as a response. The\nfunction responsible for this mapping is called the reward function. The set of transition and\nreward functions is referred to as the model of the environment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.12,
                        "section_name": "The reinforcement learning cycle",
                        "section_path": "./screenshots-images-2/chapter_1/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_12/8aae8ebf-a5a2-467b-897b-e5a1b5e977b6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The reinforcement learning cycle\nthe agent observing the\nenvironment. a fi\n\nObservation\nTransition ,e>\n\nand reward\n\nImprove\n@) Finally, the\nenvironment transitions Action\nand its internal state\nGikely) changes as a L .\nconn yummen ot the (2) it then sends an action @) The agent uses this\npr state and the to the environment in an observation and reward to\nagent's action. Then, the attempt to control it in a attempt to improve at the task.\ncycle repeats. Favorable way,\n\nThe environment commonly has a well-defined task. The goal of this task is defined through\nthe reward function. The reward-function signals can be simultaneously sequential, evalua-\ntive, and sampled. To achieve the goal, the agent needs to demonstrate intelligence, or at least\ncognitive abilities commonly associated with intelligence, such as long-term thinking, infor-\nmation gathering, and generalization.\n\nThe agent has a three-step process: the agent interacts with the environment, the agent eval-\nuates its behavior, and the agent improves its responses. The agent can be designed to learn\nmappings from observations to actions called policies. The agent can be designed to learn the\nmodel of the environment on mappings called models. The agent can be designed to learn to\nestimate the reward-to-go on mappings called value functions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.13,
                        "section_name": "Deep reinforcement learning agents improve their\nbehavior through trial-and-error learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_13/1bbdd7c2-6961-4c3b-adf8-af0f57735154.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents improve their\n\nbehavior through trial-and-error learning\n\nThe interactions between the agent and the environment go on for several cycles. Each cycle\nis called a time step. At each time step, the agent observes the environment, takes action, and\nreceives a new observation and reward. The set of the state, the action, the reward, and the\nnew state is called an experience. Every experience has an opportunity for learning and\nimproving performance.\n\nExperience tuples\nAgent Environment Time step\nAction a State s t\nReward r\nAction a\u2019 State s\u2019 t+1\n\nReward r\u2019\n\nState s\u201d t+2\nReward r\u201d\n\nAction a\u201d\n\nState s\u201d t+3\n\nExperiences:\n\nt (sar, s)\nt+1, (si) aj r,s\u201d)\nt+2, (s\u2018a%r\",s\u201d)\n\nThe task the agent is trying to solve may or may not have a natural ending. Tasks that have a\nnatural ending, such as a game, are called episodic tasks. Conversely, tasks that don\u2019t are called\ncontinuing tasks, such as learning forward motion. The sequence of time steps from the begin-\nning to the end of an episodic task is called an episode. Agents may take several time steps and\nepisodes to learn to solve a task. Agents learn through trial and error: they try something,\nobserve, learn, try something else, and so on.\n\nYou'll start learning more about this cycle in chapter 4, which contains a type of\nenvironment with a single step per episode. Starting with chapter 5, you'll learn to deal\nwith environments that require more than a single interaction cycle per episode.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.14,
                        "section_name": "Deep reinforcement learning agents\nlearn from sequential feedback",
                        "section_path": "./screenshots-images-2/chapter_1/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_14/dfa6f400-0500-449d-aa9f-aceb24b9d782.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents\nlearn from sequential feedback\n\nThe action taken by the agent may have delayed consequences. The reward may be sparse and\nonly manifest after several time steps. Thus the agent must be able to learn from sequential\nfeedback. Sequential feedback gives rise to a problem referred to as the temporal credit\nassignment problem. The temporal credit assignment problem is the challenge of determining\nwhich state and/or action is responsible for a reward. When there\u2019s a temporal component to\na problem, and actions have delayed consequences, it\u2019s challenging to assign credit for\nrewards.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.15,
                        "section_name": "The difficulty of the temporal credit assignment problem",
                        "section_path": "./screenshots-images-2/chapter_1/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_15/3c0ad7a4-1753-482e-95ec-80951b4d7793.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The difficulty of the temporal credit assignment problem\n\n\u00a9 You're in state O.\n\nAgent oe Eivironment\nTime (2) OK. V\"ll take action A.\nSL\n@) You got +23.\n@ You're in state 3.\n\n(S) Nice! Action A again, please.\n\u2014_\u2014\u2014\u2014\u2014\u2014wex\u00bb\n@ No problem, -100.\n\n@ You're in state 3.\n\n@\u00ae Ouch! Get me out of here!\nOesone Action 6?!\n\nousm Sure, \u2014100.\nGD Yow're in state 3.\n\nGa) Was it taking action A in state O that is to blame For the -100?\nSure, choosing action A in state O gave me a good immediate reward,\nbut maybe that\u2019s what sent me to state 3, which is terrible.\n\nShould | have chosen action 6 in state 0?\n\nOh, man. ... Temporal credit assignment is hard\n\nIn chapter 3, we'll study the ins and outs of sequential feedback in isolation. That is, your\nprograms learn from simultaneously sequential, supervised (as opposed to evaluative), and\nexhaustive (as opposed to sampled) feedback.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.16,
                        "section_name": "Deep reinforcement learning agents\nlearn from evaluative feedback",
                        "section_path": "./screenshots-images-2/chapter_1/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_16/690c5118-7ed6-47e4-a28b-0bf33f9567f3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents\nlearn from evaluative feedback\n\nThe reward received by the agent may be weak, in the sense that it may provide no supervi-\nsion. The reward may indicate goodness and not correctness, meaning it may contain no\ninformation about other potential rewards. Thus the agent must be able to learn from evalu-\native feedback. Evaluative feedback gives rise to the need for exploration. The agent must be\nable to balance the gathering of information with the exploitation of current information.\nThis is also referred to as the exploration versus exploitation trade-off.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.17,
                        "section_name": "The difficulty of the exploration vs. exploitation trade-off",
                        "section_path": "./screenshots-images-2/chapter_1/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_17/2380ee1d-5570-4191-8fcb-e0a7b942628c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The difficulty of the exploration vs. exploitation trade-off\n\n\u00ae You're in state 0.\nAgent Environment\n\nTime (@ Of. Vil take action A.\n@) You got +50.\n\n( You're in state 0.\n\nS) sweet! Action A again, please.\n@ No problem, +20.\n\nF\u2014~__\u00a9 you're in state 0. You're in state 0.\n\n@\u00ae) I've received lots of rewards.\n\n(9) Now, let me try action a!\n_\u2014\u2014qum\n(io) Sure, +1,000.\nG .\n\nID You're in state O.\n\nGa) Well, action A doesn't seem\nthat rewarding after all. | regret\nchoosing action A all this time!\n\nIn chapter 4, we'll study the ins and outs of evaluative feedback in isolation. That is, your\nprograms will learn from feedback that is simultaneously one-shot (as opposed to sequen-\ntial), evaluative, and exhaustive (as opposed to sampled).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.18,
                        "section_name": "Deep reinforcement learning agents\nlearn from sampled feedback",
                        "section_path": "./screenshots-images-2/chapter_1/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_18/32c3a537-c0e9-4896-b229-e2109e4356c3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents\nlearn from sampled feedback\n\nThe reward received by the agent is merely a sample, and the agent doesn\u2019t have access to the\nreward function. Also, the state and action spaces are commonly large, even infinite, so trying\nto learn from sparse and weak feedback becomes a harder challenge with samples. Therefore,\nthe agent must be able to learn from sampled feedback, and it must be able to generalize.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.19,
                        "section_name": "The difficulty of learning from sampled feedback",
                        "section_path": "./screenshots-images-2/chapter_1/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_19/992870b1-39d8-4bb0-bc0e-8566e6c186a1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The difficulty of learning from sampled feedback\n\u00ae You're in state (OI, 1.3, -1.8, 7.3).\n\nAgent Environment\n(a) what? what is that?\nTime @) Of. I'll take action A.\n(4) You. got +1. ,\n\u00a9) You're in (5, 1.3, -4.4, $.D.\n\n(@) No idea. Action 8?\n>\n@ You got +1.\nuD.\n\n@ You're in (5, 1.7, -S.4,\n\n9) Still no clue...\nGo) Action A?! | quess!?\n_\nGD You. got +1.\nGa) You're in (1.8, Li, LA, 1.4).\n\n(3) \\ have no idea. what's going on.\n\\ need function approximation . . .\n\nPerhaps, | can get a fancy deep neural\nnetwork!\n\nAgents that are designed to approximate policies are called policy-based; agents that are\ndesigned to approximate value functions are called value-based; agents that are designed to\napproximate models are called model-based; and agents that are designed to approximate\nboth policies and value functions are called actor-critic. Agents can be designed to approxi-\nmate one or more of these components.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.2,
                        "section_name": "Deep reinforcement learning agents\nuse powerful non-linear function approximation\n\n\u2014 7 . oe _ 7 ee we",
                        "section_path": "./screenshots-images-2/chapter_1/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_20/168c443e-2043-421e-b8ef-9550716b60a4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents\nuse powerful non-linear function approximation\n\nThe agent can approximate functions using a variety of ML methods and techniques, from\ndecision trees to SVMs to neural networks. However, in this book, we use only neural net-\nworks; this is what the \u201cdeep\u201d part of DRL refers to, after all. Neural networks aren\u2019t neces-\nsarily the best solution to every problem; neural networks are data hungry and challenging to\ninterpret, and you must keep these facts in mind. However, neural networks are among the\nmost potent function approximations available, and their performance is often the best.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.21,
                        "section_name": "A simple feed-forward neural network",
                        "section_path": "./screenshots-images-2/chapter_1/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_21/5fd74d91-3a00-4ba7-bfb0-93b69b41422c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A simple feed-forward neural network\n\nInput layer Hidden layers Output layer\n\n\u00a9 You're likely Familiar with these!\nWell, you'd better be!\n\nArtificial neural networks (ANN) are multi-layered non-linear function approximators loosely\ninspired by the biological neural networks in animal brains. An ANN isn\u2019t an algorithm,\nbut a structure composed of multiple layers of mathematical transformations applied to\ninput values.\n\nFrom chapter 3 through chapter 7, we only deal with problems in which agents learn from\nexhaustive (as opposed to sampled) feedback. Starting with chapter 8, we study the full DRL\nproblem; that is, using deep neural networks so that agents can learn from sampled feedback.\nRemember, DRL agents learn from feedback that\u2019s simultaneously sequential, evaluative, and\nsampled.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.22,
                        "section_name": "The past, present, and future\nof deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_22/a96f7b45-9eb3-485e-b657-ed507cec8869.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The past, present, and future\nof deep reinforcement learning\n\nHistory isn\u2019t necessary to gain skills, but it can allow you to understand the context around a\ntopic, which in turn can help you gain motivation, and therefore, skills. The history of AI and\nDRL should help you set expectations about the future of this powerful technology. At times,\nI feel the hype surrounding AI is actually productive; people get interested. But, right after\nthat, when it\u2019s time to put in work, hype no longer helps, and it\u2019s a problem. Although I'd like\nto be excited about AI, I also need to set realistic expectations.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.23,
                        "section_name": "Recent history of artificial intelligence\nand deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_23/6c7460a5-7d58-4c06-b963-36a7ba6c241a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Recent history of artificial intelligence\nand deep reinforcement learning\n\nThe beginnings of DRL could be traced back many years, because humans have been intrigued\nby the possibility of intelligent creatures other than ourselves since antiquity. But a good\nbeginning could be Alan Turing\u2019s work in the 1930s, 1940s, and 1950s that paved the way for\nmodern computer science and AI by laying down critical theoretical foundations that later\nscientists leveraged.\n\nThe most well-known of these is the Turing Test, which proposes a standard for measuring\nmachine intelligence: ifa human interrogator is unable to distinguish a machine from another\nhuman on a chat Q&A session, then the computer is said to count as intelligent. Though\nrudimentary, the Turing Test allowed generations to wonder about the possibilities of creat-\ning smart machines by setting a goal that researchers could pursue.\n\nThe formal beginnings of Al as an academic discipline can be attributed to John McCarthy,\nan influential AI researcher who made several notable contributions to the field. To name a\nfew, McCarthy is credited with coining the term \u201cartificial intelligence\u201d in 1955, leading the\nfirst AI conference in 1956, inventing the Lisp programming language in 1958, cofounding\nthe MIT AI Lab in 1959, and contributing important papers to the development of AI as a\nfield over several decades.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.24,
                        "section_name": "Artificial intelligence winters",
                        "section_path": "./screenshots-images-2/chapter_1/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_24/c9d15559-adc3-4b87-91fb-d844349d0828.png",
                            "./screenshots-images-2/chapter_1/section_24/2e8ba892-5238-4e08-b233-75ec85432704.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Artificial intelligence winters\n\nAll the work and progress of AI early on created a great deal of excitement, but there were\nalso significant setbacks. Prominent AI researchers suggested we would create human-like\nmachine intelligence within years, but this never came. Things got worse when a well-known\nresearcher named James Lighthill compiled a report criticizing the state of academic research in\nAI. All of these developments contributed to a long period of reduced funding and interest\nin AI research known as the first AI winter.\n\nThe field continued this pattern throughout the years: researchers making progress, people\ngetting overly optimistic, then overestimating\u2014leading to reduced funding by government\nand industry partners.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.25,
                        "section_name": "The current state of artificial intelligence",
                        "section_path": "./screenshots-images-2/chapter_1/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_25/a74e951e-1fb1-4337-86de-770b504fe88b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The current state of artificial intelligence\n\nWe are likely in another highly optimistic time in AI history, so we must be careful.\nPractitioners understand that Al is a powerful tool, but certain people think of AI as this\nmagic black box that can take any problem in and out comes the best solution ever. Nothing\ncan be further from the truth. Other people even worry about AI gaining consciousness, as if\nthat was relevant, as Edsger W. Dijkstra famously said: \u201cThe question of whether a computer\ncan think is no more interesting than the question of whether a submarine can swim.\u201d\n\nBut, if we set aside this Hollywood-instilled vision of AI, we can allow ourselves to get\nexcited about the recent progress in this field. Today, the most influential companies in the\nworld make the most substantial investments to AI research. Companies such as Google,\nFacebook, Microsoft, Amazon, and Apple have invested in AI research and have become\nhighly profitable thanks, in part, to Al systems. Their significant and steady investments have\ncreated the perfect environment for the current pace of AI research. Contemporary research-\ners have the best computing power available and tremendous amounts of data for their\nresearch, and teams of top researchers are working together, on the same problems, in the\nsame location, at the same time. Current AI research has become more stable and more pro-\nductive. We have witnessed one AI success after another, and it doesn\u2019t seem likely to stop\nanytime soon.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.26,
                        "section_name": "Progress in deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_26/4e8dbc2a-4ce4-4d3e-a0b1-9fae8cc2dece.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Progress in deep reinforcement learning\n\nThe use of artificial neural networks for RL problems started around the 1990s. One of the\nclassics is the backgammon-playing computer program, TD-Gammon, created by Gerald\nTesauro et al. TD-Gammon learned to play backgammon by learning to evaluate table posi-\ntions on its own through RL. Even though the techniques implemented aren\u2019t precisely con-\nsidered DRL, TD-Gammon was one of the first widely reported success stories using ANNs\nto solve complex RL problems.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.27,
                        "section_name": "TD-Gammon architecture",
                        "section_path": "./screenshots-images-2/chapter_1/section_27",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_27/085547be-42fb-4290-b284-b3b36df7e5f2.png",
                            "./screenshots-images-2/chapter_1/section_27/bdd54f14-5f9e-4c87-b218-69ad61c6655b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "TD-Gammon architecture\n\n@ Handerofted features,\n\nnot deep learning @ Not a \u201cdeep\u201d\nnetwork, but arguably\nthe beginnings of DRL\n\n@) The output of the\nnetwork was the\npredicted probability\nof winning, given the\ncurrent game state.\n\nIn 2004, Andrew Ng et al. developed an autonomous helicopter that taught itself to fly stunts\nby observing hours of human-experts flights. They used a technique known as inverse rein-\nforcement learning, in which an agent learns from expert demonstrations. The same year,\nNate Kohl and Peter Stone used a class of DRL methods known as policy-gradient methods to\ndevelop a soccer-playing robot for the RoboCup tournament. They used RL to teach the\nagent forward motion. After only three hours of training, the robot achieved the fastest\nforward-moving speed of any robot with the same hardware.\n\nThere were other successes in the 2000s, but the field of DRL really only started growing\nafter the DL field took off around 2010. In 2013 and 2015, Mnih et al. published a couple of\npapers presenting the DQN algorithm. DQN learned to play Atari games from raw pixels.\nUsing a convolutional neural network (CNN) and a single set of hyperparameters, DQN per-\nformed better than a professional human player in 22 out of 49 games.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.28,
                        "section_name": "Atari DON network architecture",
                        "section_path": "./screenshots-images-2/chapter_1/section_28",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_28/5c41613c-fd6a-4936-90ce-6258ff6eac90.png",
                            "./screenshots-images-2/chapter_1/section_28/fcc3753f-6de9-4b67-8c48-87e40cac4768.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Atari DON network architecture\n\n@ Last four frames (@) Learned @) The feed-forward (4) The output layer\nneeded to infer Features through ANN used the learned \u2014_ return the estimated\nvelocities of the ball, deep learning Features as inputs. expected value for\npaddles, and so on each action.\nLastfourframes Convolutions Feed-forward layers Output\nas input\n\nThis accomplishment started a revolution in the DRL community: In 2014, Silver et al. released\nthe deterministic policy gradient (DPG) algorithm, and a year later Lillicrap et al. improved it\nwith deep deterministic policy gradient (DDPG). In 2016, Schulman et al. released trust region\npolicy optimization (TRPO) and generalized advantage estimation (GAE) methods, Sergey\nLevine et al. published Guided Policy Search (GPS), and Silver et al. demoed AlphaGo. The fol-\nlowing year, Silver et al. demonstrated AlphaZero. Many other algorithms were released during\nthese years: double deep Q-networks (DDQN), prioritized experience replay (PER), proximal\npolicy optimization (PPO), actor-critic with experience replay (ACER), asynchronous advantage\nactor-critic (A3C), advantage actor-critic (A2C), actor-critic using Kronecker-factored trust\nregion (ACKTR), Rainbow, Unicorn (these are actual names, BTW), and so on. In 2019, Oriol\nVinyals et al. showed the AlphaStar agent beat professional players at the game of StarCraft II.\nAnd a few months later, Jakub Pachocki et al. saw their team of Dota-2-playing bots, called Five,\nbecome the first AI to beat the world champions in an e-sports game.\n\nThanks to the progress in DRL, we\u2019ve gone in two decades from solving backgammon,\nwith its 10\u00b0\u00b0 perfect-information states, to solving the game of Go, with its 10!\u201d perfect-\ninformation states, or better yet, to solving StarCraft II, with its 10\u00b0\u201d imperfect-information\nstates. It\u2019s hard to conceive a better time to enter the field. Can you imagine what the next two\ndecades will bring us? Will you be part of it? DRL is a booming field, and I expect its rate of\nprogress to continue.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.29,
                        "section_name": "Game of Go: enormous branching factor",
                        "section_path": "./screenshots-images-2/chapter_1/section_29",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_29/bbae2239-ef99-4d38-bef7-e00b6cd2b3cb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Game of Go: enormous branching factor\n\n@ From an empty\n\nboard, there are\npossible initial positions.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014HF @ out of each initia! position,\nthere are also many possible\nadditional moves.\n\n+W_____________, @) The branching continues until we have a.\ntotal of 10\" states! That's more than the\nnumber of atoms in the observable universe.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.3,
                        "section_name": "Opportunities ahead",
                        "section_path": "./screenshots-images-2/chapter_1/section_30",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_30/a8f6d981-3c0c-479c-94b3-f302e164fedc.png",
                            "./screenshots-images-2/chapter_1/section_30/ae40a441-6c85-4bea-90d5-3d8a43557aac.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Opportunities ahead\n\nI believe AI is a field with unlimited potential for positive change, regardless of what fear-\nmongers say. Back in the 1750s, there was chaos due to the start of the industrial revolution.\nPowerful machines were replacing repetitive manual labor and mercilessly displacing humans.\nEverybody was concerned: machines that can work faster, more effectively, and more cheaply\nthan humans? These machines will take all our jobs! What are we going to do for a living now?\nAnd it happened. But the fact is that many of these jobs were not only unfulfilling, but also\ndangerous.\n\nOne hundred years after the industrial revolution, the long-term effects of these changes\nwere benefiting communities. People who usually owned only a couple of shirts and a pair of\npants could get much more for a fraction of the cost. Indeed, change was difficult, but the\nlong-term effects benefited the entire world.\n\nThe digital revolution started in the 1970s with the introduction of personal computers.\nThen, the internet changed the way we do things. Because of the internet, we got big data and\ncloud computing. ML used this fertile ground for sprouting into what it is today. In the next\ncouple of decades, the changes and impact of AI on society may be difficult to accept at first,\nbut the long-lasting effects will be far superior to any setback along the way. I expect in a few\ndecades humans won\u2019t even need to work for food, clothing, or shelter because these things\nwill be automatically produced by AI. We'll thrive with abundance.\n\nWorkforce revolutions\n\nPersonal\nMechanical computer\nengine Artificial\nElectricity intelligence?\n\n| |\n\nHS\n\n1750 1800 1850 1900 1950 2000 2050\n\ntL. @ Revolutions have proven to disrupt industries and societies.\nBut in the long term, they bring abundance and progress.\n\nAs we continue to push the intelligence of machines to higher levels, certain AI researchers\nthink we might find an AI with intelligence superior to our own. At this point, we unlock a\nphenomenon known as the singularity; an AI more intelligent than humans allows for the\nimprovement of AI at a much faster pace, given that the self-improvement cycle no longer has\nthe bottleneck, namely, humans. But we must be prudent, because this is more of an ideal\nthan a practical aspect to worry about.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.31,
                        "section_name": "Singularity could be a few decades away",
                        "section_path": "./screenshots-images-2/chapter_1/section_31",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_31/b18cf126-c7c3-463b-9a1e-73f15aa52d62.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Singularity could be a few decades away\nBrace yourself, Ht could\n\n@ more than me saying that singularity will\nhappen, this graph is meant to explain what\npeople refer to when they say \u201csingularity,\u201d\n\nq\n\nCompute\n\npower\n\nSingularity\nf\nYou are here. ,\nStH Self-\nLf improving Al\nra\na\u201c -* .\nao -_* -- -\npee Human-\nproduced Al\n\nTime\n\n--%\n\n.-* * \u201cArtificial intelligence\n\nHuman intelligence, -. ---- 77\n1950\nL, @) One of the most scientific graphs you'll ever see. What? Sources? What?\n\nWhile one must be always aware of the implications of AI and strive for AI safety, the singu-\nlarity isn\u2019t an issue today. On the other hand, many issues exist with the current state of DRL,\n\nas you'll see in this book. These issues make better use of our time.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.32,
                        "section_name": "The suitability of deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_32",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_32/a52c212a-542d-4a37-9db5-0702667574c9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The suitability of deep reinforcement learning\n\nYou could formulate any ML problem as a DRL problem, but this isn\u2019t always a good idea\nfor multiple reasons. You should know the pros and cons of using DRL in general, and\nyou should be able to identify what kinds of problems and settings DRL is good and not so\ngood for.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.33,
                        "section_name": "What are the pros and cons?",
                        "section_path": "./screenshots-images-2/chapter_1/section_33",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_33/0e76fd01-8632-4623-8c82-edc4336f9277.png",
                            "./screenshots-images-2/chapter_1/section_33/8c4b42fa-cf05-4e17-89d9-7d962bcea3ff.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What are the pros and cons?\n\nBeyond a technological comparison, I'd like you to think about the inherent advantages and\ndisadvantages of using DRL for your next project. You'll see that each of the points high-\nlighted can be either a pro or a con depending on what kind of problem you're trying to solve.\nFor instance, this field is about letting the machine take control. Is this good or bad? Are you\nokay with letting the computer make the decisions for you? There\u2019s a reason why DRL\nresearch environments of choice are games: it could be costly and dangerous to have agents\ntraining directly in the real world. Can you imagine a self-driving car agent learning not to\ncrash by crashing? In DRL, the agents will have to make mistakes. Can you afford that? Are\nyou willing to risk the negative consequences\u2014actual harm\u2014to humans? Considered these\nquestions before starting your next DRL project.\n\nDeep reinforcement learning agents will explore!\nCan you afford mistakes?\n\n@\u00ae Oh look! Stocks are\n\nthe lowest they've been T\n\ni f\n\nin years: (2) \\ wonder what would\n\nhappen if | sell all my\n\npositions now?\n\nT @\u00ae Yep, give it a try.\nSell alll!\n\nYou'll also need to consider how your agent will explore its environment. For instance, most\nvalue-based methods explore by randomly selecting an action. But other methods can have\nmore strategic exploration strategies. Now, there are pros and cons to each, and this is a\ntrade-off you'll have to become familiar with.\n\nFinally, training from scratch every time can be daunting, time consuming, and resource\nintensive. However, there are a couple of areas that study how to bootstrap previously\nacquired knowledge. First, there\u2019s transfer learning, which is about transferring knowledge\ngained in tasks to new ones. For example, if you want to teach a robot to use a hammer and a\nscrewdriver, you could reuse low-level actions learned on the \u201cpick up the hammer\u201d task and\napply this knowledge to start learning the \u201cpick up the screwdriver\u201d task. This should make\nintuitive sense to you, because humans don\u2019t have to relearn low-level motions each time\nthey learn a new task. Humans seem to form hierarchies of actions as we learn. The field of\nhierarchical reinforcement learning tries to replicate this in DRL agents.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.34,
                        "section_name": "Deep reinforcement learning\u2019s strengths",
                        "section_path": "./screenshots-images-2/chapter_1/section_34",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_34/caf14c50-ee0f-48c4-8fe4-36a060611b0f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning\u2019s strengths\n\nDRL is about mastering specific tasks. Unlike SL, in which generalization is the goal, RL is\ngood at concrete, well-specified tasks. For instance, each Atari game has a particular task.\nDRL agents aren\u2019t good at generalizing behavior across different tasks; it\u2019s not true that\nbecause you train an agent to play Pong, this agent can also play Breakout. And if you naively\ntry to teach your agent Pong and Breakout simultaneously, you'll likely end up with an agent\nthat isn\u2019t good at either. SL, on the other hand, is pretty good a classifying multiple objects at\nonce. The point is the strength of DRL is well-defined single tasks.\n\nIn DRL, we use generalization techniques to learn simple skills directly from raw sensory\ninput. The performance of generalization techniques, new tips, and tricks on training deeper\nnetworks, and so on, are some of the main improvements we\u2019ve seen in recent years. Lucky\nfor us, most DL advancements directly enable new research paths in DRL.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.35,
                        "section_name": "Deep reinforcement learning\u2019s weaknesses",
                        "section_path": "./screenshots-images-2/chapter_1/section_35",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_35/5a98d55c-1563-4d25-9c9a-7926dcfa09c3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning\u2019s weaknesses\n\nOf course, DRL isn\u2019t perfect. One of the most significant issues you'll find is that in most\nproblems, agents need millions of samples to learn well-performing policies. Humans, on the\nother hand, can learn from a few interactions. Sample efficiency is probably one of the top\nareas of DRL that could use improvements. We'll touch on this topic in several chapters\nbecause it\u2019s a crucial one.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.36,
                        "section_name": "Deep reinforcement learning agents need\nlots of interaction samples!",
                        "section_path": "./screenshots-images-2/chapter_1/section_36",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_36/e7776e71-2158-41a7-9c6f-fae10d3aedc8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents need\nlots of interaction samples!\n\nEpisode 2,324,532\n\n\u00a9 | almost drove inside the lanes that last time, boss.\nLet me drive just one more car!\n\nAnother issue with DRL is with reward functions and understanding the meaning of rewards.\nIfa human expert will define the rewards the agent is trying to maximize, does that mean that\nwe\u2019re somewhat \u201csupervising\u201d this agent? And is this something good? Should the reward be\nas dense as possible, which makes learning faster, or as sparse as possible, which makes the\nsolutions more exciting and unique?\n\nWe, as humans, don\u2019t seem to have explicitly defined rewards. Often, the same person can\nsee an event as positive or negative by simply changing their perspective. Additionally, a reward\nfunction for a task such as walking isn\u2019t straightforward to design. Is it the forward motion that\nwe should target, or is it not falling? What is the \u201cperfect\u201d reward function for a human walk?!\n\nThere\u2019s ongoing interesting research on reward signals. One I\u2019m particularly interested in\nis called intrinsic motivation. Intrinsic motivation allows the agent to explore new actions just\nfor the sake of it, out of curiosity. Agents that use intrinsic motivation show improved learn-\ning performance in environments with sparse rewards, which means we get to keep exciting\nand unique solutions. The point is if you\u2019re trying to solve a task that hasn\u2019t been modeled or\ndoesn\u2019t have a distinct reward function, you'll face challenges.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.37,
                        "section_name": "Setting clear two-way expectations",
                        "section_path": "./screenshots-images-2/chapter_1/section_37",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_37/13239eb6-fe10-4d7f-8743-c82354a74bbe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Setting clear two-way expectations\n\nLet\u2019s now touch on another important point going forward. What to expect? Honestly, to me,\nthis is very important. First, I want you to know what to expect from the book so there are no\nsurprises later on. I don\u2019t want people to think that from this book, they'll be able to come up\nwith a trading agent that will make them rich. Sorry, I wouldn\u2019t be writing this book if it was\nthat simple. I also expect that people who are looking to learn put in the necessary work. The\nfact is that learning will come from the combination of me putting in the effort to make con-\ncepts understandable and you putting in the effort to understand them. I did put in the effort.\nBut, if you decide to skip a box you didn\u2019t think was necessary, we both lose.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.38,
                        "section_name": "What to expect from the book?",
                        "section_path": "./screenshots-images-2/chapter_1/section_38",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_38/38846f59-b089-4bb4-a195-e450ca421f3b.png",
                            "./screenshots-images-2/chapter_1/section_38/6385e875-375a-4ed4-9b27-e4354aac1deb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What to expect from the book?\n\nMy goal for this book is to take you, an ML enthusiast, from no prior DRL experience to\ncapable of developing state-of-the-art DRL algorithms. For this, the book is organized into\nroughly two parts. In chapters 3 through 7, you learn about agents that can learn from sequen-\ntial and evaluative feedback, first in isolation, and then in interplay. In chapters 8 through 12,\nyou dive into core DRL algorithms, methods, and techniques. Chapters 1 and 2 are about\nintroductory concepts applicable to DRL in general, and chapter 13 has concluding remarks.\n\nMy goal for the first part (chapters 3 through 7) is for you to understand \u201ctabular\u201d RL.\nThat is, RL problems that can be exhaustively sampled, problems in which there\u2019s no need for\nneural networks or function approximation of any kind. Chapter 3 is about the sequential\naspect of RL and the temporal credit assignment problem. Then, we'll study, also in isolation,\nthe challenge of learning from evaluative feedback and the exploration versus exploitation\ntrade-off in chapter 4. Last, you learn about methods that can deal with these two challenges\nsimultaneously. In chapter 5, you study agents that learn to estimate the results of fixed\nbehavior. Chapter 6 deals with learning to improve behavior, and chapter 7 shows you tech-\nniques that make RL more effective and efficient.\n\nMy goal for the second part (chapters 8 through 12) is for you to grasp the details of core\nDRL algorithms. We dive deep into the details; you can be sure of that. You learn about the\nmany different types of agents from value- and policy-based to actor-critic methods. In chap-\nters 8 through 10, we go deep into value-based DRL. In chapter 11, you learn about policy-\nbased DRL and actor-critic, and chapter 12 is about deterministic policy gradient (DPG)\nmethods, soft actor-critic (SAC) and proximal policy optimization (PPO) methods.\n\nThe examples in these chapters are repeated throughout agents of the same type to make\ncomparing and contrasting agents more accessible. You still explore fundamentally different\nkinds of problems, from small, continuous to image-based state spaces, and from discrete to\ncontinuous action spaces. But, the book\u2019s focus isn\u2019t about modeling problems, which is a\nskill of its own; instead, the focus is about solving already modeled environments.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.39,
                        "section_name": "Comparison of different algorithmic\napproaches to deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_39",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_39/e18502af-d48b-4737-ad3a-f3925f711893.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Comparison of different algorithmic\napproaches to deep reinforcement learning\n\nLess sample efficiency More sample efficiency\nLess computationally expensive More computationally expensive\nLess direct learning More direct learning\n\n\u2018 More direct use of learned function Less direct use of learned function >\n\nPolicy-based Value-based\n\nL @ In this book you learn about all these algorithmic approaches to deep\nreinforcement learning. In Fact, to me, the algorithms are the Focus and not\nSo mach the problems: Wh? Gecouse in OR once you Knew the algorithm you\ncan apply that same algorithm to similar problems with\ntuning Learning the agers uhere guget the mast ool o\u00a5 yur tana\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.4,
                        "section_name": "How to get the most out of this book",
                        "section_path": "./screenshots-images-2/chapter_1/section_40",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_40/ac6ee4f3-c1a2-4db5-ba4b-899f3a2201f8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "How to get the most out of this book\n\nThere are a few things you need to bring to the table to come out grokking deep reinforce-\nment learning. You need to bring a little prior basic knowledge of ML and DL. You need to\nbe comfortable with Python code and simple math. And most importantly, you must be\nwilling to put in the work.\n\nI assume that the reader has a solid basic understanding of ML. You should know what ML\nis beyond what\u2019s covered in this chapter; you should know how to train simple SL models,\nperhaps the Iris or Titanic datasets; you should be familiar with DL concepts such as tensors\nand matrices; and you should have trained at least one DL model, say a convolutional neural\nnetwork (CNN) on the MNIST dataset.\n\nThis book is focused on DRL topics, and there\u2019s no DL in isolation. There are many useful\nresources out there that you can leverage. But, again, you need a basic understanding; If\nyou've trained a CNN before, then you\u2019re fine. Otherwise, I highly recommend you follow a\ncouple of DL tutorials before starting the second part of the book.\n\nAnother assumption I\u2019m making is that the reader is comfortable with Python code.\nPython is a somewhat clear programming language that can be straightforward to under-\nstand, and people not familiar with it often get something out of merely reading it. Now, my\npoint is that you should be comfortable with it, willing and looking forward to reading the\ncode. If you don\u2019t read the code, then you'll miss out on a lot.\n\nLikewise, there are many math equations in this book, and that\u2019s a good thing. Math is the\nperfect language, and there\u2019s nothing that can replace it. However, I\u2019m asking people to be\ncomfortable with math, willing to read, and nothing else. The equations I show are heavily\nannotated so that people \u201cnot into math\u201d can still take advantage of the resources.\n\nFinally, I\u2019m assuming you're willing to put in the work. By that I mean you really want to\nlearn DRL. If you decide to skip the math boxes, or the Python snippets, or a section, or one\npage, or chapter, or whatever, you'll miss out on a lot of relevant information. To get the most\nout of this book, I recommend you read the entire book front to back. Because of the different\nformat, figures and sidebars are part of the main narrative in this book.\n\nAlso, make sure you run the book source code (the next section provides more details on\nhow to do this), and play around and extend the code you find most interesting.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.41,
                        "section_name": "Deep reinforcement learning development environment",
                        "section_path": "./screenshots-images-2/chapter_1/section_41",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_41/04eff179-29b0-4709-a384-c649e1977fc6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning development environment\n\nAlong with this book, you\u2019re provided with a fully tested environment and code to reproduce\nmy results. I created a Docker image and several Jupyter Notebooks so that you don\u2019t have to\nmess around with installing packages and configuring software, or copying and pasting\ncode. The only prerequisite is Docker. Please, go ahead and follow the directions at https://\ngithub.com/mimoralea/gdrl on running the code. It\u2019s pretty straightforward.\n\nThe code is written in Python, and I make heavy use of NumPy and PyTorch. I chose\nPyTorch, instead of Keras, or TensorFlow, because I found PyTorch to be a \u201cPythonic\u201d\nlibrary. Using PyTorch feels natural if you have used NumPy, unlike TensorFlow, for instance,\nwhich feels like a whole new programming paradigm. Now, my intention is not to start a\n\u201cPyTorch versus TensorFlow\u201d debate. But, in my experience from using both libraries,\nPyTorch is a library much better suited for research and teaching.\n\nDRL is about algorithms, methods, techniques, tricks, and so on, so it\u2019s pointless for us to\nrewrite a NumPy or a PyTorch library. But, also, in this book, we write DRL algorithms from\nscratch; I\u2019m not teaching you how to use a DRL library, such as Keras-RL, or Baselines, or\nRLIlib. I want you to learn DRL, and therefore we write DRL code. In the years that I\u2019ve been\nteaching RL, I\u2019ve noticed those who write RL code are more likely to understand RL. Now,\nthis isn\u2019t a book on PyTorch either; there\u2019s no separate PyTorch review or anything like that,\njust PyTorch code that I explain as we move along. If you\u2019re somewhat familiar with DL con-\ncepts, you'll be able to follow along with the PyTorch code I use in this book. Don\u2019t worry,\nyou don\u2019t need a separate PyTorch resource before you get to this book. I explain everything\nin detail as we move along.\n\nAs for the environments we use for training the agents, we use the popular OpenAI Gym\npackage and a few other libraries that I developed for this book. But we\u2019re also not going into\nthe ins and outs of Gym. Just know that Gym is a library that provides environments for\ntraining RL agents. Beyond that, remember our focus is the RL algorithms, the solutions, not\nthe environments, or modeling problems, which, needless to say, are also critical.\n\nSince you should be familiar with DL, I presume you know what a graphics processing unit\n(GPU) is. DRL architectures don\u2019t need the level of computation commonly seen on DL\nmodels. For this reason, the use of a GPU, while a good thing, is not required. Conversely,\nunlike DL models, some DRL agents make heavy use of a central processing unit (CPU) and\nthread count. If you\u2019re planning on investing in a machine, make sure to account for CPU\npower (well, technically, number of cores, not speed) as well. As you'll see later, certain algo-\nrithms massively parallelize processing, and in those cases, it\u2019s the CPU that becomes the\nbottleneck, not the GPU. However, the code runs fine in the container regardless of your\nCPU or GPU. But, if your hardware is severely limited, I recommend checking out cloud\nplatforms. I\u2019ve seen services, such as Google Colab, that offer DL hardware for free.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.42,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_1/section_42",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_42/6f660303-f01b-4a3b-b361-9cd9bfb67abc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nDeep reinforcement learning is challenging because agents must learn from feedback that is\nsimultaneously sequential, evaluative, and sampled. Learning from sequential feedback forces\nthe agent to learn how to balance immediate and long-term goals. Learning from evaluative\nfeedback makes the agent learn to balance the gathering and utilization of information.\nLearning from sampled feedback forces the agent to generalize from old to new experiences.\n\nArtificial intelligence, the main field of computer science into which reinforcement learn-\ning falls, is a discipline concerned with creating computer programs that display human-like\nintelligence. This goal is shared across many other disciplines, such as control theory and\noperations research. Machine learning is one of the most popular and successful approaches\nto artificial intelligence. Reinforcement learning is one of the three branches of machine\nlearning, along with supervised learning, and unsupervised learning. Deep learning, an\napproach to machine learning, isn\u2019t tied to any specific branch, but its power helps advance\nthe entire machine learning community.\n\nDeep reinforcement learning is the use of multiple layers of powerful function approxima-\ntors known as neural networks (deep learning) to solve complex sequential decision-making\nproblems under uncertainty. Deep reinforcement learning has performed well in many con-\ntrol problems, but, nevertheless, it\u2019s essential to have in mind that releasing human control\nfor critical decision making shouldn\u2019t be taken lightly. Several of the core needs in deep rein-\nforcement learning are algorithms with better sample complexity, better-performing explo-\nration strategies, and safe algorithms.\n\nStill, the future of deep reinforcement learning is bright, and there are perhaps dangers\nahead as the technology matures, but more importantly, there\u2019s potential in this field, and\nyou should feel excited and compelled to bring your best and embark on this journey. The\nopportunity to be part of a potential change this big happens only every few generations. You\nshould be glad you\u2019re living during these times. Now, let\u2019s be part of it.\n\nBy now, you\n\n+ Understand what deep reinforcement learning is and how it compares with other\nmachine learning approaches\n\n+ Are aware of the recent progress in the field of deep reinforcement learning, and\nintuitively understand that it has the potential to be applied to a wide variety of\nproblems\n\n+ Have a sense as to what to expect from this book, and how to get the most out of it\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 2,
                "chapter_name": "Mathematical foundations\nof reinforcement learning",
                "chapter_path": "./screenshots-images-2/chapter_2",
                "sections": [
                    {
                        "section_id": 2.1,
                        "section_name": "Mathematical foundations\nof reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_2/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_1/8784fd72-7cab-4032-bd39-bc0e94ab4407.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "You pick up this book and decide to read one more chapter despite having limited free time.\nA coach benches their best player for tonight\u2019s match ignoring the press criticism. A parent\ninvests long hours of hard work and unlimited patience in teaching their child good manners.\nThese are all examples of complex sequential decision-making under uncertainty.\n\nI want to bring to your attention three of the words in play in this phrase: complex sequen-\ntial decision-making under uncertainty. The first word, complex, refers to the fact that agents\nmay be learning in environments with vast state and action spaces. In the coaching example,\neven if you discover that your best player needs to rest every so often, perhaps resting them in\na match with a specific opponent is better than with other opponents. Learning to generalize\naccurately is challenging because we learn from sampled feedback.\n\nThe second word I used is sequential, and this one refers to the fact that in many problems,\nthere are delayed consequences. In the coaching example, again, let\u2019s say the coach benched\ntheir best player for a seemingly unimportant match midway through the season. But, what\nif the action of resting players lowers their morale and performance that only manifests in\nfinals? In other words, what if the actual consequences are delayed? The fact is that assigning\ncredit to your past decisions is challenging because we learn from sequential feedback.\n\nFinally, the word uncertainty refers to the fact that we don\u2019t know the actual inner work-\nings of the world to understand how our actions affect it; everything is left to our interpreta-\ntion. Let\u2019s say the coach did bench their best player, but they got injured in the next match.\nWas the benching decision the reason the player got injured because the player got out of\nshape? What if the injury becomes a team motivation throughout the season, and the team\nends up winning the final? Again, was benching the right decision? This uncertainty gives\nrise to the need for exploration. Finding the appropriate balance between exploration and\nexploitation is challenging because we learn from evaluative feedback.\n\nIn this chapter, you'll learn to represent these kinds of problems using a mathematical\nframework known as Markov decision processes (MDPs). The general framework of MDPs\nallows us to model virtually any complex sequential decision-making problem under uncer-\ntainty in a way that RL agents can interact with and learn to solve solely through experience.\n\nWe'll dive deep into the challenges of learning from sequential feedback in chapter 3,\nthen into the challenges of learning from evaluative feedback in chapter 4, then into the\nchallenges of learning from feedback that\u2019s simultaneously sequential and evaluative in\nchapters 5 through 7, and then chapters 8 through 12 will add complex into the mix.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.2,
                        "section_name": "Components of reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_2/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_2/c8e044df-be3f-448f-8d84-85ca38011587.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Components of reinforcement learning\n\nThe two core components in RL are the agent and the environment. The agent is the decision\nmaker, and is the solution to a problem. The environment is the representation of a problem.\nOne of the fundamental distinctions of RL from other ML approaches is that the agent and\nthe environment interact; the agent attempts to influence the environment through actions,\nand the environment reacts to the agent\u2019s actions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.3,
                        "section_name": "The reinforcement\nlearning-interaction cycle",
                        "section_path": "./screenshots-images-2/chapter_2/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_3/cea10b39-c842-4b63-8386-573b3134657c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The reinforcement\nlearning-interaction cycle\n\n@ agent perceives the environment.\nAgent @) agent takes an action.\nObservation, Action |\nreward\nEnvironment\n@ The environment reacts with\nnew observation and a reward,\n\n@ The environment goes through internal state\nchange as a. consequence of the agent\u2019s action.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.4,
                        "section_name": "Examples of problems, agents, and environments",
                        "section_path": "./screenshots-images-2/chapter_2/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_4/551bbd72-a37f-4fc2-9431-f3918ae3333d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Examples of problems, agents, and environments\n\nThe following are abbreviated examples of RL problems, agents, environments, possible\nactions, and observations:\n\n+ Problem: You're training your dog to sit. Agent: The part of your brain that makes\ndecisions. Environment: Your dog, the treats, your dog\u2019s paws, the loud neighbor,\nand so on. Actions: Talk to your dog. Wait for dog\u2019s reaction. Move your hand. Show\ntreat. Give treat. Pet. Observations: Your dog is paying attention to you. Your dog is\ngetting tired. Your dog is going away. Your dog sat on command.\n\n+ Problem: Your dog wants the treats you have. Agent: The part of your dog\u2019s brain\nthat makes decisions. Environment: You, the treats, your dog\u2019s paws, the loud\nneighbor, and so on. Actions: Stare at owner. Bark. Jump at owner. Try to steal the\ntreat. Run. Sit. Observations: Owner keeps talking loud at dog. Owner is showing\nthe treat. Owner is hiding the treat. Owner gave the dog the treat.\n\n+ Problem: A trading agent investing in the stock market. Agent: The executing DRL\ncode in memory and in the CPU. Environment: Your internet connection, the\nmachine the code is running on, the stock prices, the geopolitical uncertainty, other\ninvestors, day traders, and so on. Actions: Sell n stocks of y company. Buy n stocks of\ny company. Hold. Observations: Market is going up. Market is going down. There\nare economic tensions between two powerful nations. There\u2019s danger of war in the\ncontinent. A global pandemic is wreaking havoc in the entire world.\n\n+ Problem: You're driving your car. Agent: The part of your brain that makes\ndecisions. Environment: The make and model of your car, other cars, other drivers,\nthe weather, the roads, the tires, and so on. Actions: Steer by x, accelerate by y. Break\nby z. Turn the headlights on. Defog windows. Play music. Observations: You're\napproaching your destination. There\u2019s a traffic jam on Main Street. The car next to\nyou is driving recklessly. It\u2019s starting to rain. There\u2019s a police officer driving in front\nof you.\n\nAs you can see, problems can take many forms: from high-level decision-making problems\nthat require long-term thinking and broad general knowledge, such as investing in the stock\nmarket, to low-level control problems, in which geopolitical tensions don\u2019t seem to play a\ndirect role, such as driving a car.\n\nAlso, you can represent a problem from multiple agents\u2019 perspectives. In the dog training\nexample, in reality, there are two agents each interested in a different goal and trying to solve\na different problem.\n\nLet\u2019s zoom into each of these components independently.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.5,
                        "section_name": "The agent: The decision maker",
                        "section_path": "./screenshots-images-2/chapter_2/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_5/7d16d10f-dae3-4dea-8f1b-7375bd825825.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The agent: The decision maker\n\nAs I mentioned in chapter 1, this whole book is about agents, except for this chapter, which\nis about the environment. Starting with chapter 3, you dig deep into the inner workings of\nagents, their components, their processes, and techniques to create agents that are effective\nand efficient.\n\nFor now, the only important thing for you to know about agents is that they are the\ndecision-makers in the RL big picture. They have internal components and processes of their\nown, and that\u2019s what makes each of them unique and good at solving specific problems.\n\nIf we were to zoom in, we would see that most agents have a three-step process: all agents\nhave an interaction component, a way to gather data for learning; all agents evaluate their\ncurrent behavior; and all agents improve something in their inner components that allows\nthem to improve (or at least attempt to improve) their overall performance.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.6,
                        "section_name": "The three internal steps that every\nreinforcement learning agent goes through",
                        "section_path": "./screenshots-images-2/chapter_2/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_6/ae8255bf-61e2-4ac0-a86e-860edf40510a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The three internal steps that every\nreinforcement learning agent goes through\n\n h\na,\n\nObservation Action\n\nall agents evaluate\ntheir behavior.\n\n_\u2014~\n@ Reinforcement learning\nmeans, well, agents have\nto learn something.\n\n@) One of the\ncoolest things of\nreinforcement\nlearning is agents\ninteract with the\nproblem.\n\nC interact)\n\nWe'll continue discussing the inner workings of agents starting with the next chapter. For\nnow, let\u2019s discuss a way to represent environments, how they look, and how we should model\nthem, which is the goal of this chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.7,
                        "section_name": "The environment: Everything else",
                        "section_path": "./screenshots-images-2/chapter_2/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_7/d09b840c-de14-4889-869d-5440388e619d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The environment: Everything else\n\nMost real-world decision-making problems can be expressed as RL environments. A com-\nmon way to represent decision-making processes in RL is by modeling the problem using a\nmathematical framework known as Markov decision processes (MDPs). In RL, we assume\nall environments have an MDP working under the hood. Whether an Atari game, the stock\nmarket, a self-driving car, your significant other, you name it, every problem has an MDP\nrunning under the hood (at least in the RL world, whether right or wrong).\n\nThe environment is represented by a set of variables related to the problem. The combina-\ntion of all the possible values this set of variables can take is referred to as the state space. A\nstate is a specific set of values the variables take at any given time.\n\nAgents may or may not have access to the actual environment\u2019s state; however, one way or\nanother, agents can observe something from the environment. The set of variables the agent\nperceives at any given time is called an observation.\n\nThe combination of all possible values these variables can take is the observation space.\nKnow that state and observation are terms used interchangeably in the RL community. This\nis because often agents are allowed to see the internal state of the environment, but this isn\u2019t\nalways the case. In this book, I use state and observation interchangeably as well. But you\nneed to know that there might be a difference between states and observations, even though\nthe RL community often uses the terms interchangeably.\n\nAt every state, the environment makes available a set of actions the agent can choose from.\nOften the set of actions is the same for all states, but this isn\u2019t required. The set of all actions\nin all states is referred to as the action space.\n\nThe agent attempts to influence the environment through these actions. The environment\nmay change states as a response to the agent\u2019s action. The function that is responsible for this\ntransition is called the transition function.\n\nAfter a transition, the environment emits a new observation. The environment may also\nprovide a reward signal as a response. The function responsible for this mapping is called the\nreward function. The set of transition and reward function is referred to as the model of\nthe environment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.8,
                        "section_name": "A Concrete Example",
                        "section_path": "./screenshots-images-2/chapter_2/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_8/44f0326e-8f91-44d3-8445-f7472bd8762a.png",
                            "./screenshots-images-2/chapter_2/section_8/78c46844-373c-4850-a2df-4025f7d8ae14.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A Concrete Example\nThe bandit walk environment\n\nLet's make these concepts concrete with our first RL environment. | created this very simple\nenvironment for this book; | call it the bandit walk (BW).\n\nBW is a simple grid-world (GW) environment. GWs are a common type of environment for\nstudying RL algorithms that are grids of any size. GWs can have any model (transition and\nreward functions) you can think of and can make any kind of actions available.\n\nBut, they all commonly make move actions available to the agent: Left, Down, Right, Up\n(or West, South, East, North, which is more precise because the agent has no heading and\nusually has no visibility of the full grid, but cardinal directions can also be more confusing).\nAnd, of course, each action corresponds with its logical transition: Left goes left, and Right\ngoes right. Also, they all tend to have a fully observable discrete state and observation spaces\n(that is, state equals observation) with integers representing the cell id location of the agent.\nA\u201cwalk\" is a special case of grid-world environments with a single row. In reality, what | call a\n\u201cwalk\u201d is more commonly referred to as a \u201cCorridor.\u201d But, in this book, | use the term \u201cwalk\u201d for\nall the grid-world environments with a single row.\n\nThe bandit walk (BW) is a walk with three states, but only one non-terminal state.\nEnvironments that have a single non-terminal state are called\u201cbandit\u201d environments. \u201cBandit\u201d\nhere is an analogy to slot machines, which are also known as \u201cone-armed bandits\u201d; they have\none arm and, if you like gambling, can empty your pockets, the same way a bandit would.\n\nThe BW environment has just two actions available: a Left (action 0) and an Right (action 1)\naction. BW has a deterministic transition function: a Left action always moves the agent to\nthe Left, and a Right action always moves the agent to the right. The reward signal is a +1\nwhen landing on the rightmost cell, 0 otherwise. The agent starts in the middle cell.\n\nThe bandit walk (BW) environment\n\u2014\n\n(@ The leftmost\nstate is a Ls,\n\n@) The rightmost\nstate is the goal, and\nprovides a +! reward.\n\nA graphical representation of the BW environment would look like the following.\n\nBandit walk graph\n\u00a9 state | is a. starting state. () State a is a goal\n\n@ State 0 is a. hole, a (@ reward terminal state.\n\n1\n\n@ The transition @ feton % \u00a9 Action |,\n\nof the Left action Right @\u00ae The transition\n\nis deterministic. of the Right action\nis deterministic.\n\nI hope this raises several questions, but you'll find the answers throughout this chapter. For\ninstance, why do the terminal states have actions that transition to themselves: seems waste-\nful, doesn\u2019t? Any other questions? Like, what if the environment is stochastic? What exactly is\nan environments that is \u201cstochastic\u201d?! Keep reading.\n\nWe can also represent this environment in a table form.\n\nState Action Next state Transition probability Reward signal\n\notto | oe |\notter ah\n\nsta [1 ight\n2 (Goal) 0 (Left) 2 (Goal)\n2 (Goal) 1 (Right) 2 (Goal)\n\nInteresting, right? Let\u2019s look at another simple example.\n\nee ee\nee ee ee\n| r6start) | oie) | otro) | ro |\nee ee\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.9,
                        "section_name": "A Concrete Example",
                        "section_path": "./screenshots-images-2/chapter_2/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_9/b4eb840a-4f7d-4f02-804b-d1485f64c116.png",
                            "./screenshots-images-2/chapter_2/section_9/3469a776-50e0-4dbb-8c85-1dbccd70a89d.png",
                            "./screenshots-images-2/chapter_2/section_9/d2ed4bb6-9918-4b85-93dc-8053bb793373.png",
                            "./screenshots-images-2/chapter_2/section_9/132f79cf-72c0-4867-a355-df7fe1332792.png",
                            "./screenshots-images-2/chapter_2/section_9/9b478567-1262-4994-a756-6e6778a92fba.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A Concrete Example\nThe bandit slippery walk environment\n\nOkay, so how about we make this environment stochastic?\n\nLet's say the surface of the walk is slippery and each action has a 20% chance of sending\nthe agent backwards. | call this environment the bandit slippery walk (BSW).\n\nBSW is still a one-row-grid world, a walk, a corridor, with only Left and Right actions avail-\nable. Again, three states and two actions. The reward is the same as before, +1 when landing\nat the rightmost state (except when coming from the rightmost state\u2014from itself), and zero\notherwise.\n\nHowever, the transition function is different: 80% of the time the agent moves to the\nintended cell, and 20% of time in the opposite direction.\n\nA depiction of this environment would look as follows.\n\nThe bandit slippery walk (BSW) environment\n\n@ The agent starts in\n\u2014|\n\nstote is a hole.\n\n@ The rightmost state is the\ngoal, and provides a +1 reward,\n\nIdentical to the BW environment! Interesting .. .\nHow do we know that the action effects are stochastic? How do we represent the \u201cslip-\npery\u201d part of this problem? The graphical and table representations can help us with that.\n\nA graphical representation of the BSW environment would look like the following.\n\nBandit slippery walk graph\n@ Same os before: a. hole, starting, and goal states\n\n(@ But the transition function is different! With an 80% chance,\nwe move Forward, and with a 20% chance, we move backward!\n\nSee how the transition function is different now? The BSW environment has a stochastic\ntransition function. Let\u2019s now represent this environment in a table form as well.\n\nState Action Next state Transition probability Reward signal\n\nstr | owen [2 [ad\nist) [1h | 2G | oe | at _\nstro | vet | ono | ea \u2014\u00ab| io \u2014*d\narco | ove | 2G | ro \u00ab| io _\u2014\n[acon | oy | 26 | ro \u2014~\u00abdYC~Ci\u2018iSC*\u201d\n\nAnd, we don\u2019t have to limit ourselves to thinking about environments with discrete state and\naction spaces or even walks (corridors) or bandits (which we discuss in-depth in the next\nchapter) or grid worlds. Representing environments as MDPs is a surprisingly powerful and\nstraightforward approach to modeling complex sequential decision-making problems under\nuncertainty.\n\nHere are a few more examples of environments that are powered by underlying MDPs.\n\nHotter,\ncolder:\nGuess a\nrandomly\nselected\nnumber\nusing hints.\n\nDescription\n\nObservation space\n\nInt range 0-3.\n\nO means no guess yet\nsubmitted, 1 means\nguess is lower than the\ntarget, 2 means guess\nis equal to the target,\nand 3 means guess is\nhigher than the target.\n\nSample\n\nobservation\n\nSample\n\nAction space action\n\nFloat from\n-2000.0-\n2000.0.\nThe float\nnumber the\nagent is\nguessing.\n\nReward\nfunction\n\nThe reward is\nthe squared\npercentage\nof the way\nthe agent\nhas guessed\ntoward the\ntarget.\n\nLunar\nlander:\nNavigate a\nlander to its\nlanding\npad.\n\nA four-element vector\n\nwith ranges: from [-4.8,\n\n-Inf, -4.2, -Inf] to [4.8,\nInf, 4.2, Inf].\n\nFirst element is the cart\nposition, second is the\ncart velocity, third is\npole angle in radians,\nfourth is the pole\nvelocity at tip.\n\nAn eight-element vec-\ntor with ranges: from\n(-Inf, -Inf, -Inf, -Inf,\n-Inf, -Inf, 0, 0] to [Inf,\nInf, Inf, Inf, Inf, Inf, 1, 1).\nFirst element is the x\nposition, the second\nthe y position, the third\nis the x velocity, the\nfourth is the y velocity,\nfifth is the vehicle's\nangle, sixth is the\nangular velocity, and\nthe last two values are\nBooleans indicating\nlegs contact with the\nground.\n\n[-0.16,\n\n-1.61, 0.17,\n\n2.44)\n\n(0.36 , 0.23,\n\nInt range 0-1.\n0 means push\ncart left, 1\nmeans push\ncart right.\n\nInt range 0-3.\nNo-op (do\nnothing), fire\nleft engine,\nfire main\nengine, fire\nright engine.\n\nThe reward is\n1 for every\nstep taken,\nincluding\nthe termina-\ntion step.\n\nReward for\nlanding is\n200. There's a\nreward for\nmoving from\nthe top to\nthe landing\npad, for\ncrashing or\ncoming to\nrest, for each\nleg touching\nthe ground,\nand for firing\nthe engines.\n\n\nInt range 0-5.\n\nAction 0 is\nNo-op, 1 is\n(11246, 217, | Fire.2isup, 3 The reward is\nPong: is right, 4 is\n\n64], (55, a1 when the\nBounce the Left, 5 is\n\n184, 230], ball goes\nball past A tensor of shape 210, Down.\n\n[ 46, 231, beyond the\ntheoppo- | 160, 3. 179] Notice how ancenent\u2019\nnent, and Values ranging 0-255. \"\"~\u2122 | some actions PPO!\n\n(28, 104, c anda -1\navoid let- Represents a game don't affect\n\n249), [25, 5, when your\nting the screen image. the game in\n\n22), (173, agent's pad-\nball pass any way. In\n\n186, 1)], dle misses\nyou. V reality the the ball\n\n_ paddle can .\nonly move up\nor down, or\nnot move.\nA 44-element (or more, A 17-element ee The reward is\ndepending on the (0.6, 0.08, | vector. _ 0 4 \"| calculated\nHumanoid: | implementation) 0.9,0.0,0.0, | Values rang- 09, based on\nMake robot | vector. 0.0, 0.0,0.0, | ing from -Inf 0 5 \"| forward\nrunas fast | Values ranging from 0.045,0.0, | to Inf. ~ motion with\n: -0.2,\nas possible | -Inf to Inf. 0.47,..., | Represents 07 asmall\nand not fall. | Represents the posi- 0.32,0.0, | the forces to \u201c penalty to\n; oe -0.9,\ntions and velocities of | -0.22,...,0.] | apply to the 04 encourage a\nthe robot's joints. robot's joints. 0 8 natural gait.\n-0.1,\n0.8,\n-0.03)\n\nNotice I didn\u2019t add the transition function to this table. That\u2019s because, while you can look at\nthe code implementing the dynamics for certain environments, other implementations are\nnot easily accessible. For instance, the transition function of the cart pole environment is a\nsmall Python file defining the mass of the cart and the pole and implementing basic physics\nequations, while the dynamics of Atari games, such as Pong, are hidden inside an Atari emu-\nlator and the corresponding game-specific ROM file.\n\nNotice that what we\u2019re trying to represent here is the fact that the environment \u201creacts\u201d to\nthe agent\u2019s actions in some way, perhaps even by ignoring the agent\u2019s actions. But at the end\nof the day, there\u2019s an internal process that\u2019s uncertain (except in this and the next chapter).\nTo represent the ability to interact with an environment in an MDP, we need states, observa-\ntions, actions, a transition, and a reward function.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.1,
                        "section_name": "Process the environment goes through\nas a consequence of agent's actions",
                        "section_path": "./screenshots-images-2/chapter_2/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_10/1ba82b20-c72f-48b3-8975-173dd0a7cd13.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Process the environment goes through\n\nS) Finally, the i r\nreaction is passed as a consequence of agent's actions\nback to the agent. \u00a5 Environment\nObservation, Environment receives the\nreward wT action selected\nTransition by the agent.\n\n@ The new state and Action\nreward ore passed\nthrough a Filter: some\nproblems don't let the true\nstate of the environment\n\nstate\n\n|, Reward\n\nj/\n\nenvironment state, and the\n\n(@)... the environment\nwill transition to a new\ninternal state.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.11,
                        "section_name": "Agent-environment interaction cycle",
                        "section_path": "./screenshots-images-2/chapter_2/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_11/efb78e9c-6485-43ae-af1d-ee32f4464be3.png",
                            "./screenshots-images-2/chapter_2/section_11/746b8212-9e88-4b55-89ba-c6d106b4f76e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Agent-environment interaction cycle\n\nThe environment commonly has a well-defined task. The goal of this task is defined through\nthe reward signal. The reward signal can be dense, sparse, or anything in between. When you\ndesign environments, reward signals are the way to train your agent the way you want. The\nmore dense, the more supervision the agent will have, and the faster the agent will learn, but\nthe more bias you'll inject into your agent, and the less likely the agent will come up with\nunexpected behaviors. The more sparse, the less supervision, and therefore, the higher the\nchance of new, emerging behaviors, but the longer it'll take the agent to learn.\n\nThe interactions between the agent and the environment go on for several cycles. Each\ncycle is called a time step. A time step is a unit of time, which can be a millisecond, a second,\n1.2563 seconds, a minute, a day, or any other period of time.\n\nAt each time step, the agent observes the environment, takes action, and receives a new\nobservation and reward. Notice that, even though rewards can be negative values, they are\nstill called rewards in the RL world. The set of the observation (or state), the action, the\nreward, and the new observation (or new state) is called an experience tuple.\n\nThe task the agent is trying to solve may or may not have a natural ending. Tasks that have\na natural ending, such as a game, are called episodic tasks. Tasks that don\u2019t, such as learning\nforward motion, are called continuing tasks. The sequence of time steps from the beginning to\nthe end of an episodic task is called an episode. Agents may take several time steps and epi-\nsodes to learn to solve a task. The sum of rewards collected in a single episode is called a\nreturn. Agents are often designed to maximize the return. A time step limit is often added to\ncontinuing tasks, so they become episodic tasks, and agents can maximize the return.\n\nEvery experience tuple has an opportunity for learning and improving performance. The\nagent may have one or more components to aid learning. The agent may be designed to\nlearn mappings from observations to actions called policies. The agent may be designed\nto learn mappings from observations to new observations and/or rewards called models. The\nagent may be designed to learn mappings from observations (and possibly actions) to reward-\nto-go estimates (a slice of the return) called value functions.\n\nFor the rest of this chapter, we'll put aside the agent and the interactions, and we'll exam-\nine the environment and inner MDP in depth. In chapter 3, we'll pick back up the agent, but\nthere will be no interactions because the agent won\u2019t need them as it'll have access to the\nMDPs. In chapter 4, we'll remove the agent\u2019s access to MDPs and add interactions back into\nthe equation, but it'll be in single-state environments (bandits). Chapter 5 is about learn-\ning to estimate returns in multi-state environments when agents have no access to MDPs.\nChapters 6 and 7 are about optimizing behavior, which is the full reinforcement learning\nproblem. Chapters 5, 6, and 7 are about agents learning in environments where there\u2019s no\nneed for function approximation. After that, the rest of the book is all about agents that use\nneural networks for learning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.12,
                        "section_name": "MDPs: The engine of the environment",
                        "section_path": "./screenshots-images-2/chapter_2/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_12/8e24cb94-3c1e-4e62-b299-399fe7f5fc51.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "MDPs: The engine of the environment\n\nLet\u2019s build MDPs for a few environments as we learn about the components that make them\nup. We'll create Python dictionaries representing MDPs from descriptions of the problems. In\nthe next chapter, we\u2019ll study algorithms for planning on MDPs. These methods can devise\nsolutions to MDPs and will allow us to find optimal solutions to all problems in this chapter.\n\nThe ability to build environments yourself is an important skill to have. However, often\nyou find environments for which somebody else has already created the MDP. Also, the\ndynamics of the environments are often hidden behind a simulation engine and are too com-\nplex to examine in detail; certain dynamics are even inaccessible and hidden behind the real\nworld. In reality, RL agents don\u2019t need to know the precise MDP of a problem to learn robust\nbehaviors, but knowing about MDPs, in general, is crucial for you because agents are com-\nmonly designed with the assumption that an MDP, even if inaccessible, is running under\nthe hood.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.13,
                        "section_name": "A Concrete Example",
                        "section_path": "./screenshots-images-2/chapter_2/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_13/504679d4-5e3a-4fc8-bab3-05c218364f09.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A Concrete Example\nThe frozen lake environment\n\nThis is another, more challenging problem for which we will build an MDP in this chapter. This\nenvironment is called the frozen lake (FL).\n\nFL is a simple grid-world (GW) environment. It also has discrete state and action spaces.\nHowever, this time, four actions are available: move Left, Down, Right, or Up.\n\nThe task in the FL environment is similar to the task in the BW and BSW environments: to\ngo from a start location to a goal location while avoiding falling into holes. The challenge is\nsimilar to the BSW, in that the surface of the FL environment is slippery, it\u2019s a frozen lake after\nall. But the environment itself is larger. Let\u2019s look at a depiction of the FL.\n\nThe frozen lake (FL) environment\n\n\u00a9 poe TT note that the\n\neach trial here. sippery Frozen\n\nsurface may send\nthe agent to\nunintended places.\n\n(4) but, these are\n\nholes that, if the @ agent gets a +!\n\nagent Falls into, will when Worries here.\n\nend the episode\n\nKS IT\n\nThe FL is a4 x 4 grid (it has 16 cells, ids 0-15). The agent shows up in the START cell every new\nepisode. Reaching the GOAL cell gives a +1 reward; anything else is a 0. Because the surface\nare slippery, the agent moves only a third of the time as intended. The other two-thirds are\nsplit evenly in orthogonal directions. For example, if the agent chooses to move down, there's\na 33.3% chance it moves down, 33.3% chance it moves left, and 33.3% chance it moves right.\nThere's a fence around the lake, so if the agent tries to move out of the grid world, it will\nbounce back to the cell from which it tried to move. There are four holes in the lake. If the\nagent falls into one of these holes, it\u2019s game over.\n\nAre you ready to start building a representation of these dynamics? We need a Python\ndictionary representing the MDP as described here. Let's start building the MDP.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.14,
                        "section_name": "States: Specific configurations of the environment",
                        "section_path": "./screenshots-images-2/chapter_2/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_14/a0a673af-9bb3-4715-ad93-0eac6cd4dfae.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "States: Specific configurations of the environment\n\nA state is a unique and self-contained configuration of the problem. The set of all possible\nstates, the state space, is defined as the set S. The state space can be finite or infinite. But notice\nthat the state space is different than the set of variables that compose a single state. This other\nset must always be finite and of constant size from state to state. In the end, the state space is\na set of sets. The inner set must be of equal size and finite, as it contains the number of vari-\nables representing the states, but the outer set can be infinite depending on the types of ele-\nments of the inner sets.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.15,
                        "section_name": "State space: A set of sets",
                        "section_path": "./screenshots-images-2/chapter_2/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_15/fa8869cd-9db1-4a8e-838f-849d4e4c678e.png",
                            "./screenshots-images-2/chapter_2/section_15/dcad5a52-6976-4945-9e4c-34840bf3c739.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "State space: A set of sets\n\nFL state space Some other state space\n\nC Co), 1), Cal, Ca), C Coa, -La4, O, I, 1.44],\nC4), Cs], Cel, C7), Coal, 1.4, 0, -1, 1.44],\nCe], Co), Cio}, cu), Coal, 1.4, 0, -1, 1.44],\nCial, C13), C14], Cis) J cee 5]\n\n@ The inner set (the number of variables that compose the states) must be Finite.\nThe size of the inner set must be a positive integer.\n\n@) @ut the outer set may be infinite:\nif any of the inner sets\u2019 elements is\ncontinuous, for instance.\n\nFor the BW, BSW, and FL environments, the state is composed of a single variable containing\nthe id of the cell where the agent is at any given time. The agent\u2019s location cell id is a discrete\nvariable. But state variables can be of any kind, and the set of variables can be larger than one.\nWe could have the Euclidean distance that would be a continuous variable and an infinite\nstate space; for example, 2.124, 2.12456, 5.1, 5.1239458, and so on. We could also have mul-\ntiple variables defining the state, for instance, the number of cells away from the goal in the\nx- and y-axis. That would be two variables representing a single state. Both variables would\nbe discrete, therefore, the state space finite. However, we could also have variables of mixed\ntypes; for instance, one could be discrete, another continuous, another Boolean.\n\nWith this state representation for the BW, BSW, and FL environments, the size of the state\nspace is 3, 3, and 16, respectively. Given we have 3, 3, or 16 cells, the agent can be at any given\ntime, then we have 3, 3, and 16 possible states in the state space. We can set the ids of each cell\nstarting from zero, going left to right, top to bottom.\n\nIn the FL, we set the ids from zero to 15, left to right, top to bottom. You could set the ids\nin any other way: in a random order, or group cells by proximity, or whatever. It\u2019s up to you;\nas long as you keep them consistent throughout training, it will work. However, this repre-\nsentation is adequate, and it works well, so it\u2019s what we'll use.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.16,
                        "section_name": "States in the FL contain a single variable indicating the id of\nthe cell in which the agent is at any given time step",
                        "section_path": "./screenshots-images-2/chapter_2/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_16/f348bbc2-7eab-4ed1-9b61-b2f389f90a86.png",
                            "./screenshots-images-2/chapter_2/section_16/0dcc3409-6cc2-460c-ac6d-2743ab1f2cb8.png",
                            "./screenshots-images-2/chapter_2/section_16/60b0471a-67d3-49c7-b08c-51a7203f0eb5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "States in the FL contain a single variable indicating the id of\nthe cell in which the agent is at any given time step\n\n\u00a9 \u00a9 \u00a9 \u00a9\nCROROMS\nCROROMO\nOROROMO\n\n\u00a9 Ws just 0.4 x 4 grid!\n\nIn the case of MDPs, the states are fully observable: we can see the internal state of the envi-\nronment at each time step, that is, the observations and the states are the same. Partially\nobservable Markov decision processes (POMDPs) is a more general framework for modeling\nenvironments in which observations, which still depend on the internal state of the environ-\nment, are the only things the agent can see instead of the state. Notice that for the BW, BSW,\nand FL environments, we\u2019re creating an MDP, so the agent will be able to observe the internal\nstate of the environment.\n\nStates must contain all the variables necessary to make them independent of all other\nstates. In the FL environment, you only need to know the current state of the agent to tell its\nnext possible states. That is, you don\u2019t need the history of states visited by the agent for any-\nthing. You know that from state 2 the agent can only transition to states 1, 3, 6, or 2, and this\nis true regardless of whether the agent\u2019s previous state was 1, 3, 6, or 2.\n\nThe probability of the next state, given the current state and action, is independent of\nthe history of interactions. This memoryless property of MDPs is known as the Markov\nproperty: the probability of moving from one state s to another state s on two separate occa-\nsions, given the same action a, is the same regardless of all previous states or actions encoun-\ntered before that point.\n\n= SHow Me THE Matu\n\nThe Markov property\nJ77 Othe probability of @)... will be\nthe next state... the same...\nP(St41|S, At) a P(St41|S\u00a2, At, St\u20141, At-1, ws)\n@ ... given the @) ... as if you give\ncurrent state it the entire history\n\nand current of interactions.\n\nBut why do you care about this? Well, in the environments we\u2019ve explored so far it\u2019s not that\nobvious, and it\u2019s not that important. But because most RL (and DRL) agents are designed to\ntake advantage of the Markov assumption, you must make sure you feed your agent the nec-\nessary variables to make it hold as tightly as possible (completely keeping the Markov assump-\ntion is impractical, perhaps impossible).\n\nFor example, if you\u2019re designing an agent to learn to land a spacecraft, the agent must\nreceive all variables that indicate velocities along with its locations. Locations alone are not\nsufficient to land a spacecraft safely, and because you must assume the agent is memoryless,\nyou need to feed the agent more information than just its x, y, z coordinates away from the\nlanding pad.\n\nBut, you probably know that acceleration is to velocity what velocity is to position: the\nderivative. You probably also know that you can keep taking derivatives beyond acceleration.\nTo make the MDP completely Markovian, how deep do you have to go? This is more of an art\nthan a science: the more variables you add, the longer it takes to train an agent, but the fewer\nvariables, the higher the chance the information fed to the agent is not sufficient, and the\nharder it is to learn anything useful. For the spacecraft example, often locations and velocities\nare adequate, and for grid-world environments, only the state id location of the agent is\nsufficient.\n\nThe set of all states in the MDP is denoted S*. There is a subset of S* called the set of starting\nor initial states, denoted S'. To begin interacting with an MDP, we draw a state from S' froma\nprobability distribution. This distribution can be anything, but it must be fixed throughout\ntraining: that is, the probabilities must be the same from the first to the last episode of train-\ning and for agent evaluation.\n\nThere\u2019s a unique state called the absorbing or terminal state, and the set of all non-terminal\nstates is denoted S. Now, while it\u2019s common practice to create a single terminal state (a sink\nstate) to which all terminal transitions go, this isn\u2019t always implemented this way. What you'll\nsee more often is multiple terminal states, and that\u2019s okay. It doesn\u2019t really matter under the\nhood if you make all terminal states behave as expected.\n\nAs expected? Yes. A terminal state is a special state: it must have all available actions tran-\nsitioning, with probability 1, to itself, and these transitions must provide no reward. Note\nthat I\u2019m referring to the transitions from the terminal state, not to the terminal state.\n\nIt\u2019s very commonly the case that the end of an episode provides a non-zero reward. For\ninstance, in a chess game you win, you lose, or you draw. A logical reward signal would be +1,\n-1, and 0, respectively. But it\u2019s a compatibility convention that allows for all algorithms to\nconverge to the same solution to make all actions available in a terminal state transition from\nthat terminal state to itself with probability 1 and reward 0. Otherwise, you run the risk of\ninfinite sums and algorithms that may not work altogether. Remember how the BW and BSW\nenvironments had these terminal states?\n\nIn the FL environment, for instance, there\u2019s only one starting state (which is state 0) and\nfive terminal states (or five states that transition to a single terminal state, whichever you\nprefer). For clarity, I use the convention of multiple terminal states (5, 7, 11, 12, and 15) for\nthe illustrations and code; again, each terminal state is a separate terminal state.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.17,
                        "section_name": "States in the frozen lake environment",
                        "section_path": "./screenshots-images-2/chapter_2/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_17/605bac8c-d143-48f8-961b-e91fdf6551ff.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.18,
                        "section_name": "Actions: A mechanism to influence the environment",
                        "section_path": "./screenshots-images-2/chapter_2/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_18/cbba0767-798c-4d65-bda9-56001c2405cc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Actions: A mechanism to influence the environment\n\nMDPs make available a set of actions A that depends on the state. That is, there might be\nactions that aren\u2019t allowed in a state\u2014in fact, A is a function that takes a state as an argument;\nthat is, A(s). This function returns the set of available actions for state s. If needed, you can\ndefine this set to be constant across the state space; that is, all actions are available at every\nstate. You can also set all transitions from a state-action pair to zero if you want to deny an\naction in a given state. You could also set all transitions from state s and action a to the same\nstate s to denote action a as a no-intervene or no-op action.\n\nJust as with the state, the action space may be finite or infinite, and the set of variables of a\nsingle action may contain more than one element and must be finite. However, unlike the\nnumber of state variables, the number of variables that compose an action may not be con-\nstant. The actions available in a state may change depending on that state. For simplicity,\nmost environments are designed with the same number of actions in all states.\n\nThe environment makes the set of all available actions known in advance. Agents can select\nactions either deterministically or stochastically. This is different than saying the environ-\nment reacts deterministically or stochastically to agents\u2019 actions. Both are true statements,\nbut I\u2019m referring here to the fact that agents can either select actions from a lookup table or\nfrom per-state probability distributions.\n\nIn the BW, BSW, and FL environments, actions are singletons representing the direction\nthe agent will attempt to move. In FL, there are four available actions in all states: Up, Down,\nRight, or Left. There\u2019s one variable per action, and the size of the action space is four.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.19,
                        "section_name": "The frozen lake environment has four simple move actions",
                        "section_path": "./screenshots-images-2/chapter_2/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_19/bf82213f-d136-4814-a632-54a335d65d53.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The frozen lake environment has four simple move actions\n\n\u00a9 actions -\u2014 34 up (2) From now on,\nLeft H\u2014> 0 > Pm drawing\nKoy) 2 terminal states\nDown H\u2014p 16 Hy Right without the actions\nFor simplicity.\na\n(8) (10) @) But have in mind tha:\nterminal states ore\ndefined as states with a.\nactions with deterministi\n(14) transitions to themselves\n! : tT\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.2,
                        "section_name": "Transition function: Consequences of agent actions",
                        "section_path": "./screenshots-images-2/chapter_2/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_20/762b2fb0-e3c8-4d93-afad-d544aa38e755.png",
                            "./screenshots-images-2/chapter_2/section_20/c27ece22-6629-46ca-ab71-3afd5dc1c4d0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Transition function: Consequences of agent actions\n\nThe way the environment changes as a response to actions is referred to as the state-transition\nprobabilities, or more simply, the transition function, and is denoted by T(s, a, s'). The transi-\ntion function T maps a transition tuple s, a, s'to a probability; that is, you pass in a state s, an\naction a, and a next state s\u2019, and it'll return the corresponding probability of transition from\nstate s to state s' when taking action a. You could also represent it as T(s, a) and return a dic-\ntionary with the next states for its keys and probabilities for its values.\n\nNotice that T also describes a probability distribution p( -| s, a) determining how the system\nwill evolve in an interaction cycle from selecting action a in state s. When integrating over the\nnext states s', as any probability distribution, the sum of these probabilities must equal one.\n\n& Snow Me THE Matu\nThe transition function\n\n@ The transition @) ...as the probability @)... given action a was\n| function is defined... of transitioning to state selected on state s in the\ns'at time step t... previous time step t-\n\np(s\u2019|s,a) = P(S; = s\u2019|S;_-1 = 8, At_1 = 2)\n\n[Seeker rime\n\nexpect the sum of the probabilities\n\nacross all possible next states to sum to |.\n\nS p(s\u2019|s,a) = 1,Vs \u20ac S,Va \u20ac A(s)\n\nfi\n\ns\u2019ES\n\u00a9 That's true for all states sin the set\nof states s, and all actions a in the set of\nactions available in state s.\n\nThe BW environment was deterministic; that is, the probability of the next state s' given the\ncurrent state s and action a was always 1. There was always a single possible next state s\u2019. The\nBSW and FL environments are stochastic; that is, the probability of the next state s' given\nthe current state s and action a is less than 1. There are more than one possible next state s's.\n\nOne key assumption of many RL (and DRL) algorithms is that this distribution is stationary.\nThat is, while there may be highly stochastic transitions, the probability distribution may not\nchange during training or evaluation. Just as with the Markov assumption, the stationarity\nassumption is often relaxed to an extent. However, it\u2019s important for most agents to interact\nwith environments that at least appear to be stationary.\n\nIn the FL environment, we know that there\u2019s a 33.3% chance we'll transition to the intended\ncell (state) and a 66.6% chance we'll transition to orthogonal directions. There\u2019s also a chance\nwe'll bounce back to the state we\u2019re coming from if it\u2019s next to the wall.\n\nFor simplicity and clarity, I\u2019ve added to the following image only the transition function\nfor all actions of states 0, 2, 5,7, 11, 12, 13, and 15 of the FL environment. This subset of states\nallows for the illustration of all possible transitions without too much clutter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.21,
                        "section_name": "The transition function of the frozen lake environment",
                        "section_path": "./screenshots-images-2/chapter_2/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_21/ff953597-0a70-42c7-8f29-904a373326d8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The transition function of the frozen lake environment\n\n(@ Notice that the corner\nstates are special. You | >\nbounce back from the\nhorizontal and the vertical\nwalls.\n\n(3) Remember that terminal\nstates have all transitions from\nall actions looping back to\n\nthemselves with probability |.\n\n@) Vm not drawing all the\ntransitions, of course. This (8)\nstate, for instance, isn\u2019t\ncomplete. _\u2014_______ fT\nAER\nee.\n\n0.33 033\n\nIt might still be a bit confusing, but look at it this way: for consistency, each action in non-\nterminal states has three separate transitions (certain actions in corner states could be repre-\nsented with only two, but again, let me be consistent): one to the intended cell and two to the\ncells in orthogonal directions.\n\n(S) This environment\nis highly stochastic!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.22,
                        "section_name": "Reward signal: Carrots and sticks",
                        "section_path": "./screenshots-images-2/chapter_2/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_22/0c38c00e-5220-494b-9c51-2b16d20ccd3a.png",
                            "./screenshots-images-2/chapter_2/section_22/57c0adb2-eb28-4f2b-96fb-46ac1c4185c2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Reward signal: Carrots and sticks\n\nThe reward function R maps a transition tuple s, a, s' to a scalar. The reward function gives a\nnumeric signal of goodness to transitions. When the signal is positive, we can think of the\nreward as an income or a reward. Most problems have at least one positive signal\u2014winning\na chess match or reaching the desired destination, for example. But, rewards can also be neg-\native, and we can see these as cost, punishment, or penalty. In robotics, adding a time step\ncost is a common practice because we usually want to reach a goal, but within a number of\ntime steps. One thing to clarify is that whether positive or negative, the scalar coming out of\nthe reward function is always referred to as the reward. RL folks are happy folks.\n\nIt\u2019s also important to highlight that while the reward function can be represented as\nR(s,a,s'), which is explicit, we could also use R(s,a), or even R(s), depending on our needs.\nSometimes rewarding the agent based on state is what we need; sometimes it makes more\nsense to use the action and the state. However, the most explicit way to represent the reward\nfunction is to use a state, action, and next state triplet. With that, we can compute the mar-\nginalization over next states in R(s,a,s') to obtain R(s,a), and the marginalization over actions\nin R(s,a) to get R(s). But, once we\u2019re in R(s) we can\u2019t recover R(s,a) or R(s,a,s'), and once\nwe're in R(s,a) we can\u2019t recover R(s,a,s').\n\n& SHow Me THE MatH\n\nThe reward function\n\n\u00a9 The reward Sunction can be @) And, it\u2019s the expectation of reward ot\ntime step +, given the state-action pair in\n\nouene \u2014\u20147 i te reuse step =)\nfcarver =1(S,@) = E[Rz|S\u00a2-1 = 5, Ap_1 =a]\n\ntakes in a state-\n\n) but, it can also be defined as a function the expectation, but now\nthat takes a Full transition tuple s, a, s\u2018 given that transition tuple.\n\nr(s,a,s\u2019) = E[Ri|S:_-1 = 8, Ar_1 = a, S; = 897]\n\n+\u2014\u2014F \u00a9 The reward at time step t comes From a\nRi \u20ac R S R set of all rewards 2, which is a subset of all\n\nreal numbers.\n\nIn the FL environment, the reward function is +1 for landing in state 15, 0 otherwise. Again,\nfor clarity to the following image, I\u2019ve only added the reward signal to transitions that give a\nnon-zero reward, landing on the final state (state 15.)\n\nThere are only three ways to land on 15. (1) Selecting the Right action in state 14 will\ntransition the agent with 33.3% chance there (33.3% to state 10 and 33.3% back to 14). But,\n(2) selecting the Up and (3) the Down action from state 14 will unintentionally also transition\nthe agent there with 33.3% probability for each action. See the difference between actions and\ntransitions? It\u2019s interesting to see how stochasticity complicates things, right?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.23,
                        "section_name": "Reward signal for states with non-zero reward transitions",
                        "section_path": "./screenshots-images-2/chapter_2/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_23/cb64be7e-7815-4226-bbcd-319f2e7643e5.png",
                            "./screenshots-images-2/chapter_2/section_23/dccd329d-4711-41ce-b9b0-018aee23f2f7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Reward signal for states with non-zero reward transitions\n\n\u00a9 \u00a9 \u00ae \u00a9\n\u00ae @ \u00a9/| @\n\n<\u2014\u2014F  @ every other reward in this\nenvironment is zero, So I'm\n\n@ State |4\u2019s actions transition\nfunction and reward signal\n\n@) omitting all except state 14\u2019s.\n+ \u00a9 Notice how I'm using the most\nN explicit form, the full transition\n\nR(s,a,s\").\n\nExpanding the transition and reward functions into a table form is also useful. The following\nis the format I recommend for most problems. Notice that I\u2019ve only added a subset of the\ntransitions (rows) to the table to illustrate the exercise. Also notice that I\u2019m being explicit, and\nseveral of these transitions could be grouped and refactored (for example, corner cells).\n\nTransition probability Reward signal\n\n\nTransition probability Reward signal\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.24,
                        "section_name": "Horizon: Time changes what's optimal",
                        "section_path": "./screenshots-images-2/chapter_2/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_24/f9109dba-d9f0-4a2c-a402-a65107a47065.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Horizon: Time changes what's optimal\n\nWe can represent time in MDPs as well. A time step, also referred to as epoch, cycle, iteration,\nor even interaction, is a global clock syncing all parties and discretizing time. Having a clock\ngives rise to a couple of possible types of tasks. An episodic task is a task in which there\u2019s a\nfinite number of time steps, either because the clock stops or because the agent reaches a ter-\nminal state. There are also continuing tasks, which are tasks that go on forever; there are no\nterminal states, so there\u2019s an infinite number of time steps. In this type of task, the agent must\nbe stopped manually.\n\nEpisodic and continuing tasks can also be defined from the agent\u2019s perspective. We call it\nthe planning horizon. On the one hand, a finite horizon is a planning horizon in which the\nagent knows the task will terminate in a finite number of time steps: if we forced the agent to\ncomplete the frozen lake environment in 15 steps, for example. A special case of this kind of\nplanning horizon is called a greedy horizon, of which the planning horizon is one. The BW\nand BSW have both a greedy planning horizon: the episode terminates immediately after one\ninteraction. In fact, all bandit environments have greedy horizons.\n\nOn the other hand, an infinite horizon is when the agent doesn\u2019t have a predetermined time\nstep limit, so the agent plans for an infinite number of time steps. Such a task may still be epi-\nsodic and therefore terminate, but from the perspective of the agent, its planning horizon is\ninfinite. We refer to this type of infinite planning horizon task as an indefinite horizon task. The\nagent plans for infinite, but interactions may be stopped at any time by the environment.\n\nFor tasks in which there\u2019s a high chance the agent gets stuck in a loop and never terminates,\nit\u2019s common practice to add an artificial terminal state based on the time step: a hard time\nstep limit using the transition function. These cases require special handling of the time step\nlimit terminal state. The environment for chapters 8, 9, and 10, the cart pole environment,\nhas this kind of artificial terminal step, and you'll learn to handle these special cases in those\nchapters.\n\nThe BW, BSW, and FL environment are episodic tasks, because there are terminal states;\nthere are a clear goal and failure states. FL is an indefinite planning horizon; the agent plans\nfor infinite number of steps, but interactions may stop at any time. We won\u2019t add a time step\nlimit to the FL environment because there\u2019s a high chance the agent will terminate naturally;\nthe environment is highly stochastic. This kind of task is the most common in RL.\n\nWe refer to the sequence of consecutive time steps from the beginning to the end of an\nepisodic task as an episode, trial, period, or stage. In indefinite planning horizons, an episode\nis a collection containing all interactions between an initial and a terminal state.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.25,
                        "section_name": "Discount: The future is uncertain, value it less",
                        "section_path": "./screenshots-images-2/chapter_2/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_25/cf35a99d-3ecf-4911-9adc-0869efcb0e93.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Discount: The future is uncertain, value it less\n\nBecause of the possibility of infinite sequences of time steps in infinite horizon tasks, we need\na way to discount the value of rewards over time; that is, we need a way to tell the agent that\ngetting +1\u2019s is better sooner than later. We commonly use a positive real value less than one\nto exponentially discount the value of future rewards. The further into the future we receive\nthe reward, the less valuable it is in the present.\n\nThis number is called the discount factor, or gamma. The discount factor adjusts the impor-\ntance of rewards over time. The later we receive rewards, the less attractive they are to present\ncalculations. Another important reason why the discount factor is commonly used is to\nreduce the variance of return estimates. Given that the future is uncertain, and that the fur-\nther we look into the future, the more stochasticity we accumulate and the more variance our\nvalue estimates will have, the discount factor helps reduce the degree to which future rewards\naffect our value function estimates, which stabilizes learning for most agents.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.26,
                        "section_name": "Effect of discount factor and time on the value of rewards",
                        "section_path": "./screenshots-images-2/chapter_2/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_26/a2d5ced0-ca86-4f7b-95a7-584b9e0389da.png",
                            "./screenshots-images-2/chapter_2/section_26/7fd36692-08bf-431c-81d5-2210bc0efbee.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Effect of discount factor and time on the value of rewards\n\n1 @ The discount factor will exponentially\ndecay the value of later rewards.\n\n(@ The value of a +1 reward\n\nValue ofa | \\, at time step 0 isn't the same\n+1 reward \\ as the value of a H reward\nat time step 1000.\n\u2014 ~~\n0 \u2014- \u2014\n\n0 Time step 1000\n\nInterestingly, gamma is part of the MDP definition: the problem, and not the agent. However,\noften you'll find no guidance for the proper value of gamma to use for a given environment.\nAgain, this is because gamma is also used as a hyperparameter for reducing variance, and\ntherefore left for the agent to tune.\n\nYou can also use gamma as a way to give a sense of \u201curgency\u201d to the agent. To wrap your\nhead around that, imagine that I tell you I'll give you $1,000 once you finish reading this\nbook, but I'll discount (gamma) that reward by 0.5 daily. This means that every day I cut the\nvalue that I pay in half. You'll probably finish reading this book today. If I say gamma is 1,\nthen it doesn\u2019t matter when you finish it, you still get the full amount.\n\nFor the BW and BSW environments, a gamma of 1 is appropriate; for the FL environment,\nhowever, we'll use a gamma of 0.99, a commonly used value.\n\n& SHow Me THE Matu\nThe discount factor (gamma)\n@ The sum of all rewards obtained during the course of an episode is referred to as the return.\n\nL______., Ge = Reait+ Repo t+ Reg t..+ Rr\n\n@ but we can also use the discount factor this way and obtain the discounted return. The\ndiscounted return will downweight rewards that occur later during the episode.\n\nL_\u00a7_, Gt = Rei + yR2+ PRs t+-.+77 Rr\n@ we can simplify the equation and have a more general 0\u00b0\n\nequation, such as this one. Gr =o Rises\n@ Finally, take a. look a. this interesting recursive k=0\n\ndefinition. In the next chapter, we spend time\nexploiting this form. KA\u00bb Gy = Ry + yGt41\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.27,
                        "section_name": "Extensions to MDPs",
                        "section_path": "./screenshots-images-2/chapter_2/section_27",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_27/6fa356c5-54f1-49ad-bb98-815d3e9c4b94.png",
                            "./screenshots-images-2/chapter_2/section_27/4fe7eac7-b5b4-4ba7-ba41-197c34bd12f2.png",
                            "./screenshots-images-2/chapter_2/section_27/831a7a8f-4211-4978-9b44-2169ea2c5853.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Extensions to MDPs\n\nThere are many extensions to the MDP framework, as we\u2019ve discussed. They allow us to\ntarget slightly different types of RL problems. The following list isn\u2019t comprehensive, but it\nshould give you an idea of how large the field is. Know that the acronym MDPs is often used\nto refer to all types of MDPs. We're currently looking only at the tip of the iceberg:\n\n.\n\nPartially observable Markov decision process (POMDP): When the agent cannot\nfully observe the environment state\n\nFactored Markov decision process (FMDP): Allows the representation of the\ntransition and reward function more compactly so that we can represent large MDPs\n\nContinuous [Time|Action|State] Markov decision process: When either time, action,\nstate or any combination of them are continuous\n\nRelational Markov decision process (RMDP): Allows the combination of probabilistic\nand relational knowledge\n\nSemi-Markoy decision process (SMDP): Allows the inclusion of abstract actions that\ncan take multiple time steps to complete\n\nMulti-agent Markov decision process (MMDP): Allows the inclusion of multiple\nagents in the same environment\n\nDecentralized Markov decision process (Dec-MDP): Allows for multiple agents to\ncollaborate and maximize a common reward\n\nI Speak PYTHON\nThe bandit walk (BW) MDP\nP = {\u2014\u2014\u2014 \u00a9 The outer dictionary Keys are the states.\nos ee 0, 0.0, True)], (2) The value of the i wn\n1: [(1.0, 0, 0.0, True) is a list with all possible transiti\n\n0: [(1.0, 0, 0.0, True)],\n1: [(1.0, 2, 1.0, True)\n\n},\n\n2: {\nO: [(1.0, 0.0, True) ], @ The transition tuples have four values:\n1: [(1.0, 2, 0.0, True) ] he probability of that transition,\n\n} the next state,\n\n} \u00a9 You can also load the reward, and a Flag indicating\n\nwhether the next state is terminal.\n\nthe MOP this way. nl\n\n# import gym, gym walk\n# P = gym.make('BanditWalk-v0').env.P\n\n1 SPEAK PYTHON\n\nThe bandit slippery walk (BSW) MDP\nP = {\u00a2\u2014\u2014 \u00a9 Look at the terminal states. States 0 and a are terminal.\n\nO: {\nO: [(1.0, 0, 0.0, True)],\n1: [(1.0, 0, 0.0, True) ]\n\nGD ie hs you nul stochastic transitions. T's inte coton oF\n0: [(0.8, 0, 0.0, True), (0.2, 2, 1.0, True)],\n1: [(0.8, 2, 1.0, True), (0.2, 0, 0.0, ST\nx { @ These are the transitions after taking action | in state |.\n\n0: [(1.0, 2, 0.0, True)], () This is how you. can load\n1: [(1.0, 2, 0.0, True) ] the Bandit Slippery Walk in\n} the Notebook; make sure\n\n} to check them out!\n# import gym, gym walk\n# P = gym.make('BanditSlipperyWalk-v0') .env.P\n\n},\n1: f{\n\nI Speak PYTHON\nThe frozen lake (FL) MDP\n\nPil \u00a9 Probability of landing in state 0 when selecting action 0 in state 0\n0: {\n0: [(0.6666666666666666, 0, 0.0, False), ___]\n@ Probability of landing in state 4 when selecting action 0 in state 0\nT\u2014\u2014_\u00bb (0.3333333333333333, 4, 0.0, False)\nl, @) You can group the probabilities, such as in this line.\n<0?\n3: [(0.3333333333333333, 1, 0.0, False),\n(0.3333333333333333, 0, 0.0, False),\n(0.3333333333333333, 0, 0.0, False)\n\n} @) or be explicit, such as in these two lines.\n\n}, it works Fine either way,\n<5 0 DE\n14: { \u00a9 Lots removed from this example for clarity.\n<...> 41 @ Goto the Notebook For the complete FLMOP. (5 \u00ab140 14 ig\n1: [(0.3333333333333333, 13, 0.0, False), ey yet\n(0.3333333333333333, 14, 0.0, False), that y an\n(0.3333333333333333, 15, 1.0, True) on f\nl,\nreward, Thr:\n2: [(0.3333333333333333, 14, 0.0, False), eet foun\n(0.3333333333333333, 15, 1.0, True), te eehons havea)\n(0.3333333333333333, 10, 0.0, False) single transition\n1, that leads to\n3: [(0.3333333333333333, 15, 1.0, True), state IS. Landing\n(0.3333333333333333, 10, 0.0, False), on state [s\n(0.3333333333333333, 13, 0.0, False) provides a. +1\n] reward,\n\n{\n\n0: [(1.0, 15, 0, True)],\n1: [(1.0, 15, 0, True)],\n2: [(1.0, 15, 0, True)],\n3: [(1.0, 15, 0, True) ]\n\nHF \u00a9 State's is 0 terminal state.\n\n} Ty... @) Again, you can load the MOP like so.\n\n# import gym\n\n# P = gym.make('FrozenLake-v0') .env.P\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.28,
                        "section_name": "Putting it all together",
                        "section_path": "./screenshots-images-2/chapter_2/section_28",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_28/6ee374f9-2f60-439f-a91c-54e34ae2aab7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Putting it all together\n\nUnfortunately, when you go out to the real world, you'll find many different ways that MDPs\nare defined. Moreover, certain sources describe POMDPs and refer to them as MDPs without\nfull disclosure. All of this creates confusion to newcomers, so I have a few points to clarify for\nyou going forward. First, what you saw previously as Python code isn\u2019t a complete MDP, but\ninstead only the transition functions and reward signals. From these, we can easily infer the\nstate and action spaces. These code snippets come from a few packages containing several\nenvironments I developed for this book, and the FL environment is part of the OpenAI Gym\npackage mentioned in the first chapter. Several of the additional components of an MDP that\nare missing from the dictionaries above, such as the initial state distribution S, that comes\nfrom the set of initial state S', are handled internally by the Gym framework and not shown\nhere. Further, other components, such as the discount factor y and the horizon H, are not\nshown in the previous dictionary, and the OpenAI Gym framework doesn\u2019t provide them to\nyou. Like I said before, discount factors are commonly considered hyperparameters, for bet-\nter or worse. And the horizon is often assumed to be infinity.\n\nBut don\u2019t worry about this. First, to calculate optimal policies for the MDPs presented in\nthis chapter (which we'll do in the next chapter), we only need the dictionary shown previ-\nously containing the transition function and reward signal; from these, we can infer the state\nand action spaces, and I'll provide you with the discount factors. We'll assume horizons of\ninfinity, and won\u2019t need the initial state distribution. Additionally, the most crucial part of\nthis chapter is to give you an awareness of the components of MDPs and POMDPs. Remember,\nyou won\u2019t have to do much more building of MDPs than what you\u2019ve done in this chapter.\nNevertheless, let me define MDPs and POMDPs so we\u2019re in sync.\n\n= SHow Me THE MatH\nMDPs vs. POMDPs\n\nro? MDP(S,A,T,R,S9,7,H)\n\n@ moPs have state space S, action space A, transition Function T, reward signal 2\nThey also has a set of initial states distribution S,, the discount factor y, and the horizon H\n\n[7\u00b0 POMDPIS. A,T,R, $9.7, HO. \u20ac)\n\n@ To define a POMOP, You add the observation space Oand an emission probability &\nthat defines the probability of showing an observation 0, given a state s, Very simple.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.29,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_2/section_29",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_29/077d5560-65af-4ccf-b671-5177513cf26b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nOkay. I know this chapter is heavy on new terms, but that\u2019s its intent. The best summary for\nthis chapter is on the previous page, more specifically, the definition of an MDP. Take another\nlook at the last two equations and try to remember what each letter means. Once you do so,\nyou can be assured that you got what\u2019s necessary out of this chapter to proceed.\n\nAt the highest level, a reinforcement learning problem is about the interactions between an\nagent and the environment in which the agent exists. A large variety of issues can be modeled\nunder this setting. The Markov decision process is a mathematical framework for represent-\ning complex decision-making problems under uncertainty.\n\nMarkov decision processes (MDPs) are composed of a set of system states, a set of per-state\nactions, a transition function, a reward signal, a horizon, a discount factor, and an initial state\ndistribution. States describe the configuration of the environment. Actions allow agents to\ninteract with the environment. The transition function tells how the environment evolves\nand reacts to the agent\u2019s actions. The reward signal encodes the goal to be achieved by the\nagent. The horizon and discount factor add a notion of time to the interactions.\n\nThe state space, the set of all possible states, can be infinite or finite. The number of vari-\nables that make up a single state, however, must be finite. States can be fully observable, but\nin a more general case of MDPs, a POMDP, the states are partially observable. This means the\nagent can\u2019t observe the full state of the system, but can observe a noisy state instead, called an\nobservation.\n\nThe action space is a set of actions that can vary from state to state. However, the conven-\ntion is to use the same set for all states. Actions can be composed with more than one variable,\njust like states. Action variables may be discrete or continuous.\n\nThe transition function links a state (a next state) to a state-action pair, and it defines the\nprobability of reaching that future state given the state-action pair. The reward signal, in its\nmore general form, maps a transition tuple s, a, s' to scalar, and it indicates the goodness of\nthe transition. Both the transition function and reward signal define the model of the envi-\nronment and are assumed to be stationary, meaning probabilities stay the same throughout.\n\nBy now, you\n\n+ Understand the components of a reinforcement learning problem and how they\ninteract with each other\n\n+ Recognize Markov decision processes and know what they are composed from and\nhow they work\n\n+ Can represent sequential decision-making problems as MDPs\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 3,
                "chapter_name": "Balancing immediate\nand long-term goals",
                "chapter_path": "./screenshots-images-2/chapter_3",
                "sections": [
                    {
                        "section_id": 3.1,
                        "section_name": "Balancing immediate\nand long-term goals",
                        "section_path": "./screenshots-images-2/chapter_3/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_1/eaff8323-87ef-4173-b74e-a8f04bf023ae.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the last chapter, you built an MDP for the BW, BSW, and FL environments. MDPs are the\nmotors moving RL environments. They define the problem: they describe how the agent\ninteracts with the environment through state and action spaces, the agent\u2019s goal through the\nreward function, how the environment reacts from the agent\u2019s actions through the transition\nfunction, and how time should impact behavior through the discount factor.\n\nIn this chapter, you'll learn about algorithms for solving MDPs. We first discuss the\nobjective of an agent and why simple plans are not sufficient to solve MDPs. We then talk\nabout the two fundamental algorithms for solving MDPs under a technique called dynamic\nprogramming: value iteration (VI) and policy iteration (PI).\n\nYou'll soon notice that these methods in a way \u201ccheat\u201d: they require full access to the\nMDP, and they depend on knowing the dynamics of the environment, which is something we\ncan\u2019t always obtain. However, the fundamentals you'll learn are still useful for learning about\nmore advanced algorithms. In the end, VI and PI are the foundations from which virtually\nevery other RL (and DRL) algorithm originates.\n\nYou'll also notice that when an agent has full access to an MDP, there\u2019s no uncertainty\nbecause you can look at the dynamics and rewards and calculate expectations directly.\nCalculating expectations directly means that there\u2019s no need for exploration; that is, there\u2019s\nno need to balance exploration and exploitation. There\u2019s no need for interaction, so there\u2019s\nno need for trial-and-error learning. All of this is because the feedback we\u2019re using for learn-\ning in this chapter isn\u2019t evaluative but supervised instead.\n\nRemember, in DRL, agents learn from feedback that\u2019s simultaneously sequential (as\nopposed to one shot), evaluative (as opposed to supervised), and sampled (as opposed to\nexhaustive). What I\u2019m doing in this chapter is eliminating the complexity that comes with\nlearning from evaluative and sampled feedback, and studying sequential feedback in isola-\ntion. In this chapter, we learn from feedback that\u2019s sequential, supervised, and exhaustive.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.2,
                        "section_name": "The objective of a decision-making agent",
                        "section_path": "./screenshots-images-2/chapter_3/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_2/b6c1d9f7-d655-47a8-b986-3105de2552eb.png",
                            "./screenshots-images-2/chapter_3/section_2/3972a19b-74fa-4681-bf68-4f203876a588.png",
                            "./screenshots-images-2/chapter_3/section_2/891a4150-0759-48d8-8149-d78d5d07ecc8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The objective of a decision-making agent\n\nAt first, it seems the agent\u2019s goal is to find a sequence of actions that will maximize the return:\nthe sum of rewards (discounted or undiscounted\u2014depending on the value of gamma) during\nan episode or the entire life of the agent, depending on the task.\n\nLet me introduce a new environment to explain these concepts more concretely.\n\nConcrete Example\nThe Slippery Walk Five (SWF) environment\n\nThe Slippery Walk Five (SWF) is a one-row grid-world environment (a walk), that\u2019s stochastic,\nsimilar to the Frozen Lake, and it has only five non-terminal states (seven total if we count the\ntwo terminal).\n\nThe Slippery Walk Five environment\n\u00a9 This environment is stochastic, and\neven if the agent selects the Right\n\naction, there\u2019s a chance it goes left! H\u2014\n\n|\n6\npH) | \u201chs \u00a2p2\"|\n\n(@ so% action is success.\n() 33.33% stays in place.\n) lo.blo% goes backward,\n\nThe agent starts in S, His a hole, G is the goal and provides a +1 reward.\n\n& SHow Me THE Matu\nThe return G\n\n\u00a9 The return is the sum of rewards encounter from step \u00a3 until the Final step 7.\n\nGt = Rigi + Rise + Reig t+...+ Rr |\n\n@) As | mentioned in the previous chapter, we can combine the return and time using the\ndiscount Factor, gamma. This is then the discounted return, which prioritizes early rewards.\nGt = Rigi +R + PRs t+.. +77 Rr \u2014_\n\n@) we can simplify the equation and have a = k\nmore general equation, such as this one. Gt = > Y Rep k+1\nk=0\n\nGt = Reta + YGt41 GG @ and store at this recursive definition of \u00a2 For a.uhile.\n\n\nYou can think of returns as backward looking\u2014\u201chow much you got\u201d from a past time step;\nbut another way to look at it is as a \u201creward-to-go\u201d\u2014basically, forward looking. For example,\nimagine an episode in the SWF environment went this way: State 3 (0 reward), state 4\n(0 reward), state 5 (0 reward), state 4 (0 reward), state 5 (0 reward), state 6 (+1 reward). We\ncan shorten it: 3/0, 4/0, 5/0, 4/0, 5/0, 6/1. What\u2019s the return of this trajectory/episode?\n\nWell, if we use discounting the math would work out this way.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.3,
                        "section_name": "Discounted return in the slippery walk five environment",
                        "section_path": "./screenshots-images-2/chapter_3/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_3/64f3a66b-d803-4bb0-a476-51501e52c03a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Discounted return in the slippery walk five environment\n\nGo \u2014 1*0 + 0.99 = 0 + 0.9801 = 0 + 0.9702 \u00ab 0 + 0.9605 + 0 4+ 0.9509 + 1\nRare FR Oo a SOs SF\n\ntT\n\nthe discounted\nreward ot\n\u00a9 And soon... time step T\n@ Discounted reward at t+3 Ginal step).\n\n@ Reward at tra, discounted by gamma raised to the power |\n@) This is the reward obtained at time step t+/ (0) discounted by gamma (0.99\u00b0).\n@ Calculating the return at time step t=O\n\nIf we don\u2019t use discounting, well, the return would be 1 for this trajectory and all trajectories\nthat end in the right-most cell, state 6, and 0 for all trajectories that terminate in the left-most\ncell, state 0.\n\nIn the SWF environment, it\u2019s evident that going right is the best thing to do. It may seem,\ntherefore, that all the agent must find is something called a plan\u2014that is, a sequence of actions\nfrom the START state to the GOAL state. But this doesn\u2019t always work.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.4,
                        "section_name": "A solid plan in the SWF environment",
                        "section_path": "./screenshots-images-2/chapter_3/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_4/38b36681-1d48-4132-952a-7f6635ee15fe.png",
                            "./screenshots-images-2/chapter_3/section_4/f27087e3-d830-40ed-8e48-7f3281650277.png",
                            "./screenshots-images-2/chapter_3/section_4/b00d179d-6bb8-4121-9da1-cc757cc4c464.png",
                            "./screenshots-images-2/chapter_3/section_4/fb5325ef-a026-40cd-a0da-e0233f31875b.png",
                            "./screenshots-images-2/chapter_3/section_4/c76e5336-2ce7-4b22-aa97-ba3ded78123f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A solid plan in the SWF environment\n\nTv \u00ae This is a solid plan, but is a plan enough?\n\nSs 3 4 5\n\u2014| | \u2014\n\nTv \u00ae This is a solid plan, but is a plan enough?\n\n2|S 3 4 5\n\u2014| | \u2014\n\nIn the FL environment, a plan would look like the following.\n\nA solid plan in the FL environment\n\nSTART\n| ,_ >|,\n@ This is a solid plan. ut, in a stochastic\nenvironment, even the best of plans. Fail.\n\nRemember that in the FL environment,\nunintended action effects have even\nhigher probability: 66.00% vs. 33.33%! You.\nneed to plan for the unexpected,\n\nBut this isn\u2019t enough! The problem with plans is they don\u2019t account for stochasticity in envi-\nronments, and both the SWF and FL are stochastic; actions taken won\u2019t always work the way\nwe intend. What would happen if, due to the environment\u2019s stochasticity, our agent lands on\na cell not covered by our plan?\n\nA possible hole in our plan\no Say the agent followed the plan, but on the First environment\ntransition, the agent was sent backward to state al\n\n@ Now, what? You didn\u2019t plan an action for\nstate 2. Maybe you need a. plan 6? C? 0?\n\nSame happens in the FL environment.\n\nPlans aren't enough in stochastic environments\n\n@ Here \u2019'm showing the action and the possible action START\neffects. Notice that there\u2019s a blo.o% chance that an > > > v\n\nunintended consequence happens! | (ole|ol\n(@ Imagine that the agent is Following the plan, but in u\n\nstate 10, the agent is sent to state 9, even if it selected \u2014$frte P|\nthe down action, as it apparently is the right thing to do. . |\n\n(3) what we need is a plan For every possible state, a >\n\n\u201c4\n\nuniversal plan, a policy.\n\nWhat the agent needs to come up with is called a policy. Policies are universal plans; policies\ncover all possible states. We need to plan for every possible state. Policies can be stochastic or\ndeterministic: the policy can return action-probability distributions or single actions for a\ngiven state (or observation). For now, we're working with deterministic policies, which is\na lookup table that maps actions to states.\n\nIn the SWF environment, the optimal policy is always going right, for every single state.\nGreat, but there are still many unanswered questions. For instance, how much reward should\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.5,
                        "section_name": "Optimal policy in the SWF environment",
                        "section_path": "./screenshots-images-2/chapter_3/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_5/dc6057db-8e0b-4be3-b3b5-54c81e45e1d9.png",
                            "./screenshots-images-2/chapter_3/section_5/d011cd3f-9621-4d9e-861b-d0899af5d9d9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optimal policy in the SWF environment\n\nSTART\na | | > |\n1 2 3 4 5\n\n@ Ws kind of obvious that going always right\nis the best we can do in this environment.\n\n(@ And notice that it doesn't really matter what we do in terminal\nstates. Policies prescribe action only For non-terminal states. For\nterminal states, any action is the same, as all transitions trom all\nactions in terminal states loop back to the same terminal state.\n\nI expect from this policy? Because, even though we know how to act optimally, the environ-\nment might send our agent backward to the hole even if we always select to go toward the\ngoal. This is why returns aren\u2019t enough. The agent is really looking to maximize the expected\nreturn; that means the return taking into account the environment\u2019s stochasticity.\n\nAlso, we need a method to automatically find optimal policies, because in the FL example,\nfor instance, it isn\u2019t obvious at all what the optimal policy looks like!\n\nThere are a few components that are kept internal to the agent that can help it find optimal\nbehavior: there are policies, there can be multiple policies for a given environment, and in\nfact, in certain environments, there may be multiple optimal policies. Also, there are value\nfunctions to help us keep track of return estimates. There\u2019s a single optimal value function for\na given MDP, but there may be multiple value functions in general.\n\nLet\u2019s look at all the components internal to a reinforcement learning agent that allow them\nto learn and find optimal policies, with examples to make all of this more concrete.\n\nPolicies: Per-state action prescriptions\n\nGiven the stochasticity in the Frozen Lake environment (and most reinforcement learning\nproblems,) the agent needs to find a policy, denoted as x. A policy is a function that prescribes\nactions to take for a given nonterminal state. (Remember, policies can be stochastic: either\ndirectly over an action or a probability distribution over actions. We'll expand on stochastic\npolicies in later chapters.)\n\nHere\u2019s a sample policy.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.6,
                        "section_name": "A randomly generated policy",
                        "section_path": "./screenshots-images-2/chapter_3/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_6/13ebbef5-21b4-489c-88b4-663623001211.png",
                            "./screenshots-images-2/chapter_3/section_6/3cff1b78-9860-4388-80d5-9f3a235c27af.png",
                            "./screenshots-images-2/chapter_3/section_6/e02d660c-5592-495d-a978-644eaa4fa3bf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A randomly generated policy\n\nSTART\nO A policy generated\nuniformly at random.\n\nNothing special so Far. . .\n\n\u2014___]\n\nOne immediate question that arises when looking at a policy is this: how good is this policy?\nIf we find a way to put a number to policies, we could also ask the question, how much better\nis this policy compared to another policy?\n\nHow can we compare policies?\n\nSTART\n\n\nState-value function: What to expect from here?\n\nSomething that'd help us compare policies is to put numbers to states for a given policy. That\nis, if we\u2019re given a policy and the MDP, we should be able to calculate the expected return\nstarting from every single state (we care mostly about the START state). How can we calculate\nhow valuable being in a state is? For instance, if our agent is in state 14 (to the left of the\nGOAL), how is that better than being in state 13 (to the left of 14)? And precisely how much\nbetter is it? More importantly, under which policy would we have better results, the Go-get-it\nor the Careful policy?\n\nLet\u2019s give it a quick try with the Go-get-it policy. What is the value of being in state 14\nunder the Go-get-it policy?\n\nWhat's the value of being in state 14\nwhen running the Go-get-it policy?\n\n@ recall the @ Recording peedaens da agent \u00a9) So, what's the\n@0-aet-it poli selects action Right in si 4. value of Right on I4?\n\u201cSek policy fo) Wit 1? 0387 Sure?\n\nSTART\n\n\u2014 | \u2014 | +\u2014\n\n0 1 2 3\n\ntoll fo\no 00 004 0 0H 0 0 #\n\n8 8 10 the right answer. ut, look at this! A third of the time,\nwe get a +1 and end the episode, another third we\n\n\u2014 |\u2014 land in state 10, and the last third, back in state I4.\nOla|oie| The 0.33 is only part of the answer, but we need to\n\ntake into account the other two-thirds where the\nagent doesn\u2019t get the +.\n\nOkay, so it isn\u2019t that straightforward to calculate the value of state 14 when following the\nGo-get-it policy because of the dependence on the values of other states (10 and 14, in this\ncase), which we don\u2019t have either. It\u2019s like the chicken-or-the-egg problem. Let\u2019s keep going.\n\nWe defined the return as the sum of rewards the agent obtains from a trajectory. Now, this\nreturn can be calculated without paying attention to the policy the agent is following: you\nsum all of the rewards obtained, and you\u2019re good to go. But, the number we're looking for\nnow is the expectation of returns (from state 14) if we follow a given policy x. Remember,\nwe're under stochastic environments, so we must account for all the possible ways the envi-\nronment can react to our policy! That\u2019s what an expectation gives us.\n\nWe now define the value of a state s when following a policy 1: the value of a state s under\npolicy x is the expectation of returns if the agent follows policy x starting from state s.\nCalculate this for every state, and you get the state-value function, or V-function or value\nfunction. It represents the expected return when following policy x from state s.\n\nShow Me THE Matu\nThe state-value function V\n\n\u00a9 The value of a states... (3)... is the expectation over i.\n+ given you select\n\n@)...under poliey x. -- on) = E,[Gt|S; = 1 ep\n\n(4)... of returns at time step t...\nNeda ili falnwhe LL aha dle nial sienna yaliasalr jae\n\nUn(s) = Ex[Reyi + yRt42 + 7 Rag +. |S = 5]\n@ and define them\nrecursivelyike 30. pS? Un (8) = Ex [Resi + Ge+1|5t = 8]\n\nSS oe ibe s\n\nUx (s) = S- 7(a\\s) S > r(s', r|s,a)[r + yvz(s')], Vs \u20ac S\n=r\n\na CHRP\n(9) We get the action | (io) we also weight GD we add the reward I .\n(or actions, if the the sum over the and the discounted value ae\npolicy is stochastic) \u2014_| probabilityof next | of the landing state, soa ees\nprescribed for states | states andrewards. then Weight that by the \u2018nines\nand do 0. weighted probability of that Space.\nsum.\n\nThese equations are fascinating. A bit of a mess given the recursive dependencies, but still\ninteresting. Notice how the value of a state depends recursively on the value of possibly many\nother states, which values may also depend on others, including the original state!\n\nThe recursive relationship between states and successive states will come back in the next\nsection when we look at algorithms that can iteratively solve these equations and obtain the\nstate-value function of any policy in the FL environment (or any other environment, really).\n\nFor now, let\u2019s continue exploring other components commonly found in RL agents. We'll\nlearn how to calculate these values later in this chapter. Note that the state-value function is\noften referred to as the value function, or even the V-function, or more simply V\"(s). It may\nbe confusing, but you'll get used to it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.7,
                        "section_name": "Action-value function: What should | expect from here if | do this?",
                        "section_path": "./screenshots-images-2/chapter_3/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_7/89be5ecb-d96b-4c4a-9526-f828a1017d2f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Action-value function: What should | expect from here if | do this?\n\nAnother critical question that we often need to ask isn\u2019t merely about the value of a state but\nthe value of taking action a in a state s. Differentiating answers to this kind of question will\nhelp us decide between actions.\n\nFor instance, notice that the Go-get-it policy goes right when in state 14, but the Careful\npolicy goes down. But which action is better? More specifically, which action is better under\neach policy? That is, what is the value of going down, instead of right, and then following the\nGo-get-it policy, and what is the value of going right, instead of down, and then following\nthe Careful policy?\n\nBy comparing between different actions under the same policy, we can select better actions,\nand therefore improve our policies. The action-value function, also known as Q-function or\nQ\"(s,a), captures precisely this: the expected return if the agent follows policy x after taking\naction a in state s.\n\nIn fact, when we care about improving policies, which is often referred to as the control\nproblem, we need action-value functions. Think about it: if you don\u2019t have an MDP, how can\nyou decide what action to take merely by knowing the values of all states? V-functions don\u2019t\ncapture the dynamics of the environment. The Q-function, on the other hand, does some-\nwhat capture the dynamics of the environment and allows you to improve policies without\nthe need for MDPs. We expand on this fact in later chapters.\n\n= SHow Me tHE MatH\nThe action-value function Q\n\n@ The value of action ain @ ...is the expectation of returns, given we select\nstate s under policy =... action a in state s and Follow policy thereafter.\n\nL\u2014_, q,(s,a) = Ex[Gt|S; = 8, At = a]\n\n@ we can define this equation recursively like so:\nL\u2014, q,(s,a) = E,(Ri + yGi41|S; = 8, Ay = a]\n(4) The Bellman equation For action values is defined as Follows:\n\nQn (8, a) = Y= p(s, r|s, a)[r + yur(s\u2019)], Vs \u20ac S,Va \u20ac A(s)\n\n\u00a9 notice we dont\u201d 1 co we ao wegh, Ti eo uanat do we weigh? 1100) we ao\n\nweigh over actions however, by the The sum of the that For all\nbecause we're probabilities oF next reward and the state-action\ninterested only in a states and rewards. discounted value of pairs.\n\nspecific action. the next state.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.8,
                        "section_name": "Action-advantage function: How much better if | do that?",
                        "section_path": "./screenshots-images-2/chapter_3/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_8/e23178f9-bd35-4ca6-8be7-d8087c864cce.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Action-advantage function: How much better if | do that?\n\nAnother type of value function is derived from the previous two. The action-advantage function,\nalso known as advantage function, A-function, or A*(s, a), is the difference between the action-\nvalue function of action a in state s and the state-value function of state s under policy x.\n\n= SHow Me tHe Mat\nThe action-advantage function A\n\n@ The advantage i\u2014\u2014\u2014_ (a)... is the difference\n\nof action ain state _ _ between the value of that\n\nSunder policy 7... ax(8, a) = an(s, a) Ux (8) action and the value of the\nstate 5, both under policy x.\n\nThe advantage function describes how much better it is to take action a instead of following\npolicy : the advantage of choosing action a over the default action.\n\nLook at the different value functions for a (dumb) policy in the SWF environment.\nRemember, these values depend on the policy. In other words, the Q (s, a) assumes you'll fol-\nlow policy x (always left in the following example) and right after taking action a in state s.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.9,
                        "section_name": "State-value, action-value, and action-advantage functions",
                        "section_path": "./screenshots-images-2/chapter_3/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_9/4c3e83d1-b64f-47e0-9c7d-1b9a183f872c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "State-value, action-value, and action-advantage functions\n\n@\u00ae Notice how START\n\nus to improve 1 2 3 4 \u00a7\n\npolicy 7%, by\n\nshowing the\n\n== (-[=[=[=[=[=[=]-\naction under\n\nthe policy: 0 0 0 0 0 . +\n\n() sles rot 0.002 | 0011 | 0036 | otf | 0332 | 00\n\nth oe 1 1 1 TT ft IS)\nadvantage 1 3 4 6\n\nFor taking the\n\nsame action 0 \u20ac\nsia, oe [oe [oe [oe [oo [oo Poot\n\nrecommends.\n\n<H A,f{s,a)\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.1,
                        "section_name": "Optimality",
                        "section_path": "./screenshots-images-2/chapter_3/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_10/24315b6d-1f20-499d-b333-5e9a0ea23972.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optimality\n\nPolicies, state-value functions, action-value functions, and action-advantage functions are\nthe components we use to describe, evaluate, and improve behaviors. We call it optimality\nwhen these components are the best they can be.\n\nAn optimal policy is a policy that for every state can obtain expected returns greater than or\nequal to any other policy. An optimal state-value function is a state-value function with the\nmaximum value across all policies for all states. Likewise, an optimal action-value function is\nan action-value function with the maximum value across all policies for all state-action pairs.\nThe optimal action-advantage function follows a similar pattern, but notice an optimal\nadvantage function would be equal to or less than zero for all state-action pairs, since no\naction could have any advantage from the optimal state-value function.\n\nAlso, notice that although there could be more than one optimal policy for a given MDP,\nthere can only be one optimal state-value function, optimal action-value function, and opti-\nmal action-advantage function.\n\nYou may also notice that if you had the optimal V-function, you could use the MDP to do\na one-step search for the optimal Q-function and then use this to build the optimal policy.\nOn the other hand, if you had the optimal Q-function, you don\u2019t need the MDP at all. You\ncould use the optimal Q-function to find the optimal V-function by merely taking the maxi-\nmum over the actions. And you could obtain the optimal policy using the optimal Q-function\nby taking the argmax over the actions.\n\n& SHow Me THE Matu\nThe Bellman optimality equations\n\n@ The optimal state- @... is the state-value\nvalue function... -\u2014> Us (S$) = Max Uz(s),Vs ES Function with the highest\n65) Unseen; tho antral nation n*4 ss value across all policies.\nvalue function is the action-value 9x (8, @) = max q,(s, a), Vs \u20ac S,Va \u20ac A(s)\nfunction with the highest values.\n(4) The optimal state-value function Us (S) = max )~ p(s\u2019, r|s,a)[r + yv.(s\u2019)]\ncan be obtained this way, ;} \u2014_____\u2018* a\n\nants\n\u00a9 we take the max action... Te 5 ip ier ear ora\n@ similarly, the discounted optimal value of the next state.\noptimal action-vall\nNestonsenke qe(s,a) = )> p(s\u2019, r|s,a)[r + ymax q.(s\u2019,a\u2019)|\nobtained this way, \u2014\u2014_t sr\n\n@\u00ae Notice how the max is now on the inside.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.11,
                        "section_name": "Planning optimal sequences of actions",
                        "section_path": "./screenshots-images-2/chapter_3/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_11/e80eae02-3114-407b-9d59-b04aca3bf57b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Planning optimal sequences of actions\n\nWe have state-value functions to keep track of the values of states, action-value functions to\nkeep track of the values of state-action pairs, and action-advantage functions to show the\n\u201cadvantage\u201d of taking specific actions. We have equations for all of these to evaluate current\npolicies, that is, to go from policies to value functions, and to calculate and find optimal value\nfunctions and, therefore, optimal policies.\n\nNow that we've discussed the reinforcement learning problem formulation, and we\u2019ve\ndefined the objective we are after, we can start exploring methods for finding this objective.\nIteratively computing the equations presented in the previous section is one of the most com-\nmon ways to solve a reinforcement learning problem and obtain optimal policies when the\ndynamics of the environment, the MDPs, are known. Let\u2019s look at the methods.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.12,
                        "section_name": "Policy evaluation: Rating policies",
                        "section_path": "./screenshots-images-2/chapter_3/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_12/9ceb05b4-6bf8-44f6-b61a-c430514a0a86.png",
                            "./screenshots-images-2/chapter_3/section_12/47402027-b43d-4140-8b3f-3ab506165146.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Policy evaluation: Rating policies\n\nWe talked about comparing policies in the previous section. We established that policy x is\nbetter than or equal to policy x'if the expected return is better than or equal to 2\u2019 for all states.\nBefore we can use this definition, however, we must devise an algorithm for evaluating an\narbitrary policy. Such an algorithm is known as an iterative policy evaluation or just policy\nevaluation.\n\nThe policy-evaluation algorithm consists of calculating the V-function for a given policy\nby sweeping through the state space and iteratively improving estimates. We refer to the type\nof algorithm that takes in a policy and outputs a value function as an algorithm that solves the\nprediction problem, which is calculating the values of a predetermined policy.\n\n% SHow Me tHE Matu\nThe policy-evaluation equation\n\n@ The policy-evaluation algorithm consist of the iterative approximation of the state-value\nFunction of the policy under evaluation. The algorithm converges as 4 approaches infinity.\n\n@ Initialize v,G) For all sin s arbitrarily, and to Oif sis terminal. Then, increase\nand iteratively improve the estimates by Following the equation below.\n\nLy enea(s) = Sa(als) Yo pls\", rls, a)[r + 7005\")\n\na s',r\n\n@) Calculate the value of a. state sas the weighted sum of the |\nreward and the discounted estimated value of the next state s'.\n\nUsing this equation, we can iteratively approximate the true V-function of an arbitrary pol-\nicy. The iterative policy-evaluation algorithm is guaranteed to converge to the value function\nof the policy if given enough iterations, more concretely as we approach infinity. In practice,\nhowever, we use a small threshold to check for changes in the value function we\u2019re approxi-\nmating. Once the changes in the value function are less than this threshold, we stop.\n\nLet\u2019s see how this algorithm works in the SWF environment, for the always-left policy.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.13,
                        "section_name": "Initial calculations of policy evaluation",
                        "section_path": "./screenshots-images-2/chapter_3/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_13/d0e0159c-0ad7-4b93-9016-9cf0c82b305b.png",
                            "./screenshots-images-2/chapter_3/section_13/681bb440-3967-439c-9ede-b1e2082f59c9.png",
                            "./screenshots-images-2/chapter_3/section_13/e6ed1a42-89eb-425e-8e9a-0d448e2c26bc.png",
                            "./screenshots-images-2/chapter_3/section_13/1f6ae34d-13db-486f-942d-c05526a51b98.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Initial calculations of policy evaluation\nUe+41(S) = S- (als) So p(s\u2019, r|s, a) [r + yur (s s\u2018)|\na ___ sr\nO we hae a dereminste T @) Let's use gamma of |. ]\n\n@) An Aluways- START\nState 5, Iteration 1 (initialized to 0 in iteration 0):\n\nvil5)=p(s'=4|s=5,a=Left) *[R(S, Left, 4) + vila) 1+\n\npls'=5|s=5,a=Left) * (RIS, Left, 5) +vi(5)] +\nTl,\n\np(s'=6|s=5,a=Left) *[R(5, Left, 6) + v,(6) ]\n\nVv n5)= 0.50*(0+0) + 0.33*(0+0) + 0.166 *(1+0) = 0.166 \u20ac\u2014\u2014 \u00a9) Yep, this is the\nvalue of state S after\niteration of policy\nevaluation io\n\nYou then calculate the values for all states 0-6, and when done, move to the next iteration.\nNotice that to calculate V,*(s) you\u2019d have to use the estimates obtained in the previous itera-\ntion, V,*(s). This technique of calculating an estimate from an estimate is referred to as boot-\nstrapping, and it\u2019s a widely used technique in RL (including DRL).\n\nAlso, it\u2019s important to notice that the k\u2019s here are iterations across estimates, but they\u2019re\nnot interactions with the environment. These aren\u2019t episodes that the agent is out and about\nselecting actions and observing the environment. These aren\u2019t time steps either. Instead, these\nare the iterations of the iterative policy-evaluation algorithm. Do a couple more of these esti-\nmates. The following table shows you the results you should get.\n\nk vr(0) vr(1) v\"(2) v\"(3) v\"(4) v\"(5) v\"(6)\n0 0\n\n=< oe\npoo | oo | lo | | 667 |\n_ 0 | 0.0278 | 0.2222\n| oo\n\n0.0008 0.0093 0.0602 0.2747\n\n| oo | oo\n\n| 1 | o |\n\n| 2 | o |\n\n| 3 | o |\n\n| 4 | o |\n\n| 5 | 0 | o001 | coors | 00135 | 00705 | 02883 |\n| 6 | 0 | 00003 | 0.0029 | 0.0171 | 0.0783 | 0.2980\n| 8 | 0 | 00009 | 00050 | 0.0228 | 0.0891 | 03106\n| 9 | 0 | coor | 00059 | 00249 | 00929 | 03147 |\n| 10 | 0 | 00014 | 0.0067 | 0.0267 | 0.0959 | 0318\nee ee eee eee\n| 104 | 0 | 00027 | 0011 | 0.0357 | 0.1099 | 0.3324\n\nWhat are several of the things the resulting state-value function tells us?\n\nWell, to begin with, we can say we get a return of 0.0357 in expectation when starting an\nepisode in this environment and following the always-left policy. Pretty low.\n\nWe can also say, that even when we find ourselves in state 1 (the leftmost non-terminal\nstate), we still have a chance, albeit less than one percent, to end up in the GOAL cell (state 6).\nTo be exact, we have a 0.27% chance of ending up in the GOAL state when we\u2019re in state 1.\nAnd we select left all the time! Pretty interesting.\n\nInterestingly also, due to the stochasticity of this environment, we have a 3.57% chance\nof reaching the GOAL cell (remember this environment has 50% action success, 33.33%\nno effects, and 16.66% backward). Again, this is when under an always-left policy. Still, the\nLeft action could send us right, then right and right again, or left, right, right, right, right, and\nso on.\n\nThink about how the probabilities of trajectories combine. Also, pay attention to the iter-\nations and how the values propagate backward from the reward (transition from state 5 to\nstate 6) one step at a time. This backward propagation of the values is a common character-\nistic among RL algorithms and comes up again several times.\n\n\n| Speak PYTHON\nThe policy-evaluation algorithm\n\ndef policy evaluation(pi, P, gamma=1.0, theta=le-10):\n\u00a9 This is 0 Full implementation of the policy-evaluation algorithm.\nAll we need is the policy we're trying to evaluate and the MOP the\npolicy runs on. The discount Factor, gamma, defaults to |, and theta\nis a. small number that we use to check for convergence.\n\nre = np.zeros (len(P) )\n@ Here we initialize the first-iteration estimates of the state-value function to zero.\n\nM while True:\n@) we begin by looping \u201cForever\u201d...\n\nV = np.zeros(len(P)) * nua wusaun J\nWe initialize the current-iteration estimates to zero as well.\n\nr for s in range(len(P)):\n) And then loop through all states to estimate the state-value function.\n\n@ See here how we use the policy pi to get the possible transitions. J\n\nCon. for prob, next_state, reward, done in P[s] [pi(s)]:\n@ \u20acach transition tuple has a probability, next state, reward, and a\n\ndone Flag indicating whether the \u2018next_state\u2019 is terminal or not.\n\n@ we calculate the value of that state by\nsumming up the weighted value of that transition.\nV[{s] += prob * (reward + gamma * \\\nprev_V[next_state] * (not done) )\n\n(9) Notice how we use the \u2018done\u2019 Flag to ensure the value of the next state\nwhen landing on a terminal state is zero. We don\u2019t want infinite sums.\n\nif np.max(np.abs(prev_V - V)) < theta:\nbreak +<\u2014\u2014\u2014 io) ltt the end of each iteration (a state sweep),\nwe make sure that the state-value functions are\nchanging; otherwise, we call it converged.\nprev_V = V.copy()\n\nsen & G) Finally, \u201ccopy\u201d to get ready for the next\niteration or return the latest state-value function.\n\nLet\u2019s now run policy evaluation in the randomly generated policy presented earlier for the FL\nenvironment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.14,
                        "section_name": "Recall the randomly generated policy",
                        "section_path": "./screenshots-images-2/chapter_3/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_14/f5ea8c74-e3e8-40c9-99aa-887013323cc0.png",
                            "./screenshots-images-2/chapter_3/section_14/e37f03c9-9f47-4846-86b6-ba7ac360eae9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Recall the randomly generated policy\n\n= alte Jt a\n\n@ A policy generated randomly . . .\n\n+H \u00a9)... is the same as before.\nNo need to Hip pages!\n\nThe following shows the progress policy evaluation makes on accurately estimating the state-\nvalue function of the randomly generated policy after only eight iterations.\n\nPolicy evaluation on the randomly gen-\nerated policy for the FL environment\n\n@ The values continue to propagate and become more and more accurate.\n\nState-value function of the randomly generated policy\n\nSTART\n\na\n\no.o9ss | 0.0471 | 0.0470 | 0.0456\n\n\u2014 as @ after alg interactions,\n\n0.1469 0.0498 policy evaluation converges\n\nto these values (using a\nT J tT le-10 minimum change in\n\n0.2028 | 0.2647 | 0.1038 Values os a. stopping\n+|l\n0.4957 | 0.7417\n\nThis final state-value function is the state-value function for this policy. Note that even\nthough this is still an estimate, because we\u2019re in a discrete state and action spaces, we can\nassume this to be the actual value function when using gamma of 0.99.\n\nIn case you\u2019re wondering about the state-value functions of the two policies presented\nearlier, here are the results.\n\nResults of policy evolution\nThe Go-get-it policy: The Careful policy:\n\nSTART \u2014\n\n0.0342 | 0.0231 0.0468 | 0.0231 0.4079 | 0.3754 0.3438\nOo} fo]\n0.4840 E\n\n2/4/05)\n0.0940 _| 0.2386 | 0.2901\n\nGOAL\n@ The state-value Function of this policy (@) For this policy, the state-value Function\nconverges after bl iterations. The policy converges after S46 iterations. The policy\n\nreaches the goal state a mere 3.4% of the time. reaches the goal 3.10% of the time!\n\n@) ey the way, | calculated these values empirically by running the ]\npolicies 100 times. Therefore, these values are noisy, but you get the idea.\n\nIt seems being a Go-get-it policy doesn\u2019t pay well in the FL environment! Fascinating results,\nright? But a question arises: Are there any better policies for this environment?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.15,
                        "section_name": "Policy improvement: Using ratings to get better",
                        "section_path": "./screenshots-images-2/chapter_3/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_15/b2fda604-4697-4c95-96e0-4c84741db0b1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Policy improvement: Using ratings to get better\n\nThe motivation is clear now. You have a way of evaluating any policy. This already gives you\nsome freedom: you can evaluate many policies and rank them by the state-value function of\nthe START state. After all, that number tells you the expected cumulative reward the policy\nin question will obtain if you run many episodes. Cool, right?\n\nNo! Makes no sense. Why would you randomly generate a bunch of policies and evaluate\nthem all? First, that\u2019s a total waste of computing resources, but more importantly, it gives you\nno guarantee that you\u2019re finding better and better policies. There has to be a better way.\n\nThe key to unlocking this problem is the action-value function, the Q-function. Using the\nV-function and the MDP, you get an estimate of the Q-function. The Q-function will give\nyou a glimpse of the values of all actions for all states, and these values, in turn, can hint at\nhow to improve policies. Take a look at the Q-function of the Careful policy and ways we can\nimprove this policy:\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.16,
                        "section_name": "How can the Q-function help us improve policies ?",
                        "section_path": "./screenshots-images-2/chapter_3/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_16/c670f2ce-0dab-448f-b60d-bfd80c5be594.png",
                            "./screenshots-images-2/chapter_3/section_16/4213da90-82fb-4a91-89c8-bf51eac2fbdc.png",
                            "./screenshots-images-2/chapter_3/section_16/e1329073-47b9-4ad4-a6dc-cb6148a4f506.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "How can the Q-function help us improve policies ?\n\n@ This is the (@) Action-value Function\nCareful policy, of the Careful policy 4\n\n0.27\n0.42 0.28 0.26 0.26\n0.29 0.14\n\n0.45 0.29 0.2\n0.29 0.30 | 0.340.34 | 0.430.27\n0.31 0.48 0.39\n0.39 0.67\n0.35 0.59 | 0.57 0.71 GOAL\n0.43 0.76\n\n@ Vm calling this\nnew policy Careful+\n\nNotice how if we act greedily with respect to the Q-function of the policy, we obtain a new\npolicy: Careful+. Is this policy any better? Well, policy evaluation can tell us! Let\u2019s find out!\n\nState-value function of the Careful policy\n\nSTART\n-/|T/T/ T\n+0.1341 +0.1234 +0.1164 +0.1130\nro\n+0.1381 40.2414\n\nTlbpe|)\n+0.1464 +0.1591 +0.1824\n+]\n+0.1533 +0.1521\n\n@ after S74 iterations policy evaluation @ This new policy, Careful can\nconverges to this state-value function [ ? reach the goal state 73.40% of\nfor the Carefult policy. @) also empirically. the time. An improvement!\n\nThe new policy is better than the original policy. This is great! We used the state-value func-\ntion of the original policy and the MDP to calculate its action-value function. Then, acting\ngreedily with respect to the action-value function gave us an improved policy. This is what\nthe policy-improvement algorithm does: it calculates an action-value function using the state-\nvalue function and the MDP, and it returns a greedy policy with respect to the action-value\nfunction of the original policy. Let that sink in, it\u2019s pretty important.\n\nSHow Me tHe Matu\nThe policy-improvement equation\n\n@ To improve a. policy, we use a state-value Function and an MOP to get a one-step look-ahead and\ndetermine which of the actions lead to the highest value. This is the policy-improvement equation.\n\n@) we obtain a new policy ' by taking @) How do we get the highest-\nthe highest-valued action. valued action?\nmW (s) = argmax ) p(s\u2019, r|s, a) [r + yvx(s' )\nsr\n\n@) ey calculating, for each action, the weighted sum\nof all rewards and values of all possible next states.\n\n\u00a9) Notice that this uses the action with the highest-valued Q-function.\n\nThis is how the policy-improvement algorithm looks in Python.\n\nI Speak PYTHON\nThe policy-improvement algorithm\ndef policy improvement (V, P, gamma=1.0) :\n\n\u00ae very simple algorithm. it takes the state-value Function of the policy\nyou want to improve, V, and the MOP, P (and gamma, optionally).\n\n\u00b0\u00b0 = np.zeros((len(P), len(P[0])), dtype=np.float\u00e94)\n\n(@ Then, initialize the Q-Sunction to zero technically, you\ncan initialize these randomly, but let's Keep things simple).\n\nfor s in range(len(P)):\nfor a in range(len(P{s])):\nfor prob, next_state, reward, done in P{s] [a]:\n\n(@) Then thr the stot\n\nactors, eratereonn, G) Fag ndeating ether\nnext_state is terminal or not\n\nQ[s)[a] += prob * (reward + gamma * \\\nV[next_state] * (not done) )\n\n(S) We use those values to calculate the Q-function.\n\nnew_pi = lambda s: {s:a for s, a in enumerate(\nnp.argmax(Q, axis=1))}[s]\n\n\u00a9 Finally, we obtain a new, greedy policy by taking the argmax of the Q-function\nof the original policy. And there, you have a likely improved policy.\nreturn new pi aes\n\nThe natural next questions are these: Is there a better policy than this one? Can we do any\nbetter than Careful+? Can we evaluate the Careful+ policy, and then improve it again? Maybe!\nBut, there\u2019s only one way to find out. Let\u2019s give it a try!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.17,
                        "section_name": "Can we improve over the Careful+ policy ?\n\na oe",
                        "section_path": "./screenshots-images-2/chapter_3/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_17/3a8b4661-94e1-4770-a5dc-46905c12d3f7.png",
                            "./screenshots-images-2/chapter_3/section_17/e9264d3b-2c8e-4e9d-aa0e-23ab26a625eb.png",
                            "./screenshots-images-2/chapter_3/section_17/c15c6318-5021-4f99-bf7c-b5f963802970.png",
                            "./screenshots-images-2/chapter_3/section_17/33b9f8bc-16d5-4e96-aa79-38af80059633.png",
                            "./screenshots-images-2/chapter_3/section_17/0d7953bb-5f52-4372-a4c0-ddfc42a849eb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Can we improve over the Careful+ policy ?\n(@ Action-value function\nof the Carefult policy +\u2014\u2014\n\nCarefult policy. @) Greedy policy over\nsrant 0.50 0.47 046 | the Careful+ Q-function\noni gg | 0.340.232 | 0.440.42 | 0.31 0.30\nSTAT 0.53 0.33 0.43 0.31 STAI\n0.36 0.16\n0.56 0.37 0.360.36\n0.38 0.20\n\nO This is the\n\n0.59 0.40 0.33\n0.38 0.40 | 0.440.45 | 0620.40\n0.41 0.64 0.50\n0.46 0.74 | 0.73 0.82\nt 0.53 0.86 ) Notice, the greedy policy is\n\nthe same as the original policy.\nThere's no improvement now.\n\nIran policy evaluation on the Careful+ policy, and then policy improvement. The Q-functions\nof Careful and Careful+ are different, but the greedy policies over the Q-functions are iden-\ntical. In other words, there\u2019s no improvement this time.\n\nNo improvement occurs because the Careful+ policy is an optimal policy of the FL envi-\nronment (with gamma 0.99). We only needed one improvement over the Careful policy\nbecause this policy was good to begin with.\n\nNow, even if we start with an adversarial policy designed to perform poorly, alternating\nover policy evaluation and improvement would still end up with an optimal policy. Want\n\nproof? Let\u2019s do it! Let\u2019s make up an adversarial policy for FL environment and see what\nhappens.\n\nAdversarial policy for the FL environment\n\n@ This policy is so mean that the\nagent has 0% chance of reaching\nthe GOAL. Look at the top row!\n\n(@ it has a state-value Function\nof 0 For all states!!! mean!\n\n\nPolicy iteration: Improving upon improved behaviors\n\nThe plan with this adversarial policy is to alternate between policy evaluation and policy\nimprovement until the policy coming out of the policy-improvement phase no longer yields\na different policy. The fact is that, if instead of starting with an adversarial policy, we start\nwith a randomly generated policy, this is what an algorithm called policy iteration does.\n\n| Speak PYTHON\nThe policy-iteration algorithm\n\ndef policy iteration(P, gamma=1.0, theta=le-10): ea\n\u00a9 Policy iteration is simple, and it only needs the MOP (including gamma).\nrandom_actions = np.random.choice(\n\ntuple (P[0].keys()), len(P))\n[* = lambda s: {s:a for s, a in enumerate(\n\nrandom actions) } [s]\n@ The First step is to create a randomly generated policy. Anything here should do. |\n\\ create a list oF random actions and then map them to states.\n\nwhile True:\n\n@) Here ?'m keeping a. copy of the policy before we modify it.\nold pi = {s:pi(s) for s in range(len(P) ) }\n@ eet the state-value Function of the policy.\n\nV = policy evaluation(pi, P, gamma, theta)\n\u00a9) eet an improved policy.\n\npi = policy improvement (V, P, gamma)\n\n(&) Then, check if the new policy is any different.\n\nif old_pi == {s:pi(s) for s in range(len(P))}:\nbreak\n\nTa tits aitterent, we do it all over again.\n(\u00ae) |f it\u2019s not, we break out of the loop and return an optimal\n\nreturn V, pi dd\n\nGreat! But, let\u2019s first try it starting with the adversarial policy and see what happens.\n\nImproving upon the adversarial policy 1/2\n\nAdversarial policy ry\n\nPolicy\nevaluation\nrm\n\n0.00% success Policy\n\nimprovement\ns\n\n+\nr\u2014 Policy\n[]} evaluation\nr\u2014\u2014\u2014\u2014_>\nO\nG\n\n0.00% success\n\nPolicy\n\nimprovement\ns\n\nt\n-\u2014 Policy\nJ} evaluation\nLI\nG\n\n0.00% success\nPolic\n\ny ;\nimprovement\ns\n\nPolicy\nevaluation\n\n(260% success Policy\n\n69.20% success\n\n72.00% success\n\n13.A0% success\n\n13.20% success\n\nImproving upon the adversarial policy 2/2\n\nPolicy\n\nPolicy\n\nPolicy\n\nimprovement 7\n\nPolicy\n\n\nAs mentioned, alternating policy evaluating and policy improvement yields an optimal policy\nand state-value function regardless of the policy you start with. Now, a few points I'd like to\nmake about this sentence.\n\nNotice how I use \u201can optimal policy,\u201d but also use \u201cthe optimal state-value function.\u201d This\nis not a coincidence or a poor choice of words; this is, in fact, a property that I'd like to high-\nlight again. An MDP can have more than one optimal policy, but it can only have a single\noptimal state-value function. It\u2019s not too hard to wrap your head around that.\n\nState-value functions are collections of numbers. Numbers can have infinitesimal accu-\nracy, because they\u2019re numbers. There will be only one optimal state-value function (the col-\nlection with the highest numbers for all states). However, a state-value function may have\nactions that are equally valued for a given state; this includes the optimal state-value function.\nIn this case, there could be multiple optimal policies, each optimal policy selecting a different,\nbut equally valued, action. Take a look: the FL environment is a great example of this.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.18,
                        "section_name": "The FL environment has multiple optimal policies",
                        "section_path": "./screenshots-images-2/chapter_3/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_18/345b78aa-74e0-4821-9a54-cea4d3a5cc55.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The FL environment has multiple optimal policies\n\nSTART Dao ba ta\n0.54 0.53 | 9-34 0.32 | 0.44.0.42 | 0.31 0.30\n0.53 0.33 0.43 | ost] 34\n0.36 0.16\n0.56 0.37 0.360. | LI\n0.38 0.20\n\n@ Optimal action-\nvalue function\n\n0.59 0.40 0.33\n0.38 0.40 | 0.44 0.45 | 0.620.40\n0.41 0.64 0.50\n0.50 0.78\n0.46 0.74 | 0.73 0.82\n0.53 0.86\n\n@) Here's a. policy that goes\n@ A policy going left right in state \u00a9, and it\u2019s as\nin state 6 is optimal! @) But, look at state &. g00d, and also optimal!\n\nBy the way, it\u2019s not shown here, but all the actions in a terminal state have the same value,\nzero, and therefore a similar issue that I\u2019m highlighting in state 6.\n\nAs a final note, I want to highlight that policy iteration is guaranteed to converge to the\nexact optimal policy: the mathematical proof shows it will not get stuck in local optima.\nHowever, as a practical consideration, there\u2019s one thing to be careful about. If the action-\nvalue function has a tie (for example, right/left in state 6), we must make sure not to break ties\nrandomly. Otherwise, policy improvement could keep returning different policies, even\nwithout any real improvement. With that out of the way, let\u2019s look at another essential algo-\nrithm for finding optimal state-value functions and optimal policies.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.19,
                        "section_name": "Value iteration: Improving behaviors early",
                        "section_path": "./screenshots-images-2/chapter_3/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_19/f35cb8e1-3299-40a4-bb1f-7e7488f15103.png",
                            "./screenshots-images-2/chapter_3/section_19/fee7221e-9959-411e-aa9d-516fffe9cd60.png",
                            "./screenshots-images-2/chapter_3/section_19/4fb2dc71-a025-493d-9d2f-df5cf20aba73.png",
                            "./screenshots-images-2/chapter_3/section_19/a9473a78-fdbe-4db6-83fe-31283d6560d8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Value iteration: Improving behaviors early\n\nYou probably notice the way policy evaluation works: values propagate consistently on each\niteration, but slowly. Take a look.\n\nPolicy evaluation on the always-left\npolicy on the SWF environment\n\nSTART\ne\u2014] e] |] | ee\n1 2 3 4 5\n\nafter each state sweep\n1st iteration\n0.0 00/00 00/00 0.0/0.0 0.0/017 0.56\n1 2 3 4 5\nthe First 2nd iteration\niteration the\n\" START\nover the 1 2 3 4 5\nwas already\na different 104th iteration\nSTART\npolicy! 1 2 3 4 5\n\n@) The fully converged state-value\nFunction For the aluays-lef policy.\n\nThe image shows a single state-space sweep of policy evaluation followed by an estimation of\nthe Q-function. We do this by using the truncated estimate of the V-function and the MDP,\non each iteration. By doing so, we can more easily see that even after the first iteration, a\ngreedy policy over the early Q-function estimates would be an improvement. Look at the\nQ-values for state 5 in the first iteration; changing the action to point towards the GOAL state\nis obviously already better.\n\nIn other words, even if we truncated policy evaluation after a single iteration, we could still\nimprove upon the initial policy by taking the greedy policy of the Q-function estimation after\na single state-space sweep of policy evaluation. This algorithm is another fundamental algo-\nrithm in RL: it\u2019s called value iteration (V1).\n\nVI can be thought of \u201cgreedily greedifying policies,\u201d because we calculate the greedy policy\nas soon as we can, greedily. VI doesn\u2019t wait until we have an accurate estimate of the policy\nbefore it improves it, but instead, VI truncates the policy-evaluation phase after a single state\nsweep. Take a look at what I mean by \u201cgreedily greedifying policies.\u201d\n\nTruncated\npolicy\nevaluation\n\nTruncated\npolicy\nevaluation\n\nTruncated\n\npolicy\nevaluation\n\nTruncated\npolicy\nevaluation\n\nGreedily greedifying the always-left\npolicy of the SFW environment\n\nSTART\nele fe fe fe\n\nSTART\n0.0 00/00 00/00 00/00 00\n\n1st iteration\n\n017 05\n\nSTART\n0.0 0.0/0.0 0.0/0.0 0.0/0.08 0.25)\n\nPolicy\nimprovement\n\n2nd iteration\n\n0.33 0.67)\n\nPolicy\n\n3rd iteration\n\nSTART\n0.0 0.0/0.0 0.0/0.04 013}0.20 0.42\n\nimprovement\nSTART\nHLH} ] ||] | :\n\nPolicy\nimprovement\n\nSTART\npw fete |) [oo]\n\n|\n\n122nd iteration\n\n4th iteration\n\n0.63 0.82\n\n\u2018START\n|| | > | >\n0.37 0.67]0.79 0.89]0.93 0.96/0.98 0.99\n\n0.99 1.00\n@ This is the optimal action-value Function and optimal policy. \u2014\n\nIf we start with a randomly generated policy, instead of this adversarial policy always-left for\nthe SWF environment, VI would still converge to the optimal state-value function. VI is a\nstraightforward algorithm that can be expressed in a single equation.\n\n= SHow Me tHe Matu\nThe value-iteration equation\n@ we can merge a truncated policy-evaluation step\nand a policy improvement into the same equation.\n()... of the reward and\n\n(@) we calculate @..-using the sum | the discounted estimated\nthe value of each of the weighted value of the next state.\n\nveta(s) = max ) > p(s',r|s, a) |r + yve(s\u2019)|\n\nPpp\nthe max over the \u00a9 and add. For all probability of each\nValues of actions. transitions in the action. possible transition.\n\nNotice that in practice, in VI, we don\u2019t have to deal with policies at all. VI doesn\u2019t have any\nseparate evaluation phase that runs to convergence. While the goal of VI is the same as the\ngoal of PI\u2014to find the optimal policy for a given MDP\u2014VI happens to do this through\nthe value functions; thus the name value iteration.\n\nAgain, we only have to keep track of a V-function and a Q-function (depending on imple-\nmentation). Remember that to get the greedy policy over a Q-function, we take the argu-\nments of the maxima (argmax) over the actions of that Q-function. Instead of improving the\npolicy by taking the argmax to get a better policy and then evaluating this improved policy to\nobtain a value function again, we directly calculate the maximum (max, instead of argmax)\nvalue across the actions to be used for the next sweep over the states.\n\nOnly at the end of the VI algorithm, after the Q-function converges to the optimal values,\ndo we extract the optimal policy by taking the argmax over the actions of the Q-function, as\nbefore. You'll see it more clearly in the code snippet on the next page.\n\nOne important thing to highlight is that whereas VI and PI are two different algorithms, in\na more general view, they are two instances of generalized policy iteration (GPI). GPI is a gen-\neral idea in RL in which policies are improved using their value function estimates, and value\nfunction estimates are improved toward the actual value function for the current policy.\nWhether you wait for the perfect estimates or not is just a detail.\n\n| Speak PyTHON\nThe value-iteration algorithm\ndef value iteration(P, gamma=1.0, theta=le-10):\n\u00a9 Like policy iteration, value iteration is a method For obtaining\noptimal policies. For this, we need an MOP (including gamma).\nTheta is the convergence criteria. le-I0 is sufficiently accurate.\nC V = np.zeros(len(P), dtype=np.float64)\n\n@) First thing is to initialize a state-value function.\nKnow that a. V-Function with random numbers should work Fine.\n\nwhile True:\n[cous get step nda a etn tore\n(@ Notice this one over here has to be zero. Otherwise, the estimate would be incorrect.\nCS Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n) Then, for every transition oF every action in every state, we ...\n\nfor s in range(len(P)): |\nfor a in range(len(P[s])):\nfor prob, next_state, reward, done in P[s] [a]:\n\nLs, Q[s][a] += prob * (reward + gamma * \\\n\nV[next_state] * (not done) )\n@... notice, using v, which is the old \u201ctruncated\u201d estimate.\n\nif np.max(np.abs(V - np.max(Q, axis=1))) < theta:\n\n| break\n(\u00ae After each sweep over the state space, we make sure the state-value Function\nkeeps changing. Otherwise, we found the optimal V-function and should break out.\n\nind V = np.max(Q, axis=1)\n\n@ Thanks to this short line, we don't need a. separate policy-improvement phase. It's\nnot a direct replacement, but instead a combination of improvement and evaluation.\n\npi = lambda s: {s:a for s, a in enumerate(\nnp.argmax(Q, axis=1))}[s]\n\nL return V, pi (10) Only at the end do we extract the optimal policy and\n\nreturn it along with the optimal state-value function.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.2,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_3/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_20/e802665c-e838-4b1f-bad2-fe88e66153c0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nThe objective of a reinforcement learning agent is to maximize the expected return, which is\nthe total reward over multiple episodes. For this, agents must use policies, which can be\nthought of as universal plans. Policies prescribe actions for states. They can be deterministic,\nmeaning they return single actions, or stochastic, meaning they return probability distribu-\ntions. To obtain policies, agents usually keep track of several summary values. The main ones\nare state-value, action-value, and action-advantage functions.\n\nState-value functions summarize the expected return from a state. They indicate how\nmuch reward the agent will obtain from a state until the end of an episode in expectation.\nAction-value functions summarize the expected return from a state-action pair. This type of\nvalue function tells the expected reward-to-go after an agent selects a specific action in a given\nstate. Action-value functions allow the agent to compare across actions and therefore solve\nthe control problem. Action-advantage functions show the agent how much better than the\ndefault it can do if it were to opt for a specific state-action pair. All of these value functions\nare mapped to specific policies, perhaps an optimal policy. They depend on following what\nthe policies prescribe until the end of the episode.\n\nPolicy evaluation is a method for estimating a value function from a policy and an MDP.\nPolicy improvement is a method for extracting a greedy policy from a value function and an\nMDP. Policy iteration consists of alternating between policy-evaluation and policy improve-\nment to obtain an optimal policy from an MDP. The policy evaluation phase may run for\nseveral iterations before it accurately estimates the value function for the given policy. In\npolicy iteration, we wait until policy evaluation finds this accurate estimate. An alternative\nmethod, called value iteration, truncates the policy-evaluation phase and exits it, entering the\npolicy-improvement phase early.\n\nThe more general view of these methods is generalized policy iteration, which describes the\ninteraction of two processes to optimize policies: one moves value function estimates closer\nto the real value function of the current policy, another improves the current policy using\nits value function estimates, getting progressively better and better policies as this cycle\ncontinues.\n\nBy now, you\n\n+ Know the objective of a reinforcement learning agent and the different statistics it\nmay hold at any given time\n\n+ Understand methods for estimating value functions from policies and methods for\nimproving policies from value functions\n\n+ Can find optimal policies in sequential decision-making problems modeled by MDPs\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 4,
                "chapter_name": "Balancing the gathering\nand use of information",
                "chapter_path": "./screenshots-images-2/chapter_4",
                "sections": [
                    {
                        "section_id": 4.1,
                        "section_name": "Balancing the gathering\nand use of information",
                        "section_path": "./screenshots-images-2/chapter_4/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_1/3a0611a7-fcd6-438f-b737-c7bcaaac94f5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "No matter how small and unimportant a decision may seem, every decision you make is a\ntrade-off between information gathering and information exploitation. For example, when\nyou go to your favorite restaurant, should you order your favorite dish, yet again, or should\nyou request that dish you\u2019ve been meaning to try? If a Silicon Valley startup offers you a job,\nshould you make a career move, or should you stay put in your current role?\n\nThese kinds of questions illustrate the exploration-exploitation dilemma and are at the\ncore of the reinforcement learning problem. It boils down to deciding when to acquire knowl-\nedge and when to capitalize on knowledge previously learned. It\u2019s a challenge to know whether\nthe good we already have is good enough. When do we settle? When do we go for more? What\nare your thoughts: is a bird in the hand worth two in the bush or not?\n\nThe main issue is that rewarding moments in life are relative; you have to compare events\nto see a clear picture of their value. For example, I'll bet you felt amazed when you were\noffered your first job. You perhaps even thought that was the best thing that ever happened\nto you. But, then life continues, and you experience things that appear even more rewarding\u2014\nmaybe, when you get a promotion, a raise, or get married, who knows!\n\nAnd that\u2019s the core issue: even if you rank moments you have experienced so far by \u201chow\namazing\u201d they felt, you can\u2019t know what\u2019s the most amazing moment you could experience\nin your life\u2014life is uncertain; you don\u2019t have life\u2019s transition function and reward signal, so\nyou must keep on exploring. In this chapter, you learn about how important it is for your\nagent to explore when interacting with uncertain environments, problems in which the MDP\nisn\u2019t available for planning.\n\nIn the previous chapter, you learned about the challenges of learning from sequential feed-\nback and how to properly balance immediate and long-term goals. In this chapter, we exam-\nine the challenges of learning from evaluative feedback, and we do so in environments that\naren\u2019t sequential, but one-shot instead: multi-armed bandits (MABs).\n\nMABs isolate and expose the challenges of learning from evaluative feedback. We'll dive\ninto many different techniques for balancing exploration and exploitation in these particular\ntype of environments: single-state environments with multiple options, but a single choice.\nAgents will operate under uncertainty, that is, they won\u2019t have access to the MDP. However,\nthey will interact with one-shot environments without the sequential component.\n\nRemember, in DRL, agents learn from feedback that\u2019s simultaneously sequential (as\nopposed to one shot), evaluative (as opposed to supervised), and sampled (as opposed to\nexhaustive). In this chapter, I eliminate the complexity that comes along with learning from\nsequential and sampled feedback, and we study the intricacies of evaluative feedback in isola-\ntion. Let\u2019s get to it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.2,
                        "section_name": "The challenge of interpreting\nevaluative feedback",
                        "section_path": "./screenshots-images-2/chapter_4/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_2/dfb47c58-e9d9-4605-bb7c-c96092436cdb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The challenge of interpreting\nevaluative feedback\n\nIn the last chapter, when we solved the FL environment, we knew beforehand how the envi-\nronment would react to any of our actions. Knowing the exact transition function and reward\nsignal of an environment allows us to compute an optimal policy using planning algorithms,\nsuch as PI and VI, without having to interact with the environment at all.\n\nBut, knowing an MDP in advance oversimplifies things, perhaps unrealistically. We can-\nnot always assume we'll know with precision how an environment will react to our actions\u2014\nthat\u2019s not how the world works. We could opt for learning such things, as you'll learn in later\nchapters, but the bottom line is that we need to let our agents interact and experience the\nenvironment by themselves, learning this way to behave optimally, solely from their own\nexperience. This is what\u2019s called trial-and-error learning.\n\nIn RL, when the agent learns to behave from interaction with the environment, the envi-\nronment asks the agent the same question over and over: what do you want to do now? This\nquestion presents a fundamental challenge to a decision-making agent. What action should it\ndo now? Should the agent exploit its current knowledge and select the action with the highest\ncurrent estimate? Or should it explore actions that it hasn\u2019t tried enough? But many addi-\ntional questions follow: when do you know your estimates are good enough? How do you\nknow you have tried an apparently bad action enough? And so on.\n\nYou will learn more effective ways for dealing\nwith the exploration-exploitation trade-off\n\nThis is the key intuition: exploration builds the knowledge that allows for effective exploita-\ntion, and maximum exploitation is the ultimate goal of any decision maker.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.3,
                        "section_name": "Bandits: Single-state decision problems",
                        "section_path": "./screenshots-images-2/chapter_4/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_3/b046b601-c526-45e6-8270-bb397cf3dd50.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Bandits: Single-state decision problems\n\nMulti-armed bandits (MAB) are a special case of an RL problem in which the size of the state\nspace and horizon equal one. A MAB has multiple actions, a single state, and a greedy hori-\nzon; you can also think of it as a \u201cmany-options, single-choice\u201d environment. The name\ncomes from slot machines (bandits) with multiple arms to choose from (more realistically,\nmultiple slot machines to choose from).\n\nThere are many commercial applications for Multi-armed bandit problem\nthe methods coming out of MAB research.\n\nAdvertising companies need to find the right if Y\nway to balance showing you an ad they predict ole|7| Sug\nyou're likely to click on and showing you a new\n\nad with the potential of it being an even better \u2014_@ 9 4wo-armed bandit is a decision-maki ng\nfit for you. Websites that raise money, such as problem with two choices. You need to try\ncharities or political campaigns, need to balance \u2014_ hem both sufficient to correctly assess\nbetween showing the layout that has led to the \u2014 each option. How do you best handle the\nmost contributions and new designs that haven\u2019t \u2014_exploration-exploitation trade-of$?\n\nbeen sufficiently utilized but still have potential\n\nfor even better outcomes. Likewise, e-commerce websites need to balance recommending\nyou best-seller products as well as promising new products. In medical trials, there\u2019s a need\nto learn the effects of medicines in patients as quickly as possible. Many other problems ben-\nefit from the study of the exploration-exploitation trade-off: oil drilling, game playing, and\nsearch engines, to name a few. Our reason for studying MABs isn\u2019t so much a direct applica-\ntion to the real world, but instead how to integrate a suitable method for balancing explora-\ntion and exploitation in RL agents.\n\nSow Me tHE Matu\nMulti-armed bandit\n@ meres are mOPs with a single non-terminal state, and a single time step per episode.\n\not\n\nGo = 1*0+ 0.99 * 0 + 0.9801 * 0 + 0.9702 * 0 + 0.9605 * 0 + 0.9509 * 1\n@ The @-Sunction of action ais H\u2014\u2014> q(a) = E[R|Ar => a]\nthe expected reward, ~\u2014\u2014___________\n\ngiven a was sampled.\n(3) The best we can do in a MAB is represented\nVx = Q(ax) = max q(a) \u00ablw nse! tit nesing\nacA action that maximizes the Q-function.\nx = argZMax (A) 4G) The optimal action is the action that\nacA maximizes the optimal\n\nQ-function, and\noptimal v-Function (only one state). -\u2014> 9 (ax) = Ux\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.4,
                        "section_name": "Regret: The cost of exploration",
                        "section_path": "./screenshots-images-2/chapter_4/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_4/c5175a4c-4705-4f20-809a-8511aab6f20d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Regret: The cost of exploration\n\nThe goal of MABs is very similar to that of RL. In RL, the agent needs to maximize the expected\ncumulative discounted reward (maximize the expected return). This means to get as much\nreward (maximize) through the course of an episode (cumulative) as soon as possible (if\ndiscounted\u2014later rewards are discounted more) despite the environment\u2019s stochasticity\n(expected). This makes sense when the environment has multiple states and the agent inter-\nacts with it for multiple time steps per episode. But in MABs, while there are multiple epi-\nsodes, we only have a single chance of selecting an action in each episode.\n\nTherefore, we can exclude the words that don\u2019t apply to the MAB case from the RL goal:\nwe remove \u201ccumulative\u201d because there\u2019s only a single time step per episode, and \u201cdiscounted\u201d\nbecause there are no next states to account for. This means, in MABs, the goal is for the agent\nto maximize the expected reward. Notice that the word \u201cexpected\u201d stays there because there\u2019s\nstochasticity in the environment. In fact, that\u2019s what MAB agents need to learn: the underly-\ning probability distribution of the reward signal.\n\nHowever, if we leave the goal to \u201cmaximize the expected reward,\u201d it wouldn\u2019t be straight-\nforward to compare agents. For instance, let\u2019s say an agent learns to maximize the expected\nreward by selecting random actions in all but the final episode, while a much more sample-\nefficient agent uses a clever strategy to determine the optimal action quickly. If we only com-\npare the final-episode performance of these agents, which isn\u2019t uncommon to see in RL, these\ntwo agents would have equally good performance, which is obviously not what we want.\n\nA robust way to capture a more complete goal is for the agent to maximize the per-episode\nexpected reward while still minimizing the total expected reward loss of rewards across all\nepisodes. To calculate this value, called total regret, we sum the per-episode difference of the\ntrue expected reward of the optimal action and the true expected reward of the selected\naction. Obviously, the lower the total regret, the better. Notice I use the word true here; to\ncalculate the regret, you must have access to the MDP. That doesn\u2019t mean your agent needs\nthe MDP, only that you need it to compare agents\u2019 exploration strategy efficiency.\n\nShow Me tHe Matu\n\nTotal regret equation\nCare aimaote tne E @... the difference\nTotal regret 7; you between the optimal value\naaa p> E [v. \u2014Qx(Ae)] \u2014 of the mae and the true\nepisodes . . gual Ba value of the action selected.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.5,
                        "section_name": "Approaches to solving MAB environments",
                        "section_path": "./screenshots-images-2/chapter_4/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_5/85d47a76-7ef9-430e-a132-7b71eb74bbbb.png",
                            "./screenshots-images-2/chapter_4/section_5/c5997d85-999f-4a5f-b6ec-32a07fef3a2d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Approaches to solving MAB environments\n\nThere are three major kinds of approaches to tackling MABs. The most popular and straight-\nforward approach involves exploring by injecting randomness in our action-selection process;\nthat is, our agent will exploit most of the time, and sometimes it\u2019ll explore using randomness.\nThis family of approaches is called random exploration strategies. A basic example of this fam-\nily would be a strategy that selects the greedy action most of the time, and with an epsilon\nthreshold, it chooses uniformly at random. Now, multiple questions arise from this strategy;\nfor instance, should we keep this epsilon value constant throughout the episodes? Should we\nmaximize exploration early on? Should we periodically increase the epsilon value to ensure\nthe agent always explores?\n\nAnother approach to dealing with the exploration-exploitation dilemma is to be optimis-\ntic. Yep, your mom was right. The family of optimistic exploration strategies is a more system-\natic approach that quantifies the uncertainty in the decision-making problem and increases\nthe preference for states with the highest uncertainty. The bottom line is that being optimistic\nwill naturally drive you toward uncertain states because you'll assume that states you haven\u2019t\nexperienced yet are the best they can be. This assumption will help you explore, and as you\nexplore and come face to face with reality, your estimates will get lower and lower as they\napproach their true values.\n\nThe third approach to dealing with the exploration-exploitation dilemma is the family of\ninformation state-space exploration strategies. These strategies will model the information state\nof the agent as part of the environment. Encoding the uncertainty as part of the state space\nmeans that an environment state will be seen differently when unexplored or explored.\nEncoding the uncertainty as part of the environment is a sound approach but can also con-\nsiderably increase the size of the state space and, therefore, its complexity.\n\nIn this chapter, we'll explore a few instances of the first two approaches. We'll do this in a\nhandful of different MAB environments with different properties, pros and cons, and this will\nallow us to compare the strategies in depth.\n\nIt\u2019s important to notice that the estimation of the Q-function in MAB environments is\npretty straightforward and something all strategies will have in common. Because MABs are\none-step environments, to estimate the Q-function we need to calculate the per-action aver-\nage reward. In other words, the estimate of an action a is equal to the total reward obtained\nwhen selecting action a, divided by the number of times action a has been selected.\n\nIt\u2019s essential to highlight that there are no differences in how the strategies we evaluate in\nthis chapter estimate the Q-function; the only difference is in how each strategy uses the\nQ-function estimates to select actions.\n\nConcrete ExXAmPLe\nThe slippery bandit walk (SBW) environment is back!\n\nThe first MAB environment that we'll consider is one we have played with before: the bandit\nslippery walk (BSW).\n\nThe bandit slippery walk environment\n\nThe leftmost (2) The rightmost\nstate is a hole, and state is the goal, and\nprovides 0.0 reward. provides a +l reward,\n\nRemember, BSW is a grid world with a single row, thus, a walk. But a special feature of this\nwalk is that the agent starts at the middle, and any action sends the agent to a terminal state\nimmediately. Because it is a one-time-step, it\u2019s a bandit environment.\n\nBSW is a two-armed bandit, and it can appear to the agent as a two-armed Bernoulli\nbandit. Bernoulli bandits pay a reward of +1 with a probability p and a reward of 0 with prob-\nability q = 1 - p. In other words, the reward signal is a Bernoulli distribution.\n\nIn the BSW, the two terminal states pay either 0 or +1. If you do the math, you'll notice that\nthe probability of a +1 reward when selecting action 0 is 0.2, and when selecting action 1 is\n0.8. But your agent doesn\u2019t know this, and we won't share that info. The question we're trying\nto ask is this: how quickly can your agent figure out the optimal action? How much total\nregret will agents accumulate while learning to maximize expected rewards? Let's find out.\n\nBandit slippery walk graph\n@ Remember: a hole, starting, and goal state\n\nee\n\n0.2\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.6,
                        "section_name": "Greedy: Always exploit",
                        "section_path": "./screenshots-images-2/chapter_4/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_6/43908947-32e8-4673-81fd-159e177f7fd8.png",
                            "./screenshots-images-2/chapter_4/section_6/34d67de5-1676-4f51-8ace-889ab36b46de.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Greedy: Always exploit\n\nThe first strategy I want you to consider isn\u2019t really a strategy but a baseline, instead. I already\nmentioned we need to have some exploration in our algorithms; otherwise, we risk conver-\ngence to a suboptimal action. But, for the sake of comparison, let\u2019s consider an algorithm\nwith no exploration at all.\n\nThis baseline is called a greedy strategy, or pure exploitation strategy. The greedy action-\nselection approach consists of always selecting the action with the highest estimated value.\nWhile there\u2019s a chance for the first action we choose to be the best overall action, the likeli-\nhood of this lucky coincidence decreases as the number of available actions increases.\n\nPure exploitation in the BSW\n1st iteration\n\n> Agent\n@ The action is the index of the\nelement with the highest value Girst Q(@) Lo | | > argmax(Q)= 0\nelement when there are ties). K-\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014>\n\n\u2014> Environment\n\nLOT\n\n> Reward = +1\n\n2nd iteration t @) Let's pretend the environment\n+A goes through this transition and the\ngent\nagent gets the +1.\n@ agent selects Q\u00a2a) [+ | 2 | + argmax(Q)= 0\naction 0 again.\n\n\u2014> Environment\n\n\u00b0 ia > Reward =0\n3rd iteration TES (4) environment goes through this\n\n\u00a9 fs you can see, the +> Agent transition and gives a 0 reward,\n\nnt is already stuck [s-o]e=t]\nnh aston. > Qa) [=| \u00a9 | + argmax(Q)=0\n\nAs you might have expected, the greedy strategy gets stuck with the first action immediately.\nIf the Q-table is initialized to zero, and there are no negative rewards in the environment, the\ngreedy strategy will always get stuck with the first action.\n\n| Speak PyTHoN\nPure exploitation strategy\ndef pure exploitation(env, n_episodes=5000) :\n@ Almost all strategies have the same bookKeeping code for estimating Q-values.\n@) we initialize the Q@-function and the count array to all zeros. 7]\nQ = np.zeros((env.action_space.n) )\n\nN = np.zeros((env.action_space.n) )\n\n(@) These other variables are for calculating statistics and not necessary.\n\nQe = np.empty((n_episodes, env.action_space.n) )\nreturns = np.empty(n_episodes)\nactions np.empty(n_episodes, dtype=np.int)\n\n(@) Here we enter the main loop and interact with the environment.\n\nname = 'Pure exploitation\u2019\nfor e in taqdm(range(n_ episodes),\ndesc='Episodes for: ' + name, leave=False):\n\naction = np.argmax (Q) \u00a9 enue: ou ectorate @-anen wl\n) easy enough, we select the action that maximizes our estimated Q-values.\n\n@) Then, pass it to the environment and receive a new reward,\n\n_, reward, _, _ = env.step(action)\nt= 1\n\nN[action]\n| Qlaction] = Q[action] + (reward - Q[action]) /N[action]\n@ Finally, we update the counts and the @-table.\n\n@\u00ae Then, we update the statistics and start a new\nQefe] = Q episode.\nreturns([e] = reward\nactions[e] = action\n\nreturn name, returns, Qe, actions\n\nI want you to notice the relationship between a greedy strategy and time. If your agent only\nhas one episode left, the best thing is to act greedily. If you know you only have one day to\nlive, you'll do things you enjoy the most. To some extent, this is what a greedy strategy does:\nit does the best it can do with your current view of life assuming limited time left.\n\nAnd this is a reasonable thing to do when you have limited time left; however, if you don\u2019t,\nthen you appear to be shortsighted because you can\u2019t trade-off immediate satisfaction or\nreward for gaining of information that would allow you better long-term results.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.7,
                        "section_name": "Random: Always explore",
                        "section_path": "./screenshots-images-2/chapter_4/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_7/7f7b515f-2584-4039-a0fa-e10348364c33.png",
                            "./screenshots-images-2/chapter_4/section_7/e0614a96-731b-429c-be58-6e391e97eba1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Random: Always explore\nLet\u2019s also consider the opposite side of the spectrum: a strategy with exploration but no\nexploitation at all. This is another fundamental baseline that we can call a random strategy or\na pure exploration strategy. This is simply an approach to action selection with no exploitation\nat all. The sole goal of the agent is to gain information.\n\nDo you know people who, when starting a new project, spend a lot of time \u201cresearching\u201d\nwithout jumping into the water? Me too! They can take weeks just reading papers. Remember,\nwhile exploration is essential, it must be balanced well to get maximum gains.\n\nPure exploration in the BSW\n1st iteration\n> Agent\n\n[eno] ==*|\n@ agent selects action |, Q(a) [ \u00b0 |e | random_action =1\n\nuniformly and at random.\n> Environment a, (@ Consider this transition.\n+ Reward=0\n2nd iteration\n> Agent\nogo\n() agent selects action |, Q(a) [@ [ \u00ab | > random_action = 1\nagain, ~p\n+> Environment (4 Consider this transition.\no fa > Reward =+1\n3rd iteration (\u00a9) Now agent\n@ Agent will continue to > Agent selects action 0.\n\nselect actions randomly [2-0] ==*]\nwith total disregard for H+\u00bb _ Q(a) | \u00b0 | 05 | > random_action =0\ni i)\nthe estimates! ti @ Note, the estimates will converge to\nthe optimal values with enough episodes.\nA random strategy is obviously not a good strategy either and will also give you suboptimal\n\nresults. Similar to exploiting all the time, you don\u2019t want to explore all the time, either. We need\nalgorithms that can do both exploration and exploitation: gaining and using information.\n\n| Speak PyTHoN\nPure exploration strategy\ndef pure exploration(env, n_episodes=5000):\n\n<* @ The pure exploration baseline boilerplate is the same as before,\nSo | removed it for brevity.\n\nname = \u2018Pure exploration'\nfor e in tqdm(range(n_episodes),\ndesc='Episodes for: ' + name,\n\nleave=False):\n\n@) This is how our pure exploration baseline acts.\nBasically, it always selects an action randomly.\n\naction = np.random.randint (len (Q) )\n\n| @ 1 removed the estimation and statistics\n\nreturn name, returns, Qe, actions\n\n4+ @ tt\u2019s somewhat unfair to calll this strategy a. \u201cpure exploration strategy.\u201d It should\nbe called something more along the lines of \u201crandom strategy\u201d becouse there are\n\nother ways to explore that aren't necessarily acting randomly. Still, let's move along.\n\nI left a note in the code snippet, and I want to restate and expand on it. The pure exploration\nstrategy I presented is one way to explore, that is, random exploration. But you can think of\nmany other ways. Perhaps based on counts, that is, how many times you try one action versus\nthe others, or maybe based on the variance of the reward obtained.\n\nLet that sink in for a second: while there\u2019s only a single way to exploit, there are multiple\nways to explore. Exploiting is nothing but doing what you think is best; it\u2019s pretty straight\nforward. You think A is best, and you do A. Exploring, on the other hand, is much more\ncomplex. It\u2019s obvious you need to collect information, but how is a different question. You\ncould try gathering information to support your current beliefs. You could gather informa-\ntion to attempt proving yourself wrong. You could explore based on confidence, or based on\nuncertainty. The list goes on.\n\nThe bottom line is intuitive: exploitation is your goal, and exploration gives you informa-\ntion about obtaining your goal. You must gather information to reach your goals, that is\nclear. But, in addition to that, there are several ways to collect information, and that\u2019s where\nthe challenge lies.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.8,
                        "section_name": "Epsilon-greedy: Almost always greedy and sometimes random",
                        "section_path": "./screenshots-images-2/chapter_4/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_8/45a3e1bd-5fcd-438a-9e3f-b94196ec012f.png",
                            "./screenshots-images-2/chapter_4/section_8/487618dc-bb43-44b3-8630-a0bbb35641ef.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Epsilon-greedy: Almost always greedy and sometimes random\n\nLet\u2019s now combine the two baselines, pure exploitation and pure exploration, so that the\nagent can exploit, but also collect information to make informed decisions. The hybrid strat-\negy consists of acting greedily most of the time and exploring randomly every so often.\n\nThis strategy, referred to as the epsilon-greedy strategy, works surprisingly well. If you select\nthe action you think is best almost all the time, you'll get solid results because you're still\nselecting the action believed to be best, but you're also selecting actions you haven\u2019t tried\nsufficiently yet. This way, your action-value function has an opportunity to converge to its\ntrue value; this will, in turn, help you obtain more rewards in the long term.\n\nEpsilon-greedy in the BSW\n1st iteration\n\u2014 Agent @ The agent selects\n\n[a=o]a=1] action O greedily.\nQ(a) +> \u2014argmax(Q) =0\n\n\u2014> Environment\n0.2\n\ny<** ROA + rm \u2014> Reward =+1\n2nd iteration\nL_, @) The environment goes\n+> Agent through this transition\n\nand gives a +1 reward,\n(2) The agent selects action, Qa) |_* | 2 |-> random_action =1\nthis time randomly, es 7\n\u2014> Environment (@ Consider \u00a9) The agent\n02 this transition. receives a +!\n: reword\n0 (1) ae 2 \u2014> Reward = +1 Pama\n(Wo suppose the SS\n\nagent now selects 3rd iteration .\naction 0, and likely ,. @ Combining exploration\nstarts getting Os. > Agent and exploitation ensures\n\nthe agent doesn't get\n[aro] a=]\nQra) FEES + aromoca -0 See nea estates\n\n\n| Speak PyTHoN\nEpsilon-greedy strategy\ndef epsilon greedy(env, epsilon=0.01, n_episodes=5000):\n\nname = 'E-greedy {}'.format (epsilon)\n\nfor e in tqdm(range(n_ episodes),\ndesc='Episodes for: ' + name,\nleave=False):\n\n@ The epsilon-greedy strategy is surprisingly effective For its simplicity.\n\nKt consists of selecting an action randomly every so often.\n\nFirst thing is to draw a random number and compare to a hyperparameter \u201cepsilon.\u201d\n\nif np.random. random () 4 epsilon:\n\n(2) If the drawn number is greater than epsilon,\neee eee\n\naction = np.argmax(Q)\n\n1 @) otherwise, we explore by selecting an action randomly,\nelse:\naction = np.random.randint (len (Q) ) ____]\n\nLi cane hat he may ery we ye he eed acten because\nwe're selecting an action randomly From all available actions, including\nbut a little less than that depending on the number of actions.\n\n1 \u00a9 Removed the estimation and stats code\n<...>\n\nreturn name, returns, Qe, actions\n\nThe epsilon-greedy strategy is a random exploration strategy because we use randomness to\nselect the action. First, we use randomness to choose whether to exploit or explore, but also\nwe use randomness to select an exploratory action. There are other random-exploration\nstrategies, such as softmax (discussed later in this chapter), that don\u2019t have that first random\ndecision point.\n\nI want you to notice that if epsilon is 0.5 and you have two actions, you can\u2019t say your agent\nwill explore 50% of the time, if by \u201cexplore\u201d you mean selecting the non-greedy action. Notice\nthat the \u201cexploration step\u201d in epsilon-greedy includes the greedy action. In reality, your agent\nwill explore a bit less than the epsilon value depending on the number of actions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.9,
                        "section_name": "Decaying epsilon-greedy: First maximize exploration,\nthen exploitation",
                        "section_path": "./screenshots-images-2/chapter_4/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_9/37eaa380-1186-4a47-b849-7d6bb2cb4c33.png",
                            "./screenshots-images-2/chapter_4/section_9/517fcff6-525b-4168-abd9-dcafbcb9c52d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Decaying epsilon-greedy: First maximize exploration,\nthen exploitation\n\nIntuitively, early on when the agent hasn\u2019t experienced the environment enough is when we'd\nlike it to explore the most; while later, as it obtains better estimates of the value functions, we\nwant the agent to exploit more and more. The mechanics are straightforward: start with a\nhigh epsilon less than or equal to one, and decay its value on every step. This strategy, called\ndecaying epsilon-greedy strategy, can take many forms depending on how you change the value\nof epsilon. Here I\u2019m showing you two ways.\n\n| SPEAK PYTHON\nLinearly decaying epsilon-greedy strategy\ndef lin dec epsilon_greedy (env,\ninit_epsilon=1.0,\nmin_epsilon=0.01,\ndecay ratio=0.05,\nn_episodes=5000) :\n\n+7 0 Again, boilerplate is gone!\nSoo 0k\nname = 'Lin e-greedy {} {} {}'.format(\ninit_epsilon, min_epsilon, decay ratio)\nfor e in tqdm(range(n_ episodes),\ndesc='Episodes for: ' + name,\nleave=False):\n@ Linearly decaying epsilon-greedy consists of making epsilon decay\nlinearly with the number of steps. We start by calculating the number\nof episodes we'd like to decay epsilon to the minimum value.\ndecay episodes = n_ episodes * decay ratio\n(2) Then, calculate the value of epsilon For the current episode.\n\nepsilon = 1 - e / decay episodes\nepsilon *= init epsilon - min_epsilon\nepsilon += min_epsilon\nepsilon = np.clip(epsilon, min_epsilon, init_epsilon)\n() after that, every thing is the same as the epsilon-greedy strategy.\nif np.random.random() > epsilon:\n-_ action = np.argmax(Q)\nelse:\naction = np.random.randint (len (Q) )\n<...>0 + + \u00a9 Stott ore\n\nreturn name, returns, Qe, actions removed here.\n\nI Speak PYTHON\nExponentially decaying epsilon-greedy strategy\n\ndef exp dec epsilon greedy (env,\ninit_epsilon=1.0,\nmin_epsilon=0.01,\ndecay ratio=0.1,\nn_episodes=5000) :\n\n+ OF, not complete code\n\na @) Here we calculate the exponentially decaying epsilons. Now,\nnotice you can calculate all of these values at once, and only query\nan array of pre-computed values as you go through the loop.\n\ndecay episodes = int(n_episodes * decay ratio)\n\nrem_episodes = n_episodes - decay episodes\n\nepsilons = 0.01\n\nepsilons /= np.logspace(-2, 0, decay episodes)\n\nepsilons *= init epsilon - min_epsilon\n\nepsilons += min_epsilon\n\nepsilons = np.pad(epsilons, (0, rem_episodes), 'edge')\n\n@) everything else is the same as before.\n\nname = 'Exp e-greedy {} {} {}'.format (\ninit_epsilon, min_epsilon, decay ratio)\nfor e in tqdm(range(n_episodes),\ndesc='Episodes for: ' + name,\nleave=False):\nif np.random.random() > epsilons[e]:\naction = np.argmax (Q)\nelse:\naction = np.random.randint (len (Q) )\n<...> \u2014\u2014\u2014\u2014 7 @ And stats are removed again, of course.\nreturn name, returns, Qe, actions\n\nThere are many other ways you can handle the decaying of epsilon: from a simple 1/episode\nto dampened sine waves. There are even different implementations of the same linear and\nexponential techniques presented. The bottom line is that the agent should explore with a\nhigher chance early and exploit with a higher chance later. Early on, there\u2019s a high likelihood\nthat value estimates are wrong. Still, as time passes and you acquire knowledge, the likeli-\nhood that your value estimates are close to the actual values increases, which is when you\nshould explore less frequently so that you can exploit the knowledge acquired.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.1,
                        "section_name": "Optimistic initialization: Start off believing it\u2019s a wonderful world",
                        "section_path": "./screenshots-images-2/chapter_4/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_10/577efd91-7250-4925-9596-6824125bcdce.png",
                            "./screenshots-images-2/chapter_4/section_10/60d700ec-6365-4f50-a444-85d59ab977e0.png",
                            "./screenshots-images-2/chapter_4/section_10/550f3ba0-864a-47f6-aee9-accb8d109790.png",
                            "./screenshots-images-2/chapter_4/section_10/9e431185-c3ac-467c-909b-2ddef2e57abb.png",
                            "./screenshots-images-2/chapter_4/section_10/1e103b7f-6555-46ac-8321-81c70ea73a7f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optimistic initialization: Start off believing it\u2019s a wonderful world\n\nAnother interesting approach to dealing with the exploration-exploitation dilemma is to\ntreat actions that you haven\u2019t sufficiently explored as if they were the best possible actions\u2014\nlike you\u2019re indeed in paradise. This class of strategies is known as optimism in the face of\nuncertainty. The optimistic initialization strategy is an instance of this class.\n\nThe mechanics of the optimistic initialization strategy are straightforward: we initialize\nthe Q-function to a high value and act greedily using these estimates. Two points to clarify:\nFirst \u201ca high value\u201d is something we don\u2019t have access to in RL, which we'll address this later\nin this chapter; but for now, pretend we have that number in advance. Second, in addition to\nthe Q-values, we need to initialize the counts to a value higher than one. If we don\u2019t, the\nQ-function will change too quickly, and the effect of the strategy will be reduced.\n\nOptimistic initialization in the BSW\n\nInitial Q = 1, count = 10\nIst iteration \u00a9 Initial values, optimistic!\n\n+ Agent\n\n) The agent selects Q(a) [1 | + | \u2014 argmax(Q)=0\n\u2014__\u2014SSSS\u2014COsssssSSS\u2014C\u2014CSsCsSCSCSCS<\u2018a\n\naction 0 greedily.\n\n\u2014> Environment\n\n02\nr) rm \u2014> Reward=0\n\n2nd iteration through this transition\n\n\u2014> Agent and gives a.0 reward,\n@ This is 10/1, which is \u2014 I\n\n[s=0] a=4]\nthe total reward divided Q(a) > argmax(Q)=1 41\nby the counts. (S) The agent selects\naction | greedily.\n$1 \u00a9 Consider this transition.\n0.2\n\n0 ae 08 SH > Reward =0\nTL @ agent gets\n\n3rd iteration aOreward\n\u00ae Q-values continue + Agent\n\nlower as they converge [s=0] e=4]\nto the optimal. Ha) [ 0: | 091 | - argmax(Q) = 0\n\n\u2014 Environment\n\n| Speak PYTHON\nOptimistic initialization strategy\ndef optimistic initialization (env,\noptimistic estimate=1.0,\ninitial count=100,\nn_episodes=5000) :\n\nQ = np.full((env.action_space.n), J @ In this strategy, we start\noptimistic estimate, initializing the Q-values to an\ndtype=np. float\u00e964) optimistic value.\n\nN = np.full((env.action_space.n),\ninitial count, \u00a2\u2014\u2014\u2014\u2014\u2014\u2018+4 @ we also initialize the\ndtype=np. float6\u00e94) counts that will serve as an\n\nde @ Removed some code here higher thenre sae we\n<.20>\n\nname = \u2018Optimistic {} {}'.format (optimistic estimate,\n\ninitial count)\nfor e in taqdm(range(n_ episodes),\ndesc='Episodes for: ' + name,\nleave=False) :\n\n@) after that, we always select the action with the highest\nestimated value, similar to the \u201cpure exploitation\u201d strategy.\n\naction = np.argmax (Q)\n\n<...> \u00a2\u2014\u2014 \u00a9) Removed more code\nreturn name, returns, Qe, actions\n\nInteresting, right? Momma was right! Because the agent initially expects to obtain more\nreward than it actually can, it goes around exploring until it finds sources of reward. As it\ngains experience, the \u201cnaiveness\u201d of the agent goes away, that is, the Q-values get lower and\nlower until they converge to their actual values.\n\nAgain, by initializing the Q-function to a high value, we encourage the exploration of\nunexplored actions. As the agent interacts with the environment, our estimates will start con-\nverging to lower, but more accurate, estimates, allowing the agent to find and converge to the\naction with the actual highest payoff.\n\nThe bottom line is if you\u2019re going to act greedily, at least be optimistic.\n\nConcrete ExamPLe\nTwo-armed Bernoulli bandit environment\n\nLet's compare specific instantiations of the strategies we have presented so far on a set of\ntwo-armed Bernoulli bandit environments.\n\nTwo-armed Bernoulli bandit environments have a single non-terminal state and two\nactions. Action 0 has an a chance of paying a +1 reward, and with 1-a, it will pay 0 rewards.\nAction 1 has a B chance of paying a +1 reward, and with 1-8, it will pay 0 rewards.\n\nThis is similar to the BSW to an extent. BSW has complimentary probabilities: action 0 pays\n+1 with a probability, and action 1 pays +1 with 1-a chance. In this kind of bandit environ-\nment, these probabilities are independent; they can even be equal.\n\nLook at my depiction of the two-armed Bernoulli bandit MDP.\n\nTwo-armed Bernoulli bandit environments\n\n1-8\n\np 1\n\n@ Here's a. general moP representation For\ntwo-armed Bernoulli bandit environments.\n\nIt's crucial you notice there are many different ways of representing this environment. And in\nfact, this isn\u2019t how | have it written in code, because there\u2019s much redundant and unnecessary\ninformation.\n\nConsider, for instance, the two terminal states. One could have the two actions transition-\ning to the same terminal state. But, you know, drawing that would make the graph too\nconvoluted.\n\nThe important lesson here is you're free to build and represent environments your own\nway; there isn\u2019t a single correct answer. There are definitely multiple incorrect ways, but there\nare also multiple correct ways. Make sure to explore!\n\nYeah, | went there.\n\nTatty it Up\nSimple exploration strategies in two-armed Bernoulli bandit environments\n\nlran two hyperparameter instantiations of all strategies presented so far: the epsilon-greedy,\nthe two decaying, and the optimistic approach, along with the pure exploitation and explo-\nration baselines on five two-armed Bernoulli bandit environments with probabilities a and B\ninitialized uniformly at random, and five seeds. Results are means across 25 runs.\n\nMean Episode Reward\n\non > Pure explotation\n\nPure exploration\n\u201c7 Epsilon. Greedy 0.07\nag ~~ Epsilon Greedy 0.1\n\nLin Epson Greedy 10. 00,02\na-\u2014- . -\n: Socsueraretyuaeecs. w This is the pure exploitation strategy.\na3 \u2014 Opel @) These are all other strategies\n~ Optimvstic 1.0, SO\nMean E Episode Reward (Log scale)\n\nSS\n\n10? 10?\n\n; A\nMean Episode Reward (Zoom on best)\n\nons __ \u00a9) Highest mean episode reward: optimistic 1.0 10 -\u2014t\nous @ exp epsilon-greedy 0.3 second highest -\u2014\u2014\n\n90 ord 8 6 38 00\nTotal Regret\n\n\u2122  @ See the linear total regret of the baselines. r\u2014\u2014\u2014\u2014\u2014\u2014o\u00bb0M\n200 \u2014\n\npsilon-greedy |.0 with low total regret. imeem\n@) Optimistic LO 10 strategy with lowest total regret -\u2014\u2014\u2014\n\n90 m2 oo\u00a2 6 98 acoe\nEpisodes\n\nThe best performing strategy in this experiment is the optimistic with 1.0 initial Q-values and\n10 initial counts. All strategies perform pretty well, and these weren't highly tuned, so it\u2019s just\nfor the fun of it and nothing else. Head to chapter 4\u2019s Notebook and play, have fun.\n\nIt\u2019s IN THE DeTAILs\nSimple strategies in the two-armed Bernoulli bandit environments\nLet's talk about several of the details in this experiment.\n\nFirst, | ran five different seeds (12, 34, 56, 78, 90) to generate five different two-armed\nBernoulli bandit environments. Remember, all Bernoulli bandits pay a +1 reward with certain\nprobability for each arm.\n\nThe resulting environments and their probability of payoff look as follows:\n\nTwo-armed bandit with seed 12:\n\n+ Probability of reward: [0.41630234, 0.5545003 ]\n\nTwo-armed bandit with seed 34:\n\n+ Probability of reward: [0.88039337, 0.56881791]\n\nTwo-armed bandit with seed 56:\n\n+ Probability of reward: [0.44859284, 0.9499771 ]\n\nTwo-armed bandit with seed 78:\n\n+ Probability of reward: [0.53235706, 0.84511988]\n\nTwo -armed bandit with seed 90:\n\n+ Probability of reward: [0.56461729, 0.91744039]\nThe mean optimal value across all seeds is 0.83.\n\nAll of the strategies were run against each of the environments above with five different\nseeds (12, 34, 56, 78, 90) to smooth and factor out the randomness of the results. For instance,\n| first used seed 12 to create a Bernoulli bandit, then | used seeds 12, 34, and so on, to get the\nperformance of each strategy under the environment created with seed 12.\n\nThen, | used seed 34 to create another Bernoulli bandit and used 12, 34, and so on, to\nevaluate each strategy under the environment created with seed 34. | did this for all strate-\ngies in all five environments. Overall, the results are the means over the five environments\nand five seeds, so 25 different runs per strategy.\n\n| tuned each strategy independently but also manually. | used approximately 10 hyperpa-\nrameter combinations and picked the top two from those.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.11,
                        "section_name": "Strategic exploration",
                        "section_path": "./screenshots-images-2/chapter_4/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_11/6c190f32-736c-4e18-8df4-e502a2fcbb6e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Strategic exploration\n\nAlright, imagine you\u2019re tasked with writing a reinforcement learning agent to learn driving a\ncar. You decide to implement an epsilon-greedy exploration strategy. You flash your agent\ninto the car\u2019s computer, start the car, push that beautiful bright green button, and then your\ncar starts exploring. It will flip a coin and decide to explore with a random action, say to drive\non the other side of the road. Like it? Right, me neither. I hope this example helps to illustrate\nthe need for different exploration strategies.\n\nLet me be clear that this example is, of course, an exaggeration. You wouldn\u2019t put an\nuntrained agent directly into the real world to learn. In reality, if you\u2019re trying to use RL in a\nreal car, drone, or in the real world in general, you'd first pre-train your agent in simulation,\nand/or use more sample-efficient methods.\n\nBut, my point holds. If you think about it, while humans explore, we don\u2019t explore ran-\ndomly. Maybe infants do. But not adults. Maybe imprecision is the source of our random-\nness, but we don\u2019t randomly marry someone just because (unless you go to Vegas.) Instead,\nId argue that adults have a more strategic way of exploring. We know that we\u2019re sacrificing\nshort- for long-term satisfaction. We know we want to acquire information. We explore by\ntrying things we haven\u2019t sufficiently tried but have the potential to better our lives. Perhaps,\nour exploration strategies are a combination of estimates and their uncertainty. For instance,\nwe might prefer a dish that we\u2019re likely to enjoy, and we haven't tried, over a dish that we like\nokay, but we get every weekend. Perhaps we explore based on our \u201ccuriosity\u201d or our predic-\ntion error. For instance, we might be more inclined to try new dishes at a restaurant that we\nthought would be okay-tasting food, but it resulted in the best food you ever had. That \u201cpre-\ndiction error\u201d and that \u201csurprise\u201d could be our metric for exploration at times.\n\nIn the rest of this chapter, we'll look at slightly more advanced exploration strategies.\nSeveral are still random exploration strategies, but they apply this randomness in proportion\nto the current estimates of the actions. Other exploration strategies take into account the\nconfidence and uncertainty levels of the estimates.\n\nAll this being said, I want to reiterate that the epsilon-greedy strategy (and its decaying\nversions) is still the most popular exploration strategy in use today, perhaps because it per-\nforms well, perhaps because of its simplicity. Maybe it\u2019s because most reinforcement learning\nenvironments today live inside a computer, and there are very few safety concerns with the\nvirtual world. It\u2019s important for you to think hard about this problem. Balancing the explo-\nration versus exploitation trade-off, the gathering and utilization of information is central\nto human intelligence, artificial intelligence, and reinforcement learning. I\u2019m certain the\nadvancements in this area will have a big impact in the fields of artificial intelligence, rein-\nforcement learning, and all other fields interested in this fundamental trade-off.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.12,
                        "section_name": "Softmax: Select actions randomly in proportion\nto their estimates",
                        "section_path": "./screenshots-images-2/chapter_4/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_12/7af37473-925e-4a0e-b469-a3df69f39331.png",
                            "./screenshots-images-2/chapter_4/section_12/e59b954f-e9bb-43c9-829c-1f9115dc1bc9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Softmax: Select actions randomly in proportion\nto their estimates\n\nRandom exploration strategies make more sense if they take into account Q-value estimates.\nBy doing so, if there is an action that has a really low estimate, we\u2019re less likely to try it. There\u2019s\na strategy, called softmax strategy, that does this: it samples an action from a probability dis-\ntribution over the action-value function such that the probability of selecting an action is\nproportional to its current action-value estimate. This strategy, which is also part of the fam-\nily of random exploration strategies, is related to the epsilon-greedy strategy because of the\ninjection of randomness in the exploration phase. Epsilon-greedy samples uniformly at ran-\ndom from the full set of actions available at a given state, while softmax samples based on\npreferences of higher valued actions.\n\nBy using the softmax strategy, we\u2019re effectively making the action-value estimates an indi-\ncator of preference. It doesn\u2019t matter how high or low the values are; if you add a constant to\nall of them, the probability distribution will stay the same. You put preferences over the\nQ-function and sample an action from a probability distribution based on this preference.\nThe difference between Q-value estimates will create a tendency to select actions with the\nhighest estimates more often, and actions with the lowest estimates less frequently.\n\nWe can also add a hyperparameter to control the algorithm\u2019s sensitivity to the differences\nin Q-value estimates. That hyperparameter, called the temperature (a reference to statistical\nmechanics), works in such a way that as it approaches infinity, the preferences over the\nQ-values are equal. Basically, we sample an action uniformly. But, as the temperature value\napproaches zero, the action with the highest estimated value will be sampled with probability\nof one. Also, we can decay this hyperparameter either linearly, exponentially, or another\nway. But, in practice, for numerical stability reasons, we can\u2019t use infinity or zero as the tem-\nperature; instead, we use a very high or very low positive real number, and normalize these\nvalues.\n\n& SHow Me THE Matu\n\nSoftmax exploration strategy @... te the\n@ To calculate the probability preference of\n\nof selecting action a... selecting that action\n\nby dividing the\n\n\u00a9@) Raise that to e. Fonr: el = von eg\n\nx(a) = -\u2014\u2014\u2014\u2014\u2014\u2014\u2014 ameter tou.\n\nb [4\n| Ebsco 20)\n\n@) Finally, normalize the values by dividing by the sum of all preferences.\n\n1 Speak PYTHON\nSoftmax strategy\n\ndef softmax(env,\ninit_temp=1000.0,\nmin_temp=0.01,\ndecay ratio=0.04,\nn_episodes=5000) :\n\nname = 'SoftMax {} {} {}'.format(init_temp,\nmin_temp,\ndecay ratio)\nfor e in tqdm(range(n_ episodes),\ndesc='Episodes for: ' + name,\nleave=False) :\n@) First, we calculate the linearly decaying temperature the\nsame way we did with the linearly decaying epsilon.\ndecay episodes = n_ episodes * decay ratio\ntemp = 1 - e / decay episodes\ntemp *= init temp - min_temp\ntemp += min_temp\ntemp = np.clip(temp, min_temp, init temp)\n\n\u2014_\u2014__+\n@)\\ make sure min_temp isn\u2019t 0, to avoid div by zero. Check the Notebook For details.\n(4) Next we calculate the probabilities by applying the softmax Function to the Q-values.\nscaled Q = Q / temp \u00a9 Normalize for numeric stability.\nnorm_Q = scaled _Q - np.max(scaled Q)\nexp Q = np.exp(norm_Q)\nprobs = exp Q / np.sum(exp Q) @ Finally, we make sure we got\ngo0d probabilities and select the\n\naction based on them.\nassert np.isclose(probs.sum(), 1.0) |\n\naction = np.random.choice (np.arange(len(probs)),\nsize=1,\np=probs) [0]\n\n_, reward, _, _ = env.step (action)\n<...7\n\n4#\u2014 i @ code was removed here too.\nreturn name, returns, Qe, actions\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.13,
                        "section_name": "UCB: It\u2019s not about optimism, it\u2019s about realistic optimism",
                        "section_path": "./screenshots-images-2/chapter_4/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_13/659d41ab-8b9c-40a2-ae50-d35f164067f0.png",
                            "./screenshots-images-2/chapter_4/section_13/b8091034-faf1-437e-bf53-c4c89d92acfe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "UCB: It\u2019s not about optimism, it\u2019s about realistic optimism\n\nIn the last section, I introduced the optimistic initialization strategy. This is a clever (and\nperhaps philosophical) approach to dealing with the exploration versus exploitation trade-\noff, and it\u2019s the simplest method in the optimism in the face of uncertainty family of strate-\ngies. But, there are two significant inconveniences with the specific algorithm we looked at.\nFirst, we don\u2019t always know the maximum reward the agent can obtain from an environment.\nIf you set the initial Q-value estimates of an optimistic strategy to a value much higher than\nits actual maximum value, unfortunately, the algorithm will perform sub-optimally because\nthe agent will take many episodes (depending on the \u201ccounts\u201d hyperparameter) to bring the\nestimates near the actual values. But even worse, if you set the initial Q-values to a value lower\nthan the environment\u2019s maximum, the algorithm will no longer be optimistic, and it will no\nlonger work.\n\nThe second issue with this strategy as we presented it is that the \u201ccounts\u201d variable is a\nhyperparameter and it needs tuning, but in reality, what we\u2019re trying to represent with this\nvariable is the uncertainty of the estimate, which shouldn\u2019t be a hyperparameter. A better\nstrategy, instead of believing everything is roses from the beginning and arbitrarily setting\ncertainty measure values, follows the same principles as optimistic initialization while using\nstatistical techniques to calculate the value estimates uncertainty and uses that as a bonus for\nexploration. This is what the upper confidence bound (UCB) strategy does.\n\nIn UCB, we're still optimistic, but it\u2019s a more a realistic optimism; instead of blindly hop-\ning for the best, we look at the uncertainty of value estimates. The more uncertain a Q-value\nestimate, the more critical it is to explore it. Note that it\u2019s no longer about believing the value\nwill be the \u201cmaximum possible,\u201d though it might be! The new metric that we care about here\nis uncertainty; we want to give uncertainty the benefit of the doubt.\n\n& SHow Me tHe MatH\nUpper confidence bound (UCB) equation\n\n@ To select the action @...add the @)...andan\not episode \u20ac... Q-value estimates... | uncertainty bonus.\nIne\n\nA, = aremax Q-(a) +\u00a2 Nela)\n\nee,\n\nTo implement this strategy, we select the action with the highest sum of its Q-value estimate\nand an action-uncertainty bonus U. That is, we\u2019re going to add a bonus, upper confidence\nbound U,(a), to the Q-value estimate of action a, such that if we attempt action a only a few\ntimes, the U bonus is large, thus encouraging exploring this action. If the number of attempts\nis significant, we add only a small U bonus value to the Q-value estimates, because we are\nmore confident of the Q-value estimates; they\u2019re not as critical to explore.\n\n| Speak PyTHON\nUpper confidence bound (UCB) strategy\n\ndef upper confidence bound (env,\n\nc=2,\nn_episodes=5000) :\n-\n<\u00a5o @ Code removed For brevity.\nname = \u2018UCB {}'.format (c)\nfor e in taqdm(range(n_ episodes),\ndesc='Episodes for: ' + name,\n\nleave=False) :\n\nif e < len(Q): mF @ we first select all actions\naction =e once to avoid division by zero.\nelse:\n\n= (2) Then, proceed to calculating the confidence bounds.\n\nU = np.sqrt(c * np.log(e)/N)\n@) Last, we pick the action with the highest value with an uncertainty\nbonus: the more uncertain the value of the action, the higher the bonus.\naction = np.argmax(Q + U)\n\n$0008 C\u2014O (S) Stats code removed for brevity\nreturn name, returns, Qe, actions\n\nOn a practical level, if you plot U as a function of the episodes and counts, you'll notice it\u2019s\nmuch like an exponentially decaying function with a few differences. Instead of the smooth\ndecay exponential functions show, there\u2019s a sharp decay early on and a long tail. This makes\nit so that early on when the episodes are low, there\u2019s a higher bonus for smaller differences\nbetween actions, but as more episode pass, and counts increase, the difference in bonuses for\nuncertainty become smaller. In other words, a 0 versus 100 attempts should give a higher\nbonus to 0 than to a 100 in a 100 versus 200 attempts. Finally, the c hyperparameter controls\nthe scale of the bonus: a higher c means higher bonuses, lower c lower bonuses.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.14,
                        "section_name": "Thompson sampling: Balancing reward and risk",
                        "section_path": "./screenshots-images-2/chapter_4/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_14/6325dcaf-7966-449b-891c-7332ccf679d3.png",
                            "./screenshots-images-2/chapter_4/section_14/6523fd24-6a2e-411e-a243-09d4d43645e1.png",
                            "./screenshots-images-2/chapter_4/section_14/f6803a5b-f4ce-471a-b33a-156d171ae3c2.png",
                            "./screenshots-images-2/chapter_4/section_14/e6d11ebe-101c-4a0f-bd17-9f2da470f98f.png",
                            "./screenshots-images-2/chapter_4/section_14/69d00673-02d0-496a-915e-5944fb42e8d4.png",
                            "./screenshots-images-2/chapter_4/section_14/659fdfc1-ff44-487e-89f7-2a60a2d09c9e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Thompson sampling: Balancing reward and risk\n\nThe UCB algorithm is a frequentist approach to dealing with the exploration versus exploita-\ntion trade-off because it makes minimal assumptions about the distributions underlying the\nQ-function. But other techniques, such as Bayesian strategies, can use priors to make reason-\nable assumptions and exploit this knowledge. The Thompson sampling strategy is a sample-\nbased probability matching strategy that allows us to use Bayesian techniques to balance the\nexploration and exploitation trade-off.\n\nA simple way to implement this strategy is to keep track of each Q-value as a Gaussian\n(a.k.a. normal) distribution. In reality, you can use any other kind of probability distribution\nas prior; beta distributions, for instance, are a common choice. In our case, the Gaussian\nmean is the Q-value estimate, and the Gaussian standard deviation measures the uncertainty\nof the estimate, which are updated on each episode.\n\nComparing two action-value functions\nrepresented as Gaussian distributions\n\nCT\n\nThis Q-Sunction seems\n\nbetter because its mean is\n\nhigher than the other one.\n@) but is it? We're much\nmore uncertain about\nthe estimate of the\nother one. Shouldn't we\n\nexplore it?\n\nAs the name suggests, in Thompson sampling, we sample from these normal distributions\nand pick the action that returns the highest sample. Then, to update the Gaussian distribu-\ntions\u2019 standard deviation, we use a formula similar to the UCB strategy in which, early on\nwhen the uncertainty is higher, the standard deviation is more significant; therefore, the\nGaussian is broad. But as the episodes progress, and the means shift toward better and better\nestimates, the standard deviations gets lower, and the Gaussian distribution shrinks, and so\nits samples are more and more likely to be near the estimated mean.\n\nI Speak PYTHON\nThompson sampling strategy\n\ndef thompson_sampling (env,\nalpha=1,\nbeta=0,\nn_episodes=5000) :\n\n| @ Initialization code removed\n\n<...>\nname = 'Thompson Sampling {} {}'.format(alpha, beta)\nfor e in tqdm(range(n_ episodes),\n\ndesc='Episodes for: ' + name,\n\nleave=False) :\n@) In our implementation, we'll sample numbers From the Gaussian distributions.\nNotice how the \u2018scale\u2019 which is the width of the Gaussian (the standard deviation)\nshrinks with the number of times we try each action. Also, notice how \u2018alpha?\ncontrols the initial width of the Gaussian, and \u2018beta\u2019 the rate at which they shrink.\nsamples = np.random.normal (\nloc=Q, scale=alpha/(np.sqrt(N) + beta) )\n\nDL, ccs me @ Then, we select the action with the highest sample.\naction = np.argmax (samples)\n\n<2... 0 SH @ Stats code removed\nreturn name, returns, Qe, actions\n\nIn this particular implementation, I use two hyperparameters: alpha, to control the scale of\nthe Gaussian, or how large the initial standard deviation will be, and beta, to shift the decay\nsuch that the standard deviation shrinks more slowly. In practice, these hyperparameters\nneed little tuning for the examples in this chapter because, as you probably already know, a\nstandard deviation of just five, for instance, is almost a flat-looking Gaussian representing\nover a ten-unit spread. Given our problems have rewards (and Q-values) between 0 and 1,\nand approximately between \u20143 and 3 (the example coming up next), we wouldn\u2019t need any\nGaussian with standard deviations too much greater than 1.\n\nFinally, I want to reemphasize that using Gaussian distributions is perhaps not the most\ncommon approach to Thompson sampling. Beta distributions seem to be the favorites here.\nI prefer Gaussian for these problems because of their symmetry around the mean, and because\ntheir simplicity makes them suitable for teaching purposes. However, I encourage you to dig\nmore into this topic and share what you find.\n\nTatty it Up\n\nAdvanced exploration strategies in two-armed Bernoulli bandit environments\n\n| ran two hyperparameter instantiations of each of the new strategies introduced: the softmax,\nthe UCB, and the Thompson approach, along with the pure exploitation and exploration base-\nlines, and the top-performing simple strategies from earlier on the same five two-armed\nBernoulli bandit environments. This is again a total of 10 agents in five environments across\nfive seeds. It\u2019s a 25 runs total per strategy. The results are averages across these runs.\n\nor\nos\nos\na4\n\n03\n\nosm\n\noR?\n\n0.026\n\noss\n\n3\n\nMean Episode Reward\n\n\u2014 ern.aploheton\noscars @ advanced exploration strategies} ]\n\n\u2014\u2014 Exp Epsilon Greedy 0.3, 0.0, 0.3\n\nLun Soften inf, 0.0, 6.005 a a\n= Unsenue $60, 605. 06% (@) This is the pure explorati 5\nfee ma @) This is the pure exploitation baseline.\n00 0\n\n~ Thompson Sampving 0.5.0.5 9\n\na (ee\n\n10?\n\nMean Episode Ri Reward d (Zoom oF on best).\n\n\u00a9 east performances across all experiments soFtmax it \u2014T\n\n0?\n\na * 7 \u201d acoo\nTotal 1 Regret\nQu regret of the baselines a \u2014_\n_ el moo ns0ereeee\n\n6 opheniatis some tow regret fete\n@) Lowest regret by softmax int H\u2014\n\nyoo m2 oe 6\nEpisodes\n\nBesides the fact that the optimistic strategy uses domain knowledge that we cannot assume\nwe'll have, the results indicate that the more advanced approaches do better.\n\nConcrete EXAMPLE\n10-armed Gaussian bandit environments\n\n10-armed Gaussian bandit environments still have a single non-terminal state; they're bandit\nenvironments. As you probably can tell, they have ten arms or actions instead of two like\ntheir Bernoulli counterparts. But, . .\n\nthe probability distributions and 10-armed Gaussian bandit\n\nreward signals are different from\nthe Bernoulli bandits. First, Bernoulli\nbandits have a probability of payoff\nof p, and with 1-p, the arm won't\npay anything. Gaussian bandits, on\nthe other hand, will always pay\nsomething (unless they sample a\nO\u2014more on this next). Second,\nBernoulli bandits have a binary\nreward signal: you either get a +1 or\na 0. Instead, Gaussian bandits pay\nevery time by sampling a reward\nfrom a Gaussian distribution.\n\nTo create a 10-armed Gaussian\nbandit environment, you first sample from a standard normal (Gaussian with mean 0 and\nvariance 1) distribution 10 times to get the optimal action-value function q*(a,) for all k (10)\narms. These values will become the mean of the reward signal for each action. To get the\nreward for action k at episode e, we sample from another Gaussian with mean q*(a,), and\nvariance 1.\n\n#6 SHow Me tHE Matu\n10-armed Gaussian bandit reward function\n\u00a9 Prior to interacting with the environment, @) We do this by sampling from a standard\n\nwe create it by calculating the optimal Gaussian distribution: that\u2019s a Gaussian\n\naction-value for each arm/action k. with (se ___]\nL_,a,(s,a) = gx(s,a) \u2014 vx(s)\n\n@) Once our agent is interacting with the (4)... we sample From a Gaussian\n\nenvironment, to sample the reward 2 for distribution centered on the optimal\n\narm/action kin episode e... Q-value, and variance |.\nCy Rie ~N (n= 4\" (ax), 07 = 1) \u2014\n\nTatty it Up\nAdvanced exploration strategies in 10-armed Gaussian bandit environments\n\n| ran the same hyperparameter instantiations of the simple strategies introduced earlier, now\non five 10-armed Gaussian bandit environments. This is obviously an \u201cunfair\u201d experiment\nbecause these techniques can perform well in this environment if properly tuned, but my\ngoal is to show that the most advanced strategies still do well with the old hyperparameters,\ndespite the change of the environment. You'll see that in the next example.\n\nMean Episode Reward\n\n15. pure explonaton\nPure exploration\n\n10 Epstion Greedy 0.07\n\u2014\u2014 Epson Gracey 0.1\n\nas \u2014 UnEpston.Greedy 10.00.02 @ Simple strategies not doing that much better than the baselines\n\n\u2014-- Un Eesiton Groody 0.4.0.001.0.1\nExp Epsilon-Greedy 1.0, 0.0, 0.1\n\u2014\u2014 tap tpsilor Greedy 0.3, 0.0, 0,3\nOptimistic 2.0, 10\nOptimistic 2.0, 50 = oe\n\n400 mo\nMean Episode Reward (Log scale)\n\n~os\n\nw we\non Episode Reward (Zoom on best).\n\n\u201d ~ @ Lin e-greedy 1.0 is doing well in terms of reward.\n\nFES Sremneitetanenies tes Seiestse-~ Haste eses testi. atetetsttenieeeantestenee In iene nataeenNS\n\n12 (5) Then, we have exp epsilon-greedy L.0. - >\n\n149\n148\n\n= 0 epaton-greedy 007 Follow \u2014\u2014\n\n\u201cTotal Regret\n\n=) See the linear total regret of the baselines. K\u2014\u2014\u2014\u2014\u2014\u2014\u2014> ...- ~~ Oo\n\na Tpit \u2014\n\no 200 400 0 200 1000\nTotal Regret (Zoom on best)\n\n\u2122  @ Third-lowest total regret: epsilon-greedy 0.07 nl\nuo UD Second-lowest total regret: exp epsilon-greedy 0 -\u2014\u2014\u2014}\nuo Gi) Lowest total regret: lin epsilon-greedy 1.0 \u2014\u2014.\n\nwoo m 98 sooo\nepisodes\n\n8 995 100\n\nLook at that, several of the most straightforward strategies have the lowest total regret and\nthe highest expected reward across the five different scenarios. Think about that for a sec!\n\nTatty it Up\nAdvanced exploration strategies in 10-armed Gaussian bandit environments\n\nI then ran the advanced strategies with the same hyperparameters as before. | also added\nthe two baselines and the top two performing simple strategies in the 10-armed Gaussian\nbandit. As with all other experiments, this is a total of 25 five runs.\n\nMean Episode Reward\n\n15 = Pure explanation eet ee oncom as\nPure exploration =\nlo Un Epston-Greedy 1.0. 9.0.0.1\n\n\u2014\u2014 Bmuincec.ows. Gd) This is the pure exploitation strategy. -\u2014\u2014\u2014>\n==\" Lin SoftMax 100, 0.03, 0.08 a maccoeeccencewsvarsasee]easeasnereneracenssacenssescaseateneercaceescecceceeceeceeee\n\noo Menez @) This is the pure exploration strategy, ; __4*\n25 Trompoon Sameting 5,05 x6 suo @ These are all other strategies\n\nMean! Episode Reward (Lop sc scale)\n\nos\n\n1s\n\n18200\n16175\n\n16150 _ 7 =\n\n16123 -\n\n_ (\u00a9) Top to bottom: UC 0.3, UCB OS, Themplon OS \u2014_\n\n990 2 #6 8 000\nTotal Regret\n\nis () See the linear total regret of the baselines. >...\n\u201cme CO Sofimos:inf se longer doing tha we \u2014\u2014\n\n0\na0\no\n\n0 200 400\n\nTotal Regret (Zoom on best)\n\n+ \u00a9 Top to bottom (lower is better): Thompson 0.5, UCB O.S, UCB 0.A HK\u2014\u2014,\n\nans\n\n45.0\n\nazo\naon\nas\n\n990 wm 8 me 8 000\nEpisodes:\n\nThis time only the advanced strategies make it on top, with apretty decent total regret. What\nyou should do now is head to the Notebook and have fun! Please, also share with the com-\nmunity your results, if you run additional experiments. | can\u2019t wait to see how you extend\nthese experiments. Enjoy!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.15,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_4/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_15/e187816e-d905-47db-9d10-537d64510e55.png",
                            "./screenshots-images-2/chapter_4/section_15/8235a6b6-5359-4d54-a7fc-e2590036197d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nLearning from evaluative feedback is a fundamental challenge that makes reinforcement\nlearning unique. When learning from evaluative feedback, that is, +1, +1.345, +1.5, -100, -4,\nyour agent doesn\u2019t know the underlying MDP and therefore cannot determine what the max-\nimum reward it can obtain is. Your agent \u201cthinks\u201d: \u201cWell, I got a +1, but I don\u2019t know, maybe\nthere\u2019s a +100 under this rock?\u201d This uncertainty in the environment forces you to design\nagents that explore.\n\nBut as you learned, you can\u2019t take exploration lightly. Fundamentally, exploration wastes\ncycles that could otherwise be used for maximizing reward, for exploitation, yet, your agent\ncan\u2019t maximize reward, or at least pretend it can, without gathering information first, which\nis what exploration does. All of a sudden, your agent has to learn to balance exploration and\nexploitation; it has to learn to compromise, to find an equilibrium between two crucial yet\ncompeting sides. We\u2019ve all faced this fundamental trade-off in our lives, so these issues should\nbe intuitive to you: \u201cA bird in the hand is worth two in the bush,\u201d yet \u201cA man\u2019s reach should\nexceed his grasp.\u201d Pick your poison, and have fun doing it, just don\u2019t get stuck to either one.\nBalance them!\n\nKnowing this fundamental trade-off, we introduced several different techniques to create\nagents, or strategies, for balancing exploration and exploitation. The epsilon-greedy strategy\ndoes it by exploiting most of the time and exploring only a fraction. This exploration step is\ndone by sampling an action at random. Decaying epsilon-greedy strategies capture the fact\nthat agents need more exploration at first because they need to gather information to start\nmaking a right decision, but they should quickly begin to exploit to ensure they don\u2019t accumu-\nlate regret, which is a measure of how far from optimal we act. Decaying epsilon-greedy strat-\negies decay epsilon as episodes increase and, hopefully, as our agent gathers information.\n\nBut then we learned about other strategies that try to ensure that \u201chopefully\u201d is more\nlikely. These strategies take into account estimates and their uncertainty and potential and\nselect accordingly: optimistic initialization, UCB, Thompson sampling, and although soft-\nmax doesn\u2019t really use uncertainty measures, it explores by selecting randomly in the propor-\ntion of the estimates.\n\nBy now, you\n\n+ Understand that the challenge of learning from evaluative feedback is because agents\ncannot see the underlying MDP governing their environments\n\n+ Learned that the exploration versus exploitation trade-off rises from this problem\n\n+ Know about many strategies that are commonly used for dealing with this issue\n\nTWEETABLE Feat\nWork on your own and share your findings\n\nHere are several ideas on how to take what you've learned to the next level. If you'd like, share\nyour results with the rest of the world and make sure to check out what others have done,\ntoo. It\u2019s a win-win situation, and hopefully, you'll take advantage of it.\n\n+ #gdrl_ch04_tf01: There are many more techniques for solving bandit envi-\nronments. Try exploring other resources out there and tell us techniques that are\nimportant. Research Bayesian approaches to action selection, and also, action-\nselection strategies that are based on information gain. What is information gain,\nagain? Why is this important in the context of RL? Can you develop other interesting\naction-selection strategies, including decaying strategies that use information to\ndecay the exploration rate of agents? For instance, imagine an agent the decays epsi-\nlon based on state visit:\u2014perhaps on another metric.\n\n+ #gdrl_ch04_tf02: Can you think of a few other bandit environments that are\ninteresting to examine? Clone my bandit repository (https://github.com/mimoralea\n/gym-bandits\u2014which is forked, too,) and add a few other bandit environments to it.\n\n+ #gdri_ch04_tf03: After bandit environments, but before reinforcement learning\nalgorithms, there's another kind of environment called contextual bandit problems.\nWhat are these kinds of problems? Can you help us understand what these are? But,\ndon't just create a blog post about them. Also create a Gym environment with con-\ntextual bandits. Is that even possible? Create those environments in a Python pack-\nage, and another Python package with algorithms that can solve contextual bandit\nenvironments.\n\n+ #gdri_ch04_tf04: In every chapter, I'm using the final hashtag as a catchall\nhashtag. Feel free to use this one to discuss anything else that you worked on relevant\nto this chapter. There\u2019s no more exciting homework than that which you create for\nyourself. Make sure to share what you set yourself to investigate and your results.\n\nWrite a tweet with your findings, tag me @mimoralea (I'll retweet), and use the particular\nhashtag from this list to help interested folks find your results. There are no right or wrong\nresults; you share your findings and check others\u2019 findings. Take advantage of this to socialize,\ncontribute, and get yourself out there! We're waiting for you!\n\nHere's a tweet example:\n\n\u201cHey, @mimoralea. | created a blog post with a list of resources to study deep reinforce-\nment learning. Check it out at <link>. #gdrl_ch01_tf01\u201d\n\nI'll make sure to retweet and help others find your work.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 5,
                "chapter_name": "Evaluating\nagents\u2019 behaviors",
                "chapter_path": "./screenshots-images-2/chapter_5",
                "sections": [
                    {
                        "section_id": 5.1,
                        "section_name": "Evaluating\nagents\u2019 behaviors",
                        "section_path": "./screenshots-images-2/chapter_5/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_1/a37ef10d-e6d7-41e1-9a6d-6f5e15aca1c7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "You know how challenging it is to balance immediate and long-term goals. You probably\nexperience this multiple times a day: should you watch movies tonight or keep reading this\nbook? One has an immediate satisfaction to it; you watch the movie, and you go from poverty\nto riches, from loneliness to love, from overweight to fit, and so on, in about two hours and\nwhile eating popcorn. Reading this book, on the other hand, won\u2019t really give you much\ntonight, but maybe, and only maybe, will provide much higher satisfaction in the long term.\n\nAnd that\u2019s a perfect lead-in to precisely the other issue we discussed. How much more\nsatisfaction in the long term, exactly, you may ask. Can we tell? Is there a way to find out?\nWell, that\u2019s the beauty of life: 1 don\u2019t know, you don\u2019t know, and we won\u2019t know unless we\ntry it out, unless we explore it. Life doesn\u2019t give you its MDP; life is uncertain. This is what we\nstudied in the last chapter: balancing information gathering and information utilization.\n\nHowever, in the previous chapter, we studied this challenge in isolation from the sequen-\ntial aspect of RL. Basically, you assume your actions have no long-term effect, and your only\nconcern is to find the best thing to do for the current situation. For instance, your concern\nmay be selecting a good movie, or a good book, but without thinking how the movie or the\nbook will impact the rest of your life. Here, your actions don\u2019t have a \u201ccompounding effect.\u201d\n\nNow, in this chapter, we look at agents that learn from feedback that\u2019s simultaneously\nsequential and evaluative; agents need to simultaneously balance immediate and long-term\ngoals, and balance information gathering and utilization. Back to our \u201cmovie or book\u201d exam-\nple, where you need to decide what to do today knowing each decision you make builds up,\naccumulates, and compounds in the long term. Since you are a near-optimal decision maker\nunder uncertainty, just as most humans, will you watch a movie or keep on reading? Hint!\n\nYou're smart. . . . In this chapter, we'll study agents that can learn to estimate the value of\npolicies, similar to the policy-evaluation method, but this time without the MDP. This is\noften called the prediction problem because we're estimating value functions, and these are\ndefined as the expectation of future discounted rewards, that is, they contain values that\ndepend on the future, so we\u2019re learning to predict the future in a sense. Next chapter, we'll\nlook at optimizing policies without MDPs, which is called the control problem because we\nattempt to improve agents\u2019 behaviors. As you'll see in this book, these two are equally essen-\ntial aspects of RL. In machine learning, the saying goes, \u201cThe model is only as good as the\ndata.\u201d In RL, I say, \u201cThe policy is only as good as the estimates,\u201d or, detailed, \u201cThe improve-\nment of a policy is only as good as the accuracy and precision of its estimates.\u201d\n\nOnce again, in DRL, agents learn from feedback that\u2019s simultaneously sequential (as\nopposed to one-shot), evaluative (as opposed to supervised) and sampled (as opposed to\nexhaustive). In this chapter, we\u2019re looking at agents that learn from feedback that\u2019s simulta-\nneously sequential and evaluative. We\u2019re temporarily shelving the \u201csampled\u201d part, but we'll\nopen those gates in chapter 8, and there will be fun galore. I promise.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.2,
                        "section_name": "Learning to estimate the value of policies",
                        "section_path": "./screenshots-images-2/chapter_5/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_2/8011bdb3-6b30-435d-b4ab-83042c3eaaee.png",
                            "./screenshots-images-2/chapter_5/section_2/1cc77f17-0c58-4bbf-b8ea-e0edab7efdcc.png",
                            "./screenshots-images-2/chapter_5/section_2/e8ceeecb-e8c4-4bcf-a7d2-328ad80089e8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning to estimate the value of policies\n\nAs I mentioned before, this chapter is about learning to estimate the value of existing policies.\n\nWhen I was first introduced to this prediction problem, I didn\u2019t get the motivation. To me, if\n\nyou want to estimate the value of a policy, the straightforward way of doing it is running the\npolicy repeatedly and averaging what you get.\n\nAnd, that\u2019s definitely a valid approach, and perhaps the most natural. What I didn\u2019t realize\nback then, however, is that there are many other approaches to estimating value functions.\nEach of these approaches has advantages and disadvantages. Many of the methods can be\nseen as an exact opposite alternative, but there\u2019s also a middle ground that creates a full spec-\ntrum of algorithms.\n\nIn this chapter, we'll explore a variety of these approaches and dig into their pros and cons,\n\nshowing you how they relate.\n\nR b Wit an RL Accent\nReward vs. return vs. value function\n\nReward: Refers to the one-step reward signal the agent gets: the agent observes a state,\nselects an action, and receives a reward signal. The reward signal is the core of RL, but it is not\nwhat the agent is trying to maximize! Again, the agent isn\u2019t trying to maximize the reward!\nRealize that while your agent maximizes the one-step reward, in the long-term, it\u2019s getting\nless than it could.\n\nReturn: Refers to the total discounted rewards. Returns are calculated from any state and\nusually go until the end of the episode. That is, when a terminal state is reached, the calcula-\ntion stops. Returns are often referred to as total reward, cumulative reward, sum of rewards,\nand are commonly discounted: total discounted reward, cumulative discounted reward, sum\nof discounted reward. But, it\u2019s basically the same: a return tells you how much reward your\nagent obtained in an episode. As you can see, returns are better indicators of performance\nbecause they contain a long-term sequence, a single-episode history of rewards. But the\nreturn isn\u2019t what an agent tries to maximize, either! An agent who attempts to obtain\nthe highest possible return may find a policy that takes it through a noisy path; sometimes,\nthis path will provide a high return, but perhaps most of the time a low one.\n\nValue function: Refers to the expectation of returns. Sure, we want high returns, but high in\nexpectation (on average). If the agent is in a noisy environment, or if the agent is using a sto-\nchastic policy, it\u2019s all fine. The agent is trying to maximize the expected total discounted\nreward, after all: value functions.\n\nee Micuet\u2019s ANALOGY\n\nRewards, returns, value functions, and life\n\nHow do you approach life? Do you select actions that are the best for you, or are you one of\nthose kind folks who prioritize others before themselves?\n\nThere's no shame either way! Being selfish, to me, is an excellent reward signal. It takes you\nplaces. It drives you around. Early on in life, going after the immediate reward can be a pretty\nsolid strategy.\n\nMany people judge others for being \u201ctoo selfish,\u201d but to me, that\u2019s the way to get going. Go\non and do what you want, what you dream of, what gives you satisfaction, go after the\nrewards! You'll look selfish and greedy. But you shouldn't care.\n\nAs you keep going, you'll realize that going after the rewards isn\u2019t the best strategy, even\nfor your benefit. You start seeing a bigger picture. If you overeat candy, your tummy hurts; if\nyou spend all of your money on online shopping, you can go broke.\n\nEventually, you start looking at the returns. You start understanding that there\u2019s more to\nyour selfish and greedy motives. You drop the greedy side of you because it harms you in the\nlong run, and now you can see that. But you stay selfish, you still only think in terms of\nrewards, just now \u201ctotal\u201d rewards, returns. No shame about that, either!\n\nAt one point, you'll realize that the world moves without you, that the world has many\nmore moving parts than you initially thought, that the world has underlying dynamics that\nare difficult to comprehend. You now know that \u201cwhat goes around comes around,\u2019 one way\nor another, one day or another, but it does.\n\nYou step back once again; now instead of the going after rewards or returns, you go after\nvalue functions. You wise up! You learn that the more you help others learn, the more you\nlearn. Not sure why, but it works. The more you love your significant other, the more they love\nyou, crazy! The more you don't spend (save), the more you can spend. How strange! Notice,\nyou're still selfish!\n\nBut you become aware of the complex underlying dynamics of the world and can under-\nstand that the best for yourself is to better others\u2014a perfect win-win situation.\n\nI'd like the differences between rewards, returns, and value functions to be ingrained in\nyou, so hopefully this should get you thinking for a bit.\n\nFollow the rewards!\n\nThen, the returns!\n\nThen, the value functions.\n\nConcrete EXAMPLE\nThe random walk environment\n\nThe primary environment we'll use through this chapter is called the random walk (RW). This\nis a walk, single-row grid-world environment, with five non-terminal states. But it's peculiar,\nso | want to explain it in two ways.\n\nOn the one hand, you can think of the RW as an environment in which the probability of\ngoing left when taking the Left action is equal to the probability of going right when taking\nthe Left action, and the probability of going right when taking the Right action is equal to the\nprobability of going left when taking the Right action. In other words, the agent has no con-\ntrol of where it goes! The agent will go left with 50% and right with 50% regardless of the\naction it takes. It\u2019s a random walk, after all. Crazy!\n\nRandom walk environment MDP\n\n0.5 20.5 0.5 20.5 0.5 0.5 0.5. 0.5 0.5. 0.5\n\nSISSIES SESS\n- FOIOIOIOION -\nARKO,\n\u00a9 Transition Function is totally random! _]\n\nBut to me, that was an unsatisfactory explanation of the RW, maybe because | like the idea of\nagents controlling something. What's the point of studying RL (a framework for learning opti-\nmal control) in an environment in which there's no possible control!?\n\nTherefore, you can think of the RW as an environment with a deterministic transition func-\ntion (meaning that if the agent chooses left, the agent moves left, and it moves right if it picks\ntight\u2014as expected). But pretend the agent wants to evaluate a stochastic policy that selects\nactions uniformly at random. That's half the time it chooses left, and the other half, right.\n\nEither way, the concept is the same: we have a five non-terminal state walk in which the\nagent moves left and right uniformly at random. The goal is to estimate the expected total\ndiscounted reward the agent can obtain given these circumstances.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.3,
                        "section_name": "First-visit Monte Carlo: Improving estimates after each episode",
                        "section_path": "./screenshots-images-2/chapter_5/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_3/532319cd-801f-4a22-911e-5075f30906d8.png",
                            "./screenshots-images-2/chapter_5/section_3/d84a7e6e-6ba2-417f-9200-909c54335445.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "First-visit Monte Carlo: Improving estimates after each episode\n\nAlright! The goal is to estimate the value of a policy, that is, to learn how much total reward\nto expect from a policy. More properly, the goal is to estimate the state-value function v, (s)\nof a policy x. The most straightforward approach that comes to mind is one I already men-\ntioned: it\u2019s to run several episodes with this policy collecting hundreds of trajectories, and\nthen calculate averages for every state, just as we did in the bandit environments. This method\nof estimating value functions is called Monte Carlo prediction (MC).\n\nMC is easy to implement. The agent will first interact with the environment using policy x\nuntil the agent hits a terminal state S,. The collection of state S, action A,, reward R,,, and\nnext state S__, is called an experience tuple. A sequence of experiences is called a trajectory. The\nfirst thing you need to do is have your agent generate a trajectory.\n\nOnce you have a trajectory, you calculate the returns G, ,. for every state S, encountered. For\ninstance, for state S,, you go from time step t forward, adding up and discounting the rewards\nreceived along the way: R,,,, R,,\u00bb R,,\u00bb .-.,\u00bb R, until the end of the trajectory at time step T.\nThen, you repeat that process for state S_, adding up the discounted reward from time step\nt+] until you again reach T; then for S,,,and so on for all states except S,, which by definition\nhas a value of 0. G_,. will end up using the rewards from time step t+1, up to the end of the\nepisode at time step T. We discount those rewards with an exponentially decaying discount\nfactor: y\u2019, y', y?,..., y\u2019\". That means multiplying the corresponding discount factor y by the\nreward R, then adding up the products along the way.\n\nAfter generating a trajectory and calculating the returns for all states S, you can estimate\nthe state-value function v_(s) at the end of every episode e and final time step T by merely\naveraging the returns obtained from each state s. In other words, we\u2019re estimating an expec-\ntation with an average. As simple as that.\n\nMonte Carlo prediction\n\n[1 Mn episode 6, a trajectory $,, Ay ---\n2, S,, with areturn @, oF |\n\n1 \u00a9 The numbers are rewards.\n* Assume 0 if missing,\n\n@ The state @ The circles are non-terminal states.\nwhose value function\ncurrently estimating @ whot's a. good estimate of v,(5,)? 0.4?\n\n= SHow Me tHE Matu\n\nMonte Carlo learning\n\n\u00a9 WARNING: I\u2019m heavily abusing notation to make sure you get the whole picture. In\n\nspecific, you need to notice when each thing is calculated. For instance, when you see a\n\nsubscript \u00a2:7, that means it\u2019s derived from time step tuntil the final time step T. When you\n\nsee T, that means it\u2019s computed at the end of the episode ot the Final time step T.\n\n(@ As a reminder, the action-value\nfunction is the expectation of returns.\n\nUx (s) = E, (Gir | St = s| This is a definition good to remember.\n(2) And the returns are the total\ndiscounted reward,\n\nT-1\nGer = Ri4it+yRiet..+y Rr\n@ In mc, the First thing we do is sample the policy for a trajectory.\n\u00a9 siven thas brajectory we St, At, Ris1, Sty, oeey Rr, Sr ~ Tt:T\n\nall states encountered. }\u2014\u2014\u2014+ > Tr (St) = Tr(S:) ae Gtr\n\n(\u00a9 Then, add up the per-stote returns.\n@ And, increment a count (more on this later). tt Nr(S:) = Nr(St) qr 1\n\n@ we can estimate the expectation using the Tr(S, )\nempirical mean, so, the estimated state-value -\u2014\u2014_> Vr(S;) \u2014 Tt)\nfunction for a state is the mean return for that state. Nr(St)\n\n@) As the infinity, +\ntheetinate al approash tee nue) (8) + 00 V(s) \u2014 vn(s)\n\nGo) But, notice that means can be calculated incrementally. There\u2019s no need to keep\ntrack of the sum of returns for all states. This equation is equivalent and more efficient.\n\nVr (St) = Vr\u20141(S\u00a2) + MG) Gur - Vr\u20141(51)]\n\nGD On this one, we replace the mean For a. learning value\nthat can be time dependent or constant.\n\nMC\nerror\n\n\u2014\u2014_\u2014_\u2014_\ni Vr(St) = Vr\u20141(St) + a4 Ger Vr_1(S;)\n\n(a) Notice that V is calculated only at the end of eae\nan episode, time step T, becouse @ depends on it. MAS\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.4,
                        "section_name": "Every-visit Monte Carlo: A different way of handling state visits",
                        "section_path": "./screenshots-images-2/chapter_5/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_4/6e5a6847-5f17-4413-8f43-bfb3b3f96e33.png",
                            "./screenshots-images-2/chapter_5/section_4/98e40279-5609-4e7e-b003-4bdbb59c2b02.png",
                            "./screenshots-images-2/chapter_5/section_4/5b591175-a8c1-4fcc-8d15-b429663e321d.png",
                            "./screenshots-images-2/chapter_5/section_4/66d113a9-3bae-4fe8-9e97-fbdfd3b29864.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Every-visit Monte Carlo: A different way of handling state visits\n\nYou probably notice that in practice, there are two different ways of implementing an\naveraging-of-returns algorithm. This is because a single trajectory may contain multiple visits\nto the same state. In this case, should we calculate the returns following each of those visits\nindependently and then include all of those targets in the averages, or should we only use the\nfirst visit to each state?\n\nBoth are valid approaches, and they have similar theoretical properties. The more \u201cstan-\ndard\u201d version is first-visit MC (FVMC), and its convergence properties are easy to justify\nbecause each trajectory is an independent and identically distributed (IID) sample of v, (s), so\nas we collect infinite samples, the estimates will converge to their true values. Every-visit MC\n(EVMC) is slightly different because returns are no longer independent and identically dis-\ntributed when states are visited multiple times in the same trajectory. But, fortunately for us,\nEVMC has also been proven to converge given infinite samples.\n\n<= Bom Down\nFirst- vs. every-visit MC\n\nMC prediction estimates v (s) as the average of returns of m. FVMC uses only one return per\nstate per episode: the return following a first visit. EVMC averages the returns following all\nvisits to a state, even if in the same episode.\n\nA Br oF History\n0001\nFirst-visit Monte Carlo prediction\n\nYou've probably heard the term \u201cMonte Carlo simulations\u201d or \u201cruns\u201d before. Monte Carlo\nmethods, in general, have been around since the 1940s and are a broad class of algorithms\nthat use random sampling for estimation. They are ancient and widespread. However, it was\nin 1996 that first- and every-visit MC methods were identified in the paper \u201cReinforcement\nLearning with Replacing Eligibility Traces,\u2019 by Satinder Singh and Richard Sutton.\n\nSatinder Singh and Richard Sutton each obtained their PhD in Computer Science from the\nUniversity of Massachusetts Amherst, were advised by Professor Andy Barto, became promi-\nnent figures in RL due to their many foundational contributions, and are now Distinguished\nResearch Scientists at Google DeepMind. Rich got his PhD in 1984 and is a professor at the\nUniversity of Alberta, whereas Satinder got his PhD in 1994 and is a professor at the University\nof Michigan.\n\n| Speak PyTHoN\nExponentially decaying schedule\ndef decay schedule(init value, min value, \u00a2\u2014 @ This function allows\ndecay ratio, max_steps, you to calculate all the\nlog_start=-2, log base=10): Values for alpha for the\ndecay steps = int(max_steps * decay ratio) \u2018ull training process.\nrem_steps = max_steps - decay steps\n\n@ First, calculate the number of steps to decay the values using the decay ratio variable.\n@) Then, calculate the actual values as an inverse log curve. Notice we then normalize\nbetween 0 and |, and Finally transform the points to lay between init_value and min_value.\n\nvalues = np.logspace(log start, 0, decay steps,\nbase=log base, endpoint=True) [::-1]\nvalues = (values - values.min()) / \\\n(values.max() - values.min() )\nvalues = (init value - min_value) * values + min_ value\nvalues = np.pad(values, (0, rem_steps), \u2018'edge')\n\nreturn values\n\n| Speak PyTHON\nGenerate full trajectories\n\ndef generate trajectory(pi, env, max_steps=20):\n@ This is a straightforward function. Ht\u2019s\ndone, trajectory = False, [] Tunning a policy and extracting the collection\nwhile not done: of experience tuples (the trajectories) for\n\nstate = env.reset () off-line processing.\nfor t in count ():\n\naction = pi(state)\nnext_state, reward, done, _ = env.step (action)\nexperience = (state, action, reward,\nnext_state, done)\ntrajectory.append (experience)\nif done:\nbreak > J @ This allows you to pass a maximum\nif t >= max_steps - 1: Tumber of steps so that youcan\ntrajectory = [] truncate long trajectories if desired.\nbreak\nstate = next_state\nreturn np.array(trajectory, np.object)\n\nI Speak PYTHON\nMonte Carlo prediction 1/2\n\ndef mc_prediction(pi, 4\u00a2\u2014\u2014\u2014____, @ The me_prediction function\n\nenv, works for both First- and every-\ngamma=1.0, Visit MC. The hyperparameters\ninit_alpha=0.5, you see here are standard\nmin_alpha=0.01, Remember, the discount factor,\nalpha_decay ratio=0.3, gamma, depends on the\nn_episodes=500, environment.\n\nmax_steps=100,\nfirst_visit=True) :\n\n@) For the learning rate, alpha, I'm using a decaying value from init_alpha of 0.s down to\nmin_alpha. of 0.01, decaying within the First 30% (alpha_decay ratio of 0.) of the S00 total\nmax_episodes. We already discussed max_steps on the previous function, so I\u2019m passing the\nargument around. And first_visit toggles between FYMC and EvmC.\n\nnS = env.observation_space.n @) This is cool. 'm calculating all\n\ndiscounts = np. logspace ( possible discounts at once. This\n0, max_steps, num=max_steps, logspace function for a. gamma,\nbase=gamma, endpoint=False) oF 0.99 and a max_step of 100\n\nalphas = decay schedule ( returns a 100 number vector: Ll,\ninit_alpha, min_alpha, 0.99, 0.9801, . . ., 0.3097).\n\nalpha decay ratio, n_episodes)\n\n@) Here V'm calculating all of the alphas!\n\n() Here we're initializing variables we'll use inside the main loop: the current estimate of the\nstate-value function Vv, and a per-episode copy of Vv for offline analysis.\n\nV = np.zeros (nS)\n\nV_track = np.zeros((n_episodes, nS))\n@) we loop For every episode. Note that we're using \u2018tadm\u2019 here. This package prints a.\nprogress bar, and it\u2019s useful For impatient people like me. You may not need it (unless you're\n\nfor e in tqdm(range(n episodes), leave=False) :\n\n@ e@enerate a Full\ntrajectory = generate trajectory ( trajectory\npi, env, max_steps)\n\nvisited = np.zeros(nS, dtype=np.bool)\n\ni ed for t, (state, _, reward, _, _) in enumerate(\n\n\u00a9) This last line is repeated on the next page for your reading convenience. trajectory) :\n\n| Speak PyTHoN\nMonte Carlo prediction 2/2\nCc Go) This First line is repeated on the previous page for your reading convenience.\nfor t, (state, _, reward, _, _) in enumerate(\nGD we now loop through all experiences in the trajectory, ,_T trajectory) :\n\nCc Ga) Check if the state has already been visited on this trajectory, and doing Fvmc.\n\nif visited[state] and first_visit:\ncontinue\n\n3) And if so, we\nvisited[state] = True process the next state.\n4) If this is the First visit or we are doing EVMC, we process the current state.\n\nGs) First, calculate the number of steps from tto T-\n\n(ie) Then, n_steps = len(trajectory(t:]) ~_]\n\ncalculate G = np.sum(discounts[:n_steps] * trajectory[t:, 2])\n\nthe return. V[state] = V[state] + alphas[e] * (G - V[state])\n7 Finally, estimate the value function.\n\nV 4\u2014\u2014 08) heep track of the episode\u2019s v.\n\nreturn V.copy(), V_track \u00a2-+ (19) And return v, and the tracking when done.\n\nV_track[e] =\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.5,
                        "section_name": "Temporal-difference learning: Improving estimates\nafter each step",
                        "section_path": "./screenshots-images-2/chapter_5/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_5/f941b112-ea5a-4dbe-ba0b-8dad45115d39.png",
                            "./screenshots-images-2/chapter_5/section_5/7dc6ab89-afba-423b-8f49-f5bd925a9a77.png",
                            "./screenshots-images-2/chapter_5/section_5/b7f9f2d9-05f3-4784-9545-7f577a4d9b63.png",
                            "./screenshots-images-2/chapter_5/section_5/d197814f-d651-4761-aebc-7c76cc56cf45.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Temporal-difference learning: Improving estimates\nafter each step\n\nOne of the main drawbacks of MC is the fact that the agent has to wait until the end of an\nepisode when it can obtain the actual return G., , before it can update the state-value function\nestimate V,(S,). On the one hand, MC has pretty solid convergence properties because it\nupdates the value function estimate VAS) toward the actual return Go which is an unbiased\nestimate of the true state-value function v, (s).\n\nHowever, while the actual returns are pretty accurate estimates, they are also not very pre-\ncise. Actual returns are also high-variance estimates of the true state-value function v_(s). It\u2019s\neasy to see why: actual returns accumulate many random events in the same trajectory; all\nactions, all next states, all rewards are random events. The actual return G,_, collects and com-\npounds all of that randomness for multiple time steps, from ft to T. Again, the actual return\nG,,,is unbiased, but high variance.\n\n\u2018Also, due to the high variance of the actual returns G,,, MC can be sample inefficient. All\nof that randomness becomes noise that can only be alleviated with data, lots of data, lots of\ntrajectories, and actual return samples. One way to diminish the issues of high variance is to,\ninstead of using the actual return G.,,, estimate a return. Stop for a second and think about it\nbefore proceeding: your agent is already calculating the state-value function estimate V(s) of\nthe true state-value function v, (s). How can you use those estimates to estimate a return, even\nif just partially estimated? Think!\n\nYes! You can use a single-step reward R,, ,, and once you observe the next state S,,, you can\nuse the state-value function estimates vis, p as an estimate of the return at the next step\n\nG,, ,.\u00bb- This is the relationship in the equations that temporal-difference (TD) methods exploit.\nThese methods, unlike MC, can learn from incomplete episodes by using the one-step actual\nreturn, which is the immediate reward R,,,, but then an estimate of the return from the next\nstate onwards, which is the state-value function estimate of the next state V(S,__): that is, R\n+ yV(S,,,), which is called the TD target.\n\nt+\n\n<= Bonn Down\nTemporal-difference learning and bootstrapping\n\nTD methods estimate v _(s) using an estimate of v_(s). It bootstraps and makes a guess from a\nguess; it uses an estimated return instead of the actual return. More concretely, it uses R,,, +\nyV(S,_,) to calculate and estimate V,, (5,).\n\nBecause it also uses one step of the actual return R,_,, things work out fine. That reward\nsignal R,,, progressively \u201cinjects reality\u201d into the estimates.\n\n= SHow Me tHe Matu\nTemporal-difference learning equations\n\n\u00a9 we again stort from the definition oF the Un (s) = E, (Ger | St = s|\n\nL_\u00a7_., Gyr = Rit yRa2t-.t+y7 Rr\n\n@ From the return, we can rewrite the equation by grouping up some terms. Check it out.\nGur = Rigi +yRi2t+YVRast..+ 77 \u2018Rr\n= Riyi + 7(Riy2 + VRig3 + +7\" 7 Rr)\n\n= Riya + Y\u00a5G't4.1:T 4 @ now, the same return has a recursive style.\n() We can use this new definition to also rewrite the state-value function definition equation.\n\nfr\n=Ex(Reti + 7Gtti:r | St= 8] Sawemecae\nvalue function of the\n\nCc - E, (Resi P Yn (St41) | Se = s| next state, we get this.\na J\n\n@ This means we could estimate the\n\nstate-volue function on everytime step, = S', Ay, Re+1, Stipa ~ Met+1\n\n@ we roll out a single interaction step... ~_\u2014\u2014__________t\n\n@) ... and can obtain an estimate Gio) The key difference to realize is we're now\n\nV(s) of the true state-value function \u2014_ estimating v,(s,) with an estimate of v.(s, ,).\nv{s) a different way than with mc. We're using an estimated, not actual, return.\n\nTD\nerror\n\nOO\nVi41(S\u00a2) = ViCS\u00a2) + | Rega + Vi (Si41) \u2014Vi(S\u00a2)\nee\n\nTD\ntarget\n\nGD A big win is we can now make updates to the state-\nvalue function estimates v(s) every time step.\n\n| Speak PytHoN\nThe temporal-difference learning algorithm\n\ndef td(pi,\n\n+\u2014\u2014\u2014F 0 tis a prediction method. It takes in a\nenv,\n\ngamma=1.0 policy pi, an environment env to interact\no \u2014 with, and the discount factor gamma.\ninit_alpha=0.5,\n\nmin_alpha=0.01, 4\u2122\u2014\u2014 @ The learning method has a\n\nalpha_decay ratio=0.3, configurable hyperparameter alpha,\nn_episodes=500) : Which is the learning rate.\n\n@) One of the many ways of handling the learning rate is to exponentially decay it. The\ninitial value is init_alpha, min_alpha, the minimum value, and alpha_decay ratio is the\nFraction of episodes that it will take to decay alpha. from init_alpha to min_alpha.\n\nnS = env.observation_space.n (4) we initialize the variables needed.\n\nV = np.zeros (nS)\n\nV_track = np.zeros((n_episodes, nS))\n\nalphas = decay schedule ( \u2014\u2014 \u00a9 And we caleulate the\ninit_alpha, min_alpha, learning rate schedule\nalpha_decay ratio, n_episodes) for all episodes .. .\n\nfor e in tqdm(range(n episodes), leave=False) :\n@ we get the initial state and then enter the interaction loop.\n\nstate, done = env.reset(), False\n\nwhile not done:\n\n[1 @ First thing is to sample the policy pi\naction = pi(state) for the action to take in state.\n\n@) we then use the action to interact with the environment... We roll out the policy one step.\nTL, next _state, reward, done, _ = env.step(action)\nG0) We can immediately calculate a target to update the state-value function estimates . ..\n\ntd_target = reward + gamma * V[next_state] * \\\n(not done)\nGD... and with the target, an error.\nTH, td_error = td_target - V[state] Ga) Finally update v(s) 7\n\nV[state] V[state] + alphas[e] * td_error\n\n[oranace D next state tO) don't forget to update the state\nstate) = next=state Variable for the next iteration. 6ugs\n\nlike this can be hard to find!\n\nV_track[e] = V\nreturn V, V_track \u00a2~ (4) Andreturm the v function and the tracking variable.\n\nR b Wit an RL Accent\nTrue vs. actual vs. estimated\n\nTrue value function: Refers to the exact and perfectly accurate value function, as if given by\nan oracle. The true value function is the value function agents estimate through samples. If\nwe had the true value function, we could easily estimate returns.\n\nActual return: Refers to the experienced return, as opposed to an estimated return. Agents\ncan only experience actual returns, but they can use estimated value functions to estimate\nreturns. Actual return refers to the full experienced return.\n\nEstimated value function or estimated return: Refers to the rough calculation of the true\nvalue function or actual return. \u201cEstimated\u201d means an approximation, a guess. True value func-\ntions let you estimate returns, and estimated value functions add bias to those estimates.\n\nNow, to be clear, the TD target is a biased estimate of the true state-value function v,(s),\nbecause we use an estimate of the state-value function to calculate an estimate of the state-\nvalue function. Yeah, weird, I know. This way of updating an estimate with an estimate is\nreferred to as bootstrapping, and it\u2019s much like what the dynamic programming methods we\nlearned about in chapter 3 do. The thing is, though, DP methods bootstrap on the one-step\nexpectation while TD methods bootstrap on a sample of the one-step expectation. That word\nsample makes a whole lot of a difference.\n\nOn the good side, while the new estimated return, the TD target, is a biased estimate of the\ntrue state-value function v,(s), it also has a much lower variance than the actual return G,,.we\nuse in Monte Carlo updates. This is because the TD target depends only on a single action, a\nsingle transition, and a single reward, so there\u2019s much less randomness being accumulated. As\na consequence, TD methods usually learn much faster than MC methods.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.6,
                        "section_name": "A Bit oF History\n\nTemporal-difference learning",
                        "section_path": "./screenshots-images-2/chapter_5/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_6/b7eb4870-433e-43e7-be30-fb89224a1cbc.png",
                            "./screenshots-images-2/chapter_5/section_6/fdd85753-6399-4f99-81c3-3d8ddd96f39d.png",
                            "./screenshots-images-2/chapter_5/section_6/caec9284-8563-4a57-b7c8-7c8e772d410d.png",
                            "./screenshots-images-2/chapter_5/section_6/8afb4d2e-54ee-4de0-bc73-14b254ab186a.png",
                            "./screenshots-images-2/chapter_5/section_6/b6cb79c4-d97d-4104-be4d-eabb0743b24a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A Bit oF History\n0001\nTemporal-difference learning\n\nIn 1988, Richard Sutton released a paper titled \u201cLearning to Predict by the Methods of\nTemporal Differences\u201d in which he introduced the TD learning method. The RW environment\nwe're using in this chapter was also first presented in this paper. The critical contribution of\nthis paper was the realization that while methods such as MC calculate errors using the dif-\nferences between predicted and actual returns, TD was able to use the difference between\ntemporally successive predictions, thus, the name temporal-difference learning.\n\nTD learning is the precursor of methods such as SARSA, Q-learning, double Q-learning,\ndeep Q-networks (DQN), double deep Q-networks (DDQN), and more. We'll learn about these\nmethods in this book.\n\nTD prediction\n@ This is all we need to estimate\n\n\u2018ere Sure Thats the Key JO Pr episode 6 a trajectory Sy Ay ++ Ry\nsg v \u201c5 S,, with a return q, oF |\n\n+1\n\nare actions.\n\n(a The state \u00a9@ The circles are non-terminal states.\nwhose value Function\n\nwe're currently estimoacit\n4 8 (\u00ae) what's a. good estimate of v,(s,)? Still 0.4?\n\nIt\u2019s In THE DETAILS\nFVMC, EVMC, and TD on the RW environment\n\n| ran these three policy evaluation algorithms on the RW environment. All methods evalu-\nated an all-left policy. Now, remember, the dynamics of the environment make it such that\nany action, Left or Right, has a uniform probability of transition (50% Left and 50% Right). In\nthis case, the policy being evaluated is irrelevant.\n\n| used the same schedule for the learning rate, alpha, in all algorithms: alpha starts at 0.5,\nand it decreases exponentially to 0.01 in 250 episodes out of the 500 total episodes. That's\n50% of the total number of episodes. This hyperparameter is essential. Often, alpha is a\npositive constant less than 1. Having a constant alpha helps with learning in non-stationary\nenvironments.\n\nHowever, | chose to decay Exponentially decaying schedule (for alpha)\nalpha to show convergence. \u00b0\u00b0\nThe way I'm decaying alpha\nhelps the algorithms get close\nto converging, but because ,,\nI'm not decreasing alpha all\nthe way to zero, they don\u2019t o2\nfully converge. Other than\nthat, these results should help \u2122\nyou gain some intuition about\n\n00\n\nthe differences between these \u201c eS $ \u00a3 S $\nmethods.\n\n04\n\nTatty it Up\n\nMC and TD both nearly converge to the true state-value function\n@ Here Pll show only First-visit monte Carlo prediction (FvmC) and temporal-difference\nlearning (TO). \\F you head to the Notebook for this chapter, you'll also see the results for\nevery-visit Monte Carlo prediction, and several additional plots that may be of interest to you!\n\nJ AC FVMC estimates through time vs. true values\n\n2\ne\n\no4\n\nState-value function\n\n\u00b0\ni]\n\n\u00b0 100 200 300 400 500\nEpisodes\n(@) Take a. close look at these plots. These are the running state-value function\nestimates V(s) of an all-le# policy in the random-walk environment. As you can\nsee in these plots, both algorithms show near-convergence to the true values.\n@) Now, see the difference trends of these algorithms. FYMC running estimates\n\nTD estimates through time vs. true values\n\n\u2014 V5)\n\u2014- wa)\nsveee v3)\n--- v2)\n\n5\nS\n2\n2\ne\n=!\n$\n2 \u2014 vw)\n\u00a3\n\na\n\nEpisodes\n\n@) TD running estimates don't jump as much, but they are off-center For most of the\nepisodes. For instance v(S) is usually higher than (5), while V(/) is usually lower than\nVO). eat if usu camnoare tace Valuec with Evms. ectmate<c. usu notice o different trend,\n\nTatty it Up\n\nMC estimates are noisy; TD estimates are off-target\n\nMC\nerror\n\n=~\nVr (St) = Vr-1(St) + at | Gtr \u2014Vr\u2014-1(St)\n\n@ If we get a close-up (log-scale\nplot) of these trends, you'll see\nJjump around the true values. This is\n\ntole because of the high variance of the\nmC targets.\n@a couple of - FVMC estimates through time vs. true values (log scale)\npros though; First,\nyou can see all\nestimates get 5\nclose to their true \u00a7 06 =e\nvalues g ~ -\n2 \u2014 wip\nAlso, the 8\nestimates jump\naround the true\nvalues.\nTD @) TO estimates are of$-target most\nerror of the time, but they're less jumpy.\n\u2014\u2014\u2014 _\nV; S.) = V,(S, a; | Ge. _-V,(S This is because TD targets are low\nt+1( t) e( t) + \u00a2 Seer, e( t) Variance, though biased. They use an\nTD estimated return for target.\ntarget ]\n@) The bias TD estimates through time vs. true values (log scale)\nshows, too. In the ALLS ny pre nts)\nend, TD targets os \u2014 AWA cl\nQiveupaccuracy 5 LON POT NNO teem VTHA)\nin order to Eee j\u2014 = Me\nbecome more g = ra - 7 ys a va)\necise. Also, thet gos / es 2 ~~ va)\neke a. bit long \u00a7 7 ad TR a\nbefore estimates o2 = 7 va)\n\nramp up, at\nleast in this\nenvironment.\n\n\nTatty it Up\nMC targets high variance; TD targets bias\n\ni @ Here we can see the bias/variance\n\ntrade-off between MC and TD targets.\n\u2014 T-1 .\nGer = Rigi + yRepgt.. +7 Rr Remember, the mc target is the return,\nwhich accumulates a lot of random noise.\n\nThat means high variance targets.\n@ These plots are FVMC target sequence\nshowing the targets For Se a ee\nthe initial state in the RW\nenvironment. MC targets, \u201c\nthe returns, are eitherO F, 3,\n4 (3)\n\u00a3\n\u00b0 100 200 300 400 S00\nEstimate sequence number\n\n@) To targets are calculated using an estimated\nreturn. We use the value function to predict how much\nValue we'll get from the next state onward. This helps\n\u2014 us truncate the calculations and get more estimates\nGrey = Rea + -WVi(St+1) er code (as you can see on te x oe we have\n~IL00 estimates in SOO episodes), but because we use\nV{S,,,), which is an estimate and therefore likely\nwrong, TD targets are biased.\n\n() Here you can see the TD target sequence\nrange of the TD targetsis =, 3\nmuch lower, MC alternates 4\nexactly between | and 0, oe SO en\nand TD jumps between F 2\n\napproximatelyo.1and\"03, 2) 5 2 a.\n\u201cnext state\u201d is sampled, on | SOUIE\n\nut as the V(s,,) is an \u2018\n\nestimad is biased, .\n\nFe Fi oo =\n\noff-target, and \u00b0 200 400 600 800 1000 = 1200 14001600\ninaccurate. Estimate sequence number\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.7,
                        "section_name": "Learning to estimate from multiple steps",
                        "section_path": "./screenshots-images-2/chapter_5/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_7/dff650bc-1b78-4aec-9217-193125d6c572.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning to estimate from multiple steps\n\nIn this chapter, we looked at the two central algorithms for estimating value functions of a\ngiven policy through interaction. In MC methods, we sample the environment all the way\nthrough the end of the episode before we\n\nestimate the value function. These methods What's in the middle?\nspread the actual return, the discounted total\n\nreward, on all states. For instance, if the dis- mc TD\ncount factor is less than 1 and the return is s,\n\nonly 0 or 1, as is the case in the RW environ- A, Os there anything\n\nment, the MC target will always be either 0 or es in between?\n\n1 for every single state. The same signal gets\u201d ~*\u201d \u2014_\u2014\u2014____\u2014\u2014\u2014\u2014 >;\npushed back all the way to the beginning of By\n\nthe trajectory. This is obviously not the case \u201d\n\nfor environments with a different discount A, l\n\nfactor or reward function. 25S,\n\nOn the other hand, in TD learning, the\nagent interacts with the environment only\nonce, and it estimates the expected return to go to, then estimates the target, and then the\nvalue function. TD methods bootstrap: they form a guess from a guess. What that means is\nthat, instead of waiting until the end of an episode to get the actual return like MC methods\ndo, TD methods use a single-step reward but then an estimate of the expected return-to-go,\nwhich is the value function of the next state.\n\nBut, is there something in between? I mean, that\u2019s fine that TD bootstraps after one step,\nbut how about after two steps? Three? Four? How many steps should we wait before we esti-\nmate the expected return and bootstrap on the value function?\n\nAs it turns out, there\u2019s a spectrum of algorithms lying in between MC and TD. In this sec-\ntion, we'll look at what\u2019s in the middle. You'll see that we can tune how much bootstrapping\nour targets depend on, letting us balance bias and variance.\n\nEe Micuet\u2019s ANALOGY\n\nMC and TD have distinct personalities\n\n| like to think of MC-style algorithms as type-A personality agents and TD-style algorithms\nas type-B personality agents. If you look it up you'll see what | mean. Type-A people are\noutcome-driven, time-conscious, and businesslike, while type-B are easygoing, reflective,\nand hippie-like. The fact that MC uses actual returns and TD uses predicted returns should\nmake you wonder if there is a personality to each of these target types. Think about it for a\nwhile; I'm sure you'll be able to notice several interesting patterns to help you remember.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.8,
                        "section_name": "N-step TD learning: Improving estimates after a couple of steps",
                        "section_path": "./screenshots-images-2/chapter_5/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_8/933d4e55-0104-4efc-a72b-169c699eece9.png",
                            "./screenshots-images-2/chapter_5/section_8/d42c6b02-d03d-4678-b5a4-1af2fc8b9028.png",
                            "./screenshots-images-2/chapter_5/section_8/5eedf851-7e8f-4703-8f6d-827d2da53c3a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "N-step TD learning: Improving estimates after a couple of steps\n\nThe motivation should be clear; we have two extremes, Monte Carlo methods and temporal-\ndifference methods. One can perform better than the other, depending on the circumstances.\nMC is an infinite-step method because it goes all the way until the end of the episode.\n\nI know, \u201cinfinite\u201d may sound confusing, but recall in chapter 2, we defined a terminal state\nas a state with all actions and all transitions coming from those actions looping back to that\nsame state with no reward. This way, you can think of an agent \u201cgetting stuck\u201d in this loop\nforever and therefore doing an infinite number of steps without accumulating a reward or\nupdating the state-value function.\n\nTD, on the other hand, is a one-step method because it interacts with the environment for\na single step before bootstrapping and updating the state-value function. You can generalize\nthese two methods into an n-step method. Instead of doing a single step, like TD, or the full\nepisode like MC, why not use n-steps to calculate value functions and abstract n out? This\nmethod is called n-step TD, which does an n-step bootstrapping. Interestingly, an intermedi-\nate n value often performs the better than either extreme. You see, you shouldn\u2019t become an\nextremist!\n\n= SHow Me tHe MatH\nN-step temporal-difference equations\n\n__ St, At, Ris1, St41, wey Rens Ston \u2122~ Te:t+n\n\n@ Notice how in n-step TD we must wait n steps before we can update V(s).\n@) Now, n doesn\u2019t have to be \u00a9 like in MC, or | like in TD. Here you get to pick. In reality 7 will be\nNor less if your agent reaches a terminal state. It could be less than n, but never more.\n\nGttin = Rigi t.. + \"1 Resin +9\" Vi+n\u20141(Stin)\n\n@) Here you see how the value-\nfunction estimate gets updated n-step\napproximately every n steps. error\n\u2014eoao\u2014.\nVitn(St) = Vign\u20141(St) + O\u00a2| Gertn \u2014Vien\u20141(5S1)\nae\n) but after that, you can n-step\n\nplug in that target as usual. -\u2014\u2014\u2014\u2014\u2014>_ target\n\nI Speak PYTHON\n\nN-step TD 1/2\n\ndef ntd(pi, \u20141L__,\nenv, @ Here\u2019s my implementation of the\ngamma=1.0, nrstep TD algorithm. There are many\ninit_alpha=0.5, you can code this up; this is one oF\nmin_alpha=0.01, them for your reference.\nalpha_decay ratio=0.5,\nn_step=3,\n\nn_episodes=500) :\n\n@) Here we're using the same hyperparameters as before. Notice n_step is a default of 3.\nThat is three steps and then bootstrap, or less if we hit a terminal state, in which case we\ndon't bootstrap (again, the value of a terminal state is zero by definition.)\n\nnS = env.observation_space.n\nad +\u2014\u2014HF \u00a9 tere we have\n\nV = np.zeros (nS)\nthe usual suspects.\nV_track = np.zeros((n_episodes, nS))\n\nalphas = decay_schedule( +\u2014  @ Calculate all alphas in advance.\ninit_alpha, min_alpha,\nalpha_decay ratio, n_episodes)\n\nS) Now, here\u2019s a hybrid between MC and TD. Notice we calculate the discount\nFactors, but instead of going to max_steps like in my MC implementation, we go\n\nto n_step + | to include n steps and the bootstrapping estimate. 1]\n\ndiscounts = np. logspace (\n\n0, n_step+1, num=n_step+1, base=gamma, endpoint=False)\n\n@) We get into the episodes loop.\n\nfor e in tqdm(range(n episodes), leave=False) :\n@ This path variable will hold the n_step-most-recent experiences. A partial trajectory.\n\nstate, done, path = env.reset(), False, []\n\u00ae We're going until we hit done and the path is set to none. You'll see soon.\n\nwhile not done or path is not None:\npath = path[1:] \u00a2\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 @) tere, we're \u201cpopping\u201d the\nfirst element of the path.\n\nC_ while not done and len(path) < n_step:\nGo) This line repeats on the next page.\n\n1 Speak PYTHON\nN-step TD 2/2\n\n[7100 Same dust For you to Follow the inclentasion.\nwhile not done and len(path) < n_step:\n\nGa) This is the action = pi(state)\n\ninteraction block. \u2014\u2014> next_state, reward, done,\n\nwe're basically\n\nexperience = (state,\n\n_ = env.step (action)\nreward, next_state, done)\n\ncollecting experiences path.append (experience)\n\nuntil we hit done or state\nthe length of the path if done:\nis equal to n_step. break\n\nnext_state\n\nn = len(path)\nest_state = path[0] [0]\n\nvl\n\n3) nhere could be \u2018n_step\u2019 but it could\nalso be a smaller number if a terminal\nstate is in the \u2018path.\u2019\n\n(4) Here we're extracting the state\nwe're estimating, which isn't state.\n\nGis) rewards is a vector of all rewards encountered from the est_state until n.\n\nT\u2014\u00bb rewards = np.array (path) [\n\noF ab)|\n\n(i) partial_return is a vector of discounted rewards from est_state to n.\n\npartial return = discount\n\ns[:n] * rewards\n\nGD bs_val is the bootstrapping value. Notice that in this case next state is correct.\n\nbs_val = discounts [-1]\n\n* V[next_state]\n\n* (not done)\n\nG8) ntd_target is the sum of the partial return and bootstrapping value.\n\nntd_target\n\n9) This is the error, like we've been calculating\n\nntd_error\n\n(20) The update to the state-value Function\n\nVi[est_state] = V[est_state] + alphas[e]\n\nnp.sum(np.append (partial return,\n\nbs_val))\nall along,\n\nntd_target - V[est_state]\n\n* ntd_error\n\nGi) Here we set path to None to break out of the episode loop, if path has only one\nexperience and the done flag of that experience is True (only a terminal state in path.)\n\nif len(path)\n\npath\n\nV_track[e]\nreturn V, V_track\n\nVv\n\nNone\n\n== 1 and path[0] [3]:\n\n\u2014\\\u2014\u2014 (2a we return v and v_track as usual.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.9,
                        "section_name": "Forward-view TD(A): Improving estimates of all visited states",
                        "section_path": "./screenshots-images-2/chapter_5/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_9/55332cdb-d4e6-40ac-8c56-09719d62afe6.png",
                            "./screenshots-images-2/chapter_5/section_9/d7e85f30-4878-4608-b633-7bc26c2cac50.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Forward-view TD(A): Improving estimates of all visited states\n\nBut, a question emerges: what is a good n value, then? When should you use a one-step, two-\nstep, three-step, or anything else? I already gave practical advice that values of n higher than\noneare usually better, but we shouldn\u2019t go all the way out to actual returns either. Bootstrapping\nhelps, but its bias is a challenge.\n\nHow about using a weighted combination of all n-step targets as a single target? I mean,\nour agent could go out and calculate the n-step targets corresponding to the one-, two-,\nthree-, . . ., infinite-step target, then mix all of these targets with an exponentially decaying\nfactor. Gotta have it!\n\nThis is what a method called forward-view TD(A) does. Forward-view TD(A) is a predic-\ntion method that combines multiple n-steps into a single update. In this particular version,\nthe agent will have to wait until the end of an episode before it can update the state-value\nfunction estimates. However, another method, called, backward-view TD(A), can split the\ncorresponding updates into partial updates and apply those partial updates to the state-value\nfunction estimates on every step, like leaving a trail of TD updates along a trajectory. Pretty\ncool, right? Let\u2019s take a deeper look.\n\nGeneralized bootstrapping\n\nver (1-A)Aw! (1-)X (1-A)A 1-A\nMc TD\nco-step n-step ~ 3-step 2-step 1-step\nbootstrapping bootstrapping bootstrapping bootstrapping bootstrapping\ns,\nA,\n\u00a9 First of\nall, what a Rd Se\nbeautiful\nalgorithmic \u2019 @...uhile\napproach setting lambda.\nto unity Peet Soa to 2ero will only\nme and a activate TD, and\nTo! if - cs every other\ncee RS weight be zero.\nlambda. to a\nthe Fes\nsale ative ~ T___, @ and. value in between wil\nwa Int will Ar Give a. weighted combination\nFn S, Beautiful, right?!\n\n= SHow Me tHE Matu\nForward-view TD(A)\n\n\u00a9 Sure, this is 0 loaded equation; we'll unpack it here. The\nbottom line is that we're using all n-step returns until the final\n\nstep 7, and weighting it with an exponentially decaying value.\n\n+ @ The thing is, because\n\nT-t-1 de aloe ee\n\nGhp =(1=A) SOA\" Gtgn + ATE Geer weight the actual return\nnel \u2014\\ with 0. normalizing value\n\nWeighted <o that all weights add.\n\nSum of weighted returns final return (T)\n\nfrom 1-step to T-1 steps\n@) Ali this equation is saying is that we'll\n| calculate the one-step return and weight\nGetgi = Reza + Ve(St41) it with the Following factor ... r>1-X\n\n(4)... and also the two-step return and weight it with this Factor. -\n\nup tol.\n\nGite = Regi + Rego + Visi (St42) (1\u2014A)A\n(S) Then the same for the three-step return and this factor. -\u2014q,\nGit+s = Riga + VRege + Regs + PVi42(St43) (1 \u2014 A)?\n\nFT \u00a9 You. do this For all n-steps . ..\nGirton = Riga +--+ 7\" Rein + Y\"Vien\u20141(Stin) \u2014 (1-A)A\"\u2122?\n$1O . .. until your agent reaches a. terminal state. Then you weight by this normalizing Factor.\nGur = Riri tyRioat-.+77 ' Rr \\T-t-1\n\n@ Notice the issue with this approach is that you must sample\nan entire trajectory before you can calculate these values.\n\nSt, At, Rei, St4i,- Rr, Sr ~ Ter\n\n(9) Were you have it, v will\nbecome available at time 7... A-error\n\u2014\u2014\u2014_.\n7\nSL\n\nGo)... because of this.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.1,
                        "section_name": "TD(A): Improving estimates of all visited states after each step",
                        "section_path": "./screenshots-images-2/chapter_5/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_10/ac10b359-7b7b-431a-aca1-eb24ed941169.png",
                            "./screenshots-images-2/chapter_5/section_10/54b5743d-d11a-448f-8573-cebe675ad489.png",
                            "./screenshots-images-2/chapter_5/section_10/3a71bbe9-6bdf-40b1-a613-c192b08f673e.png",
                            "./screenshots-images-2/chapter_5/section_10/a88bc280-a6e4-4759-aa08-88f50e9f981b.png",
                            "./screenshots-images-2/chapter_5/section_10/6c40fad2-5869-4f51-9133-ca6b9933d84f.png",
                            "./screenshots-images-2/chapter_5/section_10/05665ecc-b098-4e46-97a9-04ef2660df04.png",
                            "./screenshots-images-2/chapter_5/section_10/510f4fd9-9671-4c74-8254-5dd8d8125d19.png",
                            "./screenshots-images-2/chapter_5/section_10/5ca3c199-ab75-480c-8d19-6dfdb959a5b2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "TD(A): Improving estimates of all visited states after each step\n\nMC methods are under \u201cthe curse of the time step\u201d because they can only apply updates to\nthe state-value function estimates after reaching a terminal state. With n-step bootstrapping,\nyou're still under \u201cthe curse of the time step\u201d because you still have to wait until n interac-\ntions with the environment have passed before you can make an update to the state-value\nfunction estimates. You're basically playing catch-up with an n-step delay. For instance, in a\nfive-step bootstrapping method, you'll have to wait until you've seen five (or fewer when\nreaching a terminal state) states, and five rewards before you can make any calculations, a\nlittle bit like MC methods.\n\nWith forward-view TD(A), we\u2019re back at MC in terms of the time step; the forward-view\nTD(A) must also wait until the end of an episode before it can apply the corresponding\nupdate to the state-value function estimates. But at least we gain something: we can get lower-\nvariance targets if we\u2019re willing to accept bias.\n\nIn addition to generalizing and unifying MC and TD methods, backward-view TD(A), or\nTD(A) for short, can still tune the bias/variance trade-off in addition to the ability to apply\nupdates on every time step, just like TD.\n\nThe mechanism that provides TD(A) this advantage is known as eligibility traces. An eligi-\nbility trace is a memory vector that keeps track of recently visited states. The basic idea is to\ntrack the states that are eligible for an update on every step. We keep track, not only of whether\na state is eligible or not, but also by how much, so that the corresponding update is applied\ncorrectly to eligible states.\n\nEligibility traces for a four-state environment during an eight-step episode\n@ The states visited during the episode are |, a, |, 0, 3, 3, 3, 0, 3. H\u2014\u2014 .\noo '/ ~\n\n@ The values\nare the\neligibility trace\ncoefficient.\n\n@) On the y-axis p77 t eee\nare the states. 2 : ~Jr>--------|-~-- ----\n\nOn the x-axis wR\n\nare the time steps, } } } } - 1 ~>4 = 1\nL_,> 1 2 3 4 5 6 7\n(S) For example, at time step 4, the highest credit is G0 me each tone step, we ook ob the\ngiven to state 3, a little less credit is given to state 0, eligibility of all states and apply avalue-\nthen state |, and Finally state a. function update accordingly,\n\nFor example, all eligibility traces are initialized to zero, and when you encounter a state, you\nadd a one to its trace. Each time step, you calculate an update to the value function for all\nstates and multiply it by the eligibility trace vector. This way, only eligible states will get\nupdated. After the update, the eligibility trace vector is decayed by the A (weight mix-in fac-\ntor) and y (discount factor), so that future reinforcing events have less impact on earlier\nstates. By doing this, the most recent states get more significant credit for a reward encoun-\ntered in a recent transition than those states visited earlier in the episode, given that A isn\u2019t set\nto one; otherwise, this is similar to an MC update, which gives equal credit (assuming no\ndiscounting) to all states visited during the episode.\n\n& SHow Me tHE Matu\nBackward-view TD(A) \u2014 TD(A) with eligibility traces, \u201cthe\u201d TD(A)\n\nevery new episode we set the eligibility vector to. 0. -\u2014> Ey = 0)\n@) Then, we interact with the environment one cycle. +> St As, Rei i, Seer ~ Tet+1\n@ when you encounter a state S,, make it eligible tor\nan update. Technically, you increment its eligibility byl. -\u2014> EL, (S,) = E,(S;) + 1\n(4) we then calculate the TD error just as we've done\nso For, -}\u2014$\nOEP (St) = Reta + WVi(St41) \u2014Vi(S;\n\u00a9 unlike betore, we he e:t41 (St) = Rega +(St+1) \u2014Ve(S+)\nestimated state-value function VY that is, x\nthe entire function at once, every time Tp.\nstep Notice tm not using a 1(5,) bata Visi = Ve + ct 64.44 (St) Ey\nV, instead, Because we're multiplying by \u2014\u2014\nthe eligibility vector, alll eligible states will ae.\nget the corresponding credit. \u00a9 Finally, we decay the eligibility +\u00bb E41 = EyyA\n\nA final thing I wanted to reiterate is that TD(A) when A=0 is equivalent to the TD method we\nlearned about before. For this reason, TD is often referred to as TD(0); on the other hand,\nTD(A), when A=1 is equivalent to MC, well kind of. In reality, it\u2019s equal to MC assuming\noffline updates, assuming the updates are accumulated and applied at the end of the episode.\nWith online updates, the estimated state-value function changes likely every step, and there-\nfore the bootstrapping estimates vary, changing, in turn, the progression of estimates. Still,\nTD(1) is commonly assumed equal to MC. Moreover, a recent method, called true online\nTD(A), is a different implementation of TD(A) that achieves perfect equivalence of TD(0)\nwith TD and TD(1) with MC.\n\nI Speak PYTHON\nThe TD(A) algorithm, a.k.a. backward-view TD(A)\n\ndef td_lambda (pi, \u2014\u2014L_,\nenv, @ The method td_lambda has a.\n\ngamma=1.0, signature very similar to all other\ninit_alpha=0.5, methods. The only new\nmin_alpha=0.01, hyperparameter is lambda_ (the\nalpha_decay ratio=0.3, underscore is because lambda is a\nlambda _=0.3, restricted keyword in Python).\n\nn_episodes=500):\n@ Set the usual suspects.\nnS = env.observation_space.n\nV = np. zeros (nS)\nV_track = np.zeros((n_episodes, nS) )\nE = np.zeros (nS) \u2014\u2014F @) Add. a. new guy: the eligibility trace vector.\n\nalphas = decay schedule ( rr\ninit_alpha, min_alpha, (@) Calculate alpha.\n\nalpha_decay ratio, n_episodes) for all episodes.\n) Here we enter the episode loop.\nTpfor e in tqdm(range(n episodes), leave=False):\nE.fi11(0)\n\nt___, W sete to zero every new episode.\n\nstate, done = env.reset(), False \u20ac\u2014\u2014\u2014\u2014\u2014\u2014-+4() Set initial variables.\n\nwhile not done: \u2014\u2014\u2014\u2014 \u00a9 \u20acet into the time step loop.\naction = pi(state)\n[> next_state, reward, done, _ = env.step(action)\n@) We first interact with the environment for one step and get the experience tuple.\nGo) Then, we use that experience to calculate the TD error as usual.\ntd_target = reward + gamma * V[next_state] * \\\n\n(not done)\ntd_error = td_target - V[state] .\n_ - S$ Wo we inerement the\nE[state] = E[state] + 1 i ill aaati\n\nV = V+ alphas[e] * td_error * E 44a) And apply the error\n\nE = gamma * lambda * E update to all eligible\nstates as indicated by \u20ac.\nstate = next_state (2) We decay &...\n\nV_track[e] = V een ny (ia ee onelleortirune Gur\nreturn V, V_track lives as usual\n\nTatty it Up\n\nRunning estimates that n-step TD and TD(A) produce in the RW environment\n\n@ | think the most interesting \u201c\npart of the differences and\nsimilarities of mC, TD, n-step\nTD, and TOMambda) can be\nVisualized side by side. For this,\nVhighly recommend you head to\nthe book repository and check\nout the corresponding Notebook\nfor this chapter. You'll Find\nmuch more than what ve\nshown you in the text.\n\n\u2018State-value function\n\n@) @ut For now | can highlight\nthat n-step TD curves are a\nbit more like MC: and\n\n\u2018State-value function\n\ncentered, while TOambda) is r\u2014ali\n\na. bit more like TD: smooth and\n\n) When we look at the log-\nscale plots, we can see how the\nhigh variance estimates of\nn-step TD (at least higher than\nTOMambda) in this experimend,\nand how the running estimates\nmove above and below the true\nvalues, though they're centered,\n\n@ ToWambda) values aren't\ncentered, but are also much =\nsmoother than MC. These two =\nare interesting properties. Go g\ncompare them with the rest oF i\nthe methods you've learned\nabout so Far!\n\nn-step TD estimates through time vs, true values\n\nwalS)\n\nAY he ace aoe\nNf nnn se YIM)\n\n\u2014 1S\n\u2014- 10\nvais) a)\n---\n\u2014w\n\n0 20\nEpisodes\n\n\u2014 ws\n\u2014- 10\n\nrl)\n--- wa\n\u2014w\n\nPN eens WLS)\nhy Fiat\n\n1! aa?\n\nEpisodes\nTD(A) estimates through time vs. true values (log scale)\n\nWw\nal pt\u201d WS\n- Taal Wa AO ae SF\nA ln,\n\nee i152\n\nA v we)\na x -- Mo\n\nrae we\n--- 4a\na)\n\n10\nEpisodes\n\nConcrete Example\nEvaluating the optimal policy of the Russell and Norvig\u2019s Gridworld environment\n\nLet's run all algorithms in a slightly different environment. The environment is one you've\nprobably come across multiple times in the past. It is from Russell and Norvig\u2019s book on Al.\n\nRussell and Norvig\u2019s Gridworld\n\n\u00a9 The goal state gives a +1\nto the agent and the\n\nLE\n\n@aAwall. (4) Abad terminal state agent\na.-l and the episode\n@ Initial state is down here. terminates when landing here.\n\nThis environment, which | will call Russell and Norvig's Gridworld (RNG), is a 3 x 4 grid world\nin which the agent starts at the bottom-left corner, and it has to reach the top-right corner.\nThere is a hole, similar to the frozen lake environment, south of the goal, and a wall near the\nstart. The transition function has a 20% noise; that is, 80% the action succeeds, and 20% it\nfails uniformly at random in orthogonal directions. The reward function is a -0.04 living pen-\nalty, a +1 for landing on the goal, and a -1 for landing on the hole.\n\nNow, what we're doing is evaluating a policy. | happen to include the optimal policy in\nchapter 3\u2019s Notebook: | didn\u2019t have space in that chapter to talk about it. In fact, make sure\nyou check all the Notebooks provided with the book.\n\nOptimal policy in the RNG environment\n\n@ This is the policy\nwe want to evaluate.\n\n@ @ecause this environment\u2019s transition function isn\u2019t as\nnoisy as the Frozen lake, the optimal policy is the \u201cobvious\u201d\noptimal policy. Assume a. gamma. of |.\n\nTatty it Up\n\nFVMC, TD, n-step TD, and TD(A) in the RNG environment\n\n\u00a9 tran the same exact +H!\n\n\u2018ameter as before\n\nexcept for 1000 episodes\ninstead of the SOO For the\nRw. The results shown on\nthe right are the running\nestimates of the state-\nvalue function for Five\nrandomly selected states\nout of the total la states.\n@andomly, but with the\nsame seed for each plot\nfor easy is\n\nnot really 100% random. |\nFirst Filter estimated\nvalues lower than a\nthreshold, 0.1.) | did this so\nthot you can better\nappreciate meaningful\ntrends from a handful of\nstates.\n\n@) As you can see, all four\nalgorithms (Five if you\nhead to the Notebook!)\nFind a pretty good\nestimate of the true\nstate-value Function. If\nyou look closely, you can\nsee that TD and\nTOlambda) show the two\nsmoothest curves. MC, on\nthe other hand, followed\nby n-step TD show the\nmost centered trends.\n\nFYMC estimates through time vs. true values\n\nSeg i tannnap-aannn nan ncaenananannseccnaae\u2014\u2014nam\n\nina\n\nne MB)\n\n&\n\n=\n\n\u2018State-value function\n\noo\n) 200 \u201c0 609 0 1000\nEpisodes\n\nTD estimates through time vs. true values\n\nos\n\n\u2018State-value function\n=\n\nao\n\n0 600\nEpisodes\n\nn-step TD estimates through time vs, true values\n\nos\n\u2014\u2014\u2014 vmi6)\n\nState-value function\nFy\n\n00 600 0 1600\nEpisodes\n\nTD(A) estimates through time vs. true values:\n\nState-value function\n\n\u2014 ve\n--\nan)\n--- wD\n\u2014 wo\n\n\u2014 ver\n--\nan)\n--- wD\n\u2014 wo\n\n\u2014 ve\n--\n\nwa\n--- wD\n\u2014 wo\n\n\u2014 we\n~~ way\neaten)\n\nva\n\u2014 wo\n\nTatty it Up\nRNG shows a bit better the bias and variance effects on estimates\n\n@ Alright, so | Figure | no\nprobably need to *200m in\u201d\nand show you the Front oF\nthe curves. These plots are\nnot log scale like the other\nones | showed in the past.\nThese are a slice on the\nFirst SO episodes. Also, I'm oo |\nshowing only the values , * \u201d episodes\u201d .\ngreater than 0.1, but as you\ncan see, that includes most\nstates. Value functions of LJ\nstates 3,5, and Tare 0, and\n10 and Il are far From being\nbecause the action in the\nstate 9 and & points left\n\nis away from state 10 and Il.\n\nFYMC estimates through time (close up)\n\nos\n\n\u2018State-value function\n\nTD estimates through time (close up)\n\nState-value function\n\n@ Look ot the trends this\ntime around, They're easier\nto spot. For instance, MC is\njagged, showing those up-\nand-down trends. TD is\nsmooth but slow. n-step TD\nis somewhat in between,\nand TOlambda),\nsmoothness of TD, which\nyou can probably easily\nappreciate, but also it isn't\nas slow. For instance, look at\nthe curve of v(@): it First\ncrosses the 0.4 line around\naS episodes, TD all the way\nat 45.\n\nState-value function\ngs\n\nFo) 0 \u00ab0 ~\nEpisodes\n\nTD(A) estimates through time (close up)\n\nState-value function\nef \u20ac &\n\n5\n\n19\n=> 8D\n\u2014 wo\nwa\n\nmcs]\n= wa\n\u2014 wo\n\nao)\n= Me)\n\u2014 wo\nwa\nvw\n--- WH\n\u2014 wo\n\nMe\noo WE)\n\u2014 wo\nwa\n\nve\n--- WH\n\u2014 Ko\n\na)\n--- 48\n\nver\n\u2014-\n\nvay\n--- wD\n\u2014 wo\n\nTatty it Up\nFVMC and TD targets of the RNG's initial state\n@ These Final plots are the sequence of target values of the initial state. As you might\n\nexpect, the MC targets are independent of the sequence number, becouse they're actual\nreturns and don't bootstrap on the state-value Function.\n\nJT FVMC target sequence\n\nPas\n\n@ You can as\n\nprobably also\n\nnotice re 8 oo\n\nhigh variance. 2\n\nTheseore =\n\nmostly & -0.5\n\nconcentrated\n\non top, but -10\n\nthere\u2019s a . \u00b0 \u00b0 . . .\nhandful down | -15 . . .\nhere. 0 200 400 600 800 1000\n\nEstimate sequence number\n\n@) TD targets are a bit more dependent on the sequence. Notice that early on, the targets are\nway off and somewhat noisy. However, as the targets add up, they become much more stable.\n\n@) You may ] TD target sequence\nnotice three os\n\n+o form. oO\nthese ore 3 te\ntorgetsfor >\nthe initial &\nstate, state 8. \u00a9\n\\F you look at\nyou'll notice ia\n\nthat going ~~ \u00b0 200 400 600 800 1000 1200\n\nup in state Estimate sequence number\n\nScan only \u2014_) ... with 8O% the agent lands on state 4 (up), 10% it bounces back to 8 (lef),\nhave three and 10% it lands on state 9 (right). Can you think which line on this plot\ntransitions --- corresponds to which \u2018next state\"? Why?! Run experiments!\n\nvn(8)\n\n. ee ew ennle ame Seah wwomman + fis cons 004\nee ae\n\nne\n\no4\n\n02\n\n00\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.11,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_5/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_11/0f250133-05ea-4216-bb3c-ba22961df338.png",
                            "./screenshots-images-2/chapter_5/section_11/0420f268-6748-43f9-bfb5-e9afb826895b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nLearning from sequential feedback is challenging; you learned quite a lot about it in chapter\n3. You created agents that balance immediate and long-term goals. Methods such as value\niteration (VI) and policy iteration (PI) are central to RL. Learning from evaluative feedback\nis also very difficult. Chapter 4 was all about a particular type of environment in which agents\nmust learn to balance the gathering and utilization of information. Strategies such as epsilon-\ngreedy, softmax, optimistic initialization, to name a few, are also at the core of RL.\n\nAnd I want you to stop for a second and think about these two trade-offs one more time as\nseparate problems. I\u2019ve seen 500-page and longer textbooks dedicated to each of these trade-\noffs. While you should be happy we only put 30 pages on each, you should also be wondering.\nIf you want to develop new DRL algorithms, to push the state of the art, I recommend you\nstudy these two trade-offs independently. Search for books on \u201cplanning algorithms\u201d and\n\u201cbandit algorithms,\u201d and put time and effort into understanding each of those fields. You'll\nfeel leaps ahead when you come back to RL and see all the connections. Now, if your goal is\nsimply to understand DRL, to implement a couple of methods, to use them on your own\nprojects, what\u2019s in here will do.\n\nIn this chapter, you learned about agents that can deal with feedback that\u2019s simultaneously\nsequential and evaluative. And as mentioned before, this is no small feat! To simultane-\nously balance immediate and long-term goals and the gathering and utilization of informa-\ntion is something even most humans have problems with! Sure, in this chapter, we restricted\nourselves to the prediction problem, which consists of estimating values of agents\u2019 behaviors.\nFor this, we introduced methods such as Monte Carlo prediction and temporal-difference\nlearning. Those two methods are the extremes in a spectrum that can be generalized with the\nn-step TD agent. By merely changing the step size, you can get virtually any agent in between.\nBut then we learned about TD(A) and how a single agent can combine the two extremes and\neverything in between in a very innovative way.\n\nNext chapter, we'll look at the control problem, which is nothing but improving the agents\u2019\nbehaviors. The same way we split the policy-iteration algorithm into policy evaluation and\npolicy improvement, splitting the reinforcement learning problem into the prediction prob-\nlem and the control problem allows us to dig into the details and get better methods.\n\nBy now, you\n\n+ Understand that the challenge of reinforcement learning is because agents cannot see\nthe underlying MDP governing their evolving environments\n\n+ Learned how these two challenges combine and give rise to the field of RL\n\n+ Know about many ways of calculating targets for estimating state-value functions\n\nTWEETABLE Feat\nWork on your own and share your findings\n\nHere are several ideas on how to take what you have learned to the next level. If you'd like,\nshare your results with the rest of the world and make sure to check out what others have\ndone, too. It\u2019s a win-win situation, and hopefully, you'll take advantage of it.\n\n+ #gdri_ch05_tf01: None of the methods in this chapter handle the time step limit\nthat\u2019s wrapped around many Gym environments. No idea what I'm talking about? No\nworries, | explain it in more detail in chapter 8. However, for the time being, check out\nthis file: https://github.com/openai/gym/blob/master/gym/envs/__init__.py. See how\nmany environments, including the frozen lake, have a variable max_episode_steps.\nThis is a time step limit imposed over the environments. Think about this for a while:\nhow does this time step limit affect the algorithms presented in this chapter? Go to\nthe book's Notebook, and modify the algorithms so that they handle the time step\nlimit correctly, and the value function estimates are more accurate. Do the value func-\ntions change? Why, why not? Note that if you don\u2019t understand what I'm referring to,\nyou should continue and come back once you do.\n\n+ #gdrl_ch05_tf02: Comparing and plotting the Monte Carlo and temporal-\ndifference targets is useful. One thing that would help you understand the difference\nis to do a more throughout analysis of these two types of targets, and also include the\nn-step and TD-lambda targets. Go ahead and start by that collecting the n-step tar-\ngets for different values of time steps, and do the same for different values of lambda\nin TD-lambda targets. How do these compare with MC and TD? Also, find other ways\nto compare these prediction methods. But, do the comparison with graphs, visuals!\n\n+ #gdri_chO5_tf03: In every chapter, I'm using the final hashtag as a catchall\nhashtag. Feel free to use this one to discuss anything else that you worked on relevant\nto this chapter. There\u2019s no more exciting homework than that which you create for\nyourself. Make sure to share what you set yourself to investigate and your results.\n\nWrite a tweet with your findings, tag me @mimoralea (I'll retweet), and use the particular\nhashtag from this list to help interested folks find your results. There are no right or wrong\nresults; you share your findings and check others\u2019 findings. Take advantage of this to socialize,\ncontribute, and get yourself out there! We're waiting for you!\n\nHere's a tweet example:\n\n\u201cHey, @mimoralea. | created a blog post with a list of resources to study deep reinforce-\nment learning. Check it out at <link>. #gdrl_chO1_tf01\u201d\n\nI'll make sure to retweet and help others find your work.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 6,
                "chapter_name": "Improving\nagents\u2019 behaviors",
                "chapter_path": "./screenshots-images-2/chapter_6",
                "sections": [
                    {
                        "section_id": 6.1,
                        "section_name": "Improving\nagents\u2019 behaviors",
                        "section_path": "./screenshots-images-2/chapter_6/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_1/4a35a6bd-fa97-4d40-a78c-74694d92265d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Up until this chapter, you\u2019ve studied in isolation and interplay learning from two of the\nthree different types of feedback a reinforcement learning agent must deal with: sequential,\nevaluative, and sampled. In chapter 2, you learned to represent sequential decision-making\nproblems using a mathematical framework known as the Markov decision processes. In\nchapter 3, you learned how to solve these problems with algorithms that extract policies from\nMDPs. In chapter 4, you learned to solve simple control problems that are multi-option,\nsingle-choice, decision-making problems, called Multi-Armed Bandits, when the MDP rep-\nresentation isn\u2019t available to the agent. Finally, in chapter 5, we mixed these two types of\ncontrol problems, that is, we dealt with control problems that are sequential and uncertain,\nbut we only learned to estimate value functions. We solved what's called the prediction prob-\nlem, which is learning to evaluate policies, learning to predict returns.\n\nIn this chapter, we'll introduce agents that solve the control problem, which we get\nsimply by changing two things. First, instead of estimating state-value functions, V(s), we\nestimate action-value functions, Q(s, a). The main reason for this is that Q-functions, unlike\nV-functions, let us see the value of actions without having to use an MDP. Second, after we\nobtain these Q-value estimates, we use them to improve the policies. This is similar to what\nwe did in the policy-iteration algorithm: we evaluate, we improve, then evaluate the improved\npolicy, then improve on this improved policy, and so on. As I mentioned in chapter 3, this\npattern is called generalized policy iteration (GPI), and it can help us create an architecture\nthat virtually any reinforcement learning algorithm fits under, including state-of-the-art\ndeep reinforcement learning agents.\n\nThe outline for this chapter is as follows: First, Pll expand on the generalized policy-\niteration architecture, and then you'll learn about many different types of agents that solve\nthe control problem. You'll learn about the control version of the Monte Carlo prediction\nand temporal-difference learning agents. You'll also learn about slightly different kinds of\nagents that decouple learning from behavior. What this all means in practical terms is that in\nthis chapter, you develop agents that learn to solve tasks by trial-and-error learning. These\nagents learn optimal policies solely through their interaction with the environment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.2,
                        "section_name": "The anatomy of reinforcement learning agents",
                        "section_path": "./screenshots-images-2/chapter_6/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_2/ad3db647-560e-4298-831f-0be4dc241c0b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The anatomy of reinforcement learning agents\n\nIn this section, I'd like to give you a mental model that most, if not all, reinforcement learning\nagents fit under. First, every reinforcement learning agent gathers experience samples, either\nfrom interacting with the environment or from querying a learned model of an environment.\nStill, data is generated as the agents learn. Second, every reinforcement learning agent learns\nto estimate something, perhaps a model of the environment, or possibly a policy, a value\nfunction, or just the returns. Third, every reinforcement learning agent attempts to improve\na policy; that\u2019s the whole point of RL, after all.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.3,
                        "section_name": "ReFresH My Memory\n\nRewards, returns, and value functions",
                        "section_path": "./screenshots-images-2/chapter_6/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_3/79e01c5f-d0cd-4d7a-8bda-d0749805dc10.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "RerresH My Memory\nRewards, returns, and value functions\n\nNow is a good time to refresh your memory. You need to remember the difference between\nrewards, returns, and value functions, so that this chapter makes sense to you and you can\ndevelop agents that learn optimal policies through trial-and-error learning. Allow me to\nrepeat myself.\n\nA reward is a numeric signal indicating the\ngoodness of a transition. Your agent observes\nstate S,, takes action A; then the environment\nchanges and gives a reward R,, ,, and emits anew\nstate S,_,. Rewards are that single numeric signal\nindicating the goodness of the transition occurring on every time step of an episode.\n\nA return is the summation of all the rewards received during an episode. Your agent\nreceives reward R,,,, then R,,,, and so on until it gets the final reward R, right before landing\nin the terminal state S,. Returns are the sum of all those rewards during an episode. Returns\nare often defined as the discounted sum,\ninstead of just a sum. A discounted sum puts a\npriority on rewards found early in an episode\n(depending on the discount factor, of course.)\nTechnically speaking, a discounted sum is a\nmore general definition of the return, since a\ndiscount factor of one makes it a plain sum.\n\nA value function is the\nexpected return. Expectations\nare calculated as the sum of all\npossible values, each multi-\nplied by the probability of its\noccurrence. Think of expecta-\ntions as the average of an\ninfinite number of samples;\nthe expectation of returns is\nlike sampling an infinite num-\nber of returns and averaging\nthem. When you calculate a\nreturn starting after selecting\nan action, the expectation is\nthe action-value function of\nthat state-action pair, Q/(s, a). If you disregard the action taken and count from the state s, that\nbecomes the state-value function V/s).\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.4,
                        "section_name": "Most agents gather experience samples",
                        "section_path": "./screenshots-images-2/chapter_6/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_4/f07f1fbe-987e-400d-85da-22d2caca1a21.png",
                            "./screenshots-images-2/chapter_6/section_4/80092ac1-b951-44ab-8d97-56243694fa55.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Most agents gather experience samples\n\nOne of the unique characteristics of RL is that agents learn by trial and error. The agent inter-\nacts with an environment, and as it does so, it gathers data. The unusual aspect here is that\ngathering data is a separate challenge from learning from data. And as you'll see shortly,\nlearning from data is also a different thing from improving from data. In RL, there is gather-\ning, learning, and improving. For instance, an agent that\u2019s pretty good at collecting data may\nnot be as good at learning from data; or, conversely, an agent that isn\u2019t good at collecting data\nmay be good at learning from data, and so on. We all have that friend who didn\u2019t take good\nnotes in school, yet they did well on tests, while others had everything written down, but\ndidn\u2019t do as well.\n\nIn chapter 3, when we learned about dynamic programming methods, I mentioned value\nand policy iteration shouldn\u2019t be referred to as RL, but planning methods instead, the reason\nbeing they do not gather data. There\u2019s no need for DP methods to interact with the environ-\nment because a model of the environment, the MDP, is provided beforehand.\n\nR b Wr an RL Accent\nPlanning vs. learning problems\n\nPlanning problems: Refers to problems in which a model of the environment is available\nand thus, there\u2019s no learning required. These types of problems can be solved with planning\nmethods such as value iteration and policy iteration. The goal in these types of problems is to\nfind, as opposed to learn, optimal policies. Suppose | give you a map and ask you to find the\nbest route from point A to point B; there's no learning required there, just planning.\n\nLearning problems: Refers to problems in which learning from samples is required, usually\nbecause there isn\u2019t a model of the environment available or perhaps because it\u2019s impossible\nto create one. The main challenge of learning problems is that we estimate using samples,\nand samples can have high variance, which means they'll be of poor quality and difficult to\nlearn from. Samples can also be biased, either because of being from a different distribution\nthan the one estimating or because of using estimates to estimate, which can make our esti-\nmates incorrect altogether. Suppose | don\u2019t give you a map of the area this time. How would\nyou find \u201cthe best route\u201d? By trial-and-error learning, likely.\n\nFor an algorithm to be considered a standard RL method, the aspect of interacting with the\nenvironment, with the problem we're trying to solve, should be present. Most RL agents\ngather experience samples by themselves, unlike supervised learning methods, for instance,\nwhich are given a dataset. RL agents have the additional challenge of selecting their datasets.\nMost RL agents gather experience samples because RL is often about solving interactive learn-\ning problems.\n\nR b Witn an RL Accent\nNon-interactive vs. interactive learning problems\n\nNon-interactive learning problems: Refers to a type of learning problem in which there's\nno need for or possibility of interacting with an environment. In these types of problems,\nthere's no interaction with an environment while learning, but there is learning from data\npreviously generated. The objective is to find something given the samples, usually a policy,\nbut not necessarily. For instance, in inverse RL, the objective is to recover the reward function\ngiven expert-behavior samples. In apprenticeship learning, the objective is to go from this\nrecovered reward function to a policy. In behavioral cloning, which is a form of imitation\nlearning, the goal is to go from expert-behavior samples directly to policies using supervised\nlearning.\n\nInteractive learning problems: Refers to a type of learning problem in which learning and\ninteraction are interleaved. The interesting aspect of these problems is that the learner also\ncontrols the data-gathering process. Optimal learning from samples is one challenge, and\nfinding samples for optimal learning is another.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.5,
                        "section_name": "Most agents estimate something",
                        "section_path": "./screenshots-images-2/chapter_6/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_5/9facff16-1c6d-4c29-a54e-75903bd2c2b8.png",
                            "./screenshots-images-2/chapter_6/section_5/14e35fdf-71f3-4838-9f4e-90ea32de2cf7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Most agents estimate something\n\nAfter gathering data, there are multiple things an agent can do with this data. Certain agents,\nfor instance, learn to predict expected returns or value functions. In the previous chapter, you\nlearned about many different ways of doing so, from using Monte Carlo to TD targets, from\nevery-visit to first-visit MC targets, from n-step to A-return targets. There are many different\nways of calculating targets that can be used for estimating value functions.\n\nBut value functions aren\u2019t the only thing agents can learn with experience samples. Agents\nmay be designed to learn models of the environment, too. As you'll see in the next chapter,\nmodel-based RL agents use the data collected for learning transition and reward functions. By\nlearning a model of the environment, agents can predict the next state and reward. Further,\nwith these, agents can either plan a sequence of actions similar to the way DP methods work or\nmaybe use synthetic data generated from interacting with these learned models to learn some-\nthing else. The point is that agents may be designed to learn models of the environment.\n\nMoreover, agents can be designed to improve on policies directly using estimated returns.\nIn later chapters, we'll see how policy gradient methods consist of approximating functions\nthat take in a state and output a probability distribution over actions. To improve these policy\nfunctions, we can use actual returns, in the simplest case, but also estimated value functions.\nFinally, agents can be designed to estimate multiple things at once, and this is the typical case.\nThe important thing is that most agents estimate something.\n\nRerresH My Memory\nMonte Carlo vs. temporal-difference targets\n\nOther important concepts worth repeating are the different ways value functions can be\nestimated. In general, all methods that learn value functions progressively move estimates a\nfraction of the error towards the targets. The general equation that most learning methods\nfollow is estimate = estimate + step * error. The error is simply the difference between a sam-\npled target and the current estimate: (target - estimate). The two main and opposite ways for\ncalculating these targets are Monte Carlo and temporal-difference learning.\n\n\u2014 |\n\nThe Monte Carlo target consists of the actual return: really, nothing else. Monte Carlo\nestimation consists of adjusting the estimates of the value functions using the empirical\n(observed) mean return in place of the expected (as if you could average infinite samples)\nreturn.\n\nThe temporal-difference target consists of an estimated return. Remember \u201cbootstrapping\u201d?\nIt basically means using the estimated expected return from later states, for estimating the\nexpected return from the current state. TD does that: learning a guess from a guess. The TD\ntarget is formed by using a single reward and the estimated expected return from the next\nstate using the running value function estimates.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.6,
                        "section_name": "Most agents improve a policy",
                        "section_path": "./screenshots-images-2/chapter_6/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_6/721ca9f5-2408-494c-a85b-332b26f67650.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Most agents improve a policy\n\nLastly, most agents improve a policy. This final step heavily depends on the type of agent\nbeing trained and what the agent estimates. For instance, if the agent is estimating value func-\ntions, a common thing to improve is the target policy implicitly encoded in the value function,\nwhich is the policy being learned about. The benefit of improving the target policy is that the\nbehavior policy, which is the data-generating policy, will consequently improve, therefore\nimproving the quality of data the agent will subsequently gather. If the target and behavior\npolicies are the same, then the improvement of the underlying value function explicitly\nincreases the quality of the data generated afterward.\n\nNow, if a policy is being represented explicitly instead of through value functions, such as\nin policy gradient and actor-critic methods, agents can use actual returns to improve these\npolicies. Agents can also use value functions to estimate returns for improving policies.\nFinally, in model-based RL, there are multiple options for improving policies. One can use a\nlearned model of the environment to plan a sequence of actions. In this case, there\u2019s an\nimplicit policy being improved in the planning phase. One can use the model to learn a value\nfunction, instead, which implicitly encodes a policy. One can use the model to improve the\npolicy directly, too. The bottom line is that all agents attempt to improve a policy.\n\nR b Wit an RL Accent\nGreedy vs. epsilon-greedy vs. optimal policy\n\nGreedy policy: Refers to a policy that always selects the actions believed to yield the highest\nexpected return from each and every state. It\u2019s essential to know that a \u201cgreedy policy\u201d is\ngreedy with respect to a value function. The \u201cbelieved\u201d part comes from the value function.\nThe insight here is that when someone says, \u201cthe greedy policy,\u2019 you must ask, greedy with\nrespect to what? A greedy policy with respect to a random value function is a pretty bad\npolicy.\n\nEpsilon-greedy policy: Refers to a policy that often selects the actions believed to yield the\nhighest expected return from each and every state. Same as before applies; an epsilon-\ngreedy policy is epsilon-greedy with respect to a specific value function. Always make sure\nyou understand which value function is being referenced.\n\nOptimal policy: Refers to a policy that always selects the actions actually yielding the high-\nest expected return from each and every state. While a greedy policy may or may not be an\noptimal policy, an optimal policy must undoubtedly be a greedy policy. You ask, \u201cgreedy with\nrespect to what?\u201d Well done! An optimal policy is a greedy policy with respect to a unique\nvalue function, the optimal value function.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.7,
                        "section_name": "Generalized policy iteration",
                        "section_path": "./screenshots-images-2/chapter_6/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_7/ae9bc31a-c817-40e0-862b-66d478c30b88.png",
                            "./screenshots-images-2/chapter_6/section_7/c33de815-2248-4fa2-9962-deec51e3f50d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generalized policy iteration\n\nAnother simple pattern that\u2019s more commonly used to understand the architecture of rein-\nforcement learning algorithms is called generalized policy iteration (GP). GP1 is a general idea\nthat the continuous interaction of policy evaluation and policy improvement drives policies\ntowards optimality.\n\nAs you probably remember, in the policy iteration algorithm, we had two processes: policy\nevaluation and policy improvement. The policy-evaluation phase takes in any policy, and it\nevaluates it; it estimates the policy\u2019s value function. In policy improvement, these estimates,\nthe value function, are used to obtain a better policy. Once policy evaluation and improve-\nment stabilize, that is, once their interaction no longer produces any changes, then the policy\nand the value function are optimal.\n\nNow, if you remember, after studying policy iteration, we learned about another algo-\nrithm, called value iteration. This one was similar to policy iteration; it hada policy-evaluation\nand a policy-improvement phase. The main difference, however, was that the policy-\nevaluation phase consisted of a single iteration. In other words, the evaluation of the policy\ndidn\u2019t produce the actual value function. In the policy-evaluation phase of value iteration, the\nvalue function estimates move towards the actual value function, but not all the way there.\nYet, even with this truncated policy-evaluation phase, the generalized policy-iteration pattern\nfor value iteration also produces the optimal value function and policy.\n\nThe critical insight here is that policy evaluation, in general, consists of gathering and esti-\nmating value functions, similar to the algorithms you learned about in the previous chapter.\nAnd as you know, there are multiple ways of evaluating a policy, numerous methods of esti-\nmating the value function of a policy, various approaches to choose from for checking off the\npolicy-evaluation requirement of the generalized policy-iteration pattern.\n\nFurthermore, policy improvement consists of changing a policy to make it greedier with\nrespect to a value function. In the policy improvement method of the policy-iteration algo-\nrithm, we make the policy entirely greedy with respect to the value function of the evaluated\npolicy. But, we can completely greedify the policy only because we had the MDP of the envi-\nronment. However, the policy-evaluation methods that we learned about in the previous\nchapter don\u2019t require an MDP of the environment, and this comes at a cost. We can no longer\ncompletely greedify policies; we need to have our agents explore. Going forward, instead of\ncompletely greedifying the policy, we make the policy greedier, leaving room for exploration.\nThis kind of partial policy improvement was used in chapter 4 when we used different explo-\nrations strategies for working with estimates.\n\nThere you have it. Most RL algorithms follow this GPI pattern: they have distinct policy-\nevaluation and improvement phases, and all we must do is pick and choose the methods.\n\n(DP) Micuet\u2019s Anatocy\n@\n\nGeneralized policy iteration and why you should listen to criticism\n\nGeneralized policy iteration (GPI) is similar to the eternal dance of critics and performers.\nPolicy evaluation gives the much-needed feedback that policy improvement uses to make\npolicies better. In the same way, critics provide the much-needed feedback performers can\nuse to do better.\n\nAs Benjamin Franklin said, \u201cCritics are our friends, they show us our faults.\u201d He was a smart\nguy; he allowed GPI to help him improve. You let critics tell you what they think, you use that\nfeedback to get better. It\u2019s simple! Some of the best companies out there follow this process,\ntoo. What do you think the saying \u201cdata-driven decisions\u201d means? It's saying they make sure\nto use an excellent policy-evaluation process so that their policy-improvement process yields\nsolid results; that\u2019s the same pattern as GPI! Norman Vincent Peale said, \u201cThe trouble\nwith most of us is that we'd rather be ruined by praise than saved by criticism.\u201d Go, let critics\nhelp you.\n\nJust beware! That they can indeed help you doesn\u2019t mean critics are always right or that\nyou should take their advice blindly, especially if it's feedback that you hear for the first time.\nCritics are usually biased, and so is policy evaluation! It's your job as a great performer to lis-\nten to this feedback carefully, to get smart about gathering the best possible feedback, and\nto act upon it only when sure. But, in the end, the world belongs to those who do the work.\n\nTheodore Roosevelt said it best:\n\n\u201cIt is not the critic who counts; not the man who points out how the strong man\nstumbles, or where the doer of deeds could have done them better. The credit\nbelongs to the man whois actually in the arena, whose face is marred by dust and\nsweat and blood; who strives valiantly; who errs, who comes short again and\nagain, because there is no effort without error and shortcoming; but who does\nactually strive to do the deeds; who knows great enthusiasms, the great devo-\ntions; who spends himself in a worthy cause; who at the best knows in the end\nthe triumph of high achievement, and who at the worst, if he fails, at least fails\nwhile daring greatly, so that his place shall never be with those cold and timid\nsouls who neither know victory nor defeat.\u201d\n\nIn later chapters, we'll study actor-critic methods, and you'll see how this whole analogy\nextends, believe it or not! Actors and critics help each other. Stay tuned for more.\n\nIt's awe-inspiring that patterns in optimal decision making are valid across the board.\nWhat you learn studying DRL can help you become a better decision maker, and what you\nlearn in your own life can help you create better agents.\n\nCool, right?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.8,
                        "section_name": "Learning to improve policies of behavior",
                        "section_path": "./screenshots-images-2/chapter_6/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_8/e5fe39ff-bb33-4944-8d85-879b0967bea5.png",
                            "./screenshots-images-2/chapter_6/section_8/276d8af4-1903-4d0b-bc2f-002c459ad103.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning to improve policies of behavior\n\nIn the previous chapter, you learned how to solve the prediction problem: how to make\nagents most accurately estimate the value function of a given policy. However, while this is a\nuseful ability for our agents to have, it doesn\u2019t directly make them better at any task. In this\nsection, you'll learn how to solve the control problem: how to make agents optimize policies.\nThis new ability allows agents to learn optimal behavior by trial-and-error learning, starting\nfrom arbitrary policies and ending in optimal ones. After this chapter you can develop agents\nthat can solve any task represented by an MDP. The task has to be a discrete state- and action-\nspace MDP, but other than that, it\u2019s plug-and-play.\n\nTo show you a few agents, we're going to leverage the GPI pattern you learned. That is,\nwe're going to select algorithms for the policy-evaluation phase from the ones you learned\nabout in the last chapter, and strategies for the policy-improvement phase from the ones you\nlearned about in the chapter before. Hopefully, this sets your imagination free on the possi-\nbilities. Just pick and choose algorithms for policy evaluation and improvement, and things\nwill work, that\u2019s because of the interaction of these two processes.\n\nR b Wry an RL Accent\nPrediction vs. control problem vs. policy evaluation vs. improvement\n\nPrediction problem: Refers to the problem of evaluating policies, of estimating value func-\ntions given a policy. Estimating value functions is nothing but learning to predict returns.\nState-value functions estimate expected returns from states, and action-value functions esti-\nmate expected returns from state-action pairs.\n\nControl problem: Refers to the problem of finding optimal policies. The control problem is\nusually solved by following the pattern of generalized policy iteration (GPI), where the com-\npeting processes of policy evaluation and policy improvement progressively move policies\ntowards optimality. RL methods often pair an action-value prediction method with policy\nimprovement and action-selection strategies.\n\nPolicy evaluation: Refers to algorithms that solve the prediction problem. Note that there's\na dynamic programming method called policy evaluation, but this term is also used to refer\nto all algorithms that solve the prediction problem.\n\nPolicy improvement: Refers to algorithms that make new policies that improve on an origi-\nnal policy by making it greedier than the original with respect to the value function of that\noriginal policy. Note that policy improvement by itself doesn\u2019t solve the control problem.\nOften a policy evaluation must be paired with a policy improvement to solve the control\nproblem. Policy improvement only refers to the computation for improving a policy given its\nevaluation results.\n\nConcrete EXAMPLE\nThe slippery walk seven environment\n\nFor this chapter, we use an environment called slippery walk seven (SWS). This environment is\na walk, a single-row grid-world environment, with seven non-terminal states. The particular\nthing of this environment is that it\u2019s a slippery walk; action effects are stochastic. If the agent\nchooses to go left, there is a chance it does, but there is also a chance that it goes right, or that\nit stays in place.\n\nLet me show you the MDP for this environment. Remember, though, that the agent doesn\u2019t\nhave any access to the transition probabilities. The dynamics of this environment are\nunknown to the agent. I'm only giving you this information for didactic reasons.\n\nAlso, have in mind that to the agent, there are no relationships between the states in\nadvance. The agent doesn\u2019t know that state 3 is in the middle of the entire walk, or that it\u2019s in\nbetween states 2 and 4; it doesn't even know what a \u201cwalk\u201d is! The agent doesn\u2019t know that\naction zero goes left, or one goes right. Honestly, | encourage you to go to the Notebook and\nplay with the environment yourself to gain a deeper understanding. The fact is that the agent\nonly sees the state ids, say, 0, 1, 2, and so on, and chooses either action 0 or 1.\n\nSlippery walk seven environment MDP\n\n\u00ae so% chance of going in the intended direction\n(@ 33.3% chance of staying put\n) le.6% chance of going in the opposite direction\n\nThe SWS environment is similar to the random walk (RW) environment that we learned about\nin the previous chapter, but with the ability to do control. Remember that the random walk is\nan environment in which the probability of going left, when taking the Left action, is equal to\nthe probability of going right. And the probability of going right, when taking the Right\naction, is equal to the probability of going left, so there\u2019s no control. This environment is\nnoisy, but the actions the agent selects make a difference in its performance. And also, this\nenvironment has seven non-terminal states, as opposed to the five of the RW.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.9,
                        "section_name": "Monte Carlo control: Improving policies after each episode",
                        "section_path": "./screenshots-images-2/chapter_6/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_9/33d02b07-b432-40d7-a97e-c3d7c78db9cc.png",
                            "./screenshots-images-2/chapter_6/section_9/ba65b63b-268a-42fa-92ab-aa8e0d67b966.png",
                            "./screenshots-images-2/chapter_6/section_9/48a6d76f-26df-4bbd-9d53-0dc067ee1d2a.png",
                            "./screenshots-images-2/chapter_6/section_9/f2f4f342-759a-4e10-b6e8-1c2cfe38faa5.png",
                            "./screenshots-images-2/chapter_6/section_9/e829d263-e2a5-4652-abb7-96f42f14b94d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Monte Carlo control: Improving policies after each episode\n\nLet\u2019s try to create a control method using Monte Carlo prediction for our policy-evaluation\nneeds. Let\u2019s initially assume we\u2019re using the same policy-improvement step we use for the\npolicy-iteration algorithm. That is, the policy-improvement step gets the greedy policy with\nrespect to the value function of the policy evaluated. Would this make an algorithm that helps\nus find optimal policies solely through interaction? Actually, no. There are two changes we\nneed to make before we can make this approach work.\n\nFirst, we need to make sure our\nagent estimates the action-value We need to estimate\nfunction Q(s, a), instead of the V(s, a) action-value functions\nthat we estimated in the previ-\n\nous chapter. The problem with the [e fe=[e[ + [| @ Tuo actions, Le and Right,\nV-function is that, without the [\u00b0 fofos] + | | and the v-function as shown. >\nMDP, it isn\u2019t possible to know what \u201c you tell me the best policy\u2019\nthe best action is to take from a\nstate. In other words, the policy- (@) what if | told you Left sent you right with 10% chance?\nimprovement step wouldn\u2019t work. _ @) What do you think the best policy is now?\n\nSecond, we need to make sure (4) See?! v-Function isn\u2019t enough.\nour agent explores. The problem is\nthat we\u2019re no longer using the MDP\nfor our policy-evaluation needs.\n\n. 0 +1] @ imagine start with this\nWhen we estimate from samples, we , risbe potioy\n\nWe need to explore\n\nget values for all of the state-action\npairs we visited, but what if part of\nthe best states weren\u2019t visited? @ How can you tell whether the Right action is better\nTherefore, let\u2019s use first-visit than the Left if all you estimate is the Left action?\nMonte Carlo prediction for the \u00a9 See?! Your agent needs to explore.\npolicy-evaluation phase and a decay-\ning epsilon-greedy action-selection strategy for the policy-improvement phase. And that\u2019s\nit\u2014you have a complete, model-free RL algorithm in which we evaluate policies with Monte\nCarlo prediction and improve them with decaying epsilon-greedy action-selection strategy.\nAs with value iteration, which has a truncated policy-evaluation step, we can truncate the\nMonte Carlo prediction method. Instead of rolling out several episodes for estimating the\nvalue function of a single policy using Monte Carlo prediction, as we did in the previous\nchapter, we truncate the prediction step after a single full rollout and trajectory sample esti-\nmation, and improve the policy right after that single estimation step. We alternate a single\nMC-prediction step and a single decaying epsilon-greedy action-selection improvement step.\n\nLet\u2019s look at our first RL method MC control. You'll see three functions:\n+ decay_schedule: Compute decaying values as specified in the function arguments.\n* generate_trajectory: Roll out the policy in the environment for a full episode.\n\n+ mc_control: Complete implementation of the MC control method.\n\n1 Speak PYTHON\n\nExponentially decaying schedule\ndef decay schedule( \u2014\u2014\u2014\u2014\u2014\u2014\u20141+ 09 The decay schedule we'll use for\ninit_value, min_value, both alpha. and epsilon is the same we\ndecay ratio, max_steps, used in the previous chapter for alpha.\nlog_start=-2, log base=10) : Let\u2019s go into more detail this time.\n@ what | personally like about this function is that you give it an initial value, a minimum\nvalue, and the percentage of the max_steps to decay the values from initial to minimum.\n\nC2 decay steps = int(max_steps * decay ratio)\n\u00ae This decay steps is the index where the decaying of values terminates and the\nmin_value continues until max_steps.\nc\u00b0 rem_steps is therefore the difference.\nrem_steps = max_steps - decay steps\n\n\u00a9) 'm calculating the values using the logspace starting trom log_start, which | set by\ndefault to -a, and ending on 0. The number of values in that space that | ask for is\ndecay steps and the base is log_base, which | default to 10. Notice, | reverse those values!\n\nvalues = np.logspace ( ]\nlog_start, 0, decay steps,\nbase=log_ base, endpoint=True) [::-1]\n\n@ because the values may not end exactly at 0, given it\u2019s the log, | change them to be\nbetween 0 and! so that the curve looks smooth and nice.\n\nvalues = (values - values.min()) / \\\n(values.max() - values.min() )\n\ncone we can do a linear transformation and get points between init_value and min_value.\nvalues = (init value - min_value) * values + min value\ncs This pad function just repeats the rightmost value rem_step number of times.\n\nvalues = np.pad(values, (0, rem_steps), \u2018edge')\nreturn values\n\nI Speak PYTHON\n\nGenerate exploratory policy trajectories\ndef generate trajectory( 7 00 This version of the generate_\nselect_action, Q, epsilon, trajectory function is slightly\nenv, max_steps=200): different. We now need to take in\nan action-selecting strategy,\n\ninstead of a greedy policy.\nexperiences named trajectory.\n\n[ seroetncedtaing\ndone, trajectory = False, []\nwhile not done: \u00a2\u2014\u2014\u20144 @ we then start looping through until\nthe done Flag is set to true.\nstate = env.reset() \u00a2\u20144 (4) we reset the environment to\ninteract in a new episode.\nfor t in count(): 4\u00a2\u20144 (GS) Then start counting steps +.\n\n- @) Then, use the passed \u2018select_action\u2019 function to pick an action.\n\naction = select_action(state, Q, epsilon)\n\n@ we step the environment using that action and obtain the full experience tuple.\n\nnext_state, reward, done, _ = env.step(action)\nexperience = (state,\n\naction,\n\nreward,\n\nnext state, | 1 @ we append\n\ndone) Sie eer en ee se)\ntrajectory. append (experience) the trajectory list.\nif done:\n\n\u2014\u2014\u2014\u2014\u2014FF \u00a9) If we hit a. terminal state and the \u2018done\u2019\nFlag is raised, then break and return.\n\nSt GO) And if the count of steps \u201cin the\ncurrent trajectory hits the maximum\nif t >= max_steps - 1: allowed, we clear the trajectory, break,\n\nbreak\n\ntrajectory = [] i\naes and try to obtain another trajectory.\nGD) Remember to update the state.\nstate = next_state Ga) Finally, we return a NumPy version oF\nthe trajectory for easy data manipulation.\n\nreturn np.array(trajectory, np.object)\n\nI Speak PYTHON\nMonte Carlo control 1/2\n\ndef mc_control (env, -\u2014\u2014\u2014\u2014F & me_contral is similar to\n\ngamma=1.0, me_prediction. The two main\ninit_alpha=0.5, differences is that we now\nmin_alpha=0.01, estimate the action-value function\n\nalpha_decay ratio=0.5, @, and that we need to explore.\ninit_epsilon=1.0, \u00a2\u2014\u2014+1 @Notice in the function definition\n\nmin_epsilon=0.1, we are using values for epsilon to\nepsilon decay ratio=0.9, configure a decaying schedule for\nn_episodes=3000, random exploration.\n\nmax_steps=200,\nfirst_visit=True) :\n\nnS, nA = env.observation_space.n, env.action_space.n\n\ndiscounts = np.logspace( 4\u00a2\u2014\u2014\u2014\u2014\u2014-+ (3) we calculate values for the\n\n0, max_steps, discount factors in advance.\nnum=max_steps, base=gamma, Notice we use max_steps because\nendpoint=False) that\u2019s the maximum length of a\ntri 5\nalphas = decay schedule( 44 @) we also calculate alphas in\ninit_alpha, min_alpha, advance using the passed values.\n\nalpha_decay ratio,\nn_episodes)\n\nepsilons = decay schedule ( +<\u2014\u2014\u2014J \u00a9 Finally, we repeat For epsilon,\n\ninit_epsilon, min_epsilon, and obtain an array thot will\nepsilon decay ratio, work For the full training session.\nn_episodes)\n- (\u00a9) Here we're just setting up variables,\npi_track = [] TEU :\nQ = np.zeros((nS, nA), dtype=np.float\u00e964)\n\nQ track = np.zeros((n_episodes, nS, nA), dtype=np.float\u00e964)\n\nFao aoa eu oue @ This is an epsilon-\nselect_action = lambda state, Q, epsilon: greedy strategy,\n\nnp.argmax(Q[state]) \\ though we decay\nif np.random.random() > epsilon \\ epsilon on each\nelse np.random.randint (len(Q[state] ) ) episode, not step.\n\n@ Continues...\nfor e in tqdm(range(n_ episodes), leave=False): \u2014__)\n\n| Speak PYTHON\nMonte Carlo control 2/2\n\n@ Repeating the previous line so that you can keep up with the indentation\n\nfor e in tqdm(range(n_ episodes), leave=False): ul\n0) Here we're entering the episode loop. We'll run for n_episodes.\nRemember that tqdm shows a nice progress bar, nothing out of this world.\n\n[.. trajectory = generate trajectory(select_action,\nQ,\nGd every new episode \u2018e\u2019 we generate a epsilons[e],\nnew trajectory with the exploratory policy env,\ndefined by the select_action function. we Jy max_steps)\nlimit the trajectory length to max_steps.\nGa) We now keep track of the visits to state-action pairs; this is\nanother important change from the me_prediction method.\nvisited = np.zeros((nS, nA), dtype=np.bool)\nr for t, (state, action, reward, _, _) in enumerate(\\\ntrajectory):\n(i) Notice here we're processing trajectories offline, that is, after\nthe interactions with the environment have stopped.\nif visited[state] [action] and first_visit:\n\ncontinue 1 (4) Here we check\nvisited[state] [action] = True for state-action-\npair visits and act\n\nGis) We proceed to calculating the return the same way we did with\nthe prediction method, except that we're using a Q-Function this time.\nn_steps = len(trajectory([t:])\n\nG = np.sum(discounts[:n_steps] * trajectory[t:, 2])\nQ[state] [action] = Q[state] [action] + \\\n\nCo. alphas[e] * (G - Q[state] [action] )\nGe) Notice how we're using the alphas.\n\n(7D After that, it\u2019s a matter of saving values for post analysis.\n\nQ track[e] = Q\npi_track.append(np.argmax(Q, axis=1))\nV = np.max(Q, axis=1)\nC pi = lambda s: {s:a for s, a in enumerate (\\\n(ig) At the end, we extract the state-value np.argmax(Q, axis=1))}[s]\nFunction and the greedy policy.\nreturn Q, V, pi, Q track, pi_track\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.1,
                        "section_name": "SARSA: Improving policies after each step",
                        "section_path": "./screenshots-images-2/chapter_6/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_10/74becbcb-8c6f-4543-8e2b-b6f616f26480.png",
                            "./screenshots-images-2/chapter_6/section_10/c4753cc1-5ea7-4424-a98f-e2fc24fc6617.png",
                            "./screenshots-images-2/chapter_6/section_10/e8495b3e-af13-42d4-8e27-d067a6fe19ae.png",
                            "./screenshots-images-2/chapter_6/section_10/854c84db-0132-4f11-8ec0-4cc20ba3d3c5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "SARSA: Improving policies after each step\n\nAs we discussed in the previous chapter, one of the disadvantages of Monte Carlo methods is\nthat they\u2019re offline methods in an episode-to-episode sense. What that means is that we must\nwait until we reach a terminal state before we can make any improvements to our value func-\ntion estimates. However, it\u2019s straightforward to use temporal-difference prediction for the\npolicy-evaluation phase, instead of Monte Carlo prediction. By replacing MC with TD pre-\ndiction, we now have a different algorithm, the well-known SARSA agent.\n\nComparison between planning and control methods\nPolicy iteration Value iteration\n\nIt era, tive\n\n2\noe\n\u00a9 Policy iteration consists of a Full (@ value iteration starts with an\nconvergence of iterative policy evaluation arbitrary value function and has a\n\nalternating with greedy policy improvement. truncated policy evaluation step.\n\nMonte Carlo control SARSA\n\n@ mC control estimates a Q-Function, has a. (4) SARSA has pretty much the same\ntruncated MC prediction phase Followed by as MC control except a truncated\nan epsilon-greedy policy-improvement step. TD prediction for policy evaluation.\n\nI Speak PYTHON\n\nThe SARSA agent 1/2\ndef sarsa(env, <7 The sarsa agent is the direct\ngamma=1.0, conversion of TD for control problems.\ninit_alpha=0.5, ee ee eee ete\nmin_alpha=0.01, main changes. First, it evaluates the\nalpha_decay ratio=0.5, action-value Function Q. Second, it uses\ninit_epsilon=1.0, an exploratory policy-improvement step.\n\nmin_epsilon=0.1, +\u2014H @) we're doing the same thing we did\nepsilon_decay ratio=0.9, with me_control using epsilon here.\nn_episodes=3000) :\n\n@ First, create several handy variables. Remember, pi_track will hold a greedy policy per episode.\nLG os, na = env. observation_space.n, env.action_space.n\n\npi_track = []\n@ Then, we create the Q-function. I\u2019m using \u2018np.Floate4? precision . . . perhaps overkill. 71\n\nLu Q = np.zeros((nS, nA), dtype=np.float\u00e964)\nPr Q track = np.zeros((n_episodes, nS, nA), dtype=np.float\u00e94)\n\n\u00a9) \u2018@_track\u2019 will hold the estimated @-function per episode.\n\nGsclect action = lambda tate, 0, epsilon \\ (\u00a9) The select_\nselect_action = lambda state, Q, epsilon: \\ action function is\n\nnp.argmax(Q[state]) \\ the same as before:\nif np.random.random() > epsilon \\ an epsilon-greedy\nelse np.random.randint (len(Q[state]) ) strategy.\n\n@ In SARSA, we don't need to calculate all discount factors in advance, because we won't\nuse full returns. Instead, we use estimated returns, so we can calculate discounts online.\n\nalphas = decay schedule ( mj @\u00a9 Notice we are, however,\n\ninit_alpha, min_alpha, calculating all alphas in advance.\nalpha_decay ratio, This function call returns a vector\nn_episodes) with corresponding alphas to use.\n\n@ The select_action function isn't a decaying strategy on its own. We're calculating decaying\nepsilons in advance, so our agent will be using a decaying epsilon-greedy strategy.\nepsilons = decay schedule (\ninit_epsilon, min_epsilon,\nepsilon decay ratio,\nn_episodes) , Gio) Let\u2019s continue on the next page.\n\nfor e in tqdm(range(n_ episodes), leave=False):\n\n1 Speak PYTHON\nThe SARSA agent 2/2\n\nGD Same line. You Know the drill.\nTy for e in tqdm(range(n_ episodes), leave=False):\n\nL__1669 serve nove nets the episod lop\n1 02) We start each episode by resetting the environment and the done Flag,\n\nstate, done = env.reset(), False\nin action = select_action(state, Q, epsilons([e])\n\n4) we select the action (perhaps exploratory) for the initial state.\nwhile not done: 44 (is) We repeat until we hit a terminal state.\n\u00a37 60 First, step the environment and get the experience.\n\nnext_state, reward, done, _ = env.step(action)\nCr? next_action = select_action(next_state,\nQ,\n\n(7D Notice that before we make any calculations, we epsilons([e])\n\nneed to obtain the action for the next step.\n\ntd_target = reward + gamma * \\\nQ[next_state] [next_action] * (not done)\n\nG8) we calculate the td_target using that next state-action pair. And we do the little trick For\nterminal states of multiplying by the expression (not done), which zeros out the future on terminal.\n9) Then calculate the td_error as the difference between the target and current estimate.\n\nL_, td_error = td_target - Q[state] [action]\n\n(@o) Finally, update the Q-function by moving the estimates a bit toward the error.\n\nQ[state] [action] = Q[state] [action] + \\\nalphas[e] * td_error\n(ai) We update the state and action for the next step.\nstate, action = next_state, next_action\n@a) Save the Q-function and greedy policy For analysis.\nQ track[e] = Q\npi_track.append(np.argmax(Q, axis=1))\nV = np.max(Q, axis=1)\n[\u201d pi = lambda s: {s:a for s, a in enumerate(\\\n\n. \u2019 is=1\n(2) At the end, calculate the estimated optimal wa eat octal LL\n\nv-function and its greedy policy, rent teek el\n\nreturn Q, V, pi, Q track, pi_track\n\nR b Wit an RL Accent\nBatch vs. offline vs. online learning problems and methods\n\nBatch learning problems and methods: When you hear the term \u201cbatch learning,\u2019 people\nare referring to one of two things: they mean a type of learning problem in which experience\nsamples are fixed and given in advance, or they mean a type of learning method which is\noptimized for learning synchronously from a batch of experiences, also called fitting methods.\nBatch learning methods are typically studied with non-interactive learning problems, more\nspecifically, batch learning problems. But batch learning methods can also be applied to\ninteractive learning problems. For instance, growing batch methods are batch learning\nmethods that also collect data: they \u201cgrow\u201d the batch. Also, batch learning problems don't\nhave to be solved with batch learning methods, the same way that batch learning methods\naren't designed exclusively to solve batch learning problems.\n\nOffline learning problems and methods: When you hear the term \u201coffline learning,\u2019 people\nare usually referring to one of two things: they're either talking about a problem setting in\nwhich there's a simulation available for collecting data (as opposed to a real-world, online\nenvironment), or they could also be talking about learning methods that learn offline, mean-\ning between episodes, for instance. Note that, in offline learning methods, learning and inter-\naction can still be interleaved, but performance is only optimized after samples have been\ncollected, similar to the growing batch described previouisly, but with the difference that,\nunlike growing batch methods, offline methods commonly discard old samples; they don\u2019t\ngrow a batch. MC methods, for instance, are often considered offline because learning and\ninteraction are interleaved on an episode-to-episode basis. There are two distinct phases,\ninteracting and learning; MC is interactive, but also an offline learning method.\n\nOnline learning problems and methods: When you hear the term \u201conline learning,\u201d people\nare referring to one of two things: either to learning while interacting with a live system, such\na robot, or to methods that learn from an experience as soon as it\u2019s collected, on each and\nevery time step.\n\nNote that offline and online learning are often used in different contexts. I've seen offline\nversus online to mean non-interactive versus interactive, but I've also seen them, as | men-\ntioned, for distinguishing between learning from a simulator versus a live system.\n\nMy definitions here are consistent with common uses of many RL researchers: Richard\nSutton (2018 book), David Silver (2015 lectures), Hado van Hasselt (2018 lectures), Michael\nLittman (2015 paper), and Csaba Szepesvari (2009 book).\n\nBe aware of the lingo, though. That's what's important.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.11,
                        "section_name": "Decoupling behavior from learning",
                        "section_path": "./screenshots-images-2/chapter_6/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_11/21b03f5d-96f8-4463-ab41-183d5b973802.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Decoupling behavior from learning\n\nI want you to think about the TD update equation for state-value functions for a second;\nremember, it uses R,,, + yV(S,,,) as the TD target. However, if you stare at the TD update\nequation for action-value functions instead, which is R.,, + yQ(S,, A,,,), you may notice\nthere are a few more possibilities there. Look at the action being used and what that means.\nThink about what else you can put in there. One of the most critical advancements in rein-\nforcement learning was the development of the Q-learning algorithm, a model-free off-policy\nbootstrapping method that directly approximates the optimal policy despite the policy gen-\nerating experiences. Yes, this means the agent, in theory, can act randomly and still find the\noptimal value function and policies. How is this possible?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.12,
                        "section_name": "Q-learning: Learning to act optimally, even if we choose not to",
                        "section_path": "./screenshots-images-2/chapter_6/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_12/218bb39e-be3b-403e-9b55-c9616ca7e83e.png",
                            "./screenshots-images-2/chapter_6/section_12/a96f14ce-4add-4f97-bfa8-c032c3ce0da0.png",
                            "./screenshots-images-2/chapter_6/section_12/09bb4a9e-95f7-4a48-82e8-bfa6fc1dd7fe.png",
                            "./screenshots-images-2/chapter_6/section_12/37c11456-5586-41de-8b70-74b38afcb462.png",
                            "./screenshots-images-2/chapter_6/section_12/acd8c546-4e60-438c-ac27-9a0818955cd8.png",
                            "./screenshots-images-2/chapter_6/section_12/041470ec-17d6-4a26-93ae-ab163463bb81.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Q-learning: Learning to act optimally, even if we choose not to\n\nThe SARSA algorithm is a sort of \u201clearning on the job.\u201d The agent learns about the same pol-\nicy it uses for generating experience. This type of learning is called on-policy. On-policy\nlearning is excellent\u2014we learn from our own mistakes. But, let me make it clear, in on-policy\nlearning, we learn from our own current mistakes only. What if we want to learn from our\nown previous mistakes? What if we want to learn from the mistakes of others? In on-policy\nlearning, you can\u2019t. Off-policy learning, on the other hand, is sort of \u201clearning from others.\u201d\nThe agent learns about a policy that\u2019s different from the policy-generating experiences. In\noff-policy learning, there are two policies: a behavior policy, used to generate experiences, to\ninteract with the environment, and a target policy, which is the policy we\u2019re learning about.\nSARSA is an on-policy method; Q-learning is an off-policy one.\n\n= SHow Me tHE Matu\nSARSA vs. Q-learning update equations\n\u00a9 The only difference between SARSA and Q-learning is the action used in the target.\n@ This is SARSA update equation. Sarsa\n\nerror\nQ(St, At) \u2014 Q(St, At) + at | + 7Q(St41, Atti) \u201c95. |\n\n@ It uses the action taken in tenet tet\nstate to calculate the target. ;\n(4) This one is Q-Learning\u2019s. Q-learning\n\nerror\n\nee\nQ(St, At) \u2014 Q(St, At) + a [a at ymax Q(St+1, 4) \u201c85a |\n\nQ-l . 5 A -\u2014 , \u2014__\u2014___\u2014_\u2014\n\u00a9 i) the the Q-learning\nmaximum estimated value in the next target\n\nstate, despite the action taken.\n\n| Speak PYTHON\nThe Q-learning agent 1/2\n\ndef q_learning(env, +\u2014\u2014\u2014\u2014 0 Notice that the beginning oF\ngamma=1.0, the Q-learning agent is\ninit_alpha=0.5, identical to the beginning of\nmin_alpha=0.01, the SARSA agent.\nalpha_decay ratio=0.5, \u00a2\u20144@) In fact, I'm even using the\ninit_epsilon=1.0, same exact hyperparameters\nmin_epsilon=0.1, For both algorithms.\n\nepsilon decay ratio=0.9,\nn_episodes=3000):\n@) Here are several handy variables.\nnS, nA = env.observation_space.n, env.action_space.n\npi_track = []\n@) The Q@-function and the tracking variable for offline analysis\nQ = np.zeros((nS, nA), dtype=np.float\u00e964)\nQ track = np.zeros((n_episodes, nS, nA), dtype=np.float\u00e94)\n\u00a9 The same epsilon-greedy action-selection strategy\n& select_action = lambda state, Q, epsilon: \\\nnp.argmax(Q[state]) \\\nif np.random.random() > epsilon \\\nelse np.random.randint (len(Q[state]))\n\n@) The vector with all alphas to be used during learning\n\ni alphas = decay schedule (\ninit_alpha, min_alpha,\nalpha_decay ratio,\nn_episodes)\n\n@ The vector with all epsilons to decay as desired\n\nL& epsilons = decay schedule (\ninit_epsilon, min_epsilon,\nepsilon decay ratio,\nn_episodes)\n\n@ Let\u2019s continue on the next page.\n\n& for e in tqdm(range(n_ episodes), leave=False):\n\n1 Speak PYTHON\nThe Q-learning agent 2/2\n\n\u00a9@) Same line as before\nLor e in tqdm(range(n_ episodes), leave=False):\n\n(0) were iterating over episodes. -\u2014*\n\nTe ater done = env.reset(), False \u2014\n\nGD) We reset the environment and get the initial state, set the done Hag to False.\nGa) Now enter the interaction loop For online learning (steps).\n\nL_\u00bb white not done:\n\n3) We repeat the loop until we hit a terminal state and a. done Flag is raised.\nG4) First thing we do is select an action For the current state. Notice the use of epsilons.\n\naction = select_action(state, Q, epsilons([e])\n| id next_state, reward, done, _ = env.step(action)\n\n(is) we step the environment and get a. full experience tuple (s, a, s} 15 d).\n\nUe) Next, we calculate the TD target. Q-learning is a special algorithm because it tries to\nlearn the optimal action-value Function g* even if it uses an exploratory policy such as\nthe decaying epsilon-greedy we're running, This is called of$-policy learning.\n\ntd_target = reward + gamma * \\\n\n| Q[next_state].max() * (not done)\n\nG7 Again, the \u201cnot done\u201d ensures the max value of the next state is set to zero on terminal\nstates. It\u2019s important that the agent doesn't expect any reward after death!!!\n(8) Next, we calculate the TD error as the difference between the estimate and the target.\n\nL_, td_error = td_target - Q[state] [action]\nQ[state] [action] = Q[state] [action] + \\\n\n(9) we then move the @-Sunction for the state- CUEESNCY Gel Crees\naction pair to be a bit closer to the error.\n\nstate = next_state \u20ac-4 (@0) Next, we update the state.\nQ track[e] = Q \u2014F Gi) save the @-function and the policy.\npi_track.append(np.argmax(Q, axis=1))\n@a) And obtain the v-function and Final\n\nV = np.max(Q, axis=1) policy on exit.\npi = lambda s: {s:a for s, a in enumerate(\\\nnp.argmax(Q, axis=1))}[s]\n\nreturn Q, V, pi, Q track, pi_track\n\nMicuet\u2019s ANALOGY\neS\n\nHumans also learn on-policy and off-policy\n\nOn-policy is learning about a policy that\u2019s being used to make decisions; you can think about\nit as \u201clearning on the job.\u2019 Off-policy learning is learning about a policy different from the\npolicy used for making decisions. You can think about it as \u201clearning from others\u2019 experi-\nences,\u2019 or \u201clearning to be great, without trying to be great.\u201d Both are important ways of learn-\ning and perhaps vital for a solid decision maker. Interestingly, you can see whether a person\nprefers to learn on-policy or off-policy pretty quickly.\n\nMy son, for instance, tends to prefer on-policy learning. Sometimes | see him struggle\nplaying with a toy, so | come over and try to show him how to use it, but then he complains\nuntil | leave him alone. He keeps trying and trying, and he eventually learns, but he prefers his\nown experience instead of others: On-policy learning is a straightforward and stable way of\nlearning.\n\nMy daughter, on the other hand, seems to be OK with learning off-policy. She can learn\nfrom my demonstrations before she even attempts a task. | show her how to draw a house,\nthen she tries.\n\nNow, beware; this is a stretch analogy. Imitation learning and off-policy learning are not\nthe same. Off-policy learning is more about the learner using their experience at, say, run-\nning, to get better at something else, say, playing soccer. In other words, you do something\nwhile learning about something else. I\u2019m sure you think of instances when you've done that,\nwhen you have learned about painting, while cooking. It doesn\u2019t matter where the experi-\nences come from for doing off-policy learning; as long as the target policy and the behavior\npolicy are different, then you can refer to that as off-policy learning.\n\nAlso, before you make conclusions about which one is \u201cbest,\u201d know that in RL, both have\npros and cons. On one hand, on-policy learning is intuitive and stable. If you want to get\ngood at playing the piano, why not practice the piano?\n\nOn the other hand, it seems useful to learn from sources other than your own hands-on\nexperience; after all, there's only so much time in a day. Maybe meditation can teach you\nsomething about playing the piano, and help you get better at it. But, while off-policy learn-\ning helps you learn from multiple sources (and/or multiple skills), methods using off-policy\nlearning are often of higher variance and, therefore, slower to converge.\n\nAdditionally, know that off-policy learning is one of the three elements that, when com-\nbined, have been proven to lead to divergence: off-policy learning, bootstrapping, and func-\ntion approximation. These don\u2019t play nice together. You've learned about the first two, and\nthe third one is soon to come.\n\nR b Witn an RL Accent\nGreedy in the limit with infinite exploration and stochastic approx. theory\n\nGreedy in the limit with infinite exploration (GLIE) is a set of requirements that on-policy RL\nalgorithms, such as Monte Carlo control and SARSA, must meet to guarantee convergence to\nthe optimal policy. The requirements are as follows:\n\n+ All state-action pairs must be explored infinitely often.\n+ The policy must converge on a greedy policy.\n\nWhat this means in practice is that an epsilon-greedy exploration strategy, for instance, must\nslowly decay epsilon towards zero. If it goes down too quickly, the first condition may not be\nmet; if it decays too slowly, well, it takes longer to converge.\n\nNotice that for off-policy RL algorithms, such as Q-learning, the only requirement of these\ntwo that holds is the first one. The second one is no longer a requirement because in off-\npolicy learning, the policy learned about is different than the policy we're sampling actions\nfrom. Q-learning, for instance, only requires all state-action pairs to be updated sufficiently,\nand that\u2019s covered by the first condition in this section.\n\nNow, whether you can check off with certainty that requirement using simple exploration\nstrategies such as epsilon-greedy, that\u2019s another question. In simple grid worlds and discrete\naction and state spaces, epsilon-greedy most likely works. But, it\u2019s easy to imagine intricate\nenvironments that would require more than random behavior.\n\nThere is another set of requirements for general convergence based on stochastic approx-\nimation theory that applies to all of these methods. Because we're learning from samples,\nand samples have some variance, the estimates won't converge unless we also push the\nlearning rate, alpha, towards zero:\n\n+ The sum of learning rates must be infinite.\n+ The sum of squares of learning rates must be finite.\n\nThat means you must pick a learning rate that decays but never reaches zero. For instance, if\nyou use 1/t or 1/e, the learning rate is initially large enough to ensure the algorithm doesn't\nfollow only a single sample too tightly, but becomes small enough to ensure it finds the sig-\nnal behind the noise.\n\nAlso, even though these convergence properties are useful to know for developing the\ntheory of RL algorithms, in practice, learning rates are commonly set to a small-enough\nconstant, depending on the problem. Also, know that a small constant is better for non-\nstationary environments, which are common in the real world.\n\nR b Wit an RL Accent\nOn-policy vs. off-policy learning\n\nOn-policy learning: Refers to methods that attempt to evaluate or improve the policy used\nto make decisions. It is straightforward; think about a single policy. This policy generates\nbehavior. Your agent evaluates that behavior and select areas of improvement based on\nthose estimates. Your agent learns to assess and improve the same policy it uses for generat-\ning the data.\n\nOff-policy learning: Refers to methods that attempt to evaluate or improve a policy differ-\nent from the one used to generate the data. This one is more complex. Think about two poli-\ncies. One produces the data, the experiences, the behavior; but your agent uses that data to\nevaluate, improve, and overall learn about a different policy, a different behavior. Your agent\nlearns to assess and improve a policy different than the one used for generating the data.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.13,
                        "section_name": "Double Q-learning: A max of estimates for an estimate of a max",
                        "section_path": "./screenshots-images-2/chapter_6/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_13/e011e674-c70f-4eed-8bf7-43691a87de37.png",
                            "./screenshots-images-2/chapter_6/section_13/bb85f745-836e-4aa4-973b-e70a7e03667f.png",
                            "./screenshots-images-2/chapter_6/section_13/27c1ffc8-bb77-4810-8a6d-23aa0d9e6e8a.png",
                            "./screenshots-images-2/chapter_6/section_13/df7f9116-175f-4045-9f79-7fb15ccb94ed.png",
                            "./screenshots-images-2/chapter_6/section_13/29083427-d8ef-44eb-ab36-cbbf80715133.png",
                            "./screenshots-images-2/chapter_6/section_13/8320db3c-62f4-423c-ada4-16299bd38d2e.png",
                            "./screenshots-images-2/chapter_6/section_13/f312cc3d-6543-4c65-8ca4-93762679f62c.png",
                            "./screenshots-images-2/chapter_6/section_13/e2b5cc5a-91b1-4228-b543-9202380f485d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Double Q-learning: A max of estimates for an estimate of a max\n\nQ-learning often overestimates the value function. Think about this. On every step, we take\nthe maximum over the estimates of the action-value function of the next state. But what we\nneed is the actual value of the maximum action-value function of the next state. In other\nwords, we\u2019re using the maximum over merely estimates as an estimate of the maximum.\n\nDoing this isn\u2019t only an inaccurate way of estimating the maximum value but also a more\nsignificant problem, given that these bootstrapping estimates, which are used to form TD\ntargets, are often biased. The use of a maximum of biased estimates as the estimate of the\nmaximum value is a problem known as maximization bias.\n\nIt\u2019s simple. Imagine an action-value function whose actual values are all zeros, but the\nestimates have bias: some positive, some negative: for example, 0.11, 0.65, \u20140.44, -0.26, and\nso on. We know the actual maximum of the values is zero, but the maximum over the esti-\nmates is 0.65. Now, if we sometimes pick a value with a positive bias and sometimes one with\na negative bias, then perhaps the issue wouldn\u2019t be as pronounced. But because we\u2019re always\ntaking a max, we always tend to take high values even if they have the largest bias, the biggest\nerror. Doing this over and over again compounds the errors in a negative way.\n\nWe all know someone with a positive-bias personality who has let something go wrong in\ntheir lives: someone who\u2019s blinded by shiny things that aren\u2019t as shiny. To me, this is one of\nthe reasons why many people advise against feeding the AI hype; because overestimation is\noften your enemy, and certainly something to mitigate for an improved performance.\n\nI Speak PYTHON\nThe double Q-learning agent 1/3\n\ndef double gq learning(env, \u00a2\u2014\u2014\u2014\u2014\u2014+() Rs you'd expect, double\ngamma=1.0, Q-learning takes the same\ninit_alpha=0.5, exact arguments as Q-Learning.\nmin_alpha=0.01,\nalpha_decay ratio=-0.5,\ninit_epsilon=1.0,\nmin_epsilon=0.1,\nepsilon decay ratio=0.9,\nn_episodes=3000) :\n\n@ we start with the same old handy variables.\n\nL nS, nA = env.observation_space.n, env.action_space.n\npi_track = []\n\n@) But immediately you should see a big difference here. We're using two state-value\nfunctions Ql and Qa. You can think of this similar to cross-validation: one Q-function estimates\nwill help us validate the other @-function estimates. The issue, though, is now we're splitting\nthe experience between two separate functions. This somewhat slows down training.\n\nQl = np.zeros((nS, nA), dtype=np.float6\u00e94)\n\nQ2 = np.zeros((nS, nA), dtype=np.float\u00e964)\n\nQ _trackl = np.zeros((n_episodes, nS, nA), dtype=np.float\u00e94)\n\nQ _track2 = np.zeros((n_episodes, nS, nA), dtype=np.float\u00e94)\n\n) The rest on this\nal select_action = lambda state, Q, epsilon: \\ page is pretty\n\nnp.argmax(Q[state]) \\ straightforward, and\nif np.random.random() > epsilon \\ you should already\nelse np.random.randint (len(Q[state])) Know what's\nalphas = decay schedule(init_alpha, select_action, alphas,\nmin_alpha, and epsilons are\nalpha_decay ratio, calculated the same\nn_episodes) way as before.\n\nepsilons = decay schedule(init_ epsilon,\nmin_epsilon,\nepsilon decay ratio,\nn_episodes)\n\n~ (S) Continues...\nfor e in tqdm(range(n_ episodes), leave=False) : ]\n\n| Speak PYTHON\nThe double Q-learning agent 2/3\n\n(\u00a9) From the previous page\n\nTeor e in tqdm(range(n_ episodes), leave=False) :\n\n> we're bask inside the episode loop. +\n\n@\u00ae For every new episode, we start by resetting the environment and getting an initial state.\nstate, done = env.reset(), False\n\nwhile not done: \u2014]\n\n(9) Then we repeat until we hit a terminal state (and the done flag is set to True).\nGo) For every step we select an action using our select_action Function.\n\naction = select_action(state,\n(Ql + Q2)/2.,\nepsilons([e])\n\nGD But notice something interesting: we're using the mean of our two Q-Functions!! We\ncould also use the sum of our Q-Functions here. They'll give similar results.\n\nCc next_state, reward, done, _ = env.step(action)\n\nGa) we then send the action to the environment and get the experience tuple.\n(13) Things start changing now. Notice we Flip a coin to determine an update to Qi or Qa.\n\nif np.random.randint (2) :\nargmax_Q1 = np.argmax(Ql[next_state])\n\nG4) we use the action QI thinks is best... HT\nGis) .. . but get the value From @a to calculate the TD target.\n\ntd_target = reward + gamma * \\\nQ2[next_state] [argmax_Q1] * (not done)\n\n(ic) Notice here, we get the value from @a and prescribed by gi. >\nGD Then calculate the TD error From the Qi estimate. FJ,\n\ntd_error = td target - Q1l[state] [action]\n\nG8) Finally, move our estimate closer to that target by using the error.\n\nL_\u00a7__g, Ql [state] [action] = Ql[state] [action] + \\\n\nalphas[e] * td_error\n\nto rietne repent on te epg.)\n\nI Speak PYTHON\nThe double Q-learning agent 3/3\n\n(@0) Okay. From the previous page, we were calculating Ql.\nQl[state] [action] = Ql[state] [action] + \\\nalphas[e] * td_error\n\nD) Now if the random int was 0 (So% of the times), we update the other Q-function, Qa.\n\nL_, else:\n\nargmax_Q2 = np.argmax(Q2[next_state]) 7\na) but, it\u2019s basically the mirror of the other update. We get the argmax of Qa.\n(3) Then use that action, but get the estimate from the other @-Function Ql.\ntd_target = reward + gamma * \\\nQl[next_state] [argmax_02] * (not done)\n\n(2A) raain, pay attention to the roles of @! and ga here reversed. >\n(s) we calculate the TD error from the Qa this time. ry\n\ntd_error = td_target - Q2[state] [action]\n\n(bw) And use it to update the Qa estimate of the state-action pair.\nQ2[state] [action] = Q2[state] [action] + \\\n\n_l alphas[e] * td_error\n(7D Notice how we use the \u2018alphas\u2019 vector.\n\u2014 \u00a9\u00ae) we change the valu of the state\n\nstate = next_state variable and keep looping, again until\nwe land on a terminal state and the\n\u2018done\u2019 variable is set to True.\nQ_trackl[e] = Ql 4\u00a2\u2014\u2014\u2014\u2014+ G9)Here we store Qi and Qa for\nQ track2[e] Q2 offline analysis.\npi_track.append(np.argmax((Q1 + Q2)/2., axis=1))\n\ntu @0) Notice the policy is the argmax of\nthe mean of Qi and Qa.\nQ (Ql + Q2)/2. \u00a2\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014F GI The Final @ is the mean.\nV = np.max(Q, axis=1) \u00a2\u2014\u2014\u2014\u2014+4 G@ The Final vis the max of @.\npi = lambda s: {s:a for s, a in enumerate( \\\nnp.argmax(Q, axis=1))}[s]\n2) The Final policy is the argmax of the mean of Qs.\n4) We end up returning all this.\nT return Q, V, pi, (Q_trackl + Q track2)/2., pi_track\n\nOne way of dealing with maximization bias is to track estimates in two Q-functions. At each\ntime step, we choose one of them to determine the action, to determine which estimate is the\nhighest according to that Q-function. But, then we use the other Q-function to obtain that\naction\u2019s estimate. By doing this, there\u2019s a lower chance of always having a positive bias error.\nThen, to select an action for interacting with the environment, we use the average, or the sum,\nacross the two Q-functions for that state. That is, the maximum over Q.(S,,,)+Q,(S,_,), for\ninstance. The technique of using these two Q-functions is called double learning, and the algo-\nrithm that implements this technique is called double Q-learning. In a few chapters, you'll learn\nabout a deep reinforcement learning algorithm called double deep Q-networks (DDQN),\nwhich uses a variant of this double learning technique.\n\nIt\u2019s IN THE DeTAILs\nFVMC, SARSA, Q-learning, and double Q-learning on the SWS environment\n\nLet\u2019s put it all together and test all the algorithms we just learned about in the Slippery Walk\nSeven environment.\n\nSo you're aware, | used the same hyperparameters in all algorithms, the same gamma,\nalpha, epsilon, and respective decaying schedules. Remember, if you don\u2019t decay alpha\ntoward 0, the algorithm doesn\u2019t fully converge. I'm decaying it to 0.01, which is good\nenough for this simple envi-\nronment. Epsilon should Alpha and epsilon schedules\nalso be decayed to zero for \u201c = Fpwton credo\nfull convergence, but in\npractice this is rarely done.\nIn fact, often state-of-the-\nart implementations don't\neven decay epsilon and use\na constant value instead.\nHere, we're decaying to 0.1.\n\nAnother thing: note that\nin these runs, | set the same \u00b0 - # e s \u00a3 -\nnumber of episodes for all Episodes\nalgorithms; they all run\n3,000 episodes in the SWS environment. You'll notice some algorithms don\u2019t converge in this\nmany steps, but that doesn\u2019t mean they wouldn't converge at all. Also, some of the other\nenvironments in the chapter\u2019s Notebook, such as Frozen Lake, terminate on a set number of\nsteps, that is, your agent has 100 steps to complete each episode, else it\u2019s given a done flag.\nThis is somewhat of an issue that we'll address in later chapters. But, please, go to the\nNotebook and have fun! | think you'll enjoy playing around in there.\n\ns \u00b0 \u00b0\n> a @\n\nHyperparameter values\n\ns\nS\n\n2\n\u00b0\n\nTatty it Up\nSimilar trends among bootstrapping and on-policy methods\n\n@ This First one is First-visit FVMC estimates through time vs. true values\nMonte Carlo control. See\nhow the estimates have\nhigh variance, as in the\nprediction algorithm. Also, all\nthese algorithms are using\nthe same action-selection\nstrategy. The only\ndifference is the method \u201cTt \u201cwwe\nused in the policy-evaluation\nphase! Cool, right!?\n\n) SARSA is an on-policy\nbootstrapping method;\nMC is on-policy, but not\nbootstrapping. In these\nexperiments, you can see\nhow SARSA has less variance\nthan mc, yet it takes pretty t sb de he\nmuch the same amount of Episodes\n\ntime to get +o the optimal te Q-Learning estimates through time vs. true values \u00abi\n\nState-value function\n\neen asco 3000\n\n1509\nEpisodes\n\nSarsa estimates through time vs. true values\n\nState-value function\n\n000 asco 3000\n\nPal\n\nH\n(21\n\n@ @-learning is an\noFf-policy bootstrapping\nmethod, See how much\nfaster the estimates track\nthe true values. But, also, oz\nnotice how the estimates\n\nare often higher and jump o soo od cas 200 3000\naround somewhat\n\nogpyressively,\n\n@) Double Q-learning, on\nthe other hand, is slightly\nslower than Q-learning to\nget the estimates to track\nthe optimal state-value\nfunction, but it does so in a.\nmuch more stable manner.\nThere\u2019s still a bit of over\u2014\nestimation, but it\u2019s controlled.\n\non wil)\n\nState-value function\n\nDouble Q-Learning estimates through time vs. true values\n\nState-value function\n\n\u2014 ws)\n\u2014- way\na)\n-- way\n\u2014w)\n\n\u2014 v5)\n\u2014- way\n\n~ wy\na,\n\u2014w\n\n\u2014 \u201c3\n\u2014~- way\n\n~ wy\n~-- wa)\n\u2014 wy)\n\n\u2014 ws)\n\u2014- wa\n~~ wa\n--- wa)\n\u2014 wy\n\nTatty it Up\n\nExamining the policies learned in the SWS environment\n\n@ Here are a few\ninteresting plots to\nunderstand the i\nRemember From the last\npage that Q-Learning\nreaches the optimal values\n$irst, but it overshoots.\nWell, how does that\ntranslate in terms of\nsuccess? In this plot, you\ncan see how double\nQ-learning gets to 100%\nsuccess rate earlier than\nQ-learning, Note, | define\nsuccess as reaching o\n\u201cgoal state,\u201d which in SWS\nis the rightmost cell.\n\n(@ How about the mean\nwhile training? How does\ntheir performance track\nan agent that would Follow\n\nSuccess rate %\n\nReturn (Gt:T)\n\naveraged over five random\nseeds; they're noisy, but\nthe trends should hold.\n\n@) Finally, we can look at\na moving average of the\nregret, which again is the\ndifference from optimal,\nhow much reward the\nagent left on the table\nGuhile learning, so perhaps,\ndouble Q-learning shows\nthe best performance.\n\nPolicy success rate (ma 100)\n\n\u2014.\n\n\u2014\u2014 ryeMc\n\u2014-- Sansa\nLearning\n\u2014\u2014 Double Q-Learning\n\n# \u00a3 es ae\nEpisodes\nPolicy episode return (ma 100) ne\n\n\u2014 rec\no> Sansa\n\nQLearning\n\u2014\u2014 Double Q-Learning\n\nFSF FSF\n\nEpisodes\nPolicy episode regret (ma 100)\n\n\u2014 Fyac\n---- Sarsa\nQ-Learning\n\u2014\u2014 Double Q-Learning\n\nEpisodes\n\nTatty it Up\nExamining the value functions learned in the SWS environment\n\n@ These are also Estimated expected return (ma 100)\ninteresting plots. I'm\nshowing the moving\naverage over 100 episodes\nof the estimated\nexpected return: that is,\nhow much the\n\nexpects to get For a Full\nepisode (From an initia! to\na.terminal state) versus\nhow much the agent\nshould expect to get, given \u00b0 \u00a3 ef \u00a3f \u00a3\u00a3 gS\nthe optimal v-function of seed\n\nthe initial state. State-value function estimation error (ma 100)\n\n@) In this next plot, we're _ | = we\nlooking at the state-value =?) OLeamning\nfunction, the v-function, Go25 | \\\\ ee\nestimation error. This is \\\nthe Mean absolute error\nacross all estimates from 2\ntheir respective optimal. 2).\nTake a look at how 2\n\n5\n\n=\n\nwe(a)\n\n\u2014 hvac\n-~ Sansa\nre QLearning\n' \u2014\u2014 Double O-Learning\n\nEstimated value of initial state V(4)\n\nError Mi\n\u00b0\nwe\nS\n\ndouble Q-learning gets - s - eo = - s\n+o the lowest error First. Episodes\n\nSARSA and FYMC are Action-value function estimation error (ma 100)\ncomparable in this simple 935 \u2014 Fac\n\n& 0.30 fn i staan\n\n\u00a2g \\ \u2014\u2014 Double Q-Learning\n\nthe optimal, while here, \u2018ay \u00b0 \u00a3 - ra s e s\nVm calculating the mae Episodes\nacross all actions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.14,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_6/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_14/e9512eca-8c7e-4957-9676-09708d7202a2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter, you put everything you've learned so far into practice. We learned about\nalgorithms that optimize policies through trial-and-error learning. These algorithms learn\nfrom feedback that\u2019s simultaneously sequential and evaluative; that is, these agents learn to\nsimultaneously balance immediate and long-term goals and the gathering and utilization of\ninformation. But unlike in the previous chapter, in which we restricted our agents to solving\nthe prediction problem, in this chapter, our agents learned to solve the control problem.\n\nThere are many essential concepts you learned about in this chapter. You learned that the\nprediction problem consists of evaluation policies, while the control problem consists of\noptimizing policies. You learned that the solutions to the prediction problem are in policy\nevaluation methods, such as those learned about in the previous chapter. But unexpectedly,\nthe control problem isn\u2019t solved alone by policy-improvement methods you've learned in the\npast. Instead, to solve the control problem, we need to use policy-evaluation methods that\ncan learn to estimate action-value functions merely from samples and policy-improvement\nmethods that take into account the need for exploration.\n\nThe key takeaway from this chapter is the generalized policy-iteration pattern (GPI) which\nconsists of the interaction between policy-evaluation and policy-improvement methods.\nWhile policy evaluation makes the value function consistent with the policy evaluated, policy\nimprovement reverses this consistency but produces a better policy. GPI tells us that by hav-\ning these two processes interact, we iteratively produce better and better policies until con-\nvergence to optimal policies and value functions. The theory of reinforcement learning\nsupports this pattern and tells us that, indeed, we can find optimal policies and value func-\ntions in the discrete state and action spaces with only a few requirements. You learned that\nGLIE and stochastic approximation theories apply at different levels to RL algorithms.\n\nYou learned about many other things, from on-policy to off-policy methods, from online\nto offline, and more. Double Q-learning and double learning, in general, are essential tech-\nniques that we build on later. In the next chapter, we examine advanced methods for solving\nthe control problem. As environments get challenging, we use other techniques to learn opti-\nmal policies. Next, we look at methods that are more effective in solving environments, and\nthey do so more efficiently, too. That is, they solve these environments, and do so using fewer\nexperience samples than methods we learned about in this chapter.\n\nBy now, you\n\n+ Know that most RL agents follow a pattern known as generalized policy iteration\n+ Know that GPI solves the control problem with policy evaluation and improvement\n\n+ Learned about several agents that follow the GPI pattern to solve the control problem\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 7,
                "chapter_name": "Achieving goals more\neffectively and efficiently",
                "chapter_path": "./screenshots-images-2/chapter_7",
                "sections": [
                    {
                        "section_id": 7.1,
                        "section_name": "Achieving goals more\neffectively and efficiently",
                        "section_path": "./screenshots-images-2/chapter_7/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_1/bfee108b-0f4f-4540-ab42-98064eccb3b9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In this chapter, we improve on the agents you learned about in the previous chapter. More\nspecifically, we take on two separate lines of improvement. First, we use the A-return that you\nlearned about in chapter 5 for the policy evaluation requirements of the generalized policy\niteration pattern. We explore using the A-return for both on-policy and off-policy methods.\nUsing the A-return with eligibility traces propagates credit to the right state-action pairs more\nquickly than standard methods, making the value-function estimates get near the actual val-\nues faster.\n\nSecond, we explore algorithms that use experience samples to learn a model of the envi-\nronment, a Markov decision process (MDP). By doing so, these methods extract the most out\nof the data they collect and often arrive at optimality more quickly than methods that don\u2019t.\nThe group of algorithms that attempt to learn a model of the environment is referred to as\nmodel-based reinforcement learning.\n\nIt\u2019s important to note that even though we explore these lines of improvements separately,\nnothing prevents you from trying to combine them, and it\u2019s perhaps something you should\ndo after finishing this chapter. Let\u2019s get to the details right away.\n\nRE Wr an RL Accent\nPlanning vs. model-free RL vs. model-based RL\n\nPlanning: Refers to algorithms that require a model of the environment to produce a policy.\nPlanning methods can be of state-space planning type, which means they use the state\nspace to find a policy, or they can be of plan-space planning type, meaning they search in the\nspace of all possible plans (think about genetic algorithms.) Examples of planning algorithms\nthat we've learned about in this book are value iteration and policy iteration.\n\nModel-free RL: Refers to algorithms that don\u2019t use models of the environments, but are still\nable to produce a policy. The unique characteristic here is these methods obtain policies\nwithout the use of a map, a model, or an MDP. Instead, they use trial-and-error learning to\nobtain policies. Several examples of model-free RL algorithms that we have explored in this\nbook are MC, SARSA, and Q-learning.\n\nModel-based RL: Refers to algorithms that can learn, but don\u2019t require, a model of the envi-\nronment to produce a policy. The distinction is they don\u2019t require models in advance, but can\ncertainly make good use of them if available, and more importantly, attempt to learn the\nmodels through interaction with the environment. Several examples of model-based RL\nalgorithms we learn about in this chapter are Dyna-Q and trajectory sampling.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.2,
                        "section_name": "Learning to improve policies\nusing robust targets",
                        "section_path": "./screenshots-images-2/chapter_7/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_2/a0380d90-f3f6-4a91-8007-583879455487.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning to improve policies\nusing robust targets\n\nThe first line of improvement we discuss in this chapter is using more robust targets in our\npolicy-evaluation methods. Recall that in chapter 5, we explored policy-evaluation methods\nthat use different kinds of targets for estimating value functions. You learned about the Monte\nCarlo and TD approaches, but also about a target called the A-return that uses a weighted\ncombination of targets obtained using all visited states.\n\nTD(A) is the prediction method that uses the A-return for our policy evaluation needs.\nHowever, as you remember from the previous chapter, when dealing with the control prob-\nlem, we need to use a policy-evaluation method for estimating action-value functions, and a\npolicy-improvement method that allows for exploration. In this section, we discuss control\nmethods similar to SARSA and Q-learning, but use instead the A-return.\n\nConcrete EXAMPLE\nThe slippery walk seven environment\n\nTo introduce the algorithms in this chapter, we use the same environment we used in the\nprevious chapter, called slippery walk seven (SWS). However, at the end of the chapter, we\ntest the methods in much more challenging environments.\n\nRecall that the SWS is a walk, a single-row grid-world environment, with seven non-\nterminal states. Remember that this environment is a \u201cslippery\u201d walk, meaning that it\u2019s noisy,\nthat action effects are stochastic. If the agent chooses to go left, there\u2019s a chance it does, but\nthere\u2019s also some chance that it goes right, or that it stays in place.\n\nSlippery walk seven environment MDP\n@ Same environment as in the previous chapter\n\nAs a refresher, above is the MDP of this environments. But remember and always have in\nmind that the agent doesn't have any access to the transition probabilities. The dynamics of\nthis environment are unknown to the agent. Also, to the agent, there are no relationships\nbetween the states in advance.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.3,
                        "section_name": "SARSA(A): Improving policies after each step\nbased on multi-step estimates",
                        "section_path": "./screenshots-images-2/chapter_7/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_3/3c8e34e9-98b1-4779-86bd-92d4bbe5d571.png",
                            "./screenshots-images-2/chapter_7/section_3/3315f561-0ded-4308-b011-0dff149045e7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "SARSA(A): Improving policies after each step\nbased on multi-step estimates\n\nSARSA(A) is a straightforward improvement to the original SARSA agent. The main differ-\nence between SARSA and SARSA(A) is that instead of using the one-step bootstrapping\ntarget\u2014the TD target, as we do in SARSA, in SARSA(A), we use the A-return. And that\u2019s it;\nyou have SARSA(A.). Seriously! Did you see how learning the basics makes the more complex\nconcepts easier?\n\nNow, I'd like to dig a little deeper into the concept of eligibility traces that you first read\nabout in chapter 5. The type of eligibility trace I introduced in chapter 5 is called the accumu-\nlating trace. However, in reality, there are multiple ways of tracing state or state-action pairs\nresponsible for a reward.\n\nIn this section, we dig deeper into the accumulating trace and adapt it for solving the con-\ntrol problem, but we also explore a different kind of trace called the replacing trace and use\nthem both in the SARSA(A) agent.\n\nA Bit oF History\n0001\nIntroduction of the SARSA and SARSA(A) agents\n\nIn 1994, Gavin Rummery and Mahesan Niranjan published a paper titled \u201cOnline Q-Learning\nUsing Connectionist Systems,\u2019 in which they introduced an algorithm they called at the time\n\u201cModified Connectionist Q-Learning.\u201d In 1996, Singh and Sutton dubbed this algorithm\nSARSA because of the quintuple of events that the algorithm uses: (S,, A,, R,,,, S,,,, A,,,)-\nPeople often like knowing where these names come from, and as you'll soon see, RL research-\ners can get pretty creative with these names.\n\nFunny enough, before this open and \u201cunauthorized\u201d rename of the algorithm, in 1995 in\nhis PhD thesis titled \u201cProblem Solving with Reinforcement Learning,\u2019 Gavin issued Sutton an\napology for continuing to use the name \u201cModified Q-Learning\u201d despite Sutton\u2019s preference\nfor\u201cSARSA.\" Sutton also continued to use SARSA, which is ultimately the name that stuck with\nthe algorithm in the RL community. By the way, Gavin's thesis also introduced the SARSA(A)\nagent.\n\nAfter obtaining his PhD in 1995, Gavin became a programmer and later a lead program-\nmer for the company responsible for the series of the Tomb Raider games. Gavin has had a\nsuccessful career as a game developer.\n\nMahesan, who became Gavin's PhD supervisor after the unexpected death of Gavin's orig-\ninal supervisor, followed a more traditional academic career holding lecturer and professor\nroles ever since his graduation in 1990.\n\nFor adapting the accumulating trace to solving the control problem, the only necessary\nchange is that we must now track the visited state-action pairs, instead of visited states. Instead\nof using an eligibility vector for tracking visited states, we use an eligibility matrix for tracking\nvisited state-action pairs.\n\nThe replace-trace mechanism is also straightforward. It consists of clipping eligibility\ntraces to a maximum value of one; that is, instead of accumulating eligibility without bound,\nwe allow traces to only grow to one. This strategy has the advantage that if your agents get\nstuck in a loop, the traces still don\u2019t grow out of proportion. The bottom line is that traces, in\nthe replace-trace strategy, are set to one when a state-action pair is visited, and decay based\non the A value just like in the accumulate-trace strategy.\n\nA Bit oF History\n0001\nIntroduction of the eligibility trace mechanism\n\nThe general idea of an eligibility trace mechanism is probably due to A. Harry Klopf, when, in\na 1972 paper titled \u201cBrain Function and Adaptive Systems\u2014A Heterostatic Theory,\u201d he\ndescribed how synapses would become \u201celigible\u201d for changes after reinforcing events. He\nhypothesized: \u201cWhen a neuron fires, all of its excitatory and inhibitory synapses that were\nactive during the summation of potentials leading to the response are eligible to undergo\nchanges in their transmittances.\u201d\n\nHowever, in the context of RL, Richard Sutton\u2019s PhD thesis (1984) introduced the mecha-\nnism of eligibility traces. More concretely, he introduced the accumulating trace that you've\nlearned about in this book, also known as the conventional accumulating trace.\n\nThe replacing trace, on the other hand, was introduced by Satinder Singh and Richard\nSutton in a 1996 paper titled \u201cReinforcement Learning with Replacing Eligibility Traces,\u2019 which\nwe discuss in this chapter.\n\nThey found a few interesting facts. First, they found that the replace-trace mechanism\nresults in faster and more reliable learning than the accumulate-trace one. They also found\nthat the accumulate-trace mechanism is biased, while the replace-trace one is unbiased. But\nmore interestingly, they found relationships between TD(1), MC, and eligibility traces.\n\nMore concretely, they found that TD(1) with replacing traces is related to first-visit MC and\nthat TD(1) with accumulating traces is related to every-visit MC. Moreover, they found that\nthe offline version of the replace-trace TD(1) is identical to first-visit MC. It\u2019s a small world!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.4,
                        "section_name": "Accumulating traces in the SWS environment",
                        "section_path": "./screenshots-images-2/chapter_7/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_4/f06db6ff-8b13-4d62-94de-fc7e1bbe3d84.png",
                            "./screenshots-images-2/chapter_7/section_4/6f866aca-3236-47e8-9be0-3a5633c18179.png",
                            "./screenshots-images-2/chapter_7/section_4/be2de699-5dc8-46b4-aa0b-77686eaed578.png",
                            "./screenshots-images-2/chapter_7/section_4/a0d3a9fb-d971-4e6a-aaf6-99fe88997dfd.png",
                            "./screenshots-images-2/chapter_7/section_4/02b124c4-3163-45bb-94db-f8f452087206.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Accumulating traces in the SWS environment\n\n| Gamma=0.9, Lambda=0.5\ncohelate the cH] tT | let | ft fe!\na ee ee\netunctona \u2014?Lo\u00b0letetetetete te]e |\neee, LLU TT\nandalambda = \u00ab(oT [>[>[=>] [||\n~ >LeLtetTet et: te telte te |\ney | TT sy Tt Te]\na ee ce ee\n\u2014 >i o 0 0 0 045 1 o 0 0\ney PT TT TT fee\ne Lede |e |e foo] o fe |e |\n\u2014 o o o o 0.09113 | 0.2025 | 0.45 1 0\nTD error =1, alpha=0.1\no pee fe fe fowef oe fe fo To |\n\u2014 Le Le Te [oe Jovossfosns] oo | or | o_|\n\n<> Bot Down\nae Frequency and recency heuristics in the accumulating-trace mechanism\n\nThe accumulating trace combines a frequency and a recency heuristic. When your agent tries\na state-action pair, the trace for this pair is incremented by one. Now, imagine there's a loop\nin the environment, and the agent tries the same state-action pair several times. Should we\nmake this state-action pair \u201cmore\u201d responsible for rewards obtained in the future, or should\nwe make it just responsible?\n\nAccumulating traces allow trace values higher than one while replacing traces don't.\nTraces have a way for combining frequency (how often you try a state-action pair) and\nrecency (how long ago you tried a state-action pair) heuristics implicitly encoded in the trace\n\nmechanism.\n\n@ Assume the Following\ninteractions occurred\nwith the environment.\nHere the agent chose\nleft but the environment\nKept the agent ot the\nsame stote.\n\ntT\n\n41@ These are the\n\neligibility traces for the\nactions. They're First set\n\ntol, then decayed at a\n\nrote of 0.5 * 0.9.\n\n(4) Assume there are\nmore interactions.\n\n(S) Assume we reached\nthe end of an episode\nand never found a\nreward until the end.\nWe calculate Q with\nthese values.\n\n(&) Can recover\nadditional info with what\n1 provide?\n\nE\n\nE\n\nE\n\nE\n\nQ\n\nReplacing traces in the SWS environment\n\ni @ Same trajectory as before, we only change\nthe eligibility trace &, and Q-function @.\nGamma=0.9, Lambda=0.5\n\n4 7\nae} Ty tet | tT te\n(@ Here the\ne 0 0 0 0 1 0 0 0 0 replacing traces\n>Leofetofofofo]ofo]o} vn acter\non the other actions\n7) 7 of the same state.\nA\ntrace got replaced\nby the Right trace.\neLeteototoftolto|olfolfo) T\n>Lefofotoft: fo} ofofo|\n5 7\nDRS\n1\n\n\u00ab[e}eo}o| ooo }ototo|\n*Leofto}o|o fos}: }oteoloe|\n\n[) 2 3 4 5 6 7 8\n|G\n\n(2) In the end, we\n\nzz fo | o fo | | o | o | sogetadtterent\nQ-function, perhaps\n| \u00a9 | o | o | o [ooons}ozs] oa |: | O | care rensonaiie\n\nTD error =1, alpha=0.1 edvontages of\nir OUT).\n\neLefeoltotololo}lo}o}o| fj\n\nLo fo |e | 0 fooosifooas] oo] or | 0 |\n\nt\n\n@) again, can you recover the sequence of actions the agent took? It shouldn't be too difficult.\n\nLT\n\nI Speak PYTHON\n\nThe SARSA(A) agent 1/2\ndef sarsa_lambda (env, \u2014\u2014\u2014_ ) The sarsa lambda agent\ngamma=1.0, is a mix of the SARSA and\ninit_alpha=0.5, the TD lambda methods.\n\nmin_alpha=0.01,\n\nalpha_decay ratio=-0.5,\n\ninit_epsilon=1.0,\n\nmin_epsilon=0.1,\n\nepsilon decay ratio=0.9, @ Here\u2019s the lambda_\nlambda_=0.5, + _ huperparameter (ending in _\n\nreplacing traces=True, because the word lambda. is\nn_episodes=3000) : reserved in Python).\n@) The replacing_traces variable sets the algorithm to use replacing or accumulating traces.\nnS, nA = env.observation_space.n, env.action_space.n\nr pi_track = []\n@) we use the usual variables as we have before . . .\nQ = np.zeros((nS, nA), dtype=np.float64)\nQ track = np.zeros((n_episodes, nS, nA),\ndtype=np. float64)\n@ These are the eligibility traces that will allow us to keep track of states eligible for updates.\nLB E = np.zeros((nS, nA), dtype=np.float64)\n\n- ol 1 O The rest is the\nselect_action = lambda state, Q, epsilon: \\ same as before\nnp.argmax(Q[state]) \\ with the select_\nif np.random.random() > epsilon \\ action function,\nelse np.random.randint (len(Q[state] )) and the vectors\nalphas and\nalphas = decay schedule ( epsilons.\n\ninit_alpha, min_alpha,\nalpha_decay ratio, n_episodes)\n\nepsilons = decay schedule (\ninit_epsilon, min_epsilon,\n\n: - \u2018 (@ We continue\nepsilon decay ratio, n_episodes)\n\non the next\npage with this\n\nfor e in tqdm(range(n_ episodes), leave=False): line.\n\n| Speak PyTHoN\nThe SARSA(A) agent 2/2\n(9) Continues here...\nTL\u00bb for e in tqdm(range(n_ episodes), leave=False) :\n$1 G0) every new episode, we set the eligibility of every state to zero.\nE.fi11(0) ae\n\nstate, done = env.reset(), False\n| md action = select_action(state, Q, epsilons[e])\n\nGia) we select the action of the initial state.\nwhile not done: \u00a2\u2014\u2014+ (3) we enter the interaction loop.\n\nG4) we send the action to the environment and receive the experience tuple. Fy\n\nLi, next_state, reward, done, _ = env.step(action)\nind next_action = select_action(next_state,\n\nQ\nGs) We select the action to use at the next state using epsilons[e})\n\nthe Q-table and the epsilon corresponding to this episode.\ntd_target = reward + gamma * \\\n\nQ[next_state] [next_action] * (not done)\ntd_error = td target - Q[state] [action]\nie) We calculate the TO target and the TD error as in the original SARSA.\nGD Then, we increment the state-action pair trace, and clip it to | if i's a replacing trace.\nE[state] [action] = E[state] [action] + 1\nif replacing traces: E.clip(0, 1, out=E)\nG8) And notice this! We're applying the TD error to all eligible state-action pairs at once. Even\nthough we're using the entire Q-table, will be mostly 0, and greater than zero for eligible pairs.\n\nQ + alphas[e] * td_error * E\ngamma * lambda_ * E \u00a2\u2014\u2014+ (19) we decay the eligibilities.\n\nE\n\nstate, action = next_state, next_action\n(20) update the variables.\nQ track[e] = Q \u2014\\\u2014\u2014 Gai) save @ and pi.\npi_track.append(np.argmax(Q, axis=1))\n~\u2014\u2014\u2014______._ (a) At the end of training we extract V, pi, and return.\nV = np.max(Q, axis=1)\npi = lambda s: {s:a for s, a in enumerate (\\\nnp.argmax(Q, axis=1))}[s]\nreturn Q, V, pi, Q track, pi_track\n\nMicuet\u2019s ANALOGY\neS\n\nAccumulating and replacing traces, and a gluten- and banana-free diet\n\nA few months back, my daughter was having trouble sleeping at night. Every night, she\nwould wake up multiple times, crying very loudly, but unfortunately, not telling us what the\nproblem was.\n\nAfter a few nights, my wife and | decided to do something about it and try to \u201ctrace\u201d back\nthe issue so that we could more effectively \u201cassign credit\u201d to what was causing the sleepless\nnights.\n\nWe put on our detective hats (if you're a parent, you know what this is like) and tried many\nthings to diagnose the problem. After a week or so, we narrowed the issue to foods; we knew\nthe bad nights were happening when she ate certain foods, but we couldn't determine which\nfoods exactly were to blame. | noticed that throughout the day, she would eat lots of carbs\nwith gluten, such as cereal, pasta, crackers, and bread. And, close to bedtime, she would\nsnack on fruits.\n\nAn \u201caccumulating trace\u201d in my brain pointed to the carbs. \u201cOf course!\u201d | thought, \u201cGluten is\nevil; we all know that. Plus, she is eating all that gluten throughout the day.\u201d If we trace back\nand accumulate the number of times she ate gluten, gluten was clearly eligible, was clearly\nto blame, so we did remove the gluten.\n\nBut, to our surprise, the issue only subsided, it didn\u2019t entirely disappear as we hoped. After\na few days, my wife remembered she had trouble eating bananas at night when she was a\nkid. | couldn't believe it, | mean, bananas are fruits, and fruits are only good for you, right? But\nfunny enough, in the end, removing bananas got rid of the bad nights. Hard to believe!\n\nBut, perhaps if | would've used a \u201creplacing trace\" instead of an \u201caccumulating trace,\u2019 all of\nthe carbs she ate multiple times throughout the day would have received a more conserva-\ntive amount of blame.\n\nInstead, because | was using an accumulating trace, it seemed to me that the many times\nshe ate gluten were to blame. Period. | couldn't see clearly that the recency of the bananas\nplayed a role.\n\nThe bottom line is that accumulating traces can \u201cexaggerate\u201d when confronted with fre-\nquency while replacing traces moderate the blame assigned to frequent events. This moder-\nation can help the more recent, but rare events surface and be taken into account.\n\nDon't make any conclusions, yet. Like everything in life, and in RL, it\u2019s vital for you to know\nthe tools and don't just dismiss things at first glance. I'm just showing you the available\noptions, but it\u2019s up to you to use the right tools to achieve your goals.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.5,
                        "section_name": "Watkins's Q(A): Decoupling behavior from learning, again",
                        "section_path": "./screenshots-images-2/chapter_7/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_5/9185a9eb-cfce-446e-a1bd-68b1173c88fb.png",
                            "./screenshots-images-2/chapter_7/section_5/eecbc3cd-0776-48b9-b27c-77b03fea620d.png",
                            "./screenshots-images-2/chapter_7/section_5/c523ff2e-871f-4fbc-b6fe-31ef3ad9aeee.png",
                            "./screenshots-images-2/chapter_7/section_5/6d3a35a6-6ec1-4078-ab92-d91a4db25e24.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Watkins's Q(A): Decoupling behavior from learning, again\n\nAnd, of course, there\u2019s an off-policy control version of the A algorithms. Q(A) is an extension\nof Q-learning that uses the A-return for policy-evaluation requirements of the generalized\npolicy-iteration pattern. Remember, the only change we\u2019re doing here is replacing the TD\ntarget for off-policy control (the one that uses the max over the action in the next state)\nwith a 4-return for off-policy control. There are two different ways to extend Q-learning to\neligibility traces, but, I\u2019m only introducing the original version, commonly referred to as\nWatkins\u2019s Q(X).\n\n0001 A Bit oF History\nIntroduction of the Q-learning and Q(A) agents\n\nIn 1989, the Q-learning and Q(A) methods were introduced by Chris Watkins in his PhD thesis\ntitled \u201cLearning from Delayed Rewards,\u2019 which was foundational to the development of the\ncurrent theory of reinforcement learning.\n\nQ-learning is still one of the most popular reinforcement learning algorithms, perhaps\nbecause it\u2019s simple and it works well. Q(A) is now referred to as Watkins's Q(A) because there's\na slightly different version of Q(A)\u2014due to Jing Peng and Ronald Williams\u2014that was worked\nbetween 1993 and 1996 (that version is referred to as Peng\u2019s Q(A).)\n\nIn 1992, Chris, along with Peter Dayan, published a paper titled \u201cTechnical Note Q-learning,\u2019\nin which they proved a convergence theorem for Q-learning. They showed that Q-learning\nconverges with probability 1 to the optimum action-value function, with the assumption\nthat all state-action pairs are repeatedly sampled and represented discretely.\n\nUnfortunately, Chris stopped doing RL research almost right after that. He went on to\nwork for hedge funds in London, then visited research labs, including a group led by Yann\nLeCun, always working Al-related problems, but not so much RL. For the past 22+ years, Chris\nhas been a Reader in Artificial Intelligence at the University of London.\n\nAfter finishing his 1991 PhD thesis titled \u201cReinforcing Connectionism: Learning the Statistical\nWay.\" (Yeah, connectionism is what they called neural networks back then\u2014\u201cdeep reinforce-\nment learning\u201d you say? Yep!) Peter went on a couple of postdocs, including one with Geoff\nHinton at the University of Toronto. Peter was a postdoc advisor to Demis Hassabis, cofounder\nof DeepMind. Peter has held many director positions at research labs, and the latest is the\nMax Planck Institute.\n\nSince 2018 he's been a Fellow of the Royal Society, one of the highest awards given in\nthe UK.\n\nI Speak PYTHON\nThe Watkins's Q(A) agent 1/3\n\ndef q_lambda (env, \u2014\u2014\u2014\u2014F 9 The @ lambda. agent is a\ngamma=1.0, mix between the\ninit_alpha=0.5, Q-learning and the TO\nmin_alpha=0.01, lambda. methods.\n\nalpha_decay ratio=-0.5,\ninit_epsilon=1.0,\nmin_epsilon=0.1,\nepsilon decay ratio=0.9,\nlambda_=0.5, \u2014\u2014\u2014F @ nere are the lambdo_\nreplacing traces=True, em and the replacing traces\nn_episodes=3000): hyperparameters.\n@ useful variables\n\nnS, nA = env.observation_space.n, env.action_space.n\npi_track = []\n\n@ The Q-table\n\nQ = np.zeros((nS, nA), dtype=np.float64)\nQ track = np.zeros((n_episodes, nS, nA), dtype=np.float\u00e94)\n\n\u00a9 The eligibility traces matrix For\nall state-action pairs\n\nE = np.zeros((nS, nA), dtype=np.float64)\n@) The usual\nselect_action = lambda state, Q, epsilon: \\ suspects\nnp.argmax(Q[state]) \\\nif np.random.random() > epsilon \\\nelse np.random.randint (len(Q[state]))\n\nalphas = decay schedule (\ninit_alpha, min_alpha,\nalpha_decay ratio, n_episodes)\n\nepsilons = decay schedule (\ninit_epsilon, min_epsilon,\nepsilon decay ratio, n_episodes)\n\niz @ To be continued...\n\nfor e in tqdm(range(n_ episodes), leave=False):\n\n1 Speak PYTHON\nThe Watkins's Q(A) agent 2/3\n\n@ Continues on the episodes loop +\u2014_\nfor e in tqdm(range(n episodes), leave=False) :\n\u00a9) Okay, Because Q lambda is an of{-policy method\nwe must use & with care. We're learning about the\n\n; greedy policy, but Following an exploratory poli\nE.f\u00a3111(0) chee oeiinntap ta eS\n\nG0) Reset the environment and done.\nL, state, done = env.reset(), False\n\nGD eut, notice how we are preselecting the action as in SARSA, but we didn\u2019t do that in\nQ-learning. This is because we need to check whether our next action is greedy!\n\nTL\u2014-y\u00bb action = select_action(state,\nQ,\nepsilons([e] )\n\nGa) enter the interaction loop.\n\nLy, while not done:\n(3) Step the environment and get the experience. \u2014\u2014\u2014\u2014\u2014\n\nnext_state, reward, done, _ = env.step(action)\n4) we select the next_action SarsA-style!\nnext_action = select_action(next_state,\nQ,\nepsilons[e])\nGs) And use it to verify that the action on the next step will still come from the greedy policy.\nnext_action_is greedy = \\\nQ[next_state] [next_action] == Q[next_state] .max()\nGe) On this step, we still calculate the TD target as in regular Q-learning, using the max.\ntd_target = reward + gamma * \\\n\n(7) And use the TD target to Q[next_state].max() * (not done)\ncaleulote the TD error.\n\nI td_error = td_target - Q[state] [action] \u2014_11\n\n(8) We continue From this line on the next page.\n\n| Speak PYTHON\nThe Watkins's Q(A) agent 3/3\n(cy) Again, calculate a TD error using the target and the current estimate of\nthe state-action pair. Notice, this isn\u2019t next_state, this is state!!!\ntd_error = td_target - Q[state] [action]\n\n(20) The other approach to replace-trace control methods is to zero out all\naction values of the current state and then increment the current action.\n\nif replacing traces: E[state] .fi11(0)\nCL \u00b0 We increment the eligibility of the current state-action pair by |.\n\nE[state] [action] = E[state] [action] + 1\n\n| Q=Q+ alphas[e) * td_error * E\n(@a) And as before, we multiply the entire eligibility trace matrix by the error and the\nlearning rate corresponding to episode e, then move the entire Q toward that error. by doing,\nSo, we're effectively dropping a. signal to alll visited states to a various degree.\nif next_action_is greedy:\nE = gamma * lambda * E\n\nelse:\nE.fi11(0)\n\n3) Notice this too. If the action we'll take on the next state (which we already selected) is\na. greedy action, then we decay the eligibility matrix as usual, otherwise, we must reset the\neligibility matrix to zero because we'll no longer be learning about the greedy policy.\n\n(4) At the end of the step, we update the state and action to be the next state and action.\n\nstate, action = next_state, next_action\n\nQ track[e] = Q\npi_track.append(np.argmax(Q, axis=1)) \u00a2 1 @S) We save\n\nQand pi.\n@b) And at the end of training\nwe also save V and the Final pi.\nLy = np.max(Q, axis=1)\npi = lambda s: {s:a for s, a in enumerate (\\\nnp.argmax(Q, axis=1))}[s]\n\n7D Finally, we return all this.\n\n| eeeure Q, V, pi, Q track, pi_track\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.6,
                        "section_name": "Agents that interact, learn, and plan",
                        "section_path": "./screenshots-images-2/chapter_7/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_6/c5f0b251-56ac-48ec-9065-b35776e90959.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Agents that interact, learn, and plan\n\nIn chapter 3, we discussed planning algorithms such as value iteration (VI) and policy itera-\ntion (PI). These are planning algorithms because they require a model of the environment, an\nMDP. Planning methods calculate optimal policies offline. On the other hand, in the last\nchapter I presented model-free reinforcement learning methods, perhaps even suggesting\nthat they were an improvement over planning methods. But are they?\n\nThe advantage of model-free RL over planning methods is that the former doesn\u2019t require\nMDPs. Often MDPs are challenging to obtain in advance; sometimes MDPs are even impos-\nsible to create. Imagine representing the game of Go with 10'\u201d possible states or StarCraft II\nwith 10'** states. Those are significant numbers, and that doesn\u2019t even include the action\nspaces or transition function, imagine! Not requiring an MDP in advance is a practical\nbenefit.\n\nBut, let\u2019s think about this for a second: what if we don\u2019t require an MDP in advance, but\nperhaps learn one as we interact with the environment? Think about it: as you walk around a\nnew area, you start building a map in your head. You walk around for a while, find a coffee\nshop, get coffee, and you know how to get back. The skill of learning maps should be intuitive\nto you. Can reinforcement learning agents do something similar to this?\n\nIn this section, we explore agents that interact with the environment, like the model-free\nmethods, but they also learn models of the environment from these interactions, MDPs. By\nlearning maps, agents often require fewer experience samples to learn optimal policies. These\nmethods are called model-based reinforcement learning. Note that in the literature, you often\nsee VI and PI referred to as planning methods, but you may also see them referred to as mod-\nel-based methods. I prefer to draw the line and call them planning methods because they\nrequire an MDP to do anything useful at all. SARSA and Q-learning algorithms are model-\nfree because they do not require and do not learn an MDP. The methods that you learn about\nin this section are model-based because they do not require, but do learn and use an MDP (or\nat least an approximation of an MDP).\n\nR b Witn an RL Accent\nSampling models vs. distributional models\n\nSampling models: Refers to models of the environment that produce a single sample of\nhow the environment will transition given some probabilities; you sample a transition from\nthe model.\n\nDistributional models: Refers to models of the environment that produce the probability\ndistribution of the transition and reward functions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.7,
                        "section_name": "Dyna-Q: Learning sample models",
                        "section_path": "./screenshots-images-2/chapter_7/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_7/2f4786d3-873c-4917-8de5-6418d58e58cf.png",
                            "./screenshots-images-2/chapter_7/section_7/8fe81157-8c7a-4d4a-8e2d-5fbfd27b9f91.png",
                            "./screenshots-images-2/chapter_7/section_7/928d7789-d97b-4e4f-ba9f-acedafd1829d.png",
                            "./screenshots-images-2/chapter_7/section_7/d202ced5-ecc1-4466-bc4a-9c971586f3e1.png",
                            "./screenshots-images-2/chapter_7/section_7/eaf5784d-1e85-4ed3-ab53-671d1e1ae97a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Dyna-Q: Learning sample models\n\nOne of the most well-known architectures for unifying planning and model-free methods is\ncalled Dyna-Q. Dyna-Q consists of interleaving a model-free RL method, such as Q-learning,\nand a planning method, similar to value iteration, using both experiences sampled from the\nenvironment and experiences sampled from the learned model to improve the action-value\nfunction.\n\nIn Dyna-Q, we keep track of both the transition and reward function as three-dimensional\ntensors indexed by the state, the action, and the next state. The transition tensor keeps count\nof the number of times we've seen the three-tuple (s, a, s') indicating how many times we\narrived at state s' from state s when selecting action a. The reward tensor holds the average\nreward we received on the three-tuple (s, a, s') indicating the expected reward when we select\naction a on state s and transition to state s\u2018.\n\nA model-based reinforcement\n\n@ in model-based learning architecture\nreinforcement\nlearning, we start @ we pick an @) Act in the (4) with that experience,\nwith a random value action based environment and | we update the value\nfunction as we have on the value get a response function and also a. model\nin previous methods. function. from it. of the environment.\n\nPick action Interact Model learning\n\nValue function \u2014\u2014\u2014\u2014\u2014\u2014>_ Policy > Experience \u2014\u2014\u2014\u2014\u2014\u2014\u2014> Model\n\nModel-free reinforcement learning\n(S) we use the model\n\nPlanning to improve\nthe value function.\n\n0001 A Br oF History\nIntroduction of the Dyna-Q agent\n\nIdeas related to model-based RL methods can be traced back many years, and are credited to\nseveral researchers, but there are three main papers that set the foundation for the Dyna\narchitecture.\n\nThe first is a 1981 paper by Richard Sutton and Andrew Barto titled \u201cAn Adaptive Network\nthat Constructs and Uses an Internal Model of Its World,\u201d then a 1990 paper by Richard Sutton\ntitled \u201cIntegrated Architectures for Learning, Planning, and Reacting Based on Approximating\nDynamic Programming,\u2019and, finally, a 1991 paper by Richard Sutton titled \u201cDyna, an Integrated\nArchitecture for Learning, Planning, and Reacting,\u201d in which the general architecture leading\nto the specific Dyna-Q agent was introduced.\n\nI Speak PYTHON\nThe Dyna-Q agent 1/3\n\ndef dyna_q(env, cm\u201c @ dyna-@ is similar to the\ngamma=1.0, @-learning agent, but it also\ninit_alpha=0.5, learns a model of the\nmin_alpha=0.01, environment and it uses that\nalpha_decay ratio=0.5, model to improve the estimates.\ninit_epsilon=1.0,\nmin_epsilon=0.1, (D This n planni\nepsilon decay ratio=0.9, tuperparameter the number\nn_planning-3, = \u20ac\u2014\u2014___ 68 updates to the estimates that\nn_episodes=3000) : will run From the learned model.\n\n) most of the First part of the algorithm is the same.\nnS, nA = env.observation_space.n, env.action_space.n\npi_track = []\n@ we initialize the Q-function to zero, and so on.\nLe = np.zeros((nS, nA), dtype=np.float64)\nQ track = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n(S) But then, we create a Function to keep track of the transition Function.\n[ase SESS Ae\n\nT_count = np.zeros((nS, nA, nS), dtype=np.int)\nR_model = np.zeros((nS, nA, nS), dtype=np.float64)\n\n@ Then initialize the\nselect_action = lambda state, Q, epsilon: \\ exploration strategy\nnp.argmax(Q[state]) \\ select_action, and the\nif np.random.random() > epsilon \\ alphas and epsilons\nelse np.random.randint (len(Q[state] ) ) vectors, as usual.\nalphas = decay schedule (\ninit_alpha, min_alpha,\nalpha_decay ratio, n_episodes)\nepsilons = decay schedule (\ninit_epsilon, min_epsilon,\nepsilon decay ratio, n_episodes)\n@ To be continued. . .\n\nfor e in tqdm(range(n_ episodes), leave=False) : \u00ab1\n\nI Speak PYTHON\nThe Dyna-Q agent 2/3\n\n(9) Continues on the episode loop \u2014\u2014]_\nfor e in tqdm(range(n episodes), leave=False) :\n\nGo) For each new episode, we start by resetting the environment and obtaining the\nve ate, aoe ewreg, aie\nstate, done = env.reset(), False\n\nwhile not done:\nGD we select the action, as in original Q-learning (inside the loop only).\naction = select_action(state, Q, epsilons[e])\na) we step the environment and get the experience tuple.\nL_, next_state, reward, done, _ = env.step (action)\n\n3) Then, start learning the model! We increment the transition count for the state-action-\nnext_state triplet indicating, that full transition happened once more. eo\n\nT_ count [state] [action] [next_state] += 1\n\nG4) We also attempt to calculate an incremental mean of the reward signal. Get the difference.\nr_diff = reward - \\\nR_model [state] [action] [next_state]\nGs) Then use that difference and the transition count to learn the reward signal.\nL_, R_model [state] [action] [next_state] += \\\n\n(x_diff / T_count[state] [action] [next_state] )\n\nGe) we calculate the TD target as usual, Q-learning style (of$-policy, using the max)... .\n\nLL, td_target = reward + gamma * \\\nQ[next_state].max() * (not done)\nGD ... and the TD error, too, using the TD target and the current estimate.\ntd_error = td target - Q[state] [action]\n\n|e Q[state] [action] = Q[state] [action] + \\\nalphas[e] * td_error\n(8) Finally, update the Q-Function.\n\n9) And right before we get into the planning steps, we back up the next state variable.\nbackup _next_state = next_state\nfor _ in range(n_planning) : \u00a2<\u2014\u2014\u2014 (a0) To be continued...\n\nI Speak PYTHON\nThe Dyna-Q agent 3/3\n\n@) We continue From the planning loop.\n\nfor _ in range(n_planning): stele fo sure\n\nthere have been updates to the\nSF a iaration before, otherwise,\nthere\u2019s not much to\nif Q.sum() == 0: break plan.\n(3) Then we select a state from a list of states already visited by the agent in experience.\nvisited_states = np.where( \\\nnp.sum(T count, axis=(1, 2)) > 0) [0]\nstate = np.random.choice (visited states)\nGp ie then celesk on aston that has been taken on thet state\n\nactions taken = np.where( \\\nnp.sum(T count[state], axis=1) > 0) [0]\naction = np.random.choice (actions taken)\n\nGs) we use the count matrix to calculate probabilities of a next state and then a next state.\n| probs = T_count[state] [action] / \\\n\nT_ count [state] [action] .sum()\nnext_state = np.random.choice( \\\nnp.arange(nS), size=1, p=probs) [0]\n\n(ae) use the reward model as the reward.\nLi, reward = R_model[state] [action] [next_state]\n\n@D And update td_target = reward + gamma * \\\nthe @-functi Q[next_state] .max()\nusing that td_error = td target - Q[state] [action]\nsimulated F\u2014> Q[state] [action] = Q[state] [action] + \\\nexperience! alphas[e] * td_error\nstate = backup next_state \u00a2\u2014\u2014-4 (ag) At the endof the\n\n; Planning steps, we set the\n(a9) The rest is the same. Van Sere re\nT Q track[e] = Q\n\npi_track.append(np.argmax(Q, axis=1))\nV = np.max(Q, axis=1)\npi = lambda s: {s:a for s, a in enumerate( \\\nnp.argmax(Q, axis=1))}[s]\nreturn Q, V, pi, Q track, pi_track\n\nTatty it Up\n\nModel-based methods learn the transition and reward function (transition below)\n\n@ Look at the First plot to the\nright. This one is the model\nthot Dyna-Q has learned\nafter one episode. Now, there\nare obvious issues with this Mansion\nprosabiinies \u00b0*\nmodel, but also, this is only 2a\nafter a single episode. This ek\ncould mean trouble when faye eat\non because there will be a\nbias when sampling an SWS learned MDP after 10 episodes\nincorrect model. ie\n\n@ only after 10 episodes,\nyou can see the model taking as-\n\nTronation\n\nshape. In the second plot, you IA &\nshould be able to see the -\ntogether. The axis to the\n\nright is the initial state s, the\n\naxis to the left is the landing\n\nstate, the colors are the ier\nactions, and bar heights are\n\nthe transition probabilities. =\n\noe\n\n\u2018SWS learned MDP after 1 episcdes\n\nSWS learned MOP after 100 episodes\n\nTranation\n\nprobabilities look pretty a7\nclose to the real MDP.\n\nObviously, this is a simple\n\nenvironment, so the agent\ncan gather enough\nexperience samples for SWS learned MDP after 3000 episodes\nbuilding an MOP quickly, ;\n\n(4) You can see here the\nprobabilities are good enough\nand describe the moP\ncorrectly. You Know that going\nright on state 7 should take\nyou to state 8 with about Som\nchance, to 7 with about 30%\nand to 6 with about 20%.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.8,
                        "section_name": "Trajectory sampling: Making plans for the immediate future",
                        "section_path": "./screenshots-images-2/chapter_7/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_8/5a4d9fa0-52c5-4628-9632-c415c7edcc01.png",
                            "./screenshots-images-2/chapter_7/section_8/bb34e4c9-d501-43bf-80ae-5c8114e9fa34.png",
                            "./screenshots-images-2/chapter_7/section_8/eae34a4f-141f-49c9-a475-5766bdb969b7.png",
                            "./screenshots-images-2/chapter_7/section_8/dc61ccc9-8d57-4138-93e7-609dcfbed116.png",
                            "./screenshots-images-2/chapter_7/section_8/619f9179-be61-498a-9507-fb10ae916a52.png",
                            "./screenshots-images-2/chapter_7/section_8/44d166b1-5ac5-431a-a32d-4181a605013a.png",
                            "./screenshots-images-2/chapter_7/section_8/6144afac-04d2-4477-8021-cd2a6fabd700.png",
                            "./screenshots-images-2/chapter_7/section_8/2c31de75-1fbb-4594-9d4a-d5e33a1ac883.png",
                            "./screenshots-images-2/chapter_7/section_8/b39d8c11-9ddb-49d6-ad38-d240236fc8d2.png",
                            "./screenshots-images-2/chapter_7/section_8/832cbc89-049b-44c2-97f3-8340cb00236d.png",
                            "./screenshots-images-2/chapter_7/section_8/f657a244-005d-4dd3-b6e9-c6d366b87443.png",
                            "./screenshots-images-2/chapter_7/section_8/01471a71-c799-40a8-8c28-3f188bf0706b.png",
                            "./screenshots-images-2/chapter_7/section_8/80003620-8660-4506-9aeb-86bfb961a3e9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Trajectory sampling: Making plans for the immediate future\n\nIn Dyna-Q, we learn the model as previously described, adjust action-value functions as we\ndo in vanilla Q-learning, and then run a few planning iterations at the end of the algorithm.\nNotice that if we removed the model-learning and planning lines from the code, we'd be left\nwith the same Q-learning algorithm that we had in the previous chapter.\n\nAt the planning phase, we only sample from the state-action pairs that have been visited,\nso that the agent doesn\u2019t waste resources with state-action pairs about which the model has\nno information. From those visited state-action pairs, we sample a state uniformly at random\nand then sample action from previously selected actions, also uniformly at random. Finally,\nwe obtain the next state and reward sampling from the probabilities of transition given that\nstate-action pair. But doesn\u2019t this seem intuitively incorrect? We\u2019re planning by using a state\nselected uniformly at random!\n\nCouldn\u2019t this technique be more effective if we used a state that we expect to encounter\nduring the current episode? Think about it for a second. Would you prefer prioritizing plan-\nning your day, week, month, and year, or would you instead plan a random event that \u201ccould\u201d\nhappen in your life? Say that you\u2019re a software engineer: would you prefer planning reading a\nprogramming book, and working on that side project, or a future possible career change to\nmedicine? Planning for the immediate future is the smarter approach. Trajectory sampling is\na model-based RL method that does just that.\n\n<= Bom Down\n\nTrajectory sampling\nWhile Dyna-Q samples the learned MDP uniformly at random, trajectory sampling gathers\ntrajectories, that is, transitions and rewards that can be encountered in the immediate\nfuture. You're planning your week, not a random time in your life. It makes more sense to do\nit this way.\n\nThe traditional trajectory-sampling approach is to sample from an initial state until\nreaching a terminal state using the on-policy trajectory, in other words, sampling actions\nfrom the same behavioral policy at the given time step.\n\nHowever, you shouldn't limit yourself to this approach; you should experiment. For\ninstance, my implementation samples starting from the current state, instead of an initial\nstate, to a terminal state within a preset number of steps, sampling a policy greedy with\nrespect to the current estimates.\n\nBut you can try something else. As long as you're sampling a trajectory, you can call that\ntrajectory sampling.\n\n| Speak PYTHON\nThe trajectory-sampling agent 1/3\n\ndef trajectory sampling(env, \u2014\u2014\u2014F 0 Traj\ngamma=1.0, sampling is, for the\ninit_alpha=0.5, most part, the same\n\n(a) instead of min_alpha=0.01, as Dyna-@, with a\n\nn_planning we alpha_decay ratio=0.5, Few exceptions.\n\nuse a.max_ init_epsilon=1.0,\n\ntrajectory _depth min_epsilon=0.1,\n\nto restrict the epsilon decay ratio=0.9,\n\ntrajectory length. > _ max _ trajectory depth=100,\nn_episodes=3000) :\n\n@) most of the algorithm is the same as Dyna-Q.\n\nL nS, nA = env.observation_space.n, env.action_space.n\npi_track = []\n\n@ The @-function, and so on\n\n4 Q = np.zeros((nS, nA), dtype=np.float\u00e964)\nQ track = np.zeros((n_episodes, nS, nA), dtype=np.float\u00e94)\n\n() we create the same variables to model the transition function . . .\n[i resume = np terea cca, an\n\nT_count np.zeros((nS, nA, nS), dtype=np.int)\nR_model = np.zeros((nS, nA, nS), dtype=np.float\u00e964)\n\n@ The select_action\nselect_action = lambda state, Q, epsilon: \\ $unction, the alphas\n\nnp.argmax(Q[state]) \\ vector, and epsilons\nif np.random.random() > epsilon \\ vector are all the same.\nelse np.random.randint (len(Q[state]))\n\nalphas = decay schedule (\ninit_alpha, min_alpha,\nalpha_decay ratio, n_episodes)\n\nepsilons = decay schedule (\ninit_epsilon, min_epsilon,\nepsilon decay ratio, n_episodes)\n\n@\u00ae To be continued...\nfor e in tqdm(range(n_ episodes), leave=False): \u00ab\u2014!\n\n| Speak PyTHoN\nThe trajectory-sampling agent 2/3\n@) Continues on the episode loop -\u2014\nfor e in tqdm(range(n episodes), leave=False) :\nGo) Again, each new episode, we start by resetting the environment and obtaining the\ninitial state. We also set the done flag to False and enter the step interaction loop.\nstate, done = env.reset(), False |\n\nwhile not done:\nGD we select the action.\nL_, action = select_action(state, Q, epsilons[e])\na) we step the environment and get the experience tuple.\nL_, next_state, reward, done, _ = env.step(action)\n2) we learn the model just like in Dyna-@: inerement the transition count for the state-\nT_ count [state] [action] [next_state] += 1\n\nG4) Then, again, calculate an incremental mean of the reward signal; First, get the difference.\nL_, r_diff = reward - \\\n\nR_model [state] [action] [next_state]\nGs) Then, use that difference and the transition count to learn the reward signal.\nLl _, R_model [state] [action] [next_state] += \\\n\n(xr_diff / T_count([state] [action] [next_state] )\n\nie) we calculate the TO target as usual.\ntd_target = reward + gamma * \\\nQ[next_state].max() * (not done)\nGD The TO error using the TD target and the current estimate\n\ntd_error = td target - Q[state] [action]\n\ni id Q[state] [action] = Q[state] [action] + \\\nalphas[e] * td_error\n\n8) Then, update the Q-function.\nG9) And right before we get into the planning steps, we back up the next state variable.\nbackup_next_state = next_state (0) To be continued ..\n\nfor _ in range(max_trajectory depth):\n\n| Speak PYTHON\nThe trajectory-sampling agent 3/3\n\n(ai) Notice we are now using amox_trajectory_depth Variable, but are still planning.\nLH, for _ in range(max_trajectory depth):\n\n(aa) we still check for the Q-function to have any difference, so it\u2019s worth our compute.\nLs if Q.sum() == 0: break\n\n(@3) Select the action either on-policy or of\u00a7-policy (using the greedy policy).\n\n# action = select_action(state, Q, epsilons[e])\n\naction = Q[state] .argmax()\n\n(4) If we haven't experienced the transition, planning would be a mess, so break out.\n\nLs, if not T count([state] [action] .sum(): break\n(as) Otherwise, we get the probabilities of next_state and sample the model accordingly.\nprobs = T_count[state] [action] / \\\nT_ count [state] [action] .sum()\nnext_state = np.random.choice( \\\nnp.arange(nS), size=1, p=probs) [0]\n\nGb) Then, get the reward as prescribed the reward-signal model.\nreward = R_model[state] [action] [next_state]\n\n7D And continue updating the Q\u2014Sunction as if with real experience. FY\n\n(a\u00ae) Notice herewe td_target = reward + gamma * \\\n\nupdate the state Q[next_state] .max()\nVariable right before td_error = td_ target - Q[state] [action]\n\nwe loop and continue Q[state] [action] = Q[state] [action] + \\\n\nthe on-policy alphas[e] * td_error\nplanning steps. Hp state = next_state\n\n9) Outside the planning\nstate = backup_next_state \u00a2\u2014\u2014-+ loop, we restore the\nstate, and continue real\nGo) everything else as usual interaction steps.\n\nt Q track[e] = Q\npi_track.append(np.argmax(Q, axis=1))\nV = np.max(Q, axis=1)\npi = lambda s: {s:a for s, a in enumerate( \\\nnp.argmax(Q, axis=1))}[s]\nreturn 0, V, pi, Q track, pi_ track\n\nTatty it Up\nDyna-Q and trajectory sampling sample the learned model differently\nThis First plot shows the states learned model of SW envsarment\n\nthat were sampled by the \u201c\n\nplanning phase of Dyna-@ and the _\nactions selected in those states. esos\nAs you can see, Dyna-Q samples con sos\nuniformly at random, not only the wos\nstates, but also the actions taken\n\nin those states.\n\n) with trajectory sampling, you itn aes sped\nhave a different sampling States samples from Trajectory Sampling\nstrod Remember, in the oT oe learned model of SWS enviranment\n\nenvironment the rightmost oe a a | ten\n\nstate 8, is the only non-zero\nreward state. Landing on state 8 cnt\nprovides oa. reward of +. The\nstrategy samples the model in an seo\n\nattempt to improve greedy action a I\n\nselection. This is the reason 7 sna ctstes samples\n\nthe states sampled are skewed Next states samples by Dyna-Q)\n\ntoward the goal state, state 8. vem sf =\nThe same happens with the <0\n\nsampling of the action. As you can seo\n\nsee, the right action is sampled cous\n\nfar more than the lef action \u201c con\n\nacross the board. seen\n\n) To understand the implications 200\n\nof the different sampling \u2018\n\nstrategies, ' Plotted the landing . Next states snes Sampling -_\nstates ofter sampling an action in in SWS environment from state 7 \u2014--\n\nstate 1, which is the state to the\nleft of the goal state. As we've\nseen, Dyna-Q does the sampling\nuniformly ot random so\n\nprobabilities reflect the moP.\n\n@) Trajectory sampling, on the\nother hand, lands on the goal\n\nstate far more often, therefore\n\nexperiencing non-zero rewards\nfrom the model more frequently.\n\nConcrete ExamPLe\nThe frozen lake environment\n\nIn chapter 2, we developed the MDP for an environment called frozen lake (FL). As you\nremember, FL is a simple grid-world (GW) environment. It has discrete state and action\nspaces, with 16 states and four actions.\n\nThe goal of the agent is to go from a start location to a goal location while avoiding falling\ninto holes. In this particular instantiation of the frozen lake environment, the goal is to go\nfrom state 0 to state 15. The challenge is that the surface of the lake is frozen, and therefore\nslippery, very slippery.\n\nThe frozen lake environment\n@ The agent starts each trial here.\n\n@) Slippery Frozen\nsurface may send\nthe agent to\n\nunintended places.\n\n@) The agents gets a +1\n\nwhen he arrives here.\n\nThe FL environment is a 4 x 4 grid with 16 cells, states 0-15, top-left to bottom-right. State 0\nis the only state in the initial state distribution, meaning that on every new episode, the agent\nshows up in that START state. States 5, 7, 11, 12, and 15 are terminal states: once the agent\nlands on any of those states, the episode terminates. States 5, 7, 11, and 12 are holes, and\nstate 15 is the\u201cGOAL.\u201d What makes \u201choles\u201d and \u201cGOAL\" be any different is the reward function.\nAll transitions landing on the GOAL state, state 15, provide a +1 reward, while every other\ntransition in the entire grid world provides a 0 reward, no reward. The agent will naturally try\nto get to that +1 transition, and that involves avoiding the holes. The challenge of the envi-\nronment is that actions have stochastic effects, so the agent moves only a third of the time as\nintended. The other two-thirds is split evenly in orthogonal directions. If the agent tries to\nmove out of the grid world, it will bounce back to the cell from which it tried to move.\n\nIt\u2019s In THE DeTaILs\nHyperparameter values for the frozen lake environment\n\nThe frozen lake (FL) environment is a more challenging environment than, for instance, the\nslippery walk seven (SWS) environment. Therefore, one of the most important changes we\nneed to make is to increase the number of episodes the agent interacts with the environment.\n\nWhile in the SWS environment, we allow the agent to interact for only 3,000 episodes; in\nthe FL environment, we let the agent gather experience for 10,000 episodes. This simple\nchange also automatically adjusts the decay schedule for both alpha and epsilon.\n\nChanging the value of the n_episodes parameter from 3,000 to 10,000 automatically\nchanges the amount of exploration and learning of the agent. Alpha now decays from an\ninitial value of 0.5 to a minimum value of 0.01 after 50% of the total episodes, which is 5,000\nepisodes, and epsilon decays from an initial value of 1.0 to a minimum value of 0.1 after 90%\nof the total episodes, which is 9,000 episodes.\n\nAlpha and epsilon schedules\n\n10 \u2014 Alpha schedule\n~ Epsilon schedule\n\n2 \u00b0 2\n& & \u00ae\n\nHyperparameter values\n\nbd\nn\n\n0.0\n\n. & # & & -\n\nEpisodes\n\nFinally, it\u2019s important to mention that I'm using a gamma of 0.99, and that the frozen lake\nenvironment, when used with OpenAl Gym, is automatically wrapped with a time limit Gym\nWrapper. This \u201ctime wrapper\u201d instance makes sure the agent terminates an episode with no\nmore than 100 steps. Technically speaking, these two decisions (gamma and the time wrap-\nper) change the optimal policy and value function the agent learns, and should not be taken\nlightly. | recommend playing with the FL environment in chapter 7\u2018s Notebook and changing\ngamma to different values (1, 0.5, 0) and also removing the time wrapper by getting the envi-\nronment instance attribute \u201cunwrapped,\u201d for instance, \u201cenv = env.unwrapped.\u2019 Try to under-\nstand how these two things affect the policies and value functions found.\n\nTatty it Up\nModel-based RL methods get estimates closer to actual in fewer episodes\n\nSarsa(A) replacing estimates through time vs. true values (th @ One interesting experiment\n\u201c46 you should try is training vanilla\nSARSA and Q-learning agents on\nthis environment and i\nthe results. Look at the sarsa())\nagent struggle to estimate the\noptimal state-value function.\nRemember in these plots the\nhorizontal lines represent the\non) replacing estimate through optimal state-value Function For\nreplacing estimates through time vs. true values ahandful of states. In this case, |\nj = pulled states 0, 4, &, 9, and Io.\n\n@ The @(A) agent & ofF-poliey\nand you can see it\nestimates of the optimal a\nvalue function toward the true\nvalues, unlike SARSACA) . Now, to\nbe clear, this is a matter of the\n2200 wood\n\nEpisodes number of steps; I\u2019m sure\nDyna-Q estimates through time vs. true values sarsacd) would converge to the\ntrue values if given more\nepisodes.\nj= (2) The yna-@ agent is even\n\u201cti Faster than the QCA) agent at\ntracking the true values, but\nnotice too, how there\u2019s a large\nerror spike at the beginning of\n0 wo HF ies ttt training. This is likely becouse\nthe model is incorrect early on,\nand Dyna-@ randomly samples\nstates from the learned model,\neven states not sufficiently\nvisited.\n\n(4) my implementation of\nsamples states likely to be\nencountered; perhaps, the reason\nwhy there\u2019s more stability in TS.\n\ne\n\na\n\n\u00a3\n\nState-value function\n2\n\n2\n\n00 woe\nEpisodes\n\n2\n\n\u2014 wioy\n= we\n\n2\n\nState-value function\n2\n\no8\n\nEs\n\nState-value function\n\nState-value function\n\n00 teoe\nEpisodes\n\nTatty it Up\n\nBoth traces and model-based methods are efficient at processing experiences\n\n\u00a9 Now, let\u2019s discuss how the results\nshown on the previous page relate to\nSUCCESS. AS You can see on the First plot *\nto the right, all algorithms except\nSARSAC)) reach the same success rate\nas an optimal policy. Also, model-based\nRL methods appear to get there First,\nbut not by much. Recall that \u201csuccess\u201d\nhere means the number of times the a\nagent reached the goal state (state IS in\n\nthe FL environment.\n\n@) On the second plot, to the right, you\ncan see the estimated expected return\nof the initial state. Notice how both\nmodel-based methods have a huge error\nspike at the beginning of the training\nTun, trajectory sampling stabilizes a little\nbit sooner than Dyna-@, yet the spike is\nstill significant. QCA) methods get there\nwithout the spike and soon enough, while\nsarsa()) methods never make it before\ntraining is stopped.\n@ The third plot is the actual episode\nreturn averaged over 100 episodes. As you\ncan see, both model-based methods and\nQ(A) agents obtain the expected return\nafter approximately 2,000 episodes. oo\nSARSsACA) agents don't get there before\nthe training process is stopped. Again, \u2019'm\nsure that, given enough time,\nSARSACA) agents would get there. &\n\n@ This last plot is the action-value\nfunction mean absolute error. As you can\nsee, the model-based methods also bring\nthe error down close to zero, which is the\nfastest. However, shortly after a,000\nepisodes, both model-based and Q(A)\nmethods are much the same. SARSACA)\nmethods are also slow to optimal here.\n\n109\n\n8\n\nSuccess rate %\n\nEstimated value of initial state VIO}\n\nReturn (Gt:T)\n\n2\nEy\n\nous\n\nMean Absolute Error MAE(\n2s 2\n2 2\n\n2\n3\n\nPolicy success rate (ma 100)\n\nSansa) replacing\n\noo Saraa(h) accumasating\n\nQO) reatacing\n\n== 0) accumulating\nOyna-g\n\noon Trajectory Samping\n\na & \u00e9 #\nEpisodes\nEstimated expected return (ma 100)\n\n7)\no== Trajectory Samping\n\nf \u00a3\u00a3 \u00a2 \u00a2 \u00a5\n\nEpisodes\nPolicy episode return (ma 100)\nZIRTT A ey SS aod\nvo\n\n\u2018SnevaiA} reptncing:\n\no> mena) accumsan\n\n~~ QIA replacing\n\n~ OIA) accumulating\nDyna-0\n\noo Trnjpetary Sampling\n\n\u00a3 - \u00e9 \u20ac ra\nEpisodes\n\n\u2018Action-value function estimation error (ma 100)\n\n\u2014 Seesala) reptacing\nSaeeaiA) accurm\u00e9ating\nOA) replacing\n\n~~ OIA accumulating\nDyna\n\n\u2014-- Trajectory Sameting\n\nConcrete ExamPLe\nThe frozen lake 8 x 8 environment\n\nHow about we step it up and try these algorithms in a challenging environment?\n\nThis one is called frozen lake 8 x 8 (FL8x8) and as you might expect, this is an 8-by-8 grid\nworld with properties similar to the FL. The initial state is state 0, the state on the top-left\ncorner; the terminal, and GOAL state is state 63, the state on the bottom-right corner. The\nstochasticity of action effects is the same: the agent moves to the intended cell with a mere\n33.33% chance, and the rest is split evenly in orthogonal directions.\n\nThe frozen lake 8 x 8\nenvironment\n\n\u00ae The frozen lake 8 x 8 is similar to\nthe frozen lake, only much bigger\nand therefore more challenging.\n\nThe main difference in this environment, as you can see, is that there are many more holes,\nand obviously they\u2019re in different locations. States 19, 29, 35, 41, 42, 46, 49, 52, 54, and 59 are\nholes; that\u2019s a total of 10 holes!\n\nSimilar to the original FL environment, in FL8x8, the right policy allows the agent to reach\nthe terminal state 100% of the episodes. However, in the OpenAl Gym implementation,\nagents that learn optimal policies do not find these particular policies because of gamma\nand the time wrapper we discussed. Think about it for a second: given the stochasticity of\nthese environments, a safe policy could terminate in zero rewards for the episode due to the\ntime wrapper. Also, given a gamma value less than one, the more steps the agent takes, the\nlower the reward will impact the return. For these reasons, safe policies aren\u2019t necessarily\noptimal policies; therefore, the agent doesn\u2019t learn them. Remember that the goal isn\u2019t sim-\nply to find a policy that reaches the goal 100% of the times, but to find a policy that reaches\nthe goal within 100 steps in FL and 200 steps in FL8x8. Agents may need to take risks to\naccomplish this goal.\n\nIt\u2019s In THE DeTaILs\nHyperparameter values for the frozen lake 8 x 8 environment\n\nThe frozen lake 8 x 8 (FL8x8) environment is the most challenging discrete state- and\naction-space environment that we discuss in this book. This environment is challenging\nfor a number of reasons: first, 64 states is the largest number of states we've worked with,\nbut more importantly having a single non-zero reward makes this environment particularly\nchallenging.\n\nWhat that really means is agents will only know they\u2019ve done it right once they hit the\nterminal state for the first time. Remember, this is randomly! After they find the non-zero\nreward transition, agents such as SARSA and Q-learning (not the lambda versions, but the\nvanilla ones) will only update the value of the state from which the agent transitioned to the\nGOAL state. That's a one-step back from the reward. Then, for that value function to be prop-\nagated back one more step, guess what, the agent needs to randomly hit that second-to-final\nstate. But, that\u2019s for the non-lambda versions. With SARSA(A) and Q()), the propagation of\nvalues depends on the value of lambda. For all the experiments in this chapter, | use a lambda\nof 0.5, which more or less tells the agent to propagate the values half the trajectory (also\ndepending on the type of traces being used, but as a ballpark).\n\nSurprisingly enough, the only change we make to these agents is the number of episodes\nwe let them interact with the environments. While in the SWS environment we allow the\nagent to interact for only 3,000 episodes, and in the FL environment we let the agent gather\nexperience for 10,000 episodes; in FL8x8 we let these agents gather 30,000 episodes. This\nmeans that alpha now decays from an initial value of 0.5 to a minimum value of 0.01 after\n50% of the total episodes, which is now 15,000 episodes, and epsilon decays from an initial\nvalue of 1.0 to a minimum value of 0.1 after 90% of the total episodes, which is now 27,000\nepisodes.\n\nAlpha and epsilon schedules\n\n10 \u2014\u2014~ Alpha schedule\n~ Epsilon schedule\n\n\u00b0 \u00b0 \u00b0\nrs o o\n\nHyperparameter values\n\n\u00b0\nXN\n\noo\n\nrr a a a a i\n\nEpisodes\n\nTatty it Up\nOn-policy methods no longer keep up, off-policy with traces and model-based do\n\n\u2018Sarsa(A} replacing estimates through time vs. true values\n@ Results show much thesame =\u201c path\ntrends. The SARSACA) agent ea dh\nperhaps takes too long to\nve mentioned before this is\npolicy algorithm. A you can see,\nnone of the estimates gets even\n\n@ The Q(A) agent, however,\nhas estimates that do reflect\nthe optimal values. A caveat\nthat | want to mention is that\nthe optimal values shown in\nthese graphs don't take into\naccount the time step limit\nthot the agent suffers . -\nthrough interaction. That o seo docoe - Sand 20009 28000 g0909\nshould affect the estimates.\n\n@) The Oyna-@ agent has a os\nbig advantage. Being a\nmodel-based RL method, all oF\nthe interaction steps prior to\nhitting a terminal state help\nMOP. Once the agent finds the\nreward for the First time, the H soo 10000 ==18000 \u00a92000-2000 \u00ab30009\n\nplanning phase of model- Episades\nbased RL methods propagates \u2014 as Trajectory Sampling estimates through time vs. true values\n\nthe values quickly, 07 Denese F138)\n\n@ we see a similar trend with i\n\n\u2014 ve\n\u2014- wan\n\n~ wim\n\nvO wi\n\u2014 we\n\nState-value function\n\nt$000 20000 22000 30009\nEpisodes\n\nQM) replacing estimates through time vs. true values\n\n3a)\n\u2014- wD\nwaa\n\n~~ viz)\n\u2014 wa)\n\nState-value function\n\nDyna-Q estimates through time vs. true values:\n\n2\n\n\u2014 ws\n== W399)\n~ a7)\n~- vizay\n\u2014 wa)\n\nState-value function\nFy\n\nS\n\n2\n\nv*(13)\n\niv] as \u2014 w57)\nthe trajectory-samplingagent \u00a2,, Mp was) an\nas before; the estimatesdo _,, =n\ntrack the optimal values,and saa\n\nmore importantly, there\u2019s not a. wa wat : :\nhuge spike due to model error. 0\n== a more le 5 seo | weone sone 000 ato sen\n\ncurve for the estimates.\n\nTatty it Up\nSome model-based methods show large error spikes to be aware of\n\ntoo Policy success rate {ma 100) ao Policy success rate (ma 100)\n\n# #\n\nte ye\n\ni. i.\n\n\u00bb = swsarnmoucrs fT \u2014 eal ccmtarg\n\n\u201cSe ff \u00a3 \u00a3 \u20ac * # GF fF \u00a3 KF F\nEpsoces Epsoces\n\nPolicy episode return (rma 100) Policy episode return (ma 100)\n\n\u2014 senses reptecing\nSarnath eccarmdatiryg\n\nUA ewplactieg\n\ni a a ee a a a 2 2 Sa a\nEpsodes Epsodes\n\n@ \\had to separate the plotting of policy Estimated expected return {ma 100)\n\nsuccess rate and episode return. =\n(2) on the plot to the right, you can see how in Ct vn\nfor Dyna-@ is large, while trajectory sampling 3 sana recto\nSoe SaaS wera aon\nand Q(A) agents are much more stable. You z i, a oo evtase\na ~~ Oth accomraceg\ncan see how SARSACA) agents are too off. h.. \u2018a a ne\n\n@) The action-value function estimation error _ \u00a2 # elon f \u00a3 #\nis much the same with all agents. However, you\n\nnotice that Oyna-Q is the lowest error. au Action-value function estimation sree 2001 .\nWhy do you think this is? Remember, my Fo i = \u2014 Sem arn\ntrajectory sampling implementation only Soo } Se oo\ngenerates greedy trajectory samples, which i leaner anaes\nmeans that some states won't get updates (or 2\u00b0 | im\nvisited) after a number of episodes, while goo |\nmethods such as Dyna-@ select uniformly at e\n\nrandom, which means many state-action pairs\nwill get updates, even if those are irrelevant\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.9,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_7/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_9/ac4e2f9f-967e-45b5-8909-d173157059fd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter, you learned about making RL more effective and efficient. By effective, I\nmean that agents presented in this chapter are capable of solving the environment in the lim-\nited number of episodes allowed for interaction. Other agents, such as vanilla SARSA, or\nQ-learning, or even Monte Carlo control, would have trouble solving these challenges in the\nlimited number of steps; at least, for sure, they'd have trouble solving the FL8x8 environment\nin only 30,000 episodes. That\u2019s what effectiveness means to me in this chapter; agents are\nsuccessful in producing the desired results.\n\nWe also explored more efficient algorithms. And by efficient here, I mean data-efficient; I\nmean that the agents we introduced in this chapter can do more with the same data than other\nagents. SARSA(A) and Q(A), for instance, can propagate rewards to value-function estimates\nmuch quicker than their vanilla counterparts, SARSA and Q-learning. By adjusting the A\nhyperparameter, you can even assign credit to all states visited in an episode. A value of 1 for A\nis not always the best, but at least you have the option when using SARSA(A) and Q(A).\n\nYou also learned about model-based RL methods, such as Dyna-Q and trajectory sam-\npling. These methods are sample efficient in a different way. They use samples to learn a\nmodel of the environment; if your agent lands 100% of 1M samples on state s' when taking\naction a, in state s, why not use that information to improve value functions and policies?\nAdvanced model-based deep reinforcement learning methods are often used in environ-\nments in which gathering experience samples is costly: domains such as robotic, or problems\nin which you don\u2019t have a high-speed simulation, or where hardware requires large financial\nresources.\n\nFor the rest of the book, we\u2019re moving on to discuss the subtleties that arise when using\nnon-linear function approximation with reinforcement learning. Everything that you\u2019ve\nlearned so far still applies. The only difference is that instead of using vectors and matrices for\nholding value functions and policies, now we move into the world of supervised learning and\nfunction approximation. Remember, in DRL, agents learn from feedback that\u2019s simultane-\nously sequential (as opposed to one-shot), evaluative (as opposed to supervised), and sam-\npled (as opposed to exhaustive). We haven\u2019t touched the \u201csampled\u201d part yet; agents have\nalways been able to visit all states or state-action pairs, but starting with the next chapter, we\nconcentrate on problems that cannot be exhaustively sampled.\n\nBy now, you\n\n+ Know how to develop RL agents that are more effective at reaching their goals\n+ Know how to make RL agents that are more sample efficient\n\n+ Know how to deal with feedback that is simultaneously sequential and evaluative\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 8,
                "chapter_name": "Introduction to value-based\ndeep reinforcement learning",
                "chapter_path": "./screenshots-images-2/chapter_8",
                "sections": [
                    {
                        "section_id": 8.1,
                        "section_name": "Introduction to value-based\ndeep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_8/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_1/82f8dd66-0608-40a5-8cc4-c1ddb332e137.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "We've made a great deal of progress so far, and you're ready to truly grok deep reinforcement\nlearning. In chapter 2, you learned to represent problems in a way reinforcement learning\nagents can solve using Markov decision processes (MDP). In chapter 3, you developed algo-\nrithms that solve these MDPs: that is, agents that find optimal behavior in sequential decision-\nmaking problems. In chapter 4, you learned about algorithms that solve one-step MDPs\nwithout having access to these MDPs. These problems are uncertain because the agents don\u2019t\nhave access to the MDP. Agents learn to find optimal behavior through trial-and-error learn-\ning. In chapter 5, we mixed these two types of problems\u2014sequential and uncertain\u2014so we\nexplore agents that learn to evaluate policies. Agents didn\u2019t find optimal policies but were\nable to evaluate policies and estimate value functions accurately. In chapter 6, we studied\nagents that find optimal policies on sequential decision-making problems under uncertainty.\nThese agents go from random to optimal by merely interacting with their environment and\ndeliberately gathering experiences for learning. In chapter 7, we learned about agents that are\neven better at finding optimal policies by getting the most out of their experiences.\n\nChapter 2 is a foundation for all chapters in this book. Chapter 3 is about planning algo-\nrithms that deal with sequential feedback. Chapter 4 is about bandit algorithms that deal with\nevaluative feedback. Chapters 5, 6, and 7 are about RL algorithms, algorithms that deal with\nfeedback that is simultaneously sequential and evaluative. This type of problem is what peo-\nple refer to as tabular reinforcement learning. Starting from this chapter, we dig into the details\nof deep reinforcement learning.\n\nMore specifically, in this chapter, we begin our incursion into the use of deep neural\nnetworks for solving reinforcement learning problems. In deep reinforcement learning, there\nare different ways of leveraging the power of highly non-linear function approximators,\nsuch as deep neural networks. They\u2019re value-based, policy-based, actor-critic, model-based,\nand gradient-free methods. This chapter goes in-depth on value-based deep reinforcement\nlearning methods.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.2,
                        "section_name": "Types of algorithmic approaches you learn\nabout in this book",
                        "section_path": "./screenshots-images-2/chapter_8/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_2/7729ccd7-3113-460c-936f-d7228357fbb1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Types of algorithmic approaches you learn\nabout in this book\n\nPolicy-based Value-based Model-based\n\n@ You are here For the next three chapters. _______f\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.3,
                        "section_name": "The kind of feedback\ndeep reinforcement learning agents use",
                        "section_path": "./screenshots-images-2/chapter_8/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_3/4f926843-189e-45cc-bd3b-2c01018efe89.png",
                            "./screenshots-images-2/chapter_8/section_3/b8f19902-6170-4fbc-8218-29bfdb0d45d0.png",
                            "./screenshots-images-2/chapter_8/section_3/f8238984-1930-4de6-97da-82b529270273.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The kind of feedback\ndeep reinforcement learning agents use\n\nIn deep reinforcement learning, we build agents that are capable of learning from feedback\nthat\u2019s simultaneously evaluative, sequential, and sampled. I\u2019ve emphasized this throughout\nthe book because you need to understand what that means.\n\nIn the first chapter, I mentioned that deep reinforcement learning is about complex\nsequential decision-making problems under uncertainty. You probably thought, \u201cWhat a\nbunch of words.\u201d But as I promised, all these words mean something. \u201cSequential decision-\nmaking problems\u201d is what you learned about in chapter 3. \u201cProblems under uncertainty\u201d is\nwhat you learned about in chapter 4. In chapters 5, 6, and 7, you learned about \u201csequential\ndecision-making problems under uncertainty.\u201d In this chapter, we add the \u201ccomplex\u201d part\nback to that whole sentence. Let\u2019s use this introductory section to review one last time the\nthree types of feedback a deep reinforcement learning agent uses for learning.\n\n2% Boi im Down\nKinds of feedback in deep reinforcement learning\nSequential Evaluative Sampled\n\n(as opposed (as opposed (as opposed\nto one-shot) to supervised) to exhaustive)\n\nPlanning\n(Chapter 3)\nBandits\n(Chapter 4)\n\nTabular\n\nreinforcement\nlearning\n\n(Chapters 5, 6, 7)\n\nDeep\n\nreinforcement\nlearning\n\n(Chapters 8, 9, 10, 11, 12)\n\n\nDeep reinforcement learning agents\ndeal with sequential feedback\n\nDeep reinforcement learning agents have to deal with sequential feedback. One of the main\nchallenges of sequential feedback is that your agents can receive delayed information.\n\nYou can imagine a chess game in which you make a few wrong moves early on, but the\nconsequences of those wrong moves only manifest at the end of the game when and if you\nmaterialize a loss.\n\nDelayed feedback makes it tricky to interpret the source of the feedback. Sequential feed-\nback gives rise to the temporal credit assignment problem, which is the challenge of deter-\nmining which state, action, or state-action pair is responsible for a reward. When there\u2019s a\ntemporal component to a problem and actions have delayed consequences, it becomes chal-\nlenging to assign credit for rewards.\n\nSequential feedback\n\n\u00a9 Consider this environment in which\n\none path looks obviously better than\n\nthe other even after several steps.\n\n(@ eut before the\nagent can complete this\n\u201cbetter-looking\u201d path, it\nwill get 0 high penalty,\n\n@) This is the challenge of sequential feedback, and one of the reasons\nwe use value functions to decide on actions, and not merely rewards.\n\nBut, if it isn\u2019t sequential, what is it?\n\nThe opposite of delayed feedback is immediate feedback. In other words, the opposite of\nsequential feedback is one-shot feedback. In problems that deal with one-shot feedback, such\nas supervised learning or multi-armed bandits, decisions don\u2019t have long-term consequences.\nFor example, in a classification problem, classifying an image, whether correctly or not, has\nno bearing on future performance; for instance, the images presented in the next model are\nnot any different whether the model classified the previous batch correctly or not. In DRL,\nthis sequential dependency exists.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.4,
                        "section_name": "Classification problem",
                        "section_path": "./screenshots-images-2/chapter_8/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_4/a8330183-675f-4e6e-ae90-b03dc8cbd559.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Classification problem\n\nDataset 71 Model al\n\n@ A mini-batch is fed @ model predicts and\ninto the model. calculates a loss. For\n>\n\nexample, accuracy 10%,\n\nor 80% or a%, or 100%.\n\n() But, the dataset doesn't really care how the model does. The model will be ted\nnext another randomly sampled mini-batch in total disregard of model performance.\nIn other words, there are no long-term consequences.\n\nMoreover, in bandit problems, there\u2019s also no long-term consequence, though it's perhaps a\nbit harder to see why. Bandits are one-state one-step MDPs in which episodes terminate\nimmediately after a single action selection. Therefore, actions don\u2019t have long-term conse-\nquences in the performance of the agent during that episode.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.5,
                        "section_name": "Two-armed bandit",
                        "section_path": "./screenshots-images-2/chapter_8/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_5/9da1deef-4778-4979-a359-252ec3a6100e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "T machines\n\nii\n\u2018|\n\nAn intelligent agent, you!!!\n\nTwo-armed bandit\n\nNote: we assume slot machines have o. stationary\nprobability of payors, meaning the probability of\npayot$ will not change with a pull, which is likely\nincorrect for real slot machines.\n\n@ When you go to a casino and play the slot\nmachines, your goal is to Find the machine that\npays the most, and then stick to that arm.\n\n@) In bandit problems, we assume the probability\nof payot$ stays the same after every pull. This\nmakes it a. one-shot kind of problem.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.6,
                        "section_name": "Deep reinforcement learning agents\ndeal with evaluative feedback",
                        "section_path": "./screenshots-images-2/chapter_8/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_6/73feecc0-b9f6-439f-a92c-bfdc00c6b4e8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents\ndeal with evaluative feedback\n\nThe second property we learned about is that of evaluative feedback. Deep reinforcement\nlearning, tabular reinforcement learning, and bandits all deal with evaluative feedback. The\ncrux of evaluative feedback is that the goodness of the feedback is only relative, because the\nenvironment is uncertain. We don\u2019t know the actual dynamics of the environment; we don\u2019t\nhave access to the transition function and reward signal.\n\nAs a result, we must explore the environment around us to find out what\u2019s out there. The\nproblem is that, by exploring, we miss capitalizing on our current knowledge and, therefore,\nlikely accumulate regret. Out of all this, the exploration-exploitation trade-off arises. It\u2019s a\nconstant by-product of uncertainty. While not having access to the model of the environ-\nment, we must explore to gather new information or improve on our current information.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.7,
                        "section_name": "Evaluative feedback",
                        "section_path": "./screenshots-images-2/chapter_8/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_7/580ef7fd-9af1-4f06-979b-be3827d1175b.png",
                            "./screenshots-images-2/chapter_8/section_7/393b4532-90b7-4946-9dd6-6fbb06de49cc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Evaluative feedback\n\n@ To understand the challenge of evaluative feedback, you must be aware\nthot agents don\u2019t see entire maps such as this one.\n\nSo\ncn =\n\n@) Instead, they only see the current\nstate and reward such as this one.\n\n@) So, is that 410 bad?\n\nBut, if it isn\u2019t evaluative, what is it?\n\nThe opposite of evaluative feedback is supervised feedback. In a classification problem, your\nmodel receives supervision; that is, during learning, your model is given the correct labels for\neach of the samples provided. There\u2019s no guessing. If your model makes a mistake, the correct\nanswer is provided immediately afterward. What a good life!\n\nClassification is \u201csupervised\u201d\n\nDataset \u2014] \u2014_ Model\n\u00a9 tach mini-batch contains\n\nthe correct answers (labels), (2 So, the\nwhich are given to the agent model tries\nand will be\n~ given the\nSomebody said cheating!?!?t correct\nanswers after\n\n@) out, you know life gives you no \u201cright\u201d answer! \u2014 each trial.\n\nThe fact that correct answers are given to the learning algorithm makes supervised feedback\nmuch easier to deal with than evaluative feedback. That\u2019s a clear distinction between super-\nvised learning problems and evaluative-feedback problems, such as multi-armed bandits,\ntabular reinforcement learning, and deep reinforcement learning.\n\nBandit problems may not have to deal with sequential feedback, but they do learn from\nevaluative feedback. That\u2019s the core issue bandit problems solve. When under evaluative\nfeedback, agents must balance exploration versus exploitation requirements. If the feedback\nis evaluative and sequential at the same time, the challenge is even more significant. Algorithms\nmust simultaneously balance immediate- and long-term goals and the gathering and utiliza-\ntion of information. Both tabular reinforcement learning and DRL agents learn from feed-\nback that\u2019s simultaneously sequential and evaluative.\n\nBandits deal with evaluative feedback\n\n@ You go pull the First arm and get $10. Is that good or bad? What if the other\ngives you $50? What if it gives you. #1 with every pull for the next Soo pulls?!\nYour \u2014] Slot ee\n\n@) more importantly, how do\n{you know i? you could do\n\nmachine?\n@) Nobody is there to tell you;\nthere's no supervision.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.8,
                        "section_name": "Deep reinforcement learning agents\ndeal with sampled feedback",
                        "section_path": "./screenshots-images-2/chapter_8/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_8/6296b267-1c01-4fcc-8577-3088c406000d.png",
                            "./screenshots-images-2/chapter_8/section_8/90818ef1-8248-4386-aecb-5e7dc03846e3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning agents\ndeal with sampled feedback\n\nWhat differentiates deep reinforcement learning from tabular reinforcement learning is the\ncomplexity of the problems. In deep reinforcement learning, agents are unlikely to sample all\npossible feedback exhaustively. Agents need to generalize using the gathered feedback and\ncome up with intelligent decisions based on that generalization.\n\nThink about it. You can\u2019t expect exhaustive feedback from life. You can\u2019t be a doctor and\na lawyer and an engineer all at once, at least not if you want to be good at any of these. You\nmust use the experience you gather early on to make more intelligent decisions for your\nfuture. It\u2019s basic. Were you good at math in high school? Great, then, pursue a math-related\ndegree. Were you better at the arts? Then, pursue that path. Generalizing helps you narrow\nyour path going forward by helping you find patterns, make assumptions, and connect the\ndots that help you reach your optimal self.\n\nBy the way, supervised learning deals with sampled feedback. Indeed, the core challenge\nin supervised learning is to learn from sampled feedback: to be able to generalize to new sam-\nples, which is something neither multi-armed bandit nor tabular reinforcement learning\nproblems do.\n\nSampled feedback\n@ imagine you are Feeding your agent images as states. @ They have three channels\n@) tach image is alo- i\nby-I60 pixels.\n\n@) Each pixel in an\n8-bit image can have\na value from 0 to ass.\n\n|= \u00a9 How many possible states is that, you ask?\n\n(&) That's (ass)*\"?=(W0,581,37S)\"\" = a lot!\n\n\u00a9 For giggles, \\ ran this in Python and it returns a a4a,S80-digit number. To put it in perspective, the\nKnown, observable universe has between 10\u2122 and 10\u2122 atoms, which is an 83-digit number at most.\n\nBut, if it isn\u2019t sampled, what is it?\n\nThe opposite of sampled feedback is exhaustive feedback. To exhaustively sample environ-\nments means agents have access to all possible samples. Tabular reinforcement learning and\nbandits agents, for instance, only need to sample for long enough to gather all necessary\ninformation for optimal performance. To gather exhaustive feedback is also why there are\noptimal convergence guarantees in tabular reinforcement learning. Common assumptions,\nsuch as \u201cinfinite data\u201d or \u201csampling every state-action pair infinitely often,\u201d are reasonable\nassumptions in small grid worlds with finite state and action spaces.\n\nSequential, evaluative, and exhaustive feedback\n\n@ Again, this is what sequential @ And this is what evaluative\nFeedback looks like. feedback looks like.\n\u00a5 +\n\n@) but, given you have a discrete number of states and actions, you can sample the\nenvironment exhaustively. In small state and action spaces, things are easy in practice, and\ntheory is doable. As the number of state and action spaces increases, the need For function\napproximation becomes evident.\n\nThis dimension we haven\u2019t dealt with until now. In this book so far, we surveyed the tabular\nreinforcement learning problem. Tabular reinforcement learning learns from evaluative,\nsequential, and exhaustive feedback. But, what happens when we have more complex prob-\nlems in which we cannot assume our agents will ever exhaustively sample environments?\nWhat if the state space is high dimensional, such as a Go board with 10'\u201d states? How about\nAtari games with (255\u00b0)?!\u00b0* ' at 60 Hz? What if the environment state space has continuous\nvariables, such as a robotic arm indicating joint angles? How about problems with both\nhigh-dimensional and continuous states or even high-dimensional and continuous actions?\nThese complex problems are the reason for the existence of the field of deep reinforcement\nlearning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.9,
                        "section_name": "Introduction to function approximation\nfor reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_8/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_9/fc351c31-0da0-4d7b-958e-b058c774edb9.png",
                            "./screenshots-images-2/chapter_8/section_9/350a7dd6-5360-4321-9850-5a2be39be272.png",
                            "./screenshots-images-2/chapter_8/section_9/bfae3f4f-058e-4e2a-bca4-be4050b40e4b.png",
                            "./screenshots-images-2/chapter_8/section_9/c9ea946e-190b-4470-a9e3-c7c2aaab0503.png",
                            "./screenshots-images-2/chapter_8/section_9/99db093c-f21d-4a0c-a729-84cda9701ab2.png",
                            "./screenshots-images-2/chapter_8/section_9/5fe600b9-118f-45d5-8ae9-17eb93206a1b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction to function approximation\nfor reinforcement learning\n\nIt\u2019s essential to understand why we use function approximation for reinforcement learning in\nthe first place. It\u2019s common to get lost in words and pick solutions due to the hype. You know,\nif you hear \u201cdeep learning,\u201d you get more excited than if you hear \u201cnon-linear function\napproximation,\u201d yet they\u2019re the same. That\u2019s human nature. It happens to me; it happens to\nmany, I\u2019m sure. But our goal is to remove the cruft and simplify our thinking.\n\nIn this section, I provide motivations for the use of function approximation to solve rein-\nforcement learning problems in general. Perhaps a bit more specific to value functions, than\nRL overall, but the underlying motivation applies to all forms of DRL.\n\nReinforcement learning problems\ncan have high-dimensional state and action spaces\n\nThe main drawback of tabular reinforcement learning is that the use of a table to represent\nvalue functions is no longer practical in complex problems. Environments can have high-\ndimensional state spaces, meaning that the number of variables that comprise a single state is\nvast. For example, Atari games described above are high dimensional because of the 210-by-\n160 pixels and the three color channels. Regardless of the values that these pixels can take\nwhen we talk about dimensionality, we\u2019re referring to the number of variables that make up\na single state.\n\nHigh-dimensional state spaces\n\n\u00a9 This is astate. /\u2014\u2014\u2014>\ntach state is a unique\nconfiguration of variables.\n\n@ For example,\n\nvelocity, target, location,\n\nPatel values and so on. +\u2014H \u00a9) p high-dimensional state has\nmany variables. A single image\nframe from Atari, for example,\nhas al0 x 60 x 3 = 100,800 pixels.\n\n\nReinforcement learning problems\ncan have continuous state and action spaces\n\nEnvironments can additionally have continuous variables, meaning that a variable can take\non an infinite number of values. To clarify, state and action spaces can be high dimensional\nwith discrete variables, they can be low dimensional with continuous variables, and so on.\n\nEven if the variables aren\u2019t continuous and, therefore, not infinitely large, they can still\ntake on a large number of values to make it impractical for learning without function\napproximation. This is the case with Atari, for instance, where each image-pixel can take on\n256 values (0-255 integer values.) There you have a finite state-space, yet large enough to\nrequire function approximation for any learning to occur.\n\nBut, sometimes, even low-dimension state spaces can be infinitely large state spaces. For\ninstance, imagine a problem in which only the x, y, z coordinates of a robot compose the state-\nspace. Sure, a three-variable state-space is a pretty low-dimensional state-space environment,\nbut what if any of the variables is provided in continuous form, that is, that variable can be of\ninfinitesimal precision? Say, it could be a 1.56, or 1.5683, or 1.5683256, and so on. Then, how\ndo you make a table that takes all these values into account? Yes, you could discretize the state\nspace, but let me save you time and get right to it: you need function approximation.\n\nContinuous state spaces\n\n@ This is a state. /\u2014\u2014\u2014>\n\ntach state is a. +\u2014H @ voriables can be\n\nunique configuration position, velocity,\n\nof variables. target, location, pixel,\nvalue, and so on.\n\n(3) A continuous state-space has at least State\nnumber of values. For example, position, 0.0 - 100.0\n\nangles, and altitude are variables that\n\ncan have infinitesimal accuracy: say, a.l,\nor 2.1a, or 2.1a3, and So on. a ae |\n\nConcrete Example\nThe cart-pole environment\n\nThe cart-pole environment is a classic in reinforcement learning. The state space is low\ndimensional but continuous, making it an excellent environment for developing algorithms;\ntraining is fast, yet still somewhat challenging, and function approximation can help.\n\nThis is the cart-pole environment\n\n@ The cart-pole environment\nconsists of balancing a pole. -\u2014>\n\n(@ The\n\npole is\n\nhinged to \u00a9@ The cart can move\nacart -K\u2014] le# or right along a track.\n\nIts state space is comprised of four variables:\n+ The cart position on the track (x-axis) with a range from -2.4 to 2.4\n+ The cart velocity along the track (x-axis) with a range from -inf to inf\n+ The pole angle with a range of ~-40 degrees to ~ 40 degrees\n+ The pole velocity at the tip with a range of -inf to inf\nThere are two available actions in every state:\n+ Action 0 applies a -1 force to the cart (push it left)\n+ Action 1 applies a +1 force to the cart (push it right)\nYou reach a terminal state if\n+ The pole angle is more than 12 degrees away from the vertical position\n+ The cart center is more than 2.4 units from the center of the track\n+ The episode count reaches 500 time steps (more on this later)\nThe reward function is\n+ +1 for every time step\n\nThere are advantages when using function approximation\n\nI\u2019m sure you get the point that in environments with high-dimensional or continuous state\nspaces, there are no practical reasons for not using function approximation. In earlier chap-\nters, we discussed planning and reinforcement learning algorithms. All of those methods\nrepresent value functions using tables.\n\nRerresH My Memory\nAlgorithms such as value iteration and Q-learning use tables for value functions\n\nValue iteration is a method that takes in an MDP and derives an optimal policy for such MDP\nby calculating the optimal state-value function, v*. To do this, value iteration keeps track of\nthe changing state-value function, v, over multiple iterations. In value iteration, the state-\nvalue function estimates are represented as a vector of values indexed by the states. This\nvector is stored with a lookup table for querying and updating estimates.\n\nA state-value function\n\n@ A stote-value function\n\nis indexed by the state, -\u2014\u2014\u2014\u2014- State 0\nandit returns avalue '\u2014\u2014\u2014\u2014\u2014\u2014\u2014>y [ 45\nrepresenting the expected\n\nreward-to-go ot the given state.\n\nThe Q-learning algorithm does not need an MDP and doesn't use a state-value function.\nInstead, in Q-learning, we estimate the values of the optimal action-value function, q*.\nAction-value functions are not vectors, but, instead, are represented by matrices. These\nmatrices are 2D tables indexed by states and actions.\n\nAn action-value function\nStates\n\nActions\n\nAn action-value function, @, is indexed by the state and the action, and it returns a\nvalue representing the expected reward-to-go For taking that action ot that stote.\n\n<= Boum Down\nFunction approximation can make our algorithms more efficient\n\nIn the cart-pole environment, we want to use generalization because it\u2019s a more efficient use\nof experiences. With function approximation, agents learn and exploit patterns with less data\n(and perhaps faster).\n\nA state-value function with and\nwithout function approximation\n\n0) Imagine this stote-value Function. V=[-2.5, -1.1, 0.7, 3.2, 7.6]\n\n@ without Function approximation,\neach value is independent. (]\n\n_-0-0-l).. Value\n\n@) With function approximation, the\nunderlying relationship of the states\ncan be learned and exploited,\n\n@ The benefit of using Function approximation is particularly obvious if you\nimagine these plots after even a single update.\n\n(S) without function\napproximation, the update\nonly changes one state.\n(&) with function\napproximation, the updates o 1 2 3 4\n\nchange multiple states. i}\n\n@ OF course, this is a. simplitied example, but it helps illustrate what's happening.\nWhat would be different in \u201creal\u201d examples?\n\nFirst, if we approximate an action-value Function, Q, we'd have to add another\ndimension.\n\nAlso, with a non-linear function approximator, such as a neural network, more\ncomplex relationships can be discovered.\n\n\nWhile the inability of value iteration and Q-learning to solve problems with sampled feed-\nback make them impractical, the lack of generalization makes them inefficient. What I mean\nby this is that we could find ways to use tables in environments with continuous-variable\nstates, but we'd pay a price for doing so. Discretizing values could indeed make tables possi-\nble, for instance. But, even if we could engineer a way to use tables and store value functions,\nby doing so, we\u2019d miss out on the advantages of generalization.\n\nFor example, in the cart-pole environment, function approximation would help our agents\nlearn a relationship in the x distance. Agents would likely learn that being 2.35 units away from\nthe center is a bit more dangerous than being 2.2 away. We know that 2.4 is the x boundary.\nThis additional reason for using generalization isn\u2019t to be understated. Value functions often\nhave underlying relationships that agents can learn and exploit. Function approximators,\nsuch as neural networks, can discover these underlying relationships.\n\n2% Boi mt Down\nReasons for using function approximation\n\nOur motivation for using function approximation isn\u2019t only to solve problems that aren't\nsolvable otherwise, but also to solve problems more efficiently.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.1,
                        "section_name": "NFQ: The first attempt at value-based\ndeep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_8/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_10/aece5c20-dd54-4564-952d-18b989e46c25.png",
                            "./screenshots-images-2/chapter_8/section_10/39142d12-66a0-4406-a18e-50a3ee72b12b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "NFQ: The first attempt at value-based\ndeep reinforcement learning\n\nThe following algorithm is called neural fitted Q (NFQ) iteration, and it\u2019s probably one of the\nfirst algorithms to successfully use neural networks as a function approximation to solve\nreinforcement learning problems.\n\nFor the rest of this chapter, I discuss several components that most value-based deep\nreinforcement learning algorithms have. I want you to see it as an opportunity to decide on\ndifferent parts that we could\u2019ve used. For instance, when I introduce using a loss function\nwith NFQ, I discuss a few alternatives. My choices aren\u2019t necessarily the choices that were\nmade when the algorithm was originally introduced. Likewise, when I choose an optimiza-\ntion method, whether root mean square propagation (RMSprop) or adaptive moment esti-\nmation (Adam), I give a reason why I use what I use, but more importantly, I give you context\nso you can pick and choose as you see fit.\n\nWhat I hope you notice is that my goal is not only to teach you this specific algorithm but,\nmore importantly, to show you the different places where you could try different things.\nMany RL algorithms feel this \u201cplug-and-play\u201d way, so pay attention.\n\nFirst decision point: Selecting a value function to approximate\n\nUsing neural networks to approximate value functions can be done in many different ways.\nTo begin with, there are many different value functions we could approximate.\n\nRerresH My Memory\nValue functions\nYou've learned about the following value functions:\n+ The state-value function v/s)\n+ The action-value function q(s,a)\n+ Theaction-advantage function a(s,a)\n\nYou probably remember that the state-value function v(s), though useful for many purposes,\nisn't sufficient on its own to solve the control problem. Finding v(s) helps you know how\nmuch expected total discounted reward you can obtain from state s and using policy m there-\nafter. But, to determine which action to take with a V-function, you also need the MDP of the\nenvironment so that you can do a one-step look-ahead and take into account all possible\nnext states after selecting each action.\n\nYou likely also remember that the action-value function q(s,a) allows us to solve the control\nproblem, so it\u2019s more like what we need to solve the cart-pole environment: in the cart-pole\nenvironment, we want to learn the values of actions for all states in order to balance the pole\nby controlling the cart. If we had the values of state-action pairs, we could differentiate the\nactions that would lead us to either gain information, in the case of an exploratory action, or\nmaximize the expected return, in the case of a greedy action.\n\n| want you to notice, too, that what we want to estimate the optimal action-value function\nand not just an action-value function. However, as we learned in the generalized policy\niteration pattern, we can do on-policy learning using an epsilon-greedy policy and estimate\nits values directly, or we can do off-policy learning and always estimate the policy greedy\nwith respect to the current estimates, which then becomes an optimal policy.\n\nLast, we also learned about the action-advantage function a(s,a), which can help us differ-\nentiate between values of different actions, and it also lets us easily see how much better\nthan average an action is.\n\nWe'll study how to use the v(s) and a(s) functions in a few chapters. For now, let\u2019s settle on\nestimating the action-value function q(s,a), just like in Q-learning. We refer to the approxi-\nmate action-value function estimate as Q(s,a; 0), which means the Q estimates are parame-\nterized by 0, the weights of a neural network, a state s and an action a.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.11,
                        "section_name": "Second decision point: Selecting a neural network architecture",
                        "section_path": "./screenshots-images-2/chapter_8/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_11/f1eb7edd-e9d4-4e50-93b0-db37e349082e.png",
                            "./screenshots-images-2/chapter_8/section_11/a8b0b6f6-c845-434a-bd96-7574cbcb61e4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Second decision point: Selecting a neural network architecture\n\nWe settled on learning the approximate action-value function Q(s,a; @). But although I sug-\ngested the function should be parameterized by @, s, and a, that doesn\u2019t have to be the case.\nThe next component we discuss is the neural network architecture.\n\nWhen we implemented\nthe Q-learning agent, you\nnoticed how the matrix\nholding the action-value\nfunction was indexed by\nstate and action pairs. A\nstraightforward neural net-\nwork architecture is to\ninput the state (the four\nstate variables in the cart-\npole environment), and the\naction to evaluate. The out-\nput would then be one node\nrepresenting the Q-value\nfor that state-action pair.\n\nThis architecture would\nwork fine for the cart-pole\nenvironment. But, a more\nefficient architecture con-\nsists of only inputting the\nstate (four for the cart-pole\nenvironment) to the neural\nnetwork and outputting the\nQ-values for all the actions\nin that state (two for the\ncart-pole environment).\nThis is clearly advantageous\n\nState-action-in-value-out architecture\n\nState variables in\n\n* Cart position\n\n* Cart velocity\n\n* Pole angle\n\n* Pole velocity at tip\nState s. For example,\n\nC-Ou, 1, 2.3, 1.17\n\nAction in -\u2014_>\naction a, for example, O.\nThis could also be represented as a.\none-hot vector, for example, Li, 0).\n\nState-in-values-out architecture\n\nState variables in\n* Cart position\n* Cart velocity vector of values out\n* Pole angle * Action 0 Jet)\n* Pole velocity at tip (Action | right)\n@(s), for example,\nState s, for example, 1.44, -3.5]\nC-Ou, Ll, 2.3, 11)\n\nwhen using exploration strategies such as epsilon-greedy or softmax, because having to do\nonly one pass forward to get the values of all actions for any given state yields a high-\nperformance implementation, more so in environments with a large number of actions.\n\nFor our NFQ implementation, we use the state-in-values-out architecture: that is, four\ninput nodes and two output nodes for the cart-pole environment.\n\n| Speak PYTHON\nFully connected Q-function (state-in-values-out)\n\nclass FCQ(nn.Module) :\ndef init (self,\nHaines cobliy, @ Here you are just defining the\nSUEPUERO Ay input layer. See how we take in\nhiddenddimss (52752) ir input_dim and output the First\n\nBCE vet Onmr Cen sre ui: f of the hidd i romp\nsuper(FCQ, self). init  ()\n\nself.activation_fc = activation_fc\n\nself.input_layer = nn.Linear(input_dim,\nhidden_dims[0])\nself.hidden layers = nn.ModuleList ()\nfor i in range(len(hidden_dims)~-1):\nhidden_layer = nn.Linear (\nhidden _dims[i], hidden_dims[i+1])\nself.hidden layers.append (hidden layer)\n\n@) we then create the hidden layers. Notice how Hexible this class is in that it\n\nallows you to change the number of layers and units per layer. Pass a\n\ndifferent tuple, say (64, 3a, la), to the hidden_dims variable, and it will create\n\na. network with three hidden layers of 64, 3a, and lo units, respectively.\nself.output_layer = nn.Linear( \u00a2\u2014\u2014\u20144 @) we then connect\n\nhidden _dims[-1], output_dim) the last hidden layer\nto the output layer.\ndef forward(self, state): @ In the Forward Function, we First take in\nx = state eee\nif not isinstance(x, torch.Tensor):\nx = torch.tensor (x, \u00a9) We pass it through\n@ Then we do device=self.device, *he input layer and\nthe same for all dtype-torch. float32) then through the\n\nhidden layers. x = x.unsqueeze (0) activation function.\n\nx = self.activation_fc(self.input_layer (x) )\nfor hidden layer in self.hidden_ layers:\nx = self.activation_fc(hidden_ layer (x) )\nx = self.output_layer (x)\nGOATS & T\u2014 9 hind Finally, For the output layer, notice that\nwe don't apply the activation function to the\noutput but return it directly instead,\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.12,
                        "section_name": "Third decision point: Selecting what to optimize",
                        "section_path": "./screenshots-images-2/chapter_8/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_12/8b46399f-1522-4933-8daa-a862dc9a0a12.png",
                            "./screenshots-images-2/chapter_8/section_12/99d19a23-6c9f-4f38-8ed8-833574f9c52f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Third decision point: Selecting what to optimize\n\nLet\u2019s pretend for a second that the cart-pole environment is a supervised learning problem.\nSay you have a dataset with states as inputs and a value function as labels. Which value func-\ntion would you wish to have for labels?\n\nShow Me tHe Matu\nIdeal objective\nAn ideal objective in value- @ we want to have\n\nbased deep reinforcement an estimate of q% @\nlearning would be to minimize the NEI)\nloss with respect to the optimal that optimal Function.\n\naction-value function q*\u00ae\n\nL;(9;) = Es,a |(a-(s,4) a Q(s, a; 6:)) \"|\n\nL @) \\\u00a5 we had a solid estimate of g%\n\nwe then could use a greedy action with\n\nrespect to these estimates to get near-\n@) Obviously, I'm not talking about having optimal behavior\u2014only if we had that q%\naccess to q* so that we can use it;\notherwise, there\u2019s no need for learning.\nVm talking about access to sampling the\nq* some way: regression-style ML.\n\nOf course, the dream labels for learning the optimal action-value function are the corre-\nsponding optimal Q-values (notice that a lowercase q refers to the true values; uppercase is\ncommonly used to denote estimates) for the state-action input pair. That is exactly what the\noptimal action-value function q* (s,a) represents, as you know.\n\nIf we had access to the optimal action-value function, we'd use that, but if we had access to\nsampling the optimal action-value function, we could then minimize the loss between the\napproximate and optimal action-value functions, and that would be it.\n\nThe optimal action-value function is what we\u2019re after.\n\nRerresH My Memory\nOptimal action-value function\n\n@ As a reminder, here\u2019s the definition of the optimal action-value function. \u2014\u2014]\nCT @) This is just telling us that the\n@)...is the\n\npolieythat \u00a2.(s,a) = max Ez [GilSt = s, A; = al, Ws \u20ac S,Va \u20ac A(s)\nr\u2014\u2014>\n\ngives... T\n\n@...the \u00a9)... From each\nexpected in each and\nreturn every state.\n\nBut why is this an impossible dream? Well, the visible part is we don\u2019t have the optimal\naction-value function q*(s,a), but to top that off, we cannot even sample these optimal\nQ-values because we don\u2019t have the optimal policy either.\n\nFortunately, we can use the same principles learned in generalized policy iteration in which\nwe alternate between policy-evaluation and policy-improvement processes to find good\npolicies. But just so you know, because we\u2019re using non-linear function approximation, con-\nvergence guarantees no longer exist. It\u2019s the Wild West of the \u201cdeep\u201d world.\n\nFor our NFQ implementation, we do just that. We start with a randomly initialized\naction-value function (and implicit policy.) Then, we evaluate the policy by sampling actions\nfrom it, as we learned in chapter 5. Then, improve it with an exploration strategy such as\nepsilon-greedy, as we learned in chapter 4. Finally, keep iterating until we reach the desired\nperformance, as we learned in chapters 6 and 7.\n\n== Bonn Down\nWe can't use the ideal objective\n\nWe can't use the ideal objective because we don't have access to the optimal action-value\nfunction, and we don\u2019t even have an optimal policy to sample from. Instead, we must\nalternate between evaluating a policy (by sampling actions from it), and improving it (using\nan exploration strategy, such as epsilon-greedy). It\u2019s as you learned in chapter 6, in the gen-\neralized policy iteration pattern.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.13,
                        "section_name": "Fourth decision point: Selecting the targets for policy evaluation",
                        "section_path": "./screenshots-images-2/chapter_8/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_13/f391fac5-92b0-4a54-8a62-fb28823e86fa.png",
                            "./screenshots-images-2/chapter_8/section_13/89cd4b6b-acc6-43e5-af1c-9b52a807f297.png",
                            "./screenshots-images-2/chapter_8/section_13/01f5866d-0ec2-44ba-bab0-48a263cf5df7.png",
                            "./screenshots-images-2/chapter_8/section_13/a1eed4bb-7e82-48fd-9d81-4fcc21a4307b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Fourth decision point: Selecting the targets for policy evaluation\n\nThere are multiple ways we can evaluate a policy. More specifically, there are different targets\nwe can use for estimating the action-value function of a policy 7. The core targets you learned\nabout are the Monte Carlo (MC) target, the temporal-difference (TD) target, the n-step\ntarget, and lambda target.\n\nMC, TD, n-step, and lambda targets\n\nMC TD N-step Lambda\n(n=2)\n@ me: you use all @ TO: you use @) arstep is like @ Lambda.\nreward found in a the value of the TD, but instead target mixes in an\ntrajectory from a. next state as an of bootstrapping exponentially\nstart state to the estimate of all after one step, decaying fashion\nterminal state. reward to go. you use \u201cn\u201d steps. all n-step targets\ninto one.\n\u00a9) we will be using \u2014I\nthe TD target.\n\nWe could use any of these targets and get solid results, but this time for our NFQ implemen-\ntation, we keep it simple and use the TD target for our experiments.\n\nYou remember that the TD targets can be either on-policy or off-policy, depending on\nthe way you bootstrap the target. The two main ways for bootstrapping the TD target are to\neither use the action-value function of the action the agent will take at the landing state, or\nalternatively, to use the value of the action with the highest estimate at the next state.\n\nOften in the literature, the on-policy version of this target is called the SARSA target, and\nthe off-policy version is called the Q-learning target.\n\nSuow Me tHe Matu\nOn-policy and off-policy TD targets\n\u00a9 Notice that both on-policy and off-policy targets estimate an action-value function.\n@ However, if we were to use the on-policy target, the target |\n\nwould approximate the behavioral policy; the policy generating\nbehavior and the policy being learned would be the same.\n\nyears? = Reza + YQ(St41, Atsi3 9)\n\nyer learning _ = Rist + ymax Q(S1+1, 4; i)\n\n@ This isn\u2019t true For the of\u00a7-policy target in which we always\n\napproximate the greedy policy, even if the policy generating\nbehavior isn\u2019t totally greedy.\n\nIn our NFQ implementation, we use the same off-policy TD target we used in the Q-learning\nalgorithm. At this point, to get an objective function, we need to substitute the optimal\naction-value function q*(s,a), that we had as the ideal objective equation, by the Q-learning\ntarget.\n\nSHow Me THE Matu\nThe Q-learning target, an off-policy TD target\nIn practice, an online Q-learning target would look something like this. a\n\nBottom line is we use the\nSeen ees cece ue learning \u2014 Resi + Yes Q(St41, 4; 4)\nnext state to form the target. I\u201d\nSee can Pig in a. more general\n@) out it\u2019s basically form of this Q-learning target here.\nthe same. We're using\n\n2\ntheegestaton ot > Li(Gi) = Esayrs\u2019 [(r-+7 max Q(s',a'; 04) \u2014 Q(s,056%))\u2019|\nexperience tuples .. . _____ Ft . a\n\n(5)... to minimize the loss.\n\n@ when differentiating through\n\nths equation Ws werent yo CD The gradient must only go ;\nnotice the gradient doesn\u2019t involve Tea ne Pree eae ns\nthe target. +z, one common source of error.\n\n| Speak PYTHON\nit\nQ-learning target \u00a9 First, we get the\nJ values of the Q-function\nq_sp = self.online model (next_states).detach() gy sprime (next state).\n\nThe sin next_states\n@) The \u2018detach\u2019 here is important. We\n\n. means that this is a batch\nshould not be propagating values through of next_state.\nthis. We're only calculating targets.\n@ Then, we get the max\nmax_a q Sp = q_sp.max(1) [0] .unsqueeze (1) Value of the next state max_a.\n\n(S) One important step, often overlooked, is to\nensure terminal states are grounded to zero.\n@) Also, notice is_terminals are batches of\nis_terminal Flags, which are merely Flags\nmax_a_q sp =* (1 - is terminals) indicating whether the next_state is a\n\n@ The unsqueeze adds a dimension to\nthe vector so the operations that Follow\nwork on the correct elements.\n\nterminal state or not.\ntarget_q_s = rewards + self.gamma * max_a_q sp\n4 @ we now caleulate the target.\n\nq_sa = self.online model(states).gather(1, actions)\n\nt____, @ Finally, we get the current estimate of @(s,a).\nAt this point, we're ready to create our loss function.\n\nI want to bring to your attention two issues that I, unfortunately, see often in DRL implemen-\ntations of algorithms that use TD targets.\n\nFirst, you need to make sure that you only backpropagate through the predicted values.\nLet me explain. You know that in supervised learning, you have predicted values that come\nfrom the learning model, and true values that are commonly constants provided in advance.\nIn RL, often the \u201ctrue values\u201d depend on predicted values themselves: they come from the\nmodel.\n\nFor instance, when you form a TD target, you use a reward, which is a constant, and the\ndiscounted value of the next state, which comes from the model. Notice, this value is also not\na true value, which is going to cause all sorts of problems that we'll address in the next chapter.\nBut what I also want you to notice now is that the predicted value comes from the neural\nnetwork. You have to make this predicted value a constant. In PyTorch, you do this only by\ncalling the detach method. Please look at the two previous boxes and understand these points.\nThey\u2019 re vital for the reliable implementation of DRL algorithms.\n\nThe second issue that I want to raise before we move on is the way terminal states are\nhandled when using OpenAI Gym environments. The OpenAI Gym step, which is used to\ninteract with the environment, returns after every step a handy flag indicating whether the\nagent just landed on a terminal state. This flag helps the agent force the value of terminal\nstates to zero, which, as you remember from chapter 2, is a requirement to keep the value\nfunctions from diverging. You know the value of life after death is nil.\n\nThe tricky part is that some OpenAI\nGym environments, such as the cart-pole,\nhave a wrapper code that artificially termi-\nnates an episode after some time steps. In\nCartPole-v0, the time step limit is 200, and\nin CartPole-v1 it is 500. This wrapper code\nhelps to prevent agents from taking too\nlong to complete an episode, which can be\nuseful, but it can get you in trouble. Think\nabout it: what do you think the value of\nhaving the pole straight up in time step 500\nwould be? I mean, if the pole is straight up,\nand you get +1 for every step, then the true\nvalue of straight-up is infinite. Yet, since at\n\nWhat's the value of this state?\n\n@ Take a\nWhat's the value\nof this state?\n\nLy\n\n@ HINT: This state looks pretty good to me! The\ncart-pole seems to be under control in a\nstraight-up position. Perhaps the best action is\n+o push right, but it doesn\u2019t seem like a critical\nstate. Both actions are probably similarly valued.\n\ntime step 500 your agent times out, and a terminal flag is passed to the agent, you'll bootstrap\non zero if you're not careful. This is bad. I cannot stress this enough. There is a handful of\nways you can handle this issue, and here are the two common ones. Instead of bootstrapping\non zero, bootstrap on the value of the next state as predicted by the network, if either you\n(1) reach the time step limit for the environment or (2) find the key \u201cTimeLimit.truncated\u201d\nin the info dictionary. Let me show you the second way.\n\n| Speak PyTHON\nProperly handling terminal states\n\nJF! Owe cohect an experience tuple as usual.\n\nnew_state, reward, is terminal, info = env.step(action)\nis_truncated = 'TimeLimit.truncated' in info and \\\n\nL__ eortnen creas tor the\n\ninfo['TimeLimit.truncated']\nTimeLimit-truncated,.\n\nis_failure = is_terminal and not is truncated \u00a2\u20144 @)A foilureis\n\ndefined as Follows.\n\nexperience = (state, action, reward, new_state, float(is failure) )\n\n@) Finally, we add the terminal Flag if the episode ended in failure.\nIf it isn't a Failure, we want to bootstrap on the value of the new_state.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.14,
                        "section_name": "Fifth decision point: Selecting an exploration strategy",
                        "section_path": "./screenshots-images-2/chapter_8/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_14/c655cbf3-05e2-4357-b926-f1ce0747d683.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Fifth decision point: Selecting an exploration strategy\n\nAnother thing we need to decide is which policy improvement step to use for our generalized\npolicy iteration needs. You know this from chapters 6 and 7, in which we alternate a policy\nevaluation method, such as MC or TD, and a policy improvement method that accounts for\nexploration, such as decaying e-greedy.\n\nIn chapter 4, we surveyed many different ways to balance the exploration-exploitation\ntrade-off, and almost any of those techniques would work fine. But in an attempt to keep it\nsimple, we\u2019re going to use an epsilon-greedy strategy on our NFQ implementation.\n\nBut, I want to highlight the implication of the fact that we\u2019re training an off-policy learning\nalgorithm here. What that means is that there are two policies: a policy that generates behavior,\nwhich in this case is an epsilon-greedy policy, and a policy that we\u2019re learning about, which is\nthe greedy (an ultimately optimal) policy.\n\nOne interesting fact of off-policy learning algorithms you studied in chapter 6 is that the\npolicy generating behavior can be virtually anything. That is, it can be anything as long as it\nhas broad support, which means it must ensure enough exploration of all state-action pairs.\nIn our NFQ implementation, I use an epsilon-greedy strategy that selects an action randomly\n50% of the time during training. However, when evaluating the agent, I use the action greedy\nwith respect to the learned action-value function.\n\n| Speak PyTHoN\nEpsilon-greedy exploration strategy\n\nclass EGreedyStrategy () : @ The select_action Function of the epsilon-greedy\nSao ok strategy\u2019 starts by pulling out the Q-values for state s\n\ndef select_action(self, model, state):\nwith torch.no grad():\nq_values = model (state) .cpu() .detach()\nco q_values = q_values.data.numpy() .squeeze()\n(@ | make the values \u2018NumPy friendly\" and remove an extra dimension.\nif np.random.rand() > self.epsilon: 4\u00a2\u20144@) Then, get arandom\naction = np.argmax(q_values) number and, if greater\nelse: than epsilon, act greedily.\naction = np.random.randint (len(q_ values) )\n\n@) otherwise, act randomly in the number of actions.\n\n<...> \u2014\u2014 \u00a9 NoTe: | always query the model to calculate stats. aut,\nreturn action you. shouldn't do that iF your goal is performance!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.15,
                        "section_name": "Sixth decision point: Selecting a loss function",
                        "section_path": "./screenshots-images-2/chapter_8/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_15/9dfd23e8-03a8-4e29-86a4-bff4d47f8eca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Sixth decision point: Selecting a loss function\n\nA loss function is a measure of how well our neural network predictions are. In supervised\nlearning, it\u2019s more straightforward to interpret the loss function: given a batch of predictions\nand their corresponding true values, the loss function computes a distance score indicating\nhow well the network has done in this batch.\n\nThere are many different ways for calculating this distance score, but I continue to keep\nit simple in this chapter and use one of the most common ones: MSE (mean squared error,\nor L2 loss). Still, let me restate that one challenge in reinforcement learning, as compared to\nsupervised learning, is that our \u201ctrue values\u201d use predictions that come from the network.\n\nMSE (or L2 loss) is defined as the average squared difference between the predicted and\ntrue values; in our case, the predicted values are the predicted values of the action-value\nfunction that come straight from the neural network: all good. But the true values are, yes,\nthe TD targets, which depend on a prediction also coming from the network, the value of the\nnext state.\n\nCircular dependency of the action-value function\n\n@ Trace the\nuse of the Policy produces\ndi\nfunction. is used to produces\ncalculate the\nData\nTargets Action-value\nfunction\nis used to\ncalculate the\nare used to\n\ncalculate the\n\nAs you may be thinking, this circular dependency is bad. It\u2019s not well behaved because it\ndoesn\u2019t respect several of the assumptions made in supervised learning problems. We'll cover\nwhat these assumptions are later in this chapter, and the problems that arise when we violate\nthem in the next chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.16,
                        "section_name": "Seventh decision point: Selecting an optimization method",
                        "section_path": "./screenshots-images-2/chapter_8/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_16/063e27dd-f0e5-4bca-8d6f-1c3fc82b9fb2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Seventh decision point: Selecting an optimization method\n\nGradient descent is a stable optimization method given a couple of assumptions: data must\nbe independent and identically distributed (IID), and targets must be stationary. In rein-\nforcement learning, however, we cannot ensure any of these assumptions hold, so choosing a\nrobust optimization method to minimize the loss function can often make the difference\nbetween convergence and divergence.\n\nIf you visualize a loss function as a landscape with valleys, peaks, and planes, an optimization\nmethod is the hiking strategy for finding areas of interest, usually the lowest or highest point\nin that landscape.\n\nA classic optimization method in supervised learning is called batch gradient descent. The\nbatch gradient descent algorithm takes the entire dataset at once, calculates the gradient of\nthe given dataset, and steps toward this gradient a little bit at a time. Then, it repeats this cycle\nuntil convergence. In the landscape analogy, this gradient represents a signal telling us the\ndirection we need to move. Batch gradient descent isn\u2019t the first choice of researchers because\nit isn\u2019t practical to process massive datasets at once. When you have a considerable dataset\nwith millions of samples, batch gradient descent is too slow to be practical. Moreover, in\nreinforcement learning, we don\u2019t even have a dataset in advance, so batch gradient descent\nisn\u2019t a practical method for our purpose either.\n\nAn optimization method capable of handling smaller batches of data is called mini-batch\ngradient descent. In mini-batch gradient descent, we use only a fraction of the data at a time.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.17,
                        "section_name": "Batch gradient descent",
                        "section_path": "./screenshots-images-2/chapter_8/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_17/d619f0d0-3852-4495-9d2d-01ddd0b53414.png",
                            "./screenshots-images-2/chapter_8/section_17/163f5fa8-8522-41fe-9e85-0c7b5bb56345.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Batch gradient descent\n\n\u00ae eatch gradient\ndescent goes smoothly\ntoward the target\nbecouse it uses the\nentire dataset at once,\nso lower variance is\n\nexpected,\n\nWe process a mini-batch of samples to find its loss, then backpropagate to compute the\ngradient of this loss, and then adjust the weights of the network to make the network better\nat predicting the values of that mini-batch. With mini-batch gradient descent, you can control\nthe size of the mini-batches, which allows the processing of large datasets.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.18,
                        "section_name": "Mini-batch gradient descent",
                        "section_path": "./screenshots-images-2/chapter_8/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_18/82d6cd59-c831-4a6c-9e4a-4071fc0a1065.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Mini-batch gradient descent\n\n4+\u2014 0) in mini-batch gradient\ndescent we use a uniformly\nsampled mini-batch. This results\nin noisier updates, but also\nfaster processing of the data.\n\nAt one extreme, you can set the size of your mini-batch to the size of your dataset, in which\ncase, you're back at batch gradient descent. At the other extreme, you can set the mini-batch\nsize to a single sample per step. In this case, you\u2019re using an algorithm called stochastic gradient\ndescent.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.19,
                        "section_name": "Stochastic gradient descent",
                        "section_path": "./screenshots-images-2/chapter_8/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_19/7a51146a-3de3-489a-bb05-acc81ce94612.png",
                            "./screenshots-images-2/chapter_8/section_19/62a3b471-2ee2-412f-92bf-d871c8397ebe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stochastic gradient descent\n\n+<\u2014\u2014F 00 with stochastic gradient descent, in\nevery iteration we step through only\none sample. This makes it a noi\nalgorithm. It wouldn't be surprising to\nsee several steps taking us further\nfrom the target, and later back\ntoward the target.\n\nThe larger the batch, the lower the variance the steps of the optimization method have. But\nuse a batch too large, and learning slows down considerably. Both extremes are too slow in\npractice. For these reasons, it\u2019s common to see mini-batch sizes ranging from 32 to 1024.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.2,
                        "section_name": "Zig-zag pattern of mini-batch gradient descent",
                        "section_path": "./screenshots-images-2/chapter_8/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_20/40e1b591-66cf-4d68-89f5-a28ddaaaea53.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Zig-zag pattern of mini-batch gradient descent\n\n@ It isn\u2019t uncommon to see mini-batch gradient descent develop a.\n2ig-2a9 pattern toward the target.\n\nAn improved gradient descent algorithm is called gradient descent with momentum, or\nmomentum for short. This method is a mini-batch gradient descent algorithm that updates\nthe network\u2019s weights in the direction of the moving average of the gradients, instead of the\ngradient itself.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.21,
                        "section_name": "Mini-batch gradient descent vs. momentum",
                        "section_path": "./screenshots-images-2/chapter_8/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_21/995c4837-f64b-4fef-9e25-ddb5a57a4732.png",
                            "./screenshots-images-2/chapter_8/section_21/d4918e03-e153-4578-bf40-71dd817fede7.png",
                            "./screenshots-images-2/chapter_8/section_21/47027d0d-fc3e-459f-abfd-db7af637f006.png",
                            "./screenshots-images-2/chapter_8/section_21/f82847d2-72f2-40e0-b7d2-2ea404bd7600.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Mini-batch gradient descent vs. momentum\n\n\u00ae mini-batch\ngradient descent\nrom the last image\n\n@ This would\nbe momentum.\n\nAn alternative to using momentum is called root mean square propagation (RMSprop). Both\nRMSprop and momentum do the same thing of dampening the oscillations and moving\nmore directly towards the goal, but they do so in different ways.\n\nWhile momentum takes steps in the direction of the moving average of the gradients,\nRMSprop takes the safer bet of scaling the gradient in proportion to a moving average of the\nmagnitude of gradients. It reduces oscillations by merely scaling the gradient in proportion to\nthe square root of the moving average of the square of the gradients or, more simply put, in\nproportion to the average magnitude of recent gradients.\n\n(D) Micuet\u2019s Anatocy\n\nS Optimization methods in value-based deep reinforcement learning\nTo visualize RMSprop, think of the steepness change of the surface of your loss function. If\ngradients are high, such as when going downhill, and the surface changes to a flat valley,\nwhere gradients are small, the moving average magnitude of gradients is higher than the\nmost recent gradient; therefore, the size of the step is reduced, preventing oscillations or\novershooting.\n\nIf gradients are small, such as in a near-flat surface, and they change to a significant\ngradient, as when going downhill, the average magnitude of gradients is small, and the new\ngradient large, therefore increasing the step size and speeding up learning.\n\nA final optimization method Id like to introduce is called adaptive moment estimation\n(Adam). Adam is a combination of RMSprop and momentum. The Adam method steps in\nthe direction of the velocity of the gradients, as in momentum. But, it scales updates in pro-\nportion to the moving average of the magnitude of the gradients, as in RMSprop. These\nproperties make Adam as an optimization method a bit more aggressive than RMSprop, yet\nnot as aggressive as momentum.\n\nIn practice, both Adam and RMSprop are sensible choices for value-based deep reinforce-\nment learning methods. I use both extensively in the chapters ahead. However, I do prefer\nRMSprop for value-based methods, as you'll soon notice. RMSprop is stable and less sensitive\nto hyperparameters, and this is particularly important in value-based deep reinforcement\nlearning.\n\n0001 A Br oF History\n\nIntroduction of the NFQ algorithm\nNFQ was introduced in 2005 by Martin Riedmiller in a paper called \u201cNeural Fitted Q Iteration \u2014\nFirst Experiences with a Data Efficient Neural Reinforcement Learning Method\u201d After 13 years\nworking as a professor at a number of European universities, Martin took a job as a research\nscientist at Google DeepMind.\n\nIt\u2019s in THE DetaiLs\nThe full neural fitted Q-iteration (NFQ) algorithm\nCurrently, we've made the following selections:\n+ Approximate the action-value function Q(s,q; 9).\n+ Use a state-in-values-out architecture (nodes: 4, 512,128, 2).\n\n+ Optimize the action-value function to approximate the optimal action-\nvalue function q*(s,a).\n\n+ Use off-policy TD targets (r+ y*max_a\u2019Q(s/a\u2019; 8)) to evaluate policies.\n+ Use an epsilon-greedy strategy (epsilon set to 0.5) to improve policies.\n+ Use mean squared error (MSE) for our loss function.\n+ Use RMSprop as our optimizer with a learning rate of 0.0005.\nNFQ has three main steps:\n1. Collect E experiences: (s, a, r, s; d) tuples. We use 1024 samples.\n2. Calculate the off-policy TD targets: r+ y*max_a\u2018Q(s/a\u2018; 6).\n3. Fit the action-value function Q(s,q; @) using MSE and RMSprop.\n\nThis algorithm repeats steps 2 and 3 K number of times before going back to step 1. That's\nwhat makes it fitted: the nested loop. We'll use 40 fitting steps K.\n\nNFQ\n\nCalculate\nthe off-policy\nTD targets:\nr+y max_a\u2019Q (s\u2019, a\u2019, 8)\n\nRepeat Fit the action-value\nK function Q(s, a; 6)\ntimes with RMSprop and MSE\n\nCollect\nE experience\nsamples\n\n\nTatty it Up\nNFQ passes the cart-pole environment\n\nAlthough NFQ is far from a state-of-the-art, value-based deep reinforcement learning\nmethod, in a somewhat simple environment, such as the cart-pole, NFQ shows a decent\nperformance.\n\nMoving Avg Reward (Training) @ one interesting point is that\nyou can see the training reward\nnever reaches the max of SOO\nreward per episode. The reason\nis we're using an epsilon of O.s.\nHaving such a high exploration\nrate helps with Finding more\n\nMoving Avg Reward (Evaluation) accurate value functions, but it\n400 shows worse performance during\n300 training.\nzor @) On the second Figure we\n100 plot the mean reward during\n\u00b0 evaluation steps. The evaluation\nTotal Steps steps are the best\n\u2014 @) The main issue with NFQ is\nsome that it takes too many steps\n50000 to get decent performance. In\ne other words, in terms of sample\nTraining Time efficiency, NFQ does poorly.\nvs it needs many samples before it\n100 gets decent results. It doesn't\ni | get the most out of each sample.\n: @) The next two plots are\n. related to time. You can see\nWall-clock Time how NFQ tokes approximately\ncad 80 seconds on average to pass\n1so is the time excluding evaluation\n100 steps, statistics, and so on.\n* \u00a9 wall-clock time is how long\nar soo 1 eg 8 2000 it takes to run from beginning\n\nto end\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.22,
                        "section_name": "Things that could (and do) go wrong",
                        "section_path": "./screenshots-images-2/chapter_8/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_22/44d3a15f-bcdc-4654-99a9-fe2af18e62fc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Things that could (and do) go wrong\n\nThere are two issues with our algorithm. First, because we\u2019re using a powerful function\napproximator, we can generalize across state-action pairs, which is excellent, but that also\nmeans that the neural network adjusts the values of all similar states at once.\n\nNow, think about this for a second: our target values depend on the values for the next\nstate, which we can safely assume are similar to the states we are adjusting the values of in the\nfirst place. In other words, we\u2019re creating a non-stationary target for our learning updates. As\nwe update the weights of the approximate Q-function, the targets also move and make our\nmost recent update outdated. Thus, training becomes unstable quickly.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.23,
                        "section_name": "Non-stationary target",
                        "section_path": "./screenshots-images-2/chapter_8/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_23/92e33692-68e5-4e12-af2e-cb57147c9945.png",
                            "./screenshots-images-2/chapter_8/section_23/7a188ee8-b5a3-4b94-8354-73469e9f6975.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Non-stationary target\n\n\u00a9 At First, our optimization\nwill behave as expected,\ngoing after the target. r\u2014-\n\n@) The problem is that as predictions improve,\nour target will improve, too, and change. \u2014]\n\nmethod can get into trouble. -\u2014\u2014\u2014>\n\nSecond, in NFQ, we batched 1024 experience samples collected online and update the net-\nwork from that mini-batch. As you can imagine, these samples are correlated, given that most\nof these samples come from the same trajectory and policy. That means the network learns\nfrom mini-batches of samples that are similar, and later uses different mini-batches that are\nalso internally correlated, but likely different from previous mini-batches, mainly if a differ-\nent, older policy collected the samples.\n\nAll this means that we aren\u2019t holding the IID assumption, and this is a problem because\noptimization methods assume the data samples they use for training are independent and\nidentically distributed. But we\u2019re training on almost the exact opposite: samples on our dis-\ntribution are not independent because the outcome of a new state s is dependent on our\ncurrent state s.\n\nAnd, also, our samples aren\u2019t identically distributed because the underlying data generat-\ning process, which is our policy, is changing over time. That means we don\u2019t have a fixed data\ndistribution. Instead, our policy, which is responsible for generating the data, is changing and\nhopefully improving periodically. Every time our policy changes, we receive new and likely\ndifferent experiences. Optimization methods allow us to relax the IID assumption to a certain\ndegree, but reinforcement learning problems go all the way, so we need to do something\nabout this, too.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.24,
                        "section_name": "Data correlated with time",
                        "section_path": "./screenshots-images-2/chapter_8/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_24/9db8883d-8e1d-4d28-a6db-fd9f6dbe87fd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Data correlated with time\n\n@ Imagine we generate these data points in a single trajectory,\nSay the y-axis is the position of the cart along the track, and\nthe x-axis is the step of the trajectory. You can see how likely it\nis data points at adjacent time steps will be similar, making our\nfunction approximator likely to overFit to that local region.\n\nIn the next chapter, we look at ways of mitigating these two issues. We start by improving\nNFQ with the algorithm that arguably started the deep reinforcement learning revolution,\nDQN. We then follow by exploring many of the several improvements proposed to the orig-\ninal DQN algorithm over the years. We also look at double DQN in the next chapter, and\nthen in chapter 10, we look at dueling DQN and PER.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.25,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_8/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_25/42af09f9-6cad-483d-bc90-af95c703567c.png",
                            "./screenshots-images-2/chapter_8/section_25/071ae623-e436-4cab-877b-c269eb38345d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter, we gave a high-level overview of how sampled feedback interacts with sequen-\ntial and evaluative feedback. We did so while introducing a simple deep reinforcement learn-\ning agent that approximates the Q-function, that in previous chapters, we would represent\nin tabular form, with a lookup table. This chapter was an introduction to value-based deep\nreinforcement learning methods.\n\nYou learned the difference between high-dimensional and continuous state and action\nspaces. The former indicates a large number of values that make up a single state; the latter\nhints at at least one variable that can take on an infinite number of values. You learned that\ndecision-making problems could be both high-dimensional and continuous variables, and\nthat makes the use of non-linear function approximation intriguing.\n\nYou learned that function approximation isn\u2019t only beneficial for estimating expectations\nof values for which we only have a few samples, but also for learning the underlying relation-\nships in the state and action dimensions. By having a good model, we can estimate values for\nwhich we never received samples and use all experiences across the board.\n\nYou had an in-depth overview of different components commonly used when building\ndeep reinforcement learning agents. You learned you could approximate different kinds of\nvalue functions, from the state-value function v(s) to the action-value q(s, a). And, you can\napproximate these value functions using different neural network architectures; we explored\nthe state-action pair in, value out, to the more efficient state-in, values out. You learned about\nusing the same objective we used for Q-learning, using the TD target for off-policy control.\nAnd, you know there are many different targets you can use to train your network. You sur-\nveyed exploration strategies, loss functions, and optimization methods. You learned that deep\nreinforcement learning agents are susceptible to the loss and optimization methods we select.\nYou learned about RMSprop and Adam as the stable options for optimization methods.\n\nYou learned to combine all of these components into an algorithm called neural fitted\nQ-iteration. You learned about the issues commonly occurring in value-based deep rein-\nforcement learning methods. You learned about the IID assumption and the stationarity of\nthe targets. You learned that not being careful with these two issues can get us into trouble.\n\nBy now, you\n\n+ Understand what it is to learn from feedback that is sequential, evaluative, and\nsampled\n\n+ Can solve reinforcement learning problems with continuous state-spaces\n\n+ Know about the components and issues in value-based DRL methods\n\nTWEETABLE FEAT\nWork on your own and share your findings\n\nHere are several ideas on how to take what you've learned to the next level. If you'd like, share\nyour results with the rest of the world and make sure to check out what others have done,\ntoo. It\u2019s a win-win situation, and hopefully, you'll take advantage of it.\n\n+ #gdri_chO8_tf01: After tabular reinforcement learning, and before deep reinforce-\nment learning, there are a couple things to explore. With this hashtag, explore and\nshare results for state discretization and tile coding techniques. What are those? Are\nthere any other techniques that we should know about?\n\n+ #gdri_ch08_tf02: The other thing I'd like you to explore is the use of linear func-\ntion approximation, instead of deep neural networks. Can you tell us how other func-\ntion approximation techniques compare? What techniques show promising results?\n\n+ #gdri_ch08_tf03: In this chapter, | introduced gradient descent as the type of opti-\nmization method we use for the remainder of the book. However, gradient descent is\nnot the only way to optimize a neural network; did you know? Either way, you should\ngo out there and investigate other ways to optimize a neural network, from black-box\noptimization methods, such as genetic algorithms, to other methods that aren't as\npopular. Share your findings, create a Notebook with examples, and share your results.\n\n+ #gdri_ch08_tf04:| started this chapter with a better way for doing Q-learning\nwith function approximation. Equally important to knowing a better way is to have\nan implementation of the simplest way that didn\u2019t work. Implement the minimal\nchanges to make Q-learning work with a neural network: that is, Q-learning with\nonline experiences as you learned in chapter 6. Test and share your results.\n\n+ #gdrl_ch08_tf05: In every chapter, I'm using the final hashtag as a catchall\nhashtag. Feel free to use this one to discuss anything else that you worked on relevant\nto this chapter. There\u2019s no more exciting homework than that which you create for\nyourself. Make sure to share what you set yourself to investigate and your results.\n\nWrite a tweet with your findings, tag me @mimoralea (I'll retweet), and use the particular\nhashtag from the list to help interested folks find your results. There are no right or wrong\nresults; you share your findings and check others\u2019 findings. Take advantage of this to socialize,\ncontribute, and get yourself out there! We're waiting for you!\n\nHere's a tweet example:\n\n\u201cHey, @mimoralea. | created a blog post with a list of resources to study deep reinforce-\nment learning. Check it out at <link>. #gdrl_ch01_tf01\u201d\n\nI'll make sure to retweet and help others find your work.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 9,
                "chapter_name": "More stable\nvalue-based methods",
                "chapter_path": "./screenshots-images-2/chapter_9",
                "sections": [
                    {
                        "section_id": 9.1,
                        "section_name": "More stable\nvalue-based methods",
                        "section_path": "./screenshots-images-2/chapter_9/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_1/4ccfbf73-86db-4ec5-9c5c-8f463755bc68.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the last chapter, you learned about value-based deep reinforcement learning. NFQ, the\nalgorithm we developed, is a simple solution to the two most common issues value-based\nmethods face: first, the issue that data in RL isn\u2019t independent and identically distributed. It\u2019s\nprobably the exact opposite. The experiences are dependent on the policy that generates\nthem. And, they aren\u2019t identically distributed since the policy changes throughout the train-\ning process. Second, the targets we use aren\u2019t stationary, either. Optimization methods\nrequire fixed targets for robust performance. In supervised learning, this is easy to see. We\nhave a dataset with premade labels as constants, and our optimization method uses these\nfixed targets for stochastically approximating the underlying data-generating function. In RL,\non the other hand, targets such as the TD target use the reward and the discounted predicted\nreturn from the landing state as a target. But this predicted return comes from the network\nwe\u2019re optimizing, which changes every time we execute the optimization steps. This issue\ncreates a moving target that creates instabilities in the training process.\n\nThe way NFQ addresses these issues is through the use of batching. By growing a batch, we\nhave the opportunity of optimizing several samples at the same time. The larger the batch, the\nmore the opportunity for collecting a diverse set of experience samples. This somewhat\naddresses the IID assumption. NFQ addresses the stationarity of target requirements by using\nthe same mini-batch in multiple sequential optimization steps. Remember that in NFQ, every\nE episode, we \u201cfit\u201d the neural network to the same mini-batch K times. That K allows the\noptimization method to move toward the target more stably. Gathering a batch and fitting\nthe model for multiple iterations is similar to the way we train supervised learning methods,\nin which we gather a dataset and train for multiple epochs.\n\nNFQ does an okay job, but we can do better. Now that we know the issues, we can address\nthem using better techniques. In this chapter, we explore algorithms that address not only\nthese issues, but other issues that you learn about making value-based methods more stable.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.2,
                        "section_name": "DQN: Making reinforcement learning\nmore like supervised learning",
                        "section_path": "./screenshots-images-2/chapter_9/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_2/66006a69-2668-4c1b-9ea5-97263eeaab41.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "DQN: Making reinforcement learning\nmore like supervised learning\n\nThe first algorithm that we discuss in this chapter is called deep Q-network (DQN). DQN is\none of the most popular DRL algorithms because it started a series of research innovations\nthat mark the history of RL. DQN claimed for the first time superhuman level performance\non an Atari benchmark in which agents learned from raw pixel data from mere images.\n\nThroughout the years, there have been many improvements proposed to DQN. And while\nthese days DQN in its original form is not a go-to algorithm, with the improvements, many\nof which you learn about in this book, the algorithm still has a spot among the best-performing\nDRL agents.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.3,
                        "section_name": "Common problems in value-based deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_9/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_3/ddc22ca8-0a56-4a5f-882a-2a00966a766c.png",
                            "./screenshots-images-2/chapter_9/section_3/148b39fe-fd93-40ad-bf33-7de78651ee3b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Common problems in value-based deep reinforcement learning\n\nWe must be clear and understand the two most common problems that consistently show up\nin value-based deep reinforcement learning: the violations of the IID assumption, and the\nstationarity of targets.\n\nIn supervised learning, we obtain a full dataset in advance. We preprocess it, shuffle it, and\nthen split it into sets for training. One crucial step in this process is the shuffling of the data-\nset. By doing so, we allow our optimization method to avoid developing overfitting biases;\nreduce the variance of the training process; speed up convergence; and overall learn a more\ngeneral representation of the underlying data-generating process. In reinforcement learning,\nunfortunately, data is often gathered online; as a result, the experience sample generated at\ntime step t+] correlates with the experience sample generated at time step t. Moreover, as the\npolicy is to improve, it changes the underlying data-generating process changes, too, which\nmeans that new data is locally correlated and not evenly distributed.\n\n<= Boi Down\nData isn\u2019t independently and identically distributed (IID)\n\nThe first problem is non-compliance with the IID assumption of the data. Optimization meth-\nods have been developed with the assumption that samples in the dataset we train with are\nindependent and identically distributed.\n\nWe know, however, our samples aren't independent, but instead, they come from a\nsequence, a time series, a trajectory. The sample at time step t+7 is dependent on the sample\nat time step t. Samples are correlated, and we can't prevent that from happening; it\u2019s a natural\nconsequence of online learning.\n\nBut samples are also not identically distributed because they depend on the policy that\ngenerates the actions. We know the policy is changing through time, and for us that\u2019s a good\nthing. We want policies to improve. But that also means the distribution of samples (state-\naction pairs visited) will change as we keep improving.\n\nAlso, in supervised learning, the targets used for training are fixed values on your dataset;\nthey\u2019re fixed throughout the training process. In reinforcement learning in general, and even\nmore so in the extreme case of online learning, targets move with every training step of the\nnetwork. At every training update step, we optimize the approximate value function and\ntherefore change the shape of the function, possibly the entire value function. Changing the\nvalue function means that the target values change as well, which, in turn, means the targets\nused are no longer valid. Because the targets come from the network, even before we use\nthem, we can assume targets are invalid or biased at a minimum.\n\n<= Boum Down\nNon-stationarity of targets\nThe problem of the non-stationarity of targets is depicted. These are the targets we use to\ntrain our network, but these targets are calculated using the network itself. As a result, the\nfunction changes with every update, in turn changing the targets.\nNon-stationarity of targets\n\n@ The value of next state-action\n\n. pair changed, which means the target\nQ(s,a; theta) when calculating @(s,a; theta) isn\u2019t\nO convent : G@ new stationary; it changed!\n\n(@ State-action\n\npairs sampled at\ntand t+\n\n(SA) Si.) t\n\nIn NFQ, we lessen this problem by using a batch and fitting the network to a small fixed data-\nset for multiple iterations. In NFQ, we collect a small dataset, calculating targets, and opti-\nmize the network several times before going out to collect more samples. By doing this on a\nlarge batch of samples, the updates to the neural network are composed of many points across\nthe function, additionally making changes even more stable.\n\nDQN is an algorithm that addresses the question, how do we make reinforcement learning\nlook more like supervised learning? Consider this question for a second, and think about the\ntweaks you would make to make the data look IID and the targets fixed.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.4,
                        "section_name": "Using target networks",
                        "section_path": "./screenshots-images-2/chapter_9/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_4/9507ffe1-2794-42c3-8a5e-9c74de5208a7.png",
                            "./screenshots-images-2/chapter_9/section_4/53b41b5b-6c01-4203-abdc-2322fd1c06a0.png",
                            "./screenshots-images-2/chapter_9/section_4/2a8eafc9-9938-424e-859f-fad9a70ced35.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using target networks\n\nA straightforward way to make target values more stationary is to have a separate network\nthat we can fix for multiple steps and reserve it for calculating more stationary targets. The\nnetwork with this purpose in DQN is called the target network.\n\nQ-function optimization without a target network\n\n@ pt First everything will look @) But the target will move\nnormal. We just chase the target. as our Q-function improves.\n\n7? \u201c\\\n\n@) Then, things go bad, @) And the moving targets\n@ could create divergence.\n\n/\\\\S\n\nQ-function approximation with a target network\n\n@\n\n@ That way, the optimizer can\nmake stable progress toward it.\n\n> @\n\n) we eventually update @) This allows the algorithm\nthe target, and repeat. +o make stable progress.\n\n\nBy using a target network to fix targets, we mitigate the issue of \u201cchasing your own tail\u201d by\nartificially creating several small supervised learning problems presented sequentially to the\nagent. Our targets are fixed for as many steps as we fix our target network. This improves our\nchances of convergence, but not to the optimal values because such things don\u2019t exist with\nnon-linear function approximation, but convergence in general. But, more importantly, it\nsubstantially reduces the chance of divergence, which isn\u2019t uncommon in value-based deep\nreinforcement learning methods.\n\n= SHow Me THE MatH\nTarget network gradient update\n\nVo, Li(8i) = Es,a,r,s! [(r + ymax Q(s\u2018,a\u2019;8;) \u2014 Q(s, a; 6;))Vo,Q(s, a; 64)|\n\n\u00a9 The only difference between these two\nequations is the age of the neural network weights.\n\nVo,Li(8i) = Es,a,r,5\" [(r + ymax Q(s',a\u2019;0~) \u2014 Q(s, a; 4;)) Vo, Q(s, a; 6,)|\n\n@) A target network is a previous instance of the neural network that we freeze\nfor a number of steps. The gradient update now has time to catch up to the\ntarget, which is much more stable when Frozen. This adds stability to the updates.\n\nIt\u2019s important to note that in practice we don\u2019t have two \u201cnetworks,\u201d but instead, we have two\ninstances of the neural network weights. We use the same model architecture and frequently\nupdate the weights of the target network to match the weights of the online network, which\nis the network we optimize on every step. \u201cFrequently\u201d here means something different\ndepending on the problem, unfortunately. It\u2019s common to freeze these target network weights\nfor 10 to 10,000 steps at a time, again depending on the problem. (That\u2019s time steps, not\nepisodes. Be careful there.) If you\u2019re using a convolutional neural network, such as what\nyou'd use for learning in Atari games, then a 10,000-step frequency is the norm. But for\nmore straightforward problems such as the cart-pole environment, 10-20 steps is more\nappropriate.\n\nBy using target networks, we prevent the training process from spiraling around because\nwe're fixing the targets for multiple time steps, thus allowing the online network weights to\nmove consistently toward the targets before an update changes the optimization problem,\nand a new one is set. By using target networks, we stabilize training, but we also slow down\nlearning because you're no longer training on up-to-date values; the frozen weights of the\ntarget network can be lagging for up to 10,000 steps at a time. It\u2019s essential to balance stability\nand speed and tune this hyperparameter.\n\nI Speak PYTHON\nUse of the target and online networks in DON\n\ndef optimize model(self, experiences) :\nstates, actions, rewards, \\\nnext_states, is terminals = experiences\nbatch_size = len(is terminals)\n@ Notice how we now query a target network\n+o get the estimate of the next state.\nq_sp = self.target_model (next_states) .detach ()\nin @ We grab the maximum of those values, and\nmake sure to treat terminal states appropriately.\nmax_a_q sp = q_sp.max(1) [0] .unsqueeze (1)\nmax_a_ q sp *= (1 - is terminals)\n+ \u00a9 Finally, we create the TD targets.\ntarget_q sa = rewards + self.gamma * max_a_q sp\n+ @ query the current \u201conline\u201d estimate.\nq_sa = self.online model(states).gather(1, actions)\n\u2014 \u00a9 see those values to create the errors.\ntd_error = q sa - target_q sa\nvalue_loss = td_error.pow(2) .mul(0.5) .mean()\nself.value_optimizer.zero_grad()\n\nvalue_loss.backward() \u00ab4 @) Coleulate the\nself.value_optimizer.step() loss, and optimize\n~ the online network.\n\ndef interaction _step(self, state, env):\naction = self.training strategy.select_action(\n\n@ Noti Renae tipertop eI self.online model, state)\nmodel for selecting actions.\nnew_state, reward, is terminal, _ = env.step (action)\n\n<...>\nreturn new_state, is_ terminal\n\n(\u00ae This is how the target network\nGagging network) gets updated with\ndef update network (self) : we ong KG cote\n\nfor target, online in zip(\nself.target_model.parameters(),\nself.online_ model.parameters ()) :\ntarget.data.copy (online.data)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.5,
                        "section_name": "Using larger networks",
                        "section_path": "./screenshots-images-2/chapter_9/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_5/535191af-9576-4908-af37-1c1f84ccd4eb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using larger networks\n\nAnother way you can lessen the non-stationarity issue, to some degree, is to use larger net-\nworks. With more powerful networks, subtle differences between states are more likely to be\ndetected. Larger networks reduce the aliasing of state-action pairs; the more powerful the\nnetwork, the lower the aliasing; the lower the aliasing, the less apparent correlation between\nconsecutive samples. And all of this can make target values and current estimates look more\nindependent of each other.\n\nBy \u201caliasing\u201d here I refer to the fact that two states can look like the same (or quite similar)\nstate to the neural network, but still possibly require different actions. State aliasing can occur\nwhen networks lack representational power. After all, neural networks are trying to find sim-\nilarities to generalize; their job is to find these similarities. But, too small of a network can\ncause the generalization to go wrong. The network could get fixated with simple, easy to find\npatterns.\n\nOne of the motivations for using a target network is that they allow you to differentiate\nbetween correlated states more easily. Using a more capable network helps your network\nlearn subtle differences, too.\n\nBut, a more powerful neural network takes longer to train. It needs not only more data\n(interaction time) but also more compute (processing time). Using a target network is a more\nrobust approach to mitigating the non-stationary problem, but I want you to know all the\ntricks. It\u2019s favorable for you to know how these two properties of your agent (the size of your\nnetworks, and the use of target networks, along with the update frequency), interact and\naffect final performance in similar ways.\n\n<= Bon Down\nWays to mitigate the fact that targets in reinforcement learning are non-stationary\nAllow me to restate that to mitigate the non-stationarity issue we can\n1. Create a target network that provides us with a temporarily stationary target value.\n\n2. Create large-enough networks so that they can \u201csee\u201d the small differences between\nsimilar states (like those temporally correlated).\n\nTarget networks work and work well, and have been proven to work multiple times. The tech-\nnique of \u201clarger networks\" is more of a hand-wavy solution than something scientifically\nproven to work every time. Feel free to experiment with this chapter's Notebook. You'll find it\neasy to change values and test hypotheses.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.6,
                        "section_name": "Using experience replay",
                        "section_path": "./screenshots-images-2/chapter_9/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_6/73b85373-a084-4e30-bfed-97d544ae9e47.png",
                            "./screenshots-images-2/chapter_9/section_6/8d61fbe0-7956-4a54-a5f1-5b71082c1681.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using experience replay\n\nIn our NFQ experiments, we use a mini-batch of 1,024 samples, and train with it for 40 iter-\nations, alternating between calculating new targets and optimizing the network. These 1,024\nsamples are temporally correlated because most of them belong to the same trajectory, and\nthe maximum number of steps in a cart-pole episode is 500. One way to improve on this is to\nuse a technique called experience replay. Experience replay consists of a data structure, often\nreferred to as a replay buffer or a replay memory, that holds experience samples for several\nsteps (much more than 1,024 steps), allowing the sampling of mini-batches from a broad set\nof past experiences. Having a replay buffer allows the agent two critical things. First, the train-\ning process can use a more diverse mini-batch for performing updates. Second, the agent no\nlonger has to fit the model to the same small mini-batch for multiple iterations. Adequately\nsampling a sufficiently large replay buffer yields a slow-moving target, so the agent can now\nsample and train on every time step with a lower risk of divergence.\n\n0001 A Bit oF History\nIntroduction of experience replay\n\nExperience replay was introduced by Long-Ji Lin in a paper titled \u201cSelf-Improving Reactive\nAgents Based On Reinforcement Learning, Planning and Teaching,\u201d believe it or not, pub-\nlished in 1992! That's right, 1992! Again, that\u2019s when neural networks were referred to as\n\u201cconnectionism\u201d... Sad times!\n\nAfter getting his PhD from CMU, Dr. Lin moved through several technical roles in many\ndifferent companies. Currently, he\u2019s a Chief Scientist at Signifyd, leading a team that works on\na system to predict and prevent online fraud.\n\nThere are multiple benefits to using experience replay. By sampling at random, we increase\nthe probability that our updates to the neural network have low variance. When we used the\nbatch in NFQ, most of the samples in that batch were correlated and similar. Updating with\nsimilar samples concentrates the changes on a limited area of the function, and that poten-\ntially overemphasizes the magnitude of the updates. If we sample uniformly at random from\na substantial buffer, on the other hand, chances are that our updates to the network are better\ndistributed all across, and therefore more representative of the true value function.\n\nUsing a replay buffer also gives the impression our data is IID so that the optimization\nmethod is stable. Samples appear independent and identically distributed because of the sam-\npling from multiple trajectories and even policies at once.\n\nBy storing experiences and later sampling them uniformly, we make the data entering the\noptimization method look independent and identically distributed. In practice, the replay\nbuffer needs to have considerable capacity to perform optimally, from 10,000 to 1,000,000\nexperiences depending on the problem. Once you hit the maximum size, you evict the oldest\nexperience before inserting the new one.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.7,
                        "section_name": "DQN with a replay buffer",
                        "section_path": "./screenshots-images-2/chapter_9/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_7/6d2f7e79-79ba-49a8-acb0-c6db675f85dc.png",
                            "./screenshots-images-2/chapter_9/section_7/d42b66ac-0b10-4a79-980b-6a8d5e8b02eb.png",
                            "./screenshots-images-2/chapter_9/section_7/f89e99d1-f0de-4401-8c4b-84a07bd2175b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "DQN with a replay buffer\n\nExploration\nGet Q-values strategy\nQ-function\nAction\nEnvironment\nTrain Q-function\nexperiences Transition\n\nStore\n\nexperience\nReplay buffer Ja SxPerrence State\nreward\n\nUnfortunately, the implementation becomes a little bit of a challenge when working with\nhigh-dimensional observations, because poorly implemented replay buffers hit a hardware\nmemory limit quickly in high-dimensional environments. In image-based environments, for\ninstance, where each state representation is a stack of the four latest image frames, as is com-\nmon for Atari games, you probably don\u2019t have enough memory on your personal computer\nto naively store 1,000,000 experience samples. For the cart-pole environment, this isn\u2019t so\nmuch of a problem. First, we don\u2019t need 1,000,000 samples, and we use a buffer of size 50,000\ninstead. But also, states are represented by four-element vectors, so there isn\u2019t so much of an\nimplementation performance challenge.\n\n= SHow Me tHe Mat\nReplay buffer gradient update\n\nVo, Li(O:) = Es,a,r,s' [(r + ymax Q(s',a\u2019:0-) \u2014 Q(s, a; 6;)) Vo, Q(s, a;6,)|\n\nLorne ny teens wtnan ht toque ht\nwe're now obtaining the experiences we use For training by\nsampling uniformly at random the replay buffer 0, instead\n\n[tetas\n\nVo, Lil) = Eps.ars'y~te(D) [(r + 7maxQ(s',a';0-) \u2014 Q(s,4;6;))Vo,Q(s, a; 44)|\n\nL, @ This is the Full gradient update For OGN, more precisely the one referred\nto as Nature DQN, Which is DQN with a target network and a replay buffer.\n\nNevertheless, by using a replay buffer, your data looks more IID and your targets more sta-\ntionary than in reality. By training from uniformly sampled mini-batches, you make the RL\nexperiences gathered online look more like a traditional supervised learning dataset with IID\ndata and fixed targets. Sure, data is still changing as you add new and discard old samples, but\nthese changes are happening slowly, and so they go somewhat unnoticed by the neural net-\nwork and optimizer.\n\n<% Bout Down\nExperience replay makes the data look IID, and targets somewhat stationary\n\nThe best solution to the problem of data not being IID is called experience replay.\n\nThe technique is simple, and it's been around for decades: As your agent collects experi-\nences tuples e.=(5,,A,,R,,,,5,,,) online, we insert them into a data structure, commonly referred\nto as the replay buffer D, such that D=e,, e,, ..., @,,}. M, the size of the replay buffer, is a value\noften between 10,000 to 1,000,000, depending on the problem.\n\nWe then train the agent on mini-batches sampled, usually uniformly at random, from the\nbuffer, so that each sample has equal probability of being selected. Though, as you learn in\nthe next chapter, you could possibly sample with another distribution. Just beware because\nit isn\u2019t that straightforward. We'll discuss details in the next chapter.\n\nI Speak PYTHON\n\nAsimple replay buffer\nclass ReplayBuffer(): -\u2014 This is a simple r bufSer with a\ndef _init_ (self, default maximum us| $0,000, and\n\nm_size=50000, a. default batch size of 64 samples.\nbatch_size=64):\nself.ss_ mem = np.empty(shape=(m_size), dtype=np.ndarray)\nself.as_ mem = np.empty(shape=(m_size), dtype=np.ndarray)\n<...> \u00a2\u20ac\u2014\u2014\u201440 @ we initialize Five arrays to hold states, actions, reward,\nnext states, and done fags. Shortened for brevity.\n@) we initialize several variables to do storage and sampling.\nLB self.m_size, self.batch_size = m_size, batch_size\nself. idx, self.size = 0, 0\n(@) when we store a new sample, we\ndef store(self, sample): begin by unwrapping the sample\nSs, a, Y, p, ad = sample Variable, and then setting each array\u2019s\nS) Again self.ss mem[self. idx] = s element to its corresponding value.\nremoved Self.as_mem[self. idx] =a\n\nfor <...> @) _idx points to the next index to modify, so we\nbrevity ++ increase it, and also make sure it loops back ofter\nself. idx += 1 reaching the maximum size (the end of the buffer).\n\nself. idx = self. idx % self.m_size\n+ \u00a9 Size also increases with every new sample stored, but\nself.size += 1 it doesn\u2019t loop back to 0; it stops growing instead.\nself.size = min(self.size, self.m_size)\n@ In the sample function, we begin by\ndef sample(self, batch_size=None) : determining the batch size. We use the\nif batch_size == None: \u00a2\u2014\u2014+ default of 4 if nothing else was passed.\n\nbatch size = self.batch size (9) Sample batch_size\nidxs = np.random.choice (f\u00a3 ids from 0 to size.\nself.size, batch_size, replace=False)\n\nlnm experiences = np.vstack(self.ss_ mem[{idxs]), \\\nnp.vstack(self.as_mem[idxs]), \\\n\nGo) Then earacs ne np.vstack(self.rs_mem[idxs]), \\\nexperiences from the buffer np.vstack(self.ps_mem[idxs]), \\\nusing the sampled ids. np.vstack(self.ds_mem[idxs] )\n\nreturn experiences \u00a2\u2014\u2014\\ (i) Andreturn those experiences.\n\ndef len (self): \u00a2\u20144 a) This is a handy function to return the correct\nreturn self.size size of the buffer when len(buffer is called.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.8,
                        "section_name": "Using other exploration strategies",
                        "section_path": "./screenshots-images-2/chapter_9/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_8/fb64b036-9de4-42a0-a9fd-d318a8803ce3.png",
                            "./screenshots-images-2/chapter_9/section_8/15bf9585-4c6c-4a92-9c7d-24978b8ce0b0.png",
                            "./screenshots-images-2/chapter_9/section_8/357867ba-e23b-4663-aa6e-acfb4f098198.png",
                            "./screenshots-images-2/chapter_9/section_8/97f58fda-03f7-4c03-8c04-99e6d44dd1f2.png",
                            "./screenshots-images-2/chapter_9/section_8/17251dbd-80bc-4e31-956c-fb1b365792ed.png",
                            "./screenshots-images-2/chapter_9/section_8/e8912177-2f34-4ed3-8ba3-28facc2a5eb8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using other exploration strategies\n\nExploration is a vital component of reinforcement learning. In the NFQ algorithm, we use an\nepsilon-greedy exploration strategy, which consists of acting randomly with epsilon proba-\nbility. We sample a number from a uniform distribution (0, 1). If the number is less than the\nhyperparameter constant, called epsilon, your agent selects an action uniformly at random\n(that\u2019s including the greedy action); otherwise, it acts greedily.\n\nFor the DQN experiments, I added to chapter 9\u2019s Notebook some of the other exploration\nstrategies introduced in chapter 4. I adapted them to use them with neural networks, and they\nare reintroduced next. Make sure to check out all Notebooks and play around.\n\n1 SPEAK PYTHON\nLinearly decaying epsilon-greedy exploration strategy\n\nclass EGreedyLinearStrategy () : 4110 'n a linearly decoying epson greedy\n<...> trategu, we start with a high epsilon value\ndef epsilon update (self) : noel das He alos ia Boer Fahions\nself.epsilon = 1 - self.t / self.max_steps\nself.epsilon (self.init_epsilon - self.min_epsilon) * \\\nself.epsilon + self.min_epsilon\nself.epsilon = np.clip(self.epsilon, \u2014-\nself.min_epsilon, @) we clip epsilon\nself.init_ epsilon) so bebesecn\nself.t += 1  \u00a2\u2014\u20144@Thisis a variable holding wine eee\nreturn self.epsilon the number of times epsilon minimum Volue-\n\nhas been updated. ) In the select_action\ndef select_action(self, model, state): method, we use a model\nself.exploratory action = False and a state.\nwith torch.no grad(): J \u00a9 For logging\n\nq_values = model (state) .cpu() .detach() purposes, |\nq_values = q values.data.numpy() . squeeze () always extract\n+... 1 the q_values.\nif np.random.rand() > self.epsilon: @ We draw the random number\naction = np.argmax(q_values) From a uniform distribution and\n\n[\u2019 else: compare it to epsilon.\naction = np.random.randint (len(q_ values) )\n@ If higher, we use the argmax of the\n\nself. epsilon update () q_values; otherwise, a random action.\nself.exploratory action = action != np.argmax(q_ values)\nreturn action \u00a2\u2014j(\u00ae) Finally, we update epsilon, set a variable for\n\n| Speak PYTHON\nExponentially decaying epsilon-greedy exploration strategy\n\nclass EGreedyExpStrategy ():\n<...>\n\ndef epsilon_update (self):\n\n@ In the exponentially decaying strategy, the only difference\nis that not epsiion is decaying in an exponential curve.\nself.epsilon = max(self.min_epsilon,\n\nself.decay rate * self.epsilon)\nreturn self.epsilon\n@) This is yet another way to exponentially decay epsilon, this one\nuses the exponential function. The epsilon values will be much the\nsame, but the decay rate will have to be a different scale.\n# def epsilon _update(self):\n# self.decay rate = 0.0001\n+ epsilon = self.init epsilon * np.exp( \\\n# -self.decay rate * self.t)\n#\n#\n#\n\nepsilon max(epsilon, self.min_epsilon)\nself.t t= 1\nreturn epsilon\n\ndef select_action(self, model, state):\nself.exploratory action = False\nwith torch.no grad():\nq_values = model (state) .cpu() .detach ()\nq_values = q values.data.numpy() .squeeze()\n(2) This select_action function is identical to the previous strategy. One thing | want to highlight\nis that I'm querying the q_values every time only because I'm collecting information to show to\nyou. But if you care about performance, this is a bad idea. A faster implementation would only\nquery the network after determining that a greedy action is being called For.\nif np.random.rand() > self.epsilon:\naction = np.argmax(q_ values)\nelse:\naction = np.random.randint (len(q_values) )\nself. epsilon update ()\n@) exploratory_action here is a variable used to calculate the percentage of\nexploratory actions taken per episode. Only used For logging information.\nself.exploratory action = action != np.argmax(q_ values)\nreturn action\n\n1 Speak PYTHON\n\nSoftmax exploration strategy\n\nclass SoftMaxStrategy() :\n<.2->\ndef update temp(self):\ntemp = 1 - self.t / (self.max_steps * self.explore ratio)\n| temp = (self.init_temp - self.min_temp) * \\\n@ In the softmax strategy, we use a temperature parameter, temp + self.min_temp\nwhich, the closer the value is to 0, the more pronounced the\ndifferences in the values will become, making action selection\nmore greedy. The temperature is decayed linearly.\ntemp = np.clip(temp, self.min_temp, self.init_temp)\nself.t += 1 \u00ae* ____> @ Here, after decaying the\nreturn temp temperature linearly, we clip its value to\nmake sure it\u2019s in an acceptable range.\ndef select_action(self, model, state):\nself.exploratory action = False\ntemp = self. update _temp()\nwith torch.no_grad():\n\n@ Notice that in the softmax strategy we really have no\nchance of avoiding extracting the q_values from the\nmodel. After all, actions depend directly on the values.\nq_values = model (state) .cpu() .detach()\nq_values = q values.data.numpy() .squeeze()\nL @ after extracting the values, we want to accentuate their\ndifferences (unless equals D.\nscaled_gs = q_values/temp\n\u00a9 we normalize them to avoid an overflow in the exp operation.\nnorm_qs = scaled _qs - scaled qs.max()\ne = np.exp(norm_qs) (\u00a9) Calculate the exponential.\nprobs = e / np.sum(e) @ Convert to probabilities.\nassert np.isclose(probs.sum(), 1.0)\n\n@ Finally, we use the probabilities to select an action. Notice\nhow we pass the probs variable to the p function argument.\naction = np.random.choice (np.arange(len(probs)),\n\nsize=1, p=probs) [0]\n@) And as before: Was the action the greedy or exploratory?\n\nself.exploratory action = action != np.argmax(q_values)\nreturn action\n\nIt\u2019s In THE DETAILS\n\nExploration strategies have an impactful effect on performance\n\n@ In NFQ, we used epsilon-greedy with a constant value of 0.5. Yes! That\u2019s SO% of the\ntime we acted greedily, and So% of the time, we chose uniformly at random. Given that\nthere are only two actions in this environment, the actual probability of choosing the\ngreedy action is 15%, and the chance of selecting the non-greedy action is aS%. Notice\nthat in a large action space, the probability of selecting the greedy action would be\nsmaller. In the Notebook, | output this effective probability value under \u201cex 100.\" That\nmeans \u201cratio of exploratory action over the last 100 steps.\u201d\n\nEpsilon-Greedy epsilon value Epsilon-Greedy exponentially decaying easilon value\n\n. f gf \u20ac LF F . \u00b0 # \u00a2 \u00a3\n@ In O@N and all remaining value-based algorithms in this and the Following\nchapter, | use the exponentially decaying epsilon-greedy strategy. | preter this\none because it\u2019s simple and it works well. Gut other, more advanced, strategies\nmay be worth trying. | noticed that even a small difference in hyperparameters\nmakes a. significant difference in performance. Make sure to test that yourself.\n\nEpsilon-Greedy linearty decaying epsilon value SoftMax linearly cecaying temperature value\n\nf # # & \u20ac * \u00a2 # F F F\n\n@) The plots in this box are the decaying schedules of all the different exploration\nstrategies available in chapter 9\u2019s Notebook. | highly encourage you to go through it\nand play with the many different hyperparameters and exploration strategies.\nThere\u2019s more to deep reinforcement learning than just the algorithms.\n\n\nIt\u2019s In THE DeTaILs\nThe full deep Q-network (DQN) algorithm\nOur DON implementation has components and settings similar to our NFQ:\n+ Approximate the action-value function Q(s,a; 8).\n+ Usea state-in-values-out architecture (nodes: 4, 512,128, 2).\n\n+ Optimize the action-value function to approximate the optimal action-\nvalue function q*(s,a).\n\n+ Use off-policy TD targets (r + gamma*max_a\u2019Q(s/a\u2019; 8)) to evaluate policies.\n+ Use mean squared error (MSE) for our loss function.\n+ Use RMSprop as our optimizer with a learning rate of 0.0005.\nSome of the differences are that in the DON implementation we now\n+ Use an exponentially decaying epsilon-greedy strategy to improve policies, decay-\ning from 1.0 to 0.3 in roughly 20,000 steps.\n+ Usea replay buffer with 320 samples min, 50,000 max, and mini-batches of 64.\n+ Usea target network that updates every 15 steps.\nDQN has three main steps:\n1. Collect experience: (S,,A,,R,,,,5,,, D,,,), and insert it into the replay buffer.\n2. Randomly sample a mini-batch from the buffer, and calculate the off-policy TD tar-\ngets for the whole batch: r + gamma*max_a\u2018Q(s/a\u2019; 8).\n3. Fit the action-value function Q(s,q; 6) using MSE and RMSprop.\n\n0 A Bit oF History\nIntroduction of the DQN algorithm\n\nDQN was introduced in 2013 by Volodymyr \u201cVlad\u201d Mnih in a paper called \u201cPlaying Atari with\nDeep Reinforcement Learning.\u201d This paper introduced DQN with experience replay. In 2015,\nanother paper came out, \u201cHuman-level control through deep reinforcement learning.\u201d This\nsecond paper introduced DQN with the addition of target networks; it\u2019s the full DQN version\nyou just learned about.\n\nVlad got his PhD under Geoffrey Hinton (one of the fathers of deep learning), and works as\na research scientist at Google DeepMind. He's been recognized for his DQN contributions,\nand has been included in the 2017 MIT Technology Review 35 Innovators under 35 list.\n\nTatty it Up\nDON passes the cart-pole environment\n\nThe most remarkable part of the results is that NFQ needs far more samples than DQN to\nsolve the environment; DQN is more sample efficient. However, they take about the same\ntime, both training (compute) and wall-clock time.\n\nMoving Avg Reward (Training)\n\u2014 NFO\n\nbead @ The most obvious conclusion\n200 we can draw From this First\n200\n100\n\ngraph is that DQN is more sample\npay attention to the curves, you\nMoving Avg Reward (Evaluation) than DQN. This is one of the most\n\naccomplished so Far.\n\n@) As you can see, they both\nsoo pass the cart-pole environment,\n\nbut DQN takes approximately\n\nas0 episodes while NFQ takes\n\n8\n\nsoa Total Steps almost a,S00 episodes. That\u2019s a\ntenfold reduction in samples.\n150000,\ntrend in sample efficiency, but\n70000) with time steps instead oF\n0 episodes: DQN takes about\nTraining Time $0,000 experience tuples, while\n200 2 | NFQ uses about 250,000.\n130 G) eut OGN takes more training\n100 time than NFQ to pass the\no environment. By training time,\n| mean the time from the\n\u00b0 beginning to the end of all\noso Wall-clock Time episodes, not just computation.\n200 \u00a9 In terms of wall-clock time\n150 (that\u2019s training time, statistics\n100 calculation, evaluation steps,\n~ and so on), they're both\n\u00b0 approximately Five minutes.\n\u00b0 500 1000 1500 2000,\n\nEpisodes\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.9,
                        "section_name": "Double DQN: Mitigating the overestimation\nof action-value functions",
                        "section_path": "./screenshots-images-2/chapter_9/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_9/35042bc2-d775-4513-8cbe-b3521318d104.png",
                            "./screenshots-images-2/chapter_9/section_9/1b1ad6f1-be50-4e0c-b5f4-b8efa3bc26e3.png",
                            "./screenshots-images-2/chapter_9/section_9/6ebd1762-35e4-43ae-9933-28c9e689ff67.png",
                            "./screenshots-images-2/chapter_9/section_9/87f52d9e-d9d1-4f7b-92c3-4f0dca27ada3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Double DQN: Mitigating the overestimation\nof action-value functions\n\nIn this section, we introduce one of the main improvements to DQN that have been pro-\nposed throughout the years, called double deep Q-networks (double DQN, or DDQN). This\nimprovement consists of adding double learning to our DQN agent. It\u2019s straightforward to\nimplement, and it yields agents with consistently better performance than DQN. The changes\nrequired are similar to the changes applied to Q-learning to develop double Q-learning; how-\never, there are several differences that we need to discuss.\n\nThe problem of overestimation, take two\n\nAs you can probably remember from chapter 6, Q-learning tends to overestimate action-\nvalue functions. Our DQN agent is no different; we\u2019re using the same off-policy TD target,\nafter all, with that max operator. The crux of the problem is simple: We're taking the max of\nestimated values. Estimated values are often off-center, some higher than the true values,\nsome lower, but the bottom line is that they\u2019re off. The problem is that we\u2019re always taking\nthe max of these values, so we have a preference for higher values, even if they aren\u2019t correct.\nOur algorithms show a positive bias, and performance suffers.\n\nMicuet\u2019s ANALOGY\n\nThe issue with overoptimistic agents, and people\n\nlL used to like super-positive people until | learned about double DON. No, seriously, imagine\nyou meet a very optimistic person; let\u2019s call her DON. DQN is extremely optimistic. She\u2019s expe-\nrienced many things in life, from the toughest defeat to the highest success. The problem\nwith DON, though, is she expects the sweetest possible outcome from every single thing she\ndoes, regardless of what she actually does. Is that a problem?\n\nOne day, DON went to a local casino. It was the first time, but lucky DQN got the jackpot at\nthe slot machines. Optimistic as she is, DON immediately adjusted her value function. She\nthought, \u201cGoing to the casino is quite rewarding (the value of Q/s,a) should be high) because\nat the casino you can go to the slot machines (next state s\u2018) and by playing the slot machines,\nyou get the jackpot [max_a\u2019 Q(s; a\u2019)]\"\n\nBut, there are multiple issues with this thinking. To begin with, DON doesn't play the slot\nmachines every time she goes to the casino. She likes to try new things too (she explores),\nand sometimes she tries the roulette, poker, or blackjack (tries a different action). Sometimes\nthe slot machine area is under maintenance and not accessible (the environment transitions\nher somewhere else). Additionally, most of the time when DQN plays the slot machines, she\ndoesn't get the jackpot (the environment is stochastic). After all, slot machines are called\nbandits for a reason, not those bandits, the other\u2014never mind.\n\nSeparating action selection from action evaluation\n\nOne way to better understand positive bias and how we can address it when using function\napproximation is by unwrapping the max operator in the target calculations. The max of a\nQ-function is the same as the Q-function of the argmax action.\n\nRerresH My Memory\nWhat's an argmax, again?\nThe argmax function is defined as the arguments of the maxima. The argmax action-value\nfunction, argmax Q-function, argmax,Q(s,a) is the index of the action with the maximum\nvalue at the given state s.\nFor example, if you have a Q(s) with values [-1, 0, -4, -9] for actions 0-3, the max,Q(s, a)\n\nis 0, which is the maximum value, and the argmax,Q(s, a) is 1 which is the index of the maxi-\nmum value.\n\nLet\u2019s unpack the previous sentence with the max and argmax. Notice that we made pretty\nmuch the same changes when we went from Q-learning to double Q-learning, but given that\nwe're using function approximation, we need to be cautious. At first, this unwrapping might\nseem like a silly step, but it helps me understand how to mitigate this problem.\n\n= SHow Me tHe Matu\nUnwrapping the argmax\n\nee = E(s,0,r,5')~U(D) [(r + ymax Q(s',a';0~) = Q(s,4;6;)) Vo, Q(s,0;6,)|\n\n@ what we're doing here is something silly.\nTake a. look at the equations at the top and\nbottom of the box and compare them.\n\nmax Q(s',a\u2019; a) Q(s', argmax Q(s',a\u2019; 0); 07)\n(@) There\u2019s no real difference between the two\nequations since both are using the same Q-values\nfor the target. The bottom line is that these two\n\nbits are the same thing written differently,\nVo, Li(9:) = Ejs,a,r,s.u(D) [(r +7Q(s', argmax Q(s',a': 07): 07) \u2014 Qs, a: 84,)) Vo,Q(s,0;6:)]\n\n1 Speak PYTHON\nUnwrapping the max in DON\n\n\u00a9 This is the original OQN way oF calculating targets. + @ Ws important\nq_sp = self.target_model (next_states) .detach () that we detach\n\nPr max_a_q sp = q_sp.max(1) [0] .unsqueeze(1) the target so\n\n() we pull the @-values of the next state and get their max thas we do not\nmax_a_q sp *= (1 - is terminals) backpropagate\n\ntarget_q sa = rewards + self.gamma * max_a_q_sp_ through it.\n(4) set the value of terminal states to 0, and calculate the targets.\n\n(S) This is an equivalent way of caleulati \u201cunwrapping the max.\u201d\n\new ba i iaek=|\n\n> argmax_a_q sp = self.target_model (next_states) .max(1) [1]\n(\u00a9) First, get the argmax action of the next state.\n\nrp? \u201csp = self. target_model (next_states) .detach ()\n\n@ Then, get the @-values of the next state as before.\n\n@ Now, we use the indices to get the max values of the next states.\nTy\u00bb max_a_q_sp = q sp[(np.arange (batch size), argmax_a_q sp]\n\nmax_a_q Sp = max_a_q sp.unsqueeze(1)\n\nmax_a_q sp *= (1 - is terminals)\n\ntarget_q sa = rewards + self.gamma * max_a_q sp\n(9) And proceed as before.\n\nAll we\u2019re saying here is that taking the max is like asking the network, \u201cWhat's the value of the\nhighest-valued action in state s?\u201d\n\nBut, we are really asking two questions with a single question. First, we do an argmax,\nwhich is equivalent to asking, \u201cWhich action is the highest-valued action in state s?\u201d\n\nAnd then, we use that action to get its value, equivalent to asking, \u201cWhat\u2019s the value of this\naction (which happens to be the highest-valued action) in state s?\u201d\n\nOne of the problems is that we are asking both questions to the same Q-function, which\nshows bias in the same direction in both answers. In other words, the function approximator\nwill answer, \u201cI think this one is the highest-valued action in state s, and this is its value.\u201d\n\nAsolution\n\nA way to reduce the chance of positive bias is to have two instances of the action-value func-\ntion, the way we did in chapter 6.\n\nIf you had another source of the estimates, you could ask one of the questions to one and\nthe other question to the other. It\u2019s somewhat like taking votes, or like an \u201cI cut, you choose\nfirst\u201d procedure, or like getting a second doctor\u2019s opinion on health matters.\n\nIn double learning, one estimator selects the index of what it believes to be the highest-\nvalued action, and the other estimator gives the value of this action.\n\nRerresH My Memory\nDouble learning procedure\n\nWe did this procedure with tabular reinforcement learning in chapter 6 under the double\nQ-learning agent. It goes like this:\n\n+ You create two action-value functions, Q, and Q,.\n\n+ You flip a coin to decide which action-value function to update. For example, Q, on\nheads, Q, on tails.\n\n+ Ifyou got a heads and thus get to update Q,: You select the action index to evalu-\nate from Q,, and evaluate it using the estimate Q, predicts. Then, you proceed to\nupdate Q, as usual, and leave Q, alone.\n\n+ Ifyou got a tails and thus get to update Q,, you do it the other way around: get the\nindex from Q,, and get the value estimate from Q,. Q, gets updated, and Q, is left\nalone.\n\nHowever, implementing this double-learning procedure exactly as described when using\nfunction approximation (for DQN) creates unnecessary overhead. If we did so, we\u2019d end up\nwith four networks: two networks for training (Q,, Q,) and two target networks, one for each\nonline network.\n\nAdditionally, it creates a slowdown in the training process, since we'd be training only one\nof these networks at a time. Therefore, only one network would improve per step. This is\ncertainly a waste.\n\nDoing this double-learning procedure with function approximators may still be better\nthan not doing it at all, despite the extra overhead. Fortunately for us, there\u2019s a simple modi-\nfication to the original double-learning procedure that adapts it to DQN and gives us sub-\nstantial improvements without the extra overhead.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.1,
                        "section_name": "A more practical solution",
                        "section_path": "./screenshots-images-2/chapter_9/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_10/74452782-81c3-4dd1-9404-33189905c576.png",
                            "./screenshots-images-2/chapter_9/section_10/10efa817-d6b5-476e-93c4-a1f0af343f6e.png",
                            "./screenshots-images-2/chapter_9/section_10/12e4da6f-1d08-49d8-bfd3-d220975255c0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A more practical solution\n\nInstead of adding this overhead that\u2019s a detriment to training speed, we can perform double\nlearning with the other network we already have, which is the target network. However,\ninstead of training both the online and target networks, we continue training only the online\nnetwork, but use the target network to help us, in a sense, cross-validate the estimates.\n\nWe want to be cautious as to which network to use for action selection and which network\nto use for action evaluation. Initially, we added the target network to stabilize training by\navoiding chasing a moving target. To continue on this path, we want to make sure we use the\nnetwork we're training, the online network, for answering the first question. In other words,\nwe use the online network to find the index of the best action. Then, we use the target net-\nwork to ask the second question, that is, to evaluate the previously selected action.\n\nThis is the ordering that works best in practice, and it makes sense why it works. By using\nthe target network for value estimates, we make sure the target values are frozen as needed\nfor stability. If we were to implement it the other way around, the values would come from\nthe online network, which is getting updated at every time step, and therefore changing\ncontinuously.\n\nSelecting action, evaluating action\n\neR\n\n@ The online network tells the Target network\ntarget network:\n\u201c| think action 3 is the best action.\u201d\nQ(s,0) = 3.5 Q(s,0) = 3.8\nQ(s,1) = 1.2 @ The good thing is Q\\s,1) = 1.0\nthat the torget network\nQ(s,2) = -2 has estimates, t00. 1\u00bb | Q(s,2) =-1.5\nQ(s,3) = 3.9 a(s,3) = 3.6\n(3) The target network\nThey selects its estimate of action\nOe the 3, which is what the online\n: network recommended.\n\nestimates used.\n\nA Bit oF History\n0001\nIntroduction of the double DQN algorithm\n\nDouble DQN was introduced in 2015 by Hado van Hasselt, shortly after the release of the\n2015 version of DON. (The 2015 version of DQN is sometimes referred to as Nature DQN\u2014\nbecause it was published in the Nature scientific journal, and sometimes as Vanilla DOQN\u2014\nbecause it is the first of many other improvements over the years.)\n\nIn 2010, Hado also authored the double Q-learning algorithm (double learning for the\ntabular case), as an improvement to the Q-learning algorithm. This is the algorithm you\nlearned about and implemented in chapter 6.\n\nDouble DQN, also referred to as DDQN, was the first of many improvements proposed\nover the years for DON. Back in 2015 when it was first introduced, DDQN obtained state-of-\nthe-art (best at the moment) results in the Atari domain.\n\nHado obtained his PhD from the University of Utrecht in the Netherlands in artificial intel-\nligence (reinforcement learning). After a couple of years as a postdoctoral researcher, he got\na job at Google DeepMind as a research scientist.\n\n= SHow Me THE MatH\nDDOQN gradient update\n@ So Far the gradient updates look as Follows.\n@ we sample uniformly at random From the\nreplay buffer a experience tuple (s, a, r, s\u2018).\nVo, Li(9:) = Ejs,a,r,8\")-u(D) G + 7Q(s\u2019, argmax Q(s\u2019,a';4~);4~) \u2014 Q(s,a:4;)) Vo,Q(s, a;6,)|\n\n@ we then calculate the TD target\nand error using the target network. @) Finally, calculate the\ngradients only through\n\u00a9) The only difference in DDN is now we use the predicted values.\nthe online weights to select the action, but still\nuse the frozen weights to get the estimate.\n\nVo, Li(9:) = E(s.a.r,5')~u(D) [(r + 7Q(s', argmax Q(s', a\"; 4:);8-) \u2014 Q(s, a; 8;)) Vo,Q(s, a; 4,)|\n\n1 Speak PYTHON\nDouble DON\n\ndef optimize model(self, experiences) :\nstates, actions, rewards, \\\nnext_states, is terminals = experiences\nbatch_size = len(is terminals)\n@ In Double O@N, we use the online network to get the index of the highest-valued action\nof the next state, the argmax. Note we don't detach the argmax because they're not\ndifferentiable. The max(Li] returned the index of the max, which is already \u201cdetached.\u201d\n#argmax_a_q sp = self.target_model(next_states) .max(1) [1]\nargmax_a_q sp = self.online model (next_states) .max(1) [1]\nemer extract the Q-values of the next state according to the target network.\nq_sp = self.target_model (next_states) .detach ()\n@) we then index the @-values provided by the target network\nmax_a_q sp = q_sp(np.arange(batch_ size), argmax_a_q sp]\n@) Then set up the targets as usual.\nmax_a_q sp = max_a_q sp.unsqueeze (1)\nmax_a_q sp *= (1 - is terminals)\ntarget_q sa = rewards + (self.gamma * max_a_q sp)\n\u00a9) @et the current estimates. Note this is where the gradients are Flowing through.\ncs q_sa = self.online model(states).gather(1, actions)\ntd_error = q_ sa - target_q sa\nvalue_loss = td_error.pow(2) .mul (0.5) .mean()\nself.value_optimizer.zero_grad() 4\u2014\u20144 \u00a9) Calculate the loss,\nvalue_loss.backward() and step the optimizer.\nself.value_optimizer.step()\n\ndef interaction _step(self, state, env):\naction = self.training strategy.select_action (\no> self.online model, state)\n@ Here we keep using the online network for action selection.\nnew_state, reward, is terminal, _ = env.step(action)\nreturn new state, is terminal\n\ndef update_network(self): \u00a2\u2014\u2014\u2014\u2014\u2014\u2014\u2014-+4 (\u00ae) Updating the target network\nfor target, online in zip( is still the same as before.\nself.target_model.parameters(),\nself.online_ model.parameters ()) :\ntarget.data.copy (online.data)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.11,
                        "section_name": "A more forgiving loss function",
                        "section_path": "./screenshots-images-2/chapter_9/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_11/5c1d70dd-8ba3-4131-a967-4adffa0cd0e9.png",
                            "./screenshots-images-2/chapter_9/section_11/d8df672c-bf8c-4ddb-9ea5-104548aff6fe.png",
                            "./screenshots-images-2/chapter_9/section_11/b0a1f8a0-2afb-4a6b-9652-cbfbaab4f573.png",
                            "./screenshots-images-2/chapter_9/section_11/3ed48fc8-a4d0-4874-b6ad-f232b0682730.png",
                            "./screenshots-images-2/chapter_9/section_11/78eb5049-6243-4215-a250-73075985231f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A more forgiving loss function\n\nIn the previous chapter, we selected the L2 loss, also known as mean square error (MSE), as\nour loss function, mostly for its widespread use and simplicity. And, in reality, in a problem\nsuch as the cart-pole environment, there might not be a good reason to look any further.\nHowever, because I\u2019m teaching you the ins and outs of the algorithms and not only \u201chow to\nhammer the nail,\u201d I\u2019d also like to make you aware of the different knobs available so you can\nplay around when tackling more challenging problems.\n\nMSE is a ubiquitous loss function because it\u2019s simple, it makes sense, and it works well.\nBut, one of the issues with using MSE for\nreinforcement learning is that it penalizes\nlarge errors more than small errors. This \u2014 =o\nmakes sense when doing supervised learn-\ning because our targets are the true value\nfrom the get-go, and are fixed throughout\nthe training process. That means we\u2019re con- 20\nfident that, if the model is very wrong, then __,,,,\nit should be penalized more heavily than if\nit\u2019s just wrong. 10-75-50 35S Sw\n\nBut as stated now several times, in rein-\nforcement learning, we don\u2019t have these\ntrue values, and the values we use to train our network are dependent on the agent itself.\nThat\u2019s a mind shift. Besides, targets are constantly changing; even when using target net-\nworks, they still change often. In reinforcement learning, being very wrong is something we\nexpect and welcome. At the end of the day, if you think about it, we aren\u2019t \u201ctraining\u201d agents;\nour agents learn on their own. Think about that for a second.\n\nA loss function not as unforgiving, and also more robust to outliers, is the mean absolute\nerror, also known as MAE or L1 loss. MAE is defined as the average absolute difference\nbetween the predicted and true values, that is, the predicted action-value function and the TD\ntarget. Given that MAE is a linear function as opposed to quadratic such as MSE, we can\nexpect MAE to be more successful at treat-\ning large errors the same way as small errors.\n\nThis can come in handy in our case because *\u2122\nwe expect our action-value function to give\nwrong values at some point during training,\nparticularly at the beginning. Being more\nresilient to outliers often implies errorshave\nless effect, as compared to MSE, in terms of x\nchanges to our network, which means more\nstable learning. 0 75 ewe\n\nMean Squared Error (MSE/L2)\n\n200\n\nMean Absolute Error (MAE/L1)\n\nNow, on the flip side, one of the helpful things of MSE that MAE doesn\u2019t have is the fact\nthat its gradients decrease as the loss goes to zero. This feature is helpful for optimization\nmethods because it makes it easier to reach the optima: lower gradients mean small changes\nto the network. But luckily for us, there\u2019s a loss function that\u2019s somewhat a mix of MSE and\nMAE, called the Huber loss.\n\nThe Huber loss has the same useful property as MSE of quadratically penalizing the errors\nnear zero, but it isn\u2019t quadratic all the way out for huge errors. Instead, the Huber loss is\nquadratic (curved) near-zero error, and it becomes linear (straight) for errors larger than a\npreset threshold. Having the best of both\nworlds makes the Huber loss robust to out- Huber Loss\nliers, just like MAE, and differentiable at 0,\njust like MSE.\n\nThe Huber loss uses a hyperparameter, 5,\nto set this threshold in which the loss goes\nfrom quadratic to linear, basically, from\nMSE to MAE. If is zero, you\u2019re left pre-\ncisely with MAE, and if & is infinite, then\nyou're left precisely with MSE. A typical\nvalue for 5 is 1, but be aware that your loss\nfunction, optimization, and learning rate\n\ninteract in complex ways. If you change one, MAE, MSE and Huber Loss\nyou may need to tune several of the others. rs, -- tt fom =\nCheck out the Notebook for this chapterso \u2014 wevuuneer. 6-0\n\nyou can play around. co\nInterestingly, there are at least two differ-\nent ways of implementing the Huber loss\nfunction. You could either compute the\nHuber loss as defined, or compute the MSE \u00b0\nloss instead, and then set all gradients larger :\nthan a threshold to a fixed magnitude value.\nYou clip the magnitude of the gradients. The former depends on the deep learning frame-\nwork you use, but the problem is that some frameworks don\u2019t give you access to the  hyper-\nparameter, so you're stuck with 6 set to 1, which doesn\u2019t always work, and isn\u2019t always the\nbest. The latter, often referred to as loss clipping, or better yet gradient clipping, is more flexible\nand, therefore, what I implement in the Notebook.\n\n\nI Speak PYTHON\nDouble DQN with Huber loss\n\ndef optimize model(self, experiences) :\nstates, actions, rewards, \\\nnext_states, is terminals = experiences\nbatch_size = len(is terminals)\n\n+ 0) First, you calculate the targets and get the\n<--->? current values as before using double learning.\ntd_error = q_sa - target_q sa\n\n(@ Then, calculate the loss function as\n\nMean Squared Error, as before.\nvalue_loss = td_error.pow(2) .mul(0.5) .mean()\n\n@) Zero the optimizer and calculate the\n\ngradients in a backward step.\nself.value_optimizer.zero_grad()\nvalue_loss.backward ()\n\n() Now, clip the gradients to the max_gradient_norm. This\n\nValue can be virtually any value, but Know that this interacts\n\nwith other hyperparameters, such as learning rate.\ntorch.nn.utils.clip grad _norm_( \u2014_\n\nself.online model.parameters(),\nself.max_gradient_norm)\n\nS) Finally, step the optimiagr. |)\n\nself.value_optimizer.step()\n\nKnow that there\u2019s such a thing as reward clipping, which is different than gradient clipping.\nThese are two very different things, so beware. One works on the rewards and the other on\nthe errors (the loss). Now, above all don\u2019t confuse either of these with Q-value clipping, which\nis undoubtedly a mistake.\n\nRemember, the goal in our case is to prevent gradients from becoming too large. For this,\nwe either make the loss linear outside a given absolute TD error threshold or make the gradi-\nent constant outside a max gradient magnitude threshold.\n\nIn the cart-pole environment experiments that you find in the Notebook, I implement the\nHuber loss function by using the gradient clipping technique. That is, I calculate MSE and\nthen clip the gradients. However, as I mentioned before, I set the hyperparameter setting for\nthe maximum gradient values to infinity. Therefore, it\u2019s effectively using good-old MSE. But,\nplease, experiment, play around, explore! The Notebooks I created should help you learn\nalmost as much as the book. Set yourself free over there.\n\nIt\u2019s In THE DeTaILs\nThe full double deep Q-network (DDQN) algorithm\nDDOQN is almost identical to DON, but there are still several differences:\n+ Approximate the action-value function Q(s,a; 8).\n+ Usea state-in-values-out architecture (nodes: 4, 512,128, 2).\n\n+ Optimize the action-value function to approximate the optimal action-\nvalue function q*(s,a).\n\n+ Use off-policy TD targets (r + gamma*max_a\u2019Q(s/a\u2019; 8)) to evaluate policies.\nNotice that we now\n\n+ Use an adjustable Huber loss, which, since we set the max_gradient_norm vari-\nable to \u201cfloat(\u2018inf\u2019),\u201d we're effectively using mean squared error (MSE) for our loss\nfunction.\n\n+ Use RMSprop as our optimizer with a learning rate of 0.0007. Note that before we\nused 0.0005 because without double learning (vanilla DQN), several seeds fail if we\ntrain with a learning rate of 0.0007. Perhaps stability? In DDQN, on the other hand,\ntraining with a higher learning rate works best.\n\nIn DDQN we're still using\n+ Anexponentially decaying epsilon-greedy strategy (from 1.0 to 0.3 in roughly\n20,000 steps) to improve policies.\n+ Areplay buffer with 320 samples min, 50,000 max, and a batch of 64.\n+ Atarget network that freezes for 15 steps and then updates fully.\nDDON, similar to DON, has the same three main steps:\n\n1. Collect experience: (5,,A,, R,.,,5,,,,D,,,), and insert it into the replay buffer.\n\nte ed\n2. Randomly sample a mini-batch from the buffer and calculate the off-policy TD\ntargets for the whole batch: r + gamma*max_a\u2018Q(s/a\u2019; 8).\n\n3. Fit the action-value function Q(s,q; 8) using MSE and RMSprop.\n\nThe bottom line is that the DDQN implementation and hyperparameters are identical to\nthose of DQN, except that we now use double learning and therefore train with a slightly\nhigher learning rate. The addition of the Huber loss doesn\u2019t change anything because we're\n\u201cclipping\u201d gradients to a max value of infinite, which is equivalent to using MSE. However, for\nmany other environments you'll find it useful, so tune this hyperparameter.\n\nTatty it Up\nDDON is more stable than NFQ or DON\n\nDQN and DDQN have similar performance in the cart-pole environment. However, this is a\nsimple environment with a smooth reward function. In reality, DDQN should always give\nbetter performance.\n\nMoving Avg Reward (Training)\n\u2014 vow +O Pay attention not to the mean\n\noe lines in the middle, but to the top\nthe maximum and minimum\nvalues obtained by any of the\n\nMoving Avg Reward (Evaluation) ODQN shows tighter bounds,\n\nbasically, showing more stability\non per:\n@) In the second plot, you see\nthe same pattern: DDQN has\n\nwe narrower bounds. In terms of\n\n\u00b0 performance, DQN reaches the\nTotal Steps max in fewer episodes on the\n\n0000 cart-pole environment For a.\n40000 seed, but DDQN reaches the\n30000 max in a similar number of\n20000 episodes across all seeds:\na stability,\n\no\n\n@) In this case, our seed is lucky\n\nand generates g00d random\n\n700 numbers, and DQN goes through\n\n180 more steps in Fewer episodes\n\n100 (remember the cart-pole\n\nenvironment is about \u201clasting\u201d).\n\u00b0 (4) In terms of time, DDQN takes\n\nWall-clock Time 2. bit longer than DQN to\n~ successfully pass the\naad environment.\n\n88 e 8\n\n\u00b0\n\n$s 8 8\n\nTraining Time\n\n\u00a9 For both training and wall-\nclock time\n\n0 30 100 150 200 250\nEpisodes\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.12,
                        "section_name": "Things we can still improve on",
                        "section_path": "./screenshots-images-2/chapter_9/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_12/b3d94cd4-b235-43aa-ad0f-979f12a9fedb.png",
                            "./screenshots-images-2/chapter_9/section_12/1fce29b8-59fb-47db-aaab-1896bf69ae7d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Things we can still improve on\n\nSurely our current value-based deep reinforcement learning method isn\u2019t perfect, but it\u2019s\npretty solid. DDQN can reach superhuman performance in many of the Atari games. To\nreplicate those results, you'd have to change the network to take images as input (a stack of\nfour images to be able to infer things such as direction and velocity from the images), and,\nof course, tune the hyperparameters.\n\nYet, we can still go a little further. There are at least a couple of other improvements to\nconsider that are easy to implement and impact performance in a positive way.\n\nThe first improvement requires us to reconsider the current network architecture. As\nof right now, we have a naive representation of the Q-function on our neural network\narchitecture.\n\nRerresH My Memory\nCurrent neural network architecture\n\nWe're literately \u201cmaking reinforcement learning look like supervised learning.\u201d But, we can,\nand should, break free from this constraint, and think out of the box.\n\nState-in-values-out architecture\n\nStote variables in\n\n. vector of values out\n* Cart velocity # Action 0 (lef)\n* Pole angle ; # Action | (right)\n* Pole velocity at tip @(s), for example,\nState s, for example, 0.44, -3.5]\n\nC-O41, 1, 2.3, 1.1]\n\nIs there any better way of representing the Q-function? Think about this for a second while\nyou look at the images on the next page.\n\nThe images on the right are bar plots repre-\nsenting the estimated action-value function Q,\nstate-value function V, and action-advantage\nfunction A for the cart-pole environment with\na state in which the pole is near vertical.\n\nNotice the different functions and values\nand start thinking about how to better archi-\ntect the neural network so that data is used\nmore efficiently. As a hint, let me remind you\nthat the Q-values ofa state are related through\nthe V-function. That is, the action-value func-\ntion Q has an essential relationship with\nthe state-value function V, because of both\nactions in Q(s) are indexed by the same state s\n(in the example to the right s=[0.02, -0.01,\n0.02, -0.04]).\n\nThe question is, could you learn anything\nabout Q(s, 0) if you\u2019re using a Q(s, 1) sample?\nLook at the plot showing the action-advantage\nfunction A(s) and notice how much easier it\nis for you to eyeball the greedy action with\nrespect to these estimates than when using\nthe plot with the action-value function Q(s).\nWhat can you do about this? In the next chap-\nter, we look at a network architecture called\nthe dueling network that helps us exploit these\nrelationships.\n\nPiiiadl\n\u00b0 \u00a7\n\nPill\n\ntative\n\n420209\n\nAction-value function, Qf[ 0.02 -0.01 -0.02 -0.04])\n\nAction\n\nState-value function, Vit 0.02 -0.01 -0.02 -0.04])\n\n\u20189 | 0.02 2.2 0.02 2.08),\n\u2018State\n\nAdvantage function, {f 0.02 -0.01 -0.02 -0.04})\n\nek dite iptr: Ccem\n\nThe other thing to consider improving is the way we sample experiences from the replay\nbuffer. As of now, we pull samples from the buffer uniformly at random, and I\u2019m sure your\nintuition questions this approach and suggests we can do better, and we can.\n\nHumans don\u2019t go around the world remembering random things to learn from at random\ntimes. There\u2019s a more systematic way in which intelligent agents \u201creplay memories.\u201d I\u2019m\npretty sure my dog chases rabbits in her sleep. Certain experiences are more important than\nothers to our goals. Humans often replay experiences that caused them unexpected joy or\npain. And it makes sense, and you need to learn from these experiences to generate more or\nless of them. In the next chapter, we look at ways of prioritizing the sampling of experiences\nto get the most out of each sample, when we learn about the prioritized experience replay\n\n(PER) method.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.13,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_9/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_13/792ce114-1c80-4754-ab95-cc23adb7a47b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter, you learned about the widespread issues with value-based deep reinforcement\nlearning methods. The fact that online data isn\u2019t stationary, and it also isn\u2019t independent and\nidentically distributed as most optimization methods expect, creates an enormous amount of\nproblems value-based methods are susceptible to.\n\nYou learned to stabilize value-based deep reinforcement learning methods by using a vari-\nety of techniques that have empirical results in several benchmarks, and you dug deep on\nthese components that make value-based methods more stable. Namely, you learned about\nthe advantages of using target networks and replay buffers in an algorithm known as DQN\n(nature DQN, or vanilla DQN). You learned that by using target networks, we make the tar-\ngets appear stationary to the optimizer, which is good for stability, albeit by sacrificing con-\nvergence speed. You also learned that by using replay buffers, the online data looks more IID,\nwhich, you also learned, is a source of significant issues in value-based bootstrapping meth-\nods. These two techniques combined make the algorithm sufficiently stable for performing\nwell in several deep reinforcement learning tasks.\n\nHowever, there are many more potential improvements to value-based methods. You\nimplemented a straightforward change that has a significant impact on performance, in gen-\neral. You added a double-learning strategy to the baseline DQN agent that, when using func-\ntion approximation, is known as the DDQN agent, and it mitigates the issues of overestimation\nin off-policy value-based methods.\n\nIn addition to these new algorithms, you learned about different exploration strategies\nto use with value-based methods. You learned about linearly and exponentially decaying\nepsilon-greedy and softmax exploration strategies, this time, in the context of function\napproximation. Also, you learned about different loss functions and which ones make more\nsense for reinforcement learning and why. You learned that the Huber loss function allows\nyou to tune between MSE and MAE with a single hyperparameter, and it\u2019s one of the pre-\nferred loss functions used in value-based deep reinforcement learning methods.\n\nBy now, you\n\n+ Understand why using online data for training neural network with optimizers that\nexpect stationary and IID data is a problem in value-based DRL methods\n\n+ Can solve reinforcement learning problems with continuous state-spaces with\nalgorithms that are more stable and therefore give more consistent results\n\n+ Have an understanding of state-of-the-art, value-based, deep reinforcement learning\nmethods and can solve complex problems\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 10,
                "chapter_name": "Sample-efficient\nvalue-based methods",
                "chapter_path": "./screenshots-images-2/chapter_10",
                "sections": [
                    {
                        "section_id": 10.1,
                        "section_name": "Sample-efficient\nvalue-based methods",
                        "section_path": "./screenshots-images-2/chapter_10/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_1/9f6fb371-ebdb-4f63-87ba-2df00d092c35.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the previous chapter, we improved on NFQ with the implementation of DQN and DDQN.\nIn this chapter, we continue on this line of improvements to previous algorithms by present-\ning two additional techniques for improving value-based deep reinforcement learning meth-\nods. This time, though, the improvements aren\u2019t so much about stability, although that could\neasily be a by-product. But more accurately, the techniques presented in this chapter improve\nthe sample-efficiency of DQN and other value-based DRL methods.\n\nFirst, we introduce a functional neural network architecture that splits the Q-function\nrepresentation into two streams. One stream approximates the V-function, and the other\nstream approximates the A-function. V-functions are per-state values, while A-functions\nexpress the distance of each action from their V-functions.\n\nThis is a handy fact for designing RL-specialized architectures that are capable of squeezing\ninformation from samples coming from all action in a given state into the V-function for that\nsame state. What that means is that a single experience tuple can help improve the value esti-\nmates of all the actions in that state. This improves the sample efficiency of the agent.\n\nThe second improvement we introduce in this chapter is related to the replay buffer. As\nyou remember from the previous chapter, the standard replay buffer in DQN samples expe-\nriences uniformly at random. It\u2019s crucial to understand that sampling uniformly at random\nis a good thing for keeping gradients proportional to the true data-generating underlying\ndistribution, and therefore keeping the updates unbiased. The issue is, however, that if we\ncould devise a way for prioritizing experiences, we could use the samples that are the most\npromising for learning. Therefore, in this chapter, we introduce a different technique for\nsampling experiences that allows us to draw samples that appear to provide the most infor-\nmation to the agent for actually making improvements.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.2,
                        "section_name": "Dueling DDQN: A reinforcement-learning-\naware neural network architecture",
                        "section_path": "./screenshots-images-2/chapter_10/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_2/35376e0c-26e7-4247-bdf1-4eeb2bbe33b7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Dueling DDQN: A reinforcement-learning-\naware neural network architecture\n\nLet\u2019s now dig into the details of this specialized neural network architecture called the dueling\nnetwork architecture. The dueling network is an improvement that applies only to the net-\nwork architecture and not the algorithm. That is, we won\u2019t make any changes to the algo-\nrithm, but the only modifications go into the network architecture. This property allows\ndueling networks to be combined with virtually any of the improvements proposed over the\nyears to the original DQN algorithm. For instance, we could have a dueling DQN agent, and\na dueling double DQN agent (or the way I\u2019m referring to it\u2014dueling DDQN), and more.\nMany of these improvements are just plug-and-play, which we take advantage of in this chap-\nter. Let\u2019s now implement a dueling architecture to be used in our experiments and learn\nabout it while building it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.3,
                        "section_name": "Reinforcement learning isn\u2019t a supervised learning problem",
                        "section_path": "./screenshots-images-2/chapter_10/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_3/8ed2f034-9101-433f-8dff-5bada35d1a41.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Reinforcement learning isn\u2019t a supervised learning problem\n\nIn the previous chapter, we concentrated our efforts on making reinforcement learning look\nmore like a supervised learning problem. By using a replay buffer, we made online data,\nwhich is experienced and collected sequentially by the agent, look more like an independent\nand identically distributed dataset, such as those commonly found in supervised learning.\n\nWe also made targets look more static, which is also a common trait of supervised learning\nproblems. This surely helps stabilize training, but ignoring the fact that reinforcement learning\nproblems are problems of their own isn\u2019t the smartest approach to solving these problems.\n\nOne of the subtleties value-based deep reinforcement learning agents have, and that we\nwill exploit in this chapter, is in the way the value functions relate to one another. More spe-\ncifically, we can use the fact that the state-value function V(s) and the action-value function\nQ(s, a) are related to each other through the action-advantage function A(s, a).\n\nRerresH My Memory\nValue functions recap\n\ndnr(s,a) = E,[G;|S; = s, Ay = a]\nT____ ( eecalt the action-value Function of a policy is ts expectation of returns\ngiven you take action a in state s and continue Following that policy.\nUn (s) = Ex[Gi|S; = 5]\nT\u20141 @ the state-value function of state s for a policy is the expectation of\nreturns from that state, assuming you continue Following that policy.\nAx($,a) = Gn(S, 4) \u2014 Un(S) ctatectewertam |\nt______, \u00a9 the action-advantage function tells us the difference between\ntaking action a in state s and choosing the policy's default action.\n\nE~a(s) [an (s, a)| = 0 a |\ntw Infinitely sampling the policy For the state-action pair yields 0.\nWhy? Because there\u2019s no advantage in taking the default action.\n\nQn(S, a) = v7 (8) + az(s, a) 1 \u00a9 Fray we erapup ity tie\n\nrewrite of the advantage equation\nabove. We'll use it shortly.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.4,
                        "section_name": "Nuances of value-based deep reinforcement learning methods",
                        "section_path": "./screenshots-images-2/chapter_10/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_4/85b8f7eb-8dd1-486c-a646-0d0b819cdeb4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Nuances of value-based deep reinforcement learning methods\n\nThe action-value function Q/(s, a) can be defined as the sum of the state-value function V/s)\nand the action-advantage function A(s, a). This means that we can decompose a Q-function\ninto two components: one that\u2019s shared across all actions, and another that\u2019s unique to each\naction; or, to say it another way, a component that is dependent on the action and another\nthat isn\u2019t.\n\nCurrently, we\u2019re learning the action-value function Q/(s, a) for each action separately, but\nthat\u2019s inefficient. Of course, there\u2019s a bit of generalization happening because networks are\ninternally connected. Therefore, information is shared between the nodes of a network. But,\nwhen learning about Q(s, a,), we're ignoring the fact that we could use the same information\nto learn something about Q(s, a,), Q(s, a,), and all other actions available in state s. The fact\nis that V(s) is common to all actions a,, a, a, ..., ay.\n\nEfficient use of experiences\n@ @y approximating @-Functions directly, we\nsqueeze information from each sample and\ndump it all into the same bucket.\n\n@ If we create two separate streams: one\nto collect the common information (v(s)),\nand the other to collect the differences\n\n\\\nI\n. (Technically, these ; between the actions (A(s,al) and A(s,aa),\nviperience buckets are connected | the network could get more accurate Faster.\ntuple mr a\u201d jh the network, |\ntees withme..) | eo @ Information in the\n\n! \u00bb V(s) bucket gets\n\n| used by all A(s,a).\nmen \\\n\nI\n\nI\n\nQ(s, left) Q(s, right) Vis) A(s, left) A(s, right)\n\n= Bonn Down\nThe action-value function Q(s, a) depends on the state-value function V/(s)\n\nThe bottom line is that the values of actions depend on the values of states, and it would be\nnice to leverage this fact. In the end, taking the worst action in a good state could be better\nthan taking the best action in a bad state. You see how \u201cthe values of actions depend on\nvalues of states\u201d?\n\nThe dueling network architecture uses this dependency of the action-value function Q(s, a)\non the state-value function V(s) such that every update improves the state-value function\nV(s) estimate, which is common to all actions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.5,
                        "section_name": "Advantage of using advantages",
                        "section_path": "./screenshots-images-2/chapter_10/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_5/68b2a2ac-98ca-42ec-b2a7-339185acabee.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Advantage of using advantages\n\nNow, let me give you an example. In the cart-pole environment, when the pole is in the\nupright position, the values of the left and right actions are virtually the same. It doesn\u2019t mat-\nter what you do when the pole is precisely upright (for the sake of argument, assume the cart\nis precisely in the middle of the track and that all velocities are 0). Going either left or right\nshould have the same value in this perfect state.\n\nHowever, it does matter what action you take when the pole is tilted 10 degrees to the right, for\ninstance. In this state, pushing the cart to the right to counter the tilt, is the best action the agent\ncan take. Conversely, going left, and consequently, pronouncing the tilt is probably a bad idea.\n\nNotice that this is what the action-advantage function A(s, a) represents: how much better\nthan average is taking this particular action ain the current state s?\n\nRelationship between value functions\n\n@ The state on the lef is a pretty\nstate s=[0.02, -0.01, 0.02, 0.04) good state because the pole is State s=[-0.16, -1.97, 0.24, 3.01]\nalmost in the upright position and the\ncart somewhat in the middle of the\ntrack. On the other hand, the state\nl on the right isn\u2019t as good because l\nthe pole is Falling over to the right.\n\nsucvae facie wow oro vor, @ The state-value function\n\u201c~ captures this \u201cgoodness\u201d of the\naH situation. The state on the lef is mee\n10 times more valuable than the one =\non the right (at least according to a\ndoin) sauetureten eas ont aes aoe @) The action-value function doesn't\n\u2014 capture this relationship directly, but\ninstead it helps determine several\nfavorable actions to take. On the\nleft, it isn't clear what to do, while on\nemmy RATE the right, it\u2019s obvious you should\nmove the cart right.\n\n() The action-advantage Function\nalso captures this aspect of \u201cFavor-\n\n1 wos ability,\u201d but notice how it\u2019s much ,\n\nstate on the lef helps illustrate this\nproperty fairly well.\n\nAdvantage tleaction, ([ 8.02 -0 OL -0 92 -0.84]) Asteantage tunetion, ([-0.16-197 824 3031)\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.6,
                        "section_name": "A reinforcement-learning-aware architecture",
                        "section_path": "./screenshots-images-2/chapter_10/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_6/612a2cf9-740e-4647-95dc-ab85c6adcc4e.png",
                            "./screenshots-images-2/chapter_10/section_6/f367cce1-0985-4e6a-b3d6-acbb729e042e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A reinforcement-learning-aware architecture\n\nThe dueling network architecture consists of creating two separate estimators, one of the\nstate-value function V(s), and the other of the action-advantage function A(s, a). Before split-\nting up the network, though, you want to make sure your network shares internal nodes. For\ninstance, if you\u2019re using images as inputs, you want the convolutions to be shared so that\nfeature-extraction layers are shared. In the cart-pole environment, we share the hidden\nlayers.\n\nAfter sharing most of the internal nodes and layers, the layer before the output layers splits\ninto two streams: a stream for the state-value function V(s), and another for the action-\nadvantage function A(s, a). The V-function output layer always ends in a single node because\nthe value of a state is always a single number. The output layer for the Q-function, however,\noutputs a vector of the same size as the number of actions. In the cart-pole environment, the\noutput layer of the action-advantage function stream has two nodes, one for the left action,\nand the other for the right action.\n\nDueling network architecture\n\n+\u2014F @) The stream merging\n@ Input the same vs) and ACs, a)\n(S) The dueling\n\neo NX a oe \\S @(s, a) estimate\na L_, (@) The nodes\n(@ Hidden layers estimating ACs, a)\n\nA Bit oF History\n0001\nIntroduction of the dueling network architecture\n\nThe Dueling neural network architecture was introduced in 2015 in a paper called \u201cDueling\nNetwork Architectures for Deep Reinforcement Learning\u201d by Ziyu Wang when he was a PhD\nstudent at the University of Oxford. This was arguably the first paper to introduce a custom\ndeep neural network architecture designed specifically for value-based deep reinforcement\nlearning methods.\n\nZiyu is now a research scientist at Google DeepMind, where he continues to contribute to\nthe field of deep reinforcement learning.\n\nBuilding a dueling network\n\nBuilding the dueling network is straightforward. I noticed that you could split the network\nanywhere after the input layer, and it would work just fine. I can imagine you could even have\ntwo separate networks, but I don\u2019t see the benefits of doing that. In general, my recommen-\ndation is to share as many layers as possible and split only in two heads, a layer before the\noutput layer.\n\nI] SPEAK PYTHON\n\nBuilding the dueling network\n@ The dueling network is similar to the regular\n\nclass FCDuelingQ(nn.Module): network. We need variables for the number\ndef init (self, oF nodes in the input and output layers, the\ninput_dim, shape of the hidden layers, and the activation\noutput_dim, Function, the way we did before.\n\nhidden_dims=(32,32),\n\nactivation _fc=F.relu):\nsuper (FCDuelingO, self). init ()\nself.activation_fc = activation fe\n\n@ Next, we create the input layer and \u201cconnect\u201d it to the First hidden layer. Here the\ninput_dim variable is the number of input nodes, and hidden_dimsLo] is the number oF\nnodes of the First hidden layer. nn-Linear creates a layer with inputs and outputs.\nself.input_layer = nn.Linear(input_dim,\nhidden _dims[0])\n\n@ Here we create the hidden layers by creating layers as defined in the hidden_dims\nVariable. For example, a value of (64, 3A, Ie) will create a layer with 64 input nodes and 32\noutput nodes, and then a layer with 32 input nodes and Ib output nodes.\nself.hidden_layers = nn.ModuleList ()\nfor i in range(len(hidden dims) -1):\nhidden_layer = nn.Linear (\nhidden _dims[i], hidden_dims[i+1]})\nself.hidden_layers.append(hidden_layer)\n\n@ Finally, we build the two output layers, both \u201cconnected\u201d to the last hidden\nlayer. The value_output has a single node output, and the advantage_output\nhas output_dim nodes. In the cart-pole environment, that number is two.\nself.value_output = nn.Linear(hidden_dims[-1], 1)\nself.advantage output = nn.Linear(\nhidden dims[-1], output_dim)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.7,
                        "section_name": "Reconstructing the action-value function",
                        "section_path": "./screenshots-images-2/chapter_10/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_7/7fc6975d-7ba9-474f-b05c-2403d236494c.png",
                            "./screenshots-images-2/chapter_10/section_7/19915a8f-e524-4fc3-a89d-0b0101cb144a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Reconstructing the action-value function\n\nFirst, let me clarify that the motivation of the dueling architecture is to create a new network\nthat improves on the previous network, but without having to change the underlying control\nmethod. We need changes that aren\u2019t disruptive and that are compatible with previous meth-\nods. We want to swap the neural network and be done with it.\n\nFor this, we need to find a way to aggregate the two outputs from the network and recon-\nstruct the action-value function Q(s, a), so that any of the previous methods could use the\ndueling network model. This way, we create the dueling DDQN agent when using the dueling\narchitecture with the DDQN agent. A dueling network and the DQN agent would make the\ndueling DQN agent.\n\nSuow Me tHe Matu\nDueling architecture aggregating equations\n\n@ The @-function is parameterized by theta, alpha, and beta. Theta represents the weights\nof the shared layers, alpha the weights of the action-advantage function stream, and beta\nthe weights of the state-value function stream.\n\nQ(s, 4;6,a, 3) = V(s; 6,8) + = 6,a)\n\nose) = V(s;6, 8) + (a6 a;6,a) - A, a\u2019; 0, \u00ab)\n\n@) eut because we cannot uniquely recover the @ From nena\nequation in practice. This removes one degree of Freedom from the Q@-function. The\naction-advantage and state-value Functions lose their true meaning by doing this. 6ut in\npractice, they're off-centered by a constant and are now more stable when optimizing.\n\nBut, how do we join the outputs? Some of you are thinking, add them up, right? I mean, that\u2019s\nthe definition that I provided, after all. Though, several of you may have noticed that there is\nno way to recover V(s) and A(s, a) uniquely given only Q(s, a). Think about it; if you add +10\nto V(s) and remove it from A(s, a) you obtain the same Q(s, a) with two different values for\nV(s) and A(s, a).\n\nThe way we address this issue in the dueling architecture is by subtracting the mean of the\nadvantages from the aggregated action-value function Q(s, a) estimate. Doing this shifts V(s)\nand A(s, a) off by a constant, but also stabilizes the optimization process.\n\nWhile estimates are off by a constant, they do not change the relative rank of A(s, a), and\ntherefore Q/(s, a) also has the appropriate rank. All of this, while still using the same control\nalgorithm. Big win.\n\n| Speak PYTHON\n\nThe forward pass of a dueling network\n\nclass FCDuelingQ(nn.Module): @\u00ae Notice that this is the same class as before.\n<35 0 0b removed the code for building of the network\ndef forward(self, state): for brevity,\n\n@) In the forward pass, we start by making sure the input to the network, the \u2018state,\u2019 is oF\nthe expected type and shape. We do this because sometimes we input batches of states\n(training), sometimes single states (interacting). Sometimes these are NumPy vectors.\n\nx = state\nif not isinstance(x, torch.Tensor):\n\nx = torch.tensor (x,\ndevice=self.device,\ndtype=torch.float32)\n\nxX = X.unsqueeze (0)\n\n@) At this point, we've prepped the input (again single or batch of states) variable xto what\nthe network expects. we pass the variable x to the input layer, which, remember, takes in\ninput_dim variables and outputs hidden_dimLo] variables; those will then pass through the\nactivation Function.\n\nx = self.activation_fc(self.input_layer (x) )\n@) we use that output as the input for our First hidden layer. We pass the variable x\n\nwhich you can think of as the current state of a pulse wave that goes From the input to the\noutput of the network, sequentially to each hidden layer and the activation function.\n\nfor hidden_layer in self.hidden_ layers:\nx = self.activation_fc(hidden_layer (x) )\n\n\u00a9 xnow contains the values that came out of the last hidden layer and its respective\nactivation. We use those as the input to the advantage_output and the value_output layers.\nSince vis a. single value that will be added to a, we expand it.\n\na = self.advantage_ output (x)\n\nv = self.value_ output (x)\n\nv v.expand_ as (a)\n\u00a9 Finally, we add. vand a.and subtract the mean of a From it. That\u2019s our @(s, ) estimate,\ncontaining the estimates of all actions for all states.\n\nq =v +a- a.mean(1, keepdim=True) .expand_as (a)\n\nreturn q\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.8,
                        "section_name": "Continuously updating the target network",
                        "section_path": "./screenshots-images-2/chapter_10/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_8/35058465-bf91-42f9-a583-e8c5c0726908.png",
                            "./screenshots-images-2/chapter_10/section_8/3a99c8d7-ea62-4e46-b5e7-2c006a7b53d6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Continuously updating the target network\n\nCurrently, our agent is using a target network that can be outdated for several steps before it\ngets a big weight update when syncing with the online network. In the cart-pole environment,\nthat\u2019s merely ~15 steps apart, but in more complex environments, that number can rise to\ntens of thousands.\n\nFull target network update\n\n@ Target network weights\nare held constant for a @ Creating a progressi\nnumber of steps. inereasing lag \u201cs\n\nee e . . . . . .\neee ese\nten = t+#nt+]) t+nt+2 tent3 tente4 ttnt5S t+nt+6 t+2n t+2n+1 t+2n+2\n@ every n steps we update\nthe target network weights.\n\nThere are at least a couple of issues with this approach. On the one hand, we\u2019re freezing the\nweights for several steps and calculating estimates with progressively increasing stale data. As\nwe reach the end of an update cycle, the likelihood of the estimates being of no benefit to the\ntraining progress of the network is higher. On the other hand, every so often, a huge update\nis made to the network. Making a big update likely changes the whole landscape of the loss\nfunction all at once. This update style seems to be both too conservative and too aggressive at\nthe same time, if that\u2019s possible.\n\nWe got into this issue because we wanted our network not to move too quickly and there-\nfore create instabilities, and we still want to preserve those desirable traits. But, can you think\nof other ways we can accomplish something similar but in a smooth manner? How about\nslowing down the target network, instead of freezing it?\n\nWe can do that. The technique is called Polyak Averaging, and it consists of mixing in\nonline network weights into the target network on every step. Another way of seeing it is that,\nevery step, we create a new target network composed of a large percentage of the target net-\nwork weights and a small percentage of the online network weights. We add ~1% of new\ninformation every step to the network. Therefore, the network always lags, but by a much\nsmaller gap. Additionally, we can now update the network on each step.\n\n= SHow Me tHe Mato\n\nPolyak averaging\n+ 0) instead of making the target network equal to the online\nnetwork every A/time steps, keep it Frozen in the mean time.\n\n@ why not mix the\ntarget network with a \u2014\u2014 A _ =\nKeak o the online _ 0; = 70; + (1\u20147)6;\n\nnetwork more Fr\nperhaps every time step\u00e9 @) Here tau is the mixing factor.\n\n@) Since we're doing this with a dueli \u2014-~\u2014 . \u2014 nn\nsi ch paronters nabsirg tenes Oi =Ta,+(1-7T)a;\n\nStream, wil be mixed in. i\n\n| Speak PYTHON\n\nMixing in target and online network weights\n\nclass DuelingDDON():\n\n<---> 4 0) This is the same dueling DD@N class, but with\n\nmost of the code removed for brevity.\ndef update network(self, tau=None):\n\n@ tauis a variable representing the ratio of the online network that will\nbe mixed into the target network. A value of | is equivalent to a full update.\ntau = self.tau if tau is None else tau\n\nfor target, online in zip( \u00a2 1 @) zip takes iterables\nself.target_model.parameters(), and returns an\nself.online_model.parameters()): iterator of tuples.\n\n() Now, we calculate the ratios we're taking trom the target and online wei\ntarget_ratio = (1.0 - self.tau) * target.data\nonline ratio self.tau * online.data\n\n(S) Finally, we mix the weights and copy the new values into the target network.\nmixed weights = target_ratio + online ratio\ntarget.data.copy (mixed_weights)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.9,
                        "section_name": "What does the dueling network bring to the table?",
                        "section_path": "./screenshots-images-2/chapter_10/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_9/374cfdf4-ff54-4ec9-8faf-ebbd672461b5.png",
                            "./screenshots-images-2/chapter_10/section_9/bd59e635-7297-4c01-91df-03053d92735d.png",
                            "./screenshots-images-2/chapter_10/section_9/4af23e3d-5de1-4ea7-8fbc-014da9e69f1d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What does the dueling network bring to the table?\n\nAction-advantages are particularly useful when you have many similarly valued actions, as\nyou've seen for yourself. Technically speaking, the dueling architecture improves policy eval-\nuation, especially in the face of many actions with similar values. Using a dueling network,\nour agent can more quickly and accurately compare similarly valued actions, which is some-\nthing useful in the cart-pole environment.\n\nFunction approximators, such as a neural network, have errors; that\u2019s expected. In a net-\nwork with the architecture we were using before, these errors are potentially different for all\nof the state-actions pairs, as they\u2019re all separate. But, given the fact that the state-value func-\ntion is the component of the action-value function that\u2019s common to all actions in a state, by\nusing a dueling architecture, we reduce the function error and error variance. This is because\nnow the error in the component with the most significant magnitude in similarly valued\nactions (the state-value function V/(s)) is now the same for all actions.\n\nIf the dueling network is improving policy evaluation in our agent, then a fully trained\ndueling DDQN agent should have better performance than the DDQN when the left and\nright actions have almost the same value. I ran an experiment by collecting the states of 100\nepisodes for both, the DDQN and the dueling DDQN agents. My intuition tells me that if one\nagent is better than the other at evaluating similarly valued actions, then the better agent\nshould have a smaller range along the track. This is because a better agent should learn the\ndifference between going left and right, even when the pole is exactly upright. Warning! I\ndidn\u2019t do ablation studies, but the results of my hand-wavy experiment suggest that the duel-\ning DDQN agent is indeed able to evaluate in those states better.\n\nState-space visited by fully trained cart-pole agents\n\nOm Not going to draw any conclusions here. Gut, notice the states\nVisited by fully trained DDQN and dueling DDGN agents across Five seeds.\n\nRange of state-variable values for DDQN Range of state-variable values for DuelingODQN\nr ? \u00b0 = S =\n3 | t\nse\n\n@ See how the Fully trained DDQN agents on the lef visit cart positions that are Far to\nthe right, all the way to over LS units, while the Fully trained dueling DDQN agents trained\nwith the same hyperparameters stay near the center. Could this suggest a better policy\nevaluation for dueling DDQN agents? Think about it, and experiment yourself.\n\nIt\u2019s In THE DeTaILs\nThe dueling double deep Q-network (dueling DDQN) algorithm\n\nDueling DDQN is almost identical to DDQN, and DON, with only a few tweaks. My intention is\nto keep the differences of the algorithms to a minimal while still showing you the many\ndifferent improvements that can be made. I\u2019m certain that changing only a few hyperparam-\neters by a little bit has big effects in performance of many of these algorithms; therefore, |\ndon't optimize the agents. That being said, now let me go through the things that are still the\nsame as before:\n+ Network outputs the action-value function Q(s,q; 4).\n+ Optimize the action-value function to approximate the optimal action-value\nfunction q*(s,a).\n+ Use off-policy TD targets (r + gamma*max_a\u2018Q(s/a\u2019; 8)) to evaluate policies.\n+ Use an adjustable Huber loss, but still with the max_gradient_norm variable set to\nfloat(\u2018inf\u2019). Therefore, we're using MSE.\n+ Use RMSprop as our optimizer with a learning rate of 0.0007.\n+ Anexponentially decaying epsilon-greedy strategy (from 1.0 to 0.3 in roughly\n20,000 steps) to improve policies.\n+ Agreedy action selection strategy for evaluation steps.\n+ Areplay buffer with 320 samples min, 50,000 max, and a batch of 64.\nWe replaced\n+ The neural network architecture. We now use a state-in-values-out dueling net-\nwork architecture (nodes: 4, 512,128, 1; 2, 2).\n\n+ The target network that used to freeze for 15 steps and update fully now uses\nPolyak averaging: every time step, we mix in 0.1 of the online network and 0.9 of\nthe target network to form the new target network weights.\n\nDueling DDQN is the same exact algorithm as DDQN, but a different network:\n\n1. Collect experience: (5, A, R,,,, 5,,;, D,,,), and insert into the replay buffer.\n\n2. Pull a batch out of the buffer and calculate the off-policy TD targets: R +\n\ngamma*max_a\u2019Q(s/a\u2019; 8), using double learning.\n\n3. Fit the action-value function Q(s,q; 6), using MSE and RMSprop.\nOne cool thing to notice is that all of these improvements are like Lego\u2122 blocks for you to get\ncreative. Maybe you want to try dueling DON, without the double learning; maybe you want\nthe Huber loss to clip gradients; or maybe you like the Polyak averaging to mix 50:50 every\n\n5 time steps. It\u2019s up to you! Hopefully, the way I\u2019ve organized the code will give you the free-\ndom to try things out.\n\nTatty it Up\nDueling DDQN is more data efficient than all previous methods\n\nDueling DDQN and DDQN have similar performance in the cart-pole environment. Dueling\nDDON is slightly more data efficient. The number of samples DDQN needs to pass the envi-\nronment is higher than that of dueling DDQN. However, dueling DDQN takes slightly longer\nthan DDQN.\n\n@ The training curves of\ndueling DUQN are narrower\nand end sooner than DDQN.\nThis suggests that dueling\nfewer samples, but also\nlearning more stable policies.\n\nMoving Avg Reward (Training)\n\n|\n\nDusting DOQN\n\n88 38 3\ni\n\n\u00b0\n\nMoving Avg Reward (Evaluation)\n\n@) The evaluation plot\n\nshows the same pattern. One\ninteresting thing to note is that\nbump at around episode SO.\ndueling DDQN has a higher\n\nTotal Steps lower bound throughout the\nentire training process.\n\n8388 8\n\n\u00b0\n\n() dueling DDQN consumes less\ndata and fewer steps.\n\n\u2014\n\n() but it takes longer to train,\nabout SO seconds longer on\naverage! Why would this be?\n\nTraining Time\n\nL\n\niW\niH\n\nWall-clock Time and Find out!\n\n:\n:\n\n\u00b0\nLy\n\n100 350 200 250\nEpisodes\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.1,
                        "section_name": "PER: Prioritizing the replay\nof meaningful experiences",
                        "section_path": "./screenshots-images-2/chapter_10/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_10/9ae01458-a52f-4d59-8246-4ef86bd13526.png",
                            "./screenshots-images-2/chapter_10/section_10/48a242a2-c6a2-4934-b49d-a1a5997bf416.png",
                            "./screenshots-images-2/chapter_10/section_10/39ddc54c-7c0b-4723-8878-55cef6d5be78.png",
                            "./screenshots-images-2/chapter_10/section_10/46ec6ce0-f6b5-478c-b406-2eeb94820eda.png",
                            "./screenshots-images-2/chapter_10/section_10/28ba3f5b-2904-4de2-8371-9c26db57c3e0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "PER: Prioritizing the replay\nof meaningful experiences\n\nIn this section, we introduce a more intelligent experience replay technique. The goal is to\nallocate resources for experience tuples that have the most significant potential for learning.\nprioritized experience replay (PER) is a specialized replay buffer that does just that.\n\nA smarter way to replay experiences\n\nAt the moment, our agent samples experience tuples from the replay buffer uniformly at\nrandom. Mathematically speaking, this feels right, and it is. But intuitively, this seems like an\ninferior way of replaying experiences. Replaying uniformly at random allocates resources to\nunimportant experiences. It doesn\u2019t feel right that our agent spends time and compute power\n\u201clearning\u201d things that have nothing to offer to the current state of the agent\n\nBut, let\u2019s be careful here: while it\u2019s evident that uniformly at random isn\u2019t good enough, it\u2019s\nalso the case that human intuition might not work well in determining a better learning sig-\nnal. When I first implemented a prioritized replay buffer, before reading the PER paper, my\nfirst thought was, \u201cWell, I want the agent to get the highest cumulative discounted rewards\npossible; I should have it replay experiences with high reward only.\u201d Yeah, that didn\u2019t work.\nI then realized agents also need negative experiences, so I thought, \u201cAha! I should have the\nagent replay experiences with the highest reward magnitude! Besides, I love using that \u2018abs\u2019\nfunction!\u201d But that didn\u2019t work either. Can you think why these experiments didn\u2019t work?\nIt makes sense that if 1 want the agent to learn to experience rewarding states, I should have it\nreplay those the most so that it learns to get there. Right?\n\n\u2019\nEe Micuet\u2019s ANALOGY\ns.\n\nHuman intuition and the relentless pursuit of happiness\n\nlove my daughter. I love her so much. In fact, so much that | want her to experience only the\ngood things in life. No, seriously, if you're a parent, you know what | mean.\n\nI noticed she likes chocolate a lot, or as she would say, \u201ca bunch,\" so, | started opening up\nto giving her candies every so often. And then more often than not. But, then she started\ngetting mad at me when | didn\u2019t think she should get a candy.\n\nToo much high-reward experiences, you think? You bet! Agents (maybe even humans)\nneed to be reminded often of good and bad experiences alike, but they also need mundane\nexperiences with low-magnitude rewards. In the end, none of these experiences give you the\nmost learning, which is what we're after. Isn't that counterintuitive?\n\nThen, what's a good measure of \u201cimportant\u201d experiences?\n\nWhat we're looking for is to learn from experiences with unexpected value, surprising expe-\nriences, experiences we thought should be valued this much, and ended up valued that much.\nThat makes more sense; these experiences bring reality to us. We have a view of the world, we\nanticipate outcomes, and when the difference between expectation and reality is significant,\nwe know we need to learn something from that.\n\nIn reinforcement learning, this measure of surprise is given by the TD error! Well, technically,\nthe absolute TD error. The TD error provides us with the difference between the agent\u2019s cur-\nrent estimate and target value. The current estimate indicates the value our agent thinks it's\ngoing to get for acting in a specific way. The target value suggests a new estimate for the same\nstate-action pair, which can be seen as a reality check. The absolute difference between these\nvalues indicates how far off we are, how unexpected this experience is, and how much new\ninformation we received, which makes it a good indicator for learning opportunity.\n\n& SHow Me tHE Matu\nThe absolute TD error is the priority\n\n@ vm calling it \u201cdueling ODQN\u201d target to be specific that we're using a target network,\nand o. dueling architecture. However, this could be more simply called TO target.\n\n[di] = |r + 7Q(s\u2019, argmax Q(s\u2019, a\u2019; 0;, a4, 8:);0~ a7, B~) \u2014Q(s, a; 8;, a, Bi) |\n\nDueling DDQN\ntarget\n\nDueling DDQN\nerror\n\nAbsolute Dueling DDQN\nerror\n\nThe TD error isn\u2019t the perfect indicator of the highest learning opportunity, but maybe the\nbest reasonable proxy for it. In reality, the best criterion for learning the most is inside the\nnetwork and hidden behind parameter updates. But, it seems impractical to calculate gradi-\nents for all experiences in the replay buffer every time step. The good things about the TD\nerror are that the machinery to calculate it is in there already, and of course, the fact that the\nTD error is still a good signal for prioritizing the replay of experiences.\n\nGreedy prioritization by TD error\n\nLet\u2019s pretend we use TD errors for prioritizing experiences are follows:\n+ Take action ain state s and receive a new state s\u2019, a reward r, and a done flag d.\n* Query the network for the estimate of the current state Q(s, a; 8).\n\n+ Calculate a new target value for that experience as target = r + gamma*max_a'Q\n(s,a'; @).\n\n+ Calculate the absolute TD error as atd_err = abs(Q(s, a; 0) \u2014 target).\n+ Insert experience into the replay buffer as a tuple (s, a, 1, s', d, atd_err).\n+ Pull out the top experiences from the buffer when sorted by atd_err.\n\n+ Train with these experiences, and repeat.\n\nThere are multiple issues with this approach, but let\u2019s try to get them one by one. First, we are\ncalculating the TD errors twice: we calculate the TD error before inserting it into the buffer,\nbut then again when we train with the network. In addition to this, we\u2019re ignoring the fact\nthat TD errors change every time the network changes because they\u2019re calculated using the\nnetwork. But, the solution can\u2019t be updating all of the TD errors every time step. It\u2019s simply\nnot cost effective.\n\nA workaround for both these problems is to update the TD errors only for experiences that\nare used to update the network (the replayed experiences) and insert new experiences with\nthe highest magnitude TD error in the buffer to ensure they\u2019re all replayed at least once.\n\nHowever, from this workaround, other issues arise. First, a TD error of zero in the first\nupdate means that experience will likely never be replayed again. Second, when using func-\ntion approximators, errors shrink slowly, and this means that updates concentrate heavily in\na small subset of the replay buffer. And finally, TD errors are noisy.\n\nFor these reasons, we need a strategy for sampling experiences based on the TD errors, but\nstochastically, not greedily. If we sample prioritized experiences stochastically, we can simul-\ntaneously ensure all experiences have a chance of being replayed, and that the probabilities of\nsampling experiences are monotonic in the absolute TD error.\n\n<= Boum Down\nTD errors, priorities, and probabilities\n\nThe most important takeaway from this page is that TD errors aren't enough; we'll use TD\nerrors to calculate priorities, and from priorities we calculate probabilities.\n\nSampling prioritized experiences stochastically\n\nAllow me to dig deeper into why we need stochastic prioritization. In highly stochastic envi-\nronments, learning from experiences sampled greedily based on the TD error may lead us to\nwhere the noise takes us.\n\nTD errors depend on the one-step reward and the action-value function of the next state,\nboth of which can be highly stochastic. Highly stochastic environments can have higher vari-\nance TD errors. In such environments, we can get ourselves into trouble if we let our agents\nstrictly follow the TD error. We don\u2019t want our agents to get fixated with surprising situa-\ntions; that\u2019s not the point. An additional source of noise in the TD error is the neural net-\nwork. Using highly non-linear function approximators also contributes to the noise in TD\nerrors, especially early during training when errors are the highest. If we were to sample\ngreedily solely based on TD errors, much of the training time would be spent on the experi-\nences with potentially inaccurately large magnitude TD error.\n\n<= Bom Down\nSampling prioritized experiences stochastically\n\nTD errors are noisy and shrink slowly. We don\u2019t want to stop replaying experiences that, due\nto noise, get a TD error value of zero. We don\u2019t want to get stuck with noisy experiences that,\ndue to noise, get a significant TD error. And, we don't want to fixate on experiences with an\ninitially high TD error.\n\nA Br oF History\n0001\nIntroduction of the prioritized experience replay buffer\n\nThe paper \u201cPrioritized Experience Replay\u201d was introduced simultaneously with the dueling\narchitecture paper in 2015 by the Google DeepMind folks.\n\nTom Schaul, a Senior Research Scientist at Google DeepMind, is the main author of the\nPER paper. Tom obtained his PhD in 2011 from the Technical University of Munich. After two\nyears as a postdoc at New York University, Tom joined DeepMind Technologies, which six\nmonths later would be acquired by Google and turned into what today is Google DeepMind.\n\nTom is a core developer of the PyBrain framework, a modular machine learning library for\nPython. PyBrain was probably one of the earlier frameworks to implement machine learning,\nreinforcement learning and black-box optimization algorithms. He's also a core developer of\nPyVGDL, a high-level video game description language built on top of pygame.\n\nProportional prioritization\n\nLet\u2019s calculate priorities for each sample in the buffer based on TD errors. A first approach to\ndo so is to sample experiences in proportion to their absolute TD error. We can use the abso-\nlute TD error of each experience and add a small constant, epsilon, to make sure zero TD\nerror samples still have a chance of being replayed.\n\n= SHow Me tHE Matu\nProportional prioritization\n\n\u00a9 The priority of sample i... | JA@... is the absolute TO error.\n\npi = |6;| ten\n@) A small constant, epsilon,\nto avoid zero priority\n\nWe scale this priority value by exponentiating it to alpha, a hyperparameter between zero and\none. That allows us to interpolate between uniform and prioritized sampling. It allows us to\nperform the stochastic prioritization we discussed.\n\nWhen alpha is zero, all values become one, therefore, an equal priority. When alpha is one,\nall values stay the same as the absolute TD error; therefore, the priority is proportional to the\nabsolute TD error\u2014a value in between blends the two sampling strategies.\n\nThese scaled priorities are converted to actual probabilities only by dividing their values by\nthe sum of the values. Then, we can use these probabilities for drawing samples from the\nreplay buffer.\n\n= SHow Me tHe MatH\nPriorities to probabilities\n\n@ we calculate the J. bu caising the priorities by\nprobabilities . .. jy alpha. to blend uniform and\npe prioritized experience replay.\n\nP(i)=\n\n= a\nVe Pe Tre woabines std upwome\n(3) And then normalize them so that the sum of the probabilities add. up to one.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.11,
                        "section_name": "Rank-based prioritization",
                        "section_path": "./screenshots-images-2/chapter_10/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_11/19a6f46a-6a29-4b2e-96aa-b9e7230d1bef.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Rank-based prioritization\nOne issue with the proportional-prioritization approach is that it\u2019s sensitive to outliers. That\nmeans experiences with much higher TD error than the rest, whether by fact or noise, are\nsampled more often than those with low magnitudes, which may be an undesired side effect.\nAslightly different experience prioritization approach to calculating priorities is to sample\nthem using the rank of the samples when sorted by their absolute TD error.\nRank here means the position of the sample when sorted in descending order by the\nabsolute TD error\u2014nothing else. For instance, prioritizing based on the rank makes the\nexperience with the highest absolute TD error rank 1, the second rank 2, and so on.\n\n= SHow Me tHe Mato\nRank-based prioritization\n\n( For rank-based prioritization, 1\nwe caleulate the priorities as the B= TTF\nreciprocal of the rank of that sample. rank(t)\n\nAfter we rank them by TD error, we calculate their priorities as the reciprocal of the rank. And\nagain, for calculating priorities, we proceed by scaling the priorities with alpha, the same as\nwith the proportional strategy. And then, we calculate actual probabilities from these priori-\nties, also, as before, normalizing the values so that the sum is one.\n\n== Bons Down\n\nRank-based prioritization\nWhile proportional prioritization uses the absolute TD error and a small constant for includ-\ning zero TD error experiences, rank-based prioritization uses the reciprocal of the rank of the\n\nsample when sorted in descending order by absolute TD error.\nBoth prioritization strategies then create probabilities from priorities the same way.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.12,
                        "section_name": "Prioritization bias",
                        "section_path": "./screenshots-images-2/chapter_10/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_12/5409df01-7124-47a2-a7d8-d80ed24385dd.png",
                            "./screenshots-images-2/chapter_10/section_12/42fad1be-15f8-49a1-9e5b-509a859567cc.png",
                            "./screenshots-images-2/chapter_10/section_12/0e0b7516-aabe-435e-ac10-d9f5683ecc85.png",
                            "./screenshots-images-2/chapter_10/section_12/bf16c8c4-f157-480f-bb43-6aebc18a034b.png",
                            "./screenshots-images-2/chapter_10/section_12/3746bfa8-10d8-4e79-a64c-fee3586c8acc.png",
                            "./screenshots-images-2/chapter_10/section_12/f585278e-3d4e-4c92-9ba7-a280509797c1.png",
                            "./screenshots-images-2/chapter_10/section_12/33768646-c65c-4829-bb3e-eb53cb7cc994.png",
                            "./screenshots-images-2/chapter_10/section_12/ae0c6cec-82df-4e29-8a38-cf67d149349a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Prioritization bias\n\nUsing one distribution for estimating another one introduces bias in the estimates. Because\nwe\u2019re sampling based on these probabilities, priorities, and TD errors, we need to account\nfor that.\n\nFirst, let me explain the problem in more depth. The distribution of the updates must be\nfrom the same distribution as its expectation. When we update the action-value function of\nstate sand an action a, we must be cognizant that we always update with targets.\n\nTargets are samples of expectations. That means the reward and state at the next step could\nbe stochastic; there could be many possible different rewards and states when taking action a\nin a state s.\n\nIf we were to ignore this fact and update a single sample more often than it appears in that\nexpectation, we'd create a bias toward this value. This issue is particularly impactful at the\nend of training when our methods are near convergence.\n\nThe way to mitigate this bias is to use a technique called weighted importance sampling. It\nconsists of scaling the TD errors by weights calculated with the probabilities of each sample.\n\nWhat weighted importance sampling does is change the magnitude of the updates so that\nit appears the samples came from a uniform distribution.\n\n= SHow Me tHE MatH\nWeighted importance-sampling weights calculation\n\n@ we calculate the importance- |\n\nsath prbabiey tthe nese wi = (NP(i))~?\nof samples in the replay buffer. (a) We then raise that value to\nthe additive inverse of beta.\nWi\nWie=\n\n== al\nso that the largest weights are |,\nand everything else is lower.\n\nTo do weighted importance-sampling effectively with a prioritized replay buffer, we add a\nconvenient hyperparameter, beta, that allows us to tune the degree of the corrections. When\nbeta is zero, there\u2019s no correction; when beta is one, there\u2019s a full correction of the bias.\n\nAdditionally, we want to normalize the weights by their max so that the max weight\nbecomes one, and all other weights scale down the TD errors. This way, we keep TD errors\nfrom growing too much and keep training stable.\n\nThese importance-sampling weights are used in the loss function. Instead of using the TD\nerrors straight in the gradient updates, in PER, we multiply them by the importance-sampling\nweights and scale all TD errors down to compensate for the mismatch in the distributions.\n\n= SHow Me tHE MatH\nDueling DDQN with PER gradient update\n\n@ | don't want to keep bloating this equation, so 'm\nonly using theta to represent all parameters, the\nshared, for the action-advantage Function, alpha,\nand for the state-value function, beta.\n\nVo, Li(i) = Eqw.s.a,r,s\")~P(D) [w(r + 7Q(s', argmax Q(s\u2019, a\u2019; 0;);0~) \u2014 Q(s, a; 9;)) Vo, Q(s, a; 6;)}\n\n(@) Notice how |\n\nchanged the U for a. @) Finally, notice how we're using the\n\nP, because we're normalized importance-sampling weights\ndoing a. prioritized to modify the magnitude of the TD error.\nsampling, and not\n\nuniformly at random.\n\n| Speak PyTHoN\nPrioritized replay buffer 1/2\n\nclass PrioritizedReplayBuffer ():\n<.2->\ndef store(self, sample):\n@ The store function of the PrioritizedReplaysutfer class is straightforward. The First\nthing we do is calculate the priority for the sample. Remember, we set the priority to the\nmaximum. The Following code shows | as default; then it\u2019s overwritten with the max value.\npriority = 1.0\nif self.n_entries > 0:\npriority = self.memory[\n:self.n_entries,\nself.td_error_ index] .max()\n(@) with the priority and sample (experience) in hand, we insert it into the memory.\n\nself.memory([self.next_index,\n\nself.td_error_index] = priority\nself.memory[self.next_index,\nself.sample index] = np.array(sample)\n\n@) we increase the variable that indicates the number of experiences in the buffer, but\nwe need to make sure the buffer doesn't increase beyond the max_samples.\nself.n_entries = min(self.n entries + 1,\nself.max_samples)\n(4) This next variable indicates the index ot which the next experience will be inserted.\nThis variable loops back around from max_samples to 0 and goes back up.\nself.next_index += 1\nself.next_index = self.next_index % self.max_samples\n\ndef update(self, idxs, td_errors):\n(S) The update function takes an array of experiences ids, and new TD error values.\n[raver nate Toners be gt poe\nself.memory([idxs,\nself.td_error_index] = np.abs(td_errors)\n@) \\$ we're doing rank-based sampling, we additionally sort the array, Notice that arrays\nare sub-optimal For implementing a prioritized replay buffer, mainly because of this sort\nthat depends on the number of samples. Not g00d For performance.\nif self.rank_ based:\nsorted arg = self.memory[:self.n_entries,\nself.td_error_ index] .argsort() [::-1]\nself.memory[:self.n_entries] = self.memory[\nsorted arg]\n\n| Speak PYTHON\nPrioritized replay buffer 2/2\n\nclass PrioritizedReplayBuffer ():\n\nSonat\n\ndef sample(self, batch _size=None):\n\n@ Calculate the batch_size, anneal \u2018beta,\u2019 and remove zeroed rows From entries.\nbatch_size = self.batch_size if batch_size == None \\\n\nelse batch size\n\nself. update beta()\nentries = self.memory[:self.n_entries]\n\n(@) We now calculate priorities. If it\u2019s a rank-based prioritization, it\u2019s one over the rank\n(we sorted these in the update Function). Proportional is the absolute TD error plus a small\nconstant epsilon to avoid zero priorities.\n\nif self.rank_based:\npriorities = 1/(np.arange(self.n_entries) + 1)\nelse: # proportional\npriorities = entries[:, self.td_error_ index] + EPS\n(3) Now, we go From priorities to probabilities. First, we blend with uniform, then probs.\nscaled priorities = priorities**self.alpha\nCL pri_sum = np.sum(scaled priorities)\nprobs = np.array(scaled priorities/pri_sum,\ndtype=np. float6\u00e94)\ninate o teeldin enteins @ ponessO cet beta\nweights = (self.n_entries * probs) **-self.beta\n[72S wormalice the megs The masirram weit wile\nnormalized weights = weights/weights.max()\n[71.\u00a9 we sample indices of Ane expervences in the per using the probobbiies\nidxs = np.random.choice(self.n_entries,\nbatch size, replace=False, p=probs)\n\n[1 setthe samp out of the baer:\n\nsamples = np.array([entries[idx] for idx in idxs])\n\n\u00ae) Finally, stack the samples by ids, weights, and experience tuples, and return them.\nsamples stacks = [np.vstack(batch_type) for \\\n\nbatch _type in np.vstack(samples[:, self.sample index]) .T]\nidxs_stack = np.vstack(idxs)\nweights stack = np.vstack(normalized weights [idxs] )\nreturn idxs stack, weights stack, samples_stacks\n\n| Speak PyTHoN\nPrioritized replay buffer loss function 1/2\n\nclass PER():\n<...> \u00a2\u20144 @As I\u2019ve pointed out on other occasions, this is part of the code.\nThese are snippets that | Feel are worth showing here.\n\ndef optimize model(self, experiences) :\n\n(2) One thing to notice is that now we have ids and weights coming,\nalong with the experiences.\nidxs, weights, \\\n(states, actions, rewards,\nnext_states, is_terminals) = experiences\nSo 00\n@) we calculate the target values, as before.\nargmax_a_q_sp = self.online model (next_states) .max(1) [1]\nq_sp = self.target_model (next_states) .detach ()\nmax_a_q sp = q_sp[np.arange(batch size), argmax_a_q sp]\nmax_a_q sp = max_a_q sp.unsqueeze(1)\nmax_a_q sp *= (1 - is terminals)\ntarget_q sa = rewards + (self.gamma * max_a_q sp)\nG@) we query the current estimates: nothing new.\nq_sa = self.online model(states).gather(1, actions)\n\u00a9 we calculate the TD errors, the same way.\ntd_error = q sa - target_q sa\n\n@) ut, now the loss function has TD errors downscaled by the weights.\ncs value_loss = (weights * td_error) .pow(2) .mul (0.5) .mean()\n\n@ we continue the optimization as before.\n\nself.value_optimizer.zero_grad()\nvalue_loss.backward()\n\ntorch.nn.utils.clip grad _norm_(\nself.online model.parameters(),\n\nself.max_gradient_norm)\nself.value_optimizer.step()\n\n@ And we update the priorities of the replayed batch using the absolute TD errors.\nLC priorities = np.abs(td_error.detach() .cpu() .numpy())\nself.replay buffer.update(idxs, priorities)\n\n| Speak PYTHON\nPrioritized replay buffer loss function 2/2\n\nclass PER(): @ This is the same PeR class, but we're\nSooo! Now in the train function.\n\ndef train(self, make_env_fn, make env_kargs, seed, gamma,\nmax_minutes, max_episodes, goal_mean_100_ reward):\n\nAe $71 @ inside the episode loop\n\nfor episode in range(l, max_episodes + 1):\n\n<...> $1 @ inside the time step loop\nfor step in count():\nstate, is_terminal = \\\nself.interaction_step(state, env)\n\n<...> @) every time step during training time\n\nif len(self.replay buffer) > min_samples:\n\u00a9 Look how we pull the experiences From the buffer.\n\nexperiences = self.replay buffer.sample()\n(@ From the experiences, we pull the idxs, weights, and experience tuple.\nNotice how we load the samples variables into the GPU.\n\nidxs, weights, samples = experiences\n\nexperiences = self.online model.load(\n\nsamples)\n@ Then, we stack the variables again. Note that we did that only to load the\nexperiences = (idxs, weights) + \\\n\n(experiences, )\n@ Then, we optimize the model (this is the function in the previous page).\nself.optimize model (experiences)\n@) And, everything proceeds as usual.\nif np.sum(self.episode timestep) % \\\nself.update target_every steps ==\nself.update_ network ()\n\nif is terminal:\nbreak\n\nIt\u2019s In THE DetaiLs\nThe dueling DDQN with the prioritized replay buffer algorithm\n\nOne final time, we improve on all previous value-based deep reinforcement learning\nmethods. This time, we do so by improving on the replay buffer. As you can imagine, most\nhyperparameters stay the same as the previous methods. Let's go into the details. These are\nthe things that are still the same as before:\n\n.\n\nNetwork outputs the action-value function Q(s,q; 4).\nWe use a state-in-values-out dueling network architecture (nodes: 4, 512,128, 1; 2, 2).\n\nOptimize the action-value function to approximate the optimal action-value\nfunction q*(s, a).\n\nUse off-policy TD targets (r + gamma*max_a\u2019\u2018Q(s/a\u2019; 8) to evaluate policies.\n\nUse an adjustable Huber loss with max_gradient_norm variable set to float(\u2018inf\u2019).\nTherefore, we're using MSE.\n\nUse RMSprop as our optimizer with a learning rate of 0.0007.\n\nAn exponentially decaying epsilon-greedy strategy (from 1.0 to 0.3 in roughly\n20,000 steps) to improve policies.\n\nA greedy action selection strategy for evaluation steps.\n\nA target network that updates every time step using Polyak averaging with a tau\n(the mix-in factor) of 0.1.\n\nA replay buffer with 320 samples minimum and a batch of 64.\n\nThings we've changed:\n\nUse weighted important sampling to adjust the TD errors (which changes the loss\nfunction).\n\nUse a prioritized replay buffer with proportional prioritization, with a max number\nof samples of 10,000, an alpha (degree of prioritization versus uniform\u20141 is full\npriority) value of 0.6, a beta (initial value of beta, which is bias correction\u20141 is\n\nfull correction) value of 0.1 and a beta annealing rate of 0.99992 (fully annealed in\nroughly 30,000 time steps).\n\nPER is the same base algorithm as dueling DDQN, DDQN, and DQN:\n\niB\n2.\n\nCollect experience: (5, A,, R,,,, 5,.,, D,.,), and insert into the replay buffer.\n\ntee\n\nPull a batch out of the buffer and calculate the off-policy TD targets: R +\ngamma*max_a\u2019\u2018Q(s;a\u2019; 8), using double learning.\n\nFit the action-value function Q(s,q; 8), using MSE and RMSprop.\n\nAdjust TD errors in the replay buffer.\n\nTatty it Up\nPER improves data efficiency even more\n\nThe prioritized replay buffer uses fewer samples than any of the previous methods. And as\nyou can see it in the graphs below, it even makes things look more stable. Maybe?\n\nMoving Avg Reward (Training)\n\n400 ~~ Dveling DOON\nsoo \u2014_\u2014\u2122 more efficiently, and as\nyou can see, it passes the\nenvironment in fewer\n100 episodes.\n\\ @ Nothing really different\nos Moving Avg Reward (Evaluation) in the evaluation plot in\nPs terms of sample complexity,\na but you can also see a bit\nmore stability than previous\n200\nmethods near the SO episode\nwe mark.\n\u00b0\nTotal Steps .\nsoa of sample complexity is\nooo the number of steps, not\nsoo0e contain a variable number\nsve of steps in this environment.\na However, the pattern is the\nsame. PER is more sample\nTraining Time\ncoo efficient than all previous\nmethods.\n\u201c @ out look at this! Pee is\n200 much slower than dueling\nDDGQN. But Know that this is\n\u00b0 an implementation-specific\nWall-clock Time issue. IF you get a high-\nPER, this shouldn't happen.\n#00\n\u00a9 Again, not much difference\n700\n\nbetween the two time plots.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.13,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_10/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_13/83c298ba-e93e-4e12-9b06-9c80efd95214.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nThis chapter concludes a survey of value-based DRL methods. In this chapter, we explored\nways to make value-based methods more data efficient. You learned about the dueling archi-\ntecture, and how it leverages the nuances of value-based RL by separating the Q(s, a) into its\ntwo components: the state-value function V(s) and the action-advantage function A(s, a).\nThis separation allows every experience used for updating the network to add information to\nthe estimate of the state-value function V(s), which is common to all actions. By doing this,\nwe arrive at the correct estimates more quickly, reducing sample complexity.\n\nYou also looked into the prioritization of experiences. You learned that TD errors are a\ngood criterion for creating priorities and that from priorities, you can calculate probabilities.\nYou learned that we must compensate for changing the distribution of the expectation we\u2019re\nestimating. Thus, we use importance sampling, which is a technique for correcting the bias.\n\nIn the past three chapters, we dove headfirst into the field of value-based DRL. We started\nwith a simple approach, NFQ. Then, we made this technique more stable with the improve-\nments presented in DQN and DDQN. Then, we made it more sample-efficient with dueling\nDDQN and PER. Overall, we have a pretty robust algorithm. But, as with everything, value-\nbased methods also have cons. First, they\u2019re sensitive to hyperparameters. This is well known,\nbut you should try it for yourself; change any hyperparameter. You can find more values that\ndon\u2019t work than values that do. Second, value-based methods assume they interact with a\nMarkovian environment, that the states contain all information required by the agent. This\nassumption dissipates as we move away from bootstrapping and value-based methods in\ngeneral. Last, the combination of bootstrapping, off-policy learning, and function approxi-\nmators are known conjointly as \u201cthe deadly triad.\u201d While the deadly triad is known to pro-\nduce divergence, researchers still don\u2019t know exactly how to prevent it.\n\nBy no means am I saying that value-based methods are inferior to the methods we survey\nin future chapters. Those methods have issues of their own, too. The fundamental takeaway\nis to know that value-based deep reinforcement learning methods are well known to diverge,\nand that\u2019s their weakness. How to fix it is still a research question, but sound practical advice\nis to use target networks, replay buffers, double learning, sufficiently small learning rates (but\nnot too small), and maybe a little bit of patience. I\u2019m sorry about that; I don\u2019t make the rules.\n\nBy now, you\n\n+ Can solve reinforcement learning problems with continuous state spaces\n+ Know how to stabilize value-based DRL agents\n\n+ Know how to make value-based DRL agents more sample efficient\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 11,
                "chapter_name": "Policy-gradient and\nactor-critic methods",
                "chapter_path": "./screenshots-images-2/chapter_11",
                "sections": [
                    {
                        "section_id": 11.1,
                        "section_name": "Policy-gradient and\nactor-critic methods",
                        "section_path": "./screenshots-images-2/chapter_11/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_1/adae36c7-fc3d-408f-a682-83162c554a7f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In this book, we\u2019ve explored methods that can find optimal and near-optimal policies with\nthe help of value functions. However, all of those algorithms learn value functions when what\nwe need are policies.\n\nIn this chapter, we explore the other side of the spectrum and what's in the middle. We\nstart exploring methods that optimize policies directly. These methods, referred to as policy-\nbased or policy-gradient methods, parameterize a policy and adjust it to maximize expected\nreturns.\n\nAfter introducing foundational policy-gradient methods, we explore a combined class of\nmethods that learn both policies and value functions. These methods are referred to as\nactor-critic because the policy, which selects actions, can be seen as an actor, and the value\nfunction, which evaluates policies, can be seen as a critic. Actor-critic methods often perform\nbetter than value-based or policy-gradient methods alone on many of the deep reinforcement\nlearning benchmarks. Learning about these methods allows you to tackle more challenging\nproblems.\n\nThese methods combine what you learned in the previous three chapters concerning\nlearning value functions and what you learn about in the first part of this chapter, about\nlearning policies. Actor-critic methods often yield state-of-the-art performance in diverse\nsets of deep reinforcement learning benchmarks.\n\nPolicy-based, value-based, and actor-critic methods\n\n\u00a9 For the last three\nchapters, you were here.\n\n@ You're here for the\n\n@)..-and here through\nthe end of the chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.2,
                        "section_name": "REINFORCE: Outcome-based policy learning",
                        "section_path": "./screenshots-images-2/chapter_11/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_2/cb78e8e0-ff6d-444d-8e16-b30a06d32297.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "REINFORCE: Outcome-based policy learning\n\nIn this section, we begin motivating the use of policy-based methods, first with an introduc-\ntion; then we discuss several of the advantages you can expect when using these kinds of meth-\nods; and finally, we introduce the simplest policy-gradient algorithm, called REINFORCE.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.3,
                        "section_name": "Introduction to policy-gradient methods",
                        "section_path": "./screenshots-images-2/chapter_11/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_3/a31037b3-482d-46de-b926-9333659b5cac.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction to policy-gradient methods\n\nThe first point I\u2019d like to emphasize is that in policy-gradient methods, unlike in value-based\nmethods, we\u2019re trying to maximize a performance objective. In value-based methods, the\nmain focus is to learn to evaluate policies. For this, the objective is to minimize a loss between\npredicted and target values. More specifically, our goal is to match the true action-value func-\ntion of a given policy, and therefore, we parameterized a value function and minimized the\nmean squared error between predicted and target values. Note that we didn\u2019t have true target\nvalues, and instead, we used actual returns in Monte Carlo methods or predicted returns in\nbootstrapping methods.\n\nIn policy-based methods, on the other hand, the objective is to maximize the performance\nof a parameterized policy, so we\u2019re running gradient ascent (or executing regular gradient\ndescent on the negative performance). It\u2019s rather evident that the performance of an agent is\nthe expected total discounted reward from the initial state, which is the same thing as the\nexpected state-value function from all initial states of a given policy.\n\n26 SHow Me tHE Matu\nValue-based vs. policy-based methods objectives\n@ In value-based methods, the\n\nobjective is to minimize the | (@.) = _ -@. \u2018|\nsiestnistomininantieless  1.(0;) = Esa | (de(8,2) \u2014 Q(s, 0 6%))\n\nerror between the true Q-function @ In policy-based methods, the objective\n\nand the parameterized Q-function. is to maximize a performance measure,\n_ which is the true value-function of the\n\nJi (9:) _ Es, ~Po Une, (s0)| parameterized policy from alll initial states.\n\nR b Witn an RL Accent\nValue-based vs. policy-based vs. policy-gradient vs. actor-critic methods\n\nValue-based methods: Refers to algorithms that learn value functions and only value func-\ntions. Q-learning, SARSA, DON, and company are all value-based methods.\n\nPolicy-based methods: Refers to a broad range of algorithms that optimize policies, includ-\ning black-box optimization methods, such as genetic algorithms.\n\nPolicy-gradient methods: Refers to methods that solve an optimization problem on the\ngradient of the performance of a parameterized policy, methods you'll learn in this chapter.\n\nActor-critic methods: Refers to methods that learn both a policy and a value function,\nprimarily if the value function is learned with bootstrapping and used as the score for the\nstochastic policy gradient. You learn about these methods in this and the next chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.4,
                        "section_name": "Advantages of policy-gradient methods",
                        "section_path": "./screenshots-images-2/chapter_11/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_4/af06b126-fcc8-4d65-b18d-ed6000c4d424.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Advantages of policy-gradient methods\n\nThe main advantage of learning parameterized policies is that policies can now be any learn-\nable function. In value-based methods, we worked with discrete action spaces, mostly because\nwe calculate the maximum value over the actions. In high-dimensional action spaces, this\nmax could be prohibitively expensive. Moreover, in the case of continuous action spaces,\nvalue-based methods are severely limited.\n\nPolicy-based methods, on the other hand, can more easily learn stochastic policies, which\nin turn has multiple additional advantages. First, learning stochastic policies means better\nperformance under partially observable environments. The intuition is that because we can\nlearn arbitrary probabilities of actions, the agent is less dependent on the Markov assump-\ntion. For example, if the agent can\u2019t distinguish a handful of states from their emitted obser-\nvations, the best strategy is often to act randomly with specific probabilities.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.5,
                        "section_name": "Learning stochastic policies could get us out of trouble",
                        "section_path": "./screenshots-images-2/chapter_11/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_5/caed4d16-c836-4221-aed2-0c245cebd569.png",
                            "./screenshots-images-2/chapter_11/section_5/f658f65d-cef3-459f-a55d-1ae8a774b9e7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning stochastic policies could get us out of trouble\n\n@ Consider 0. Foggy lake environment in which we don't slip as in the\nfrozen lake, but instead we can't see which state we're in.\n\n\u2018START \u2018START\n\u201c| |\n0\n\nSen\n|| |\n(O|>[>] \u00ab|\n\n(@ If we could see well in (2) \\f we couldn't see in (4) The more partially\nevery state, the optimal these two states, the observable, the more\npolicy would be something optimal action in these complex the probability\nlike this. states would be something distribution to learn for\n\nlike SO% left and So% right. optimal action selection.\n\nInterestingly, even though we\u2019re learning stochastic policies, nothing prevents the learning\nalgorithm from approaching a deterministic policy. This is unlike value-based methods, in\nwhich, throughout training, we have to force exploration with some probability to ensure\noptimality. In policy-based methods with stochastic policies, exploration is embedded in the\nlearned function, and converging to a deterministic policy for a given state while training is\npossible.\n\nAnother advantage of learning stochastic policies is that it could be more straightforward\nfor function approximation to represent a policy than a value function. Sometimes value\nfunctions are too much information for what\u2019s truly needed. It could be that calculating the\nexact value of a state or state-action pair is complicated or unnecessary.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.6,
                        "section_name": "Learning policies could be an easier, more\ngeneralizable problem to solve",
                        "section_path": "./screenshots-images-2/chapter_11/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_6/b685309d-bac2-47d3-8921-94df5b37b3a2.png",
                            "./screenshots-images-2/chapter_11/section_6/65bde9a2-8daa-444c-86a3-9a36f068d448.png",
                            "./screenshots-images-2/chapter_11/section_6/05fa0bf2-4b81-4d8c-bd87-14a784a0c82f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning policies could be an easier, more\ngeneralizable problem to solve\n\n@ Consider a near-infinite corridor deterministic environment in which there\u2019s a large\nnumber of cells, sayy |,000,001. There are two goals, one in the leftmost cell, the other\nin the rightmost cell, and every non-terminal state is in the set of initial states.\n\nSTART START START START START START START\n\n@) In an environment like this, the optimal policy would look as shown\nhere. In the middle cell, cell $00,000, a SO% left and a. SO% right is\noptimal. The rest of the actions should point to the closest goal.\n\n@) The optimal policy in this environment is rather obvious, but what isn\u2019t so\nobvious is that learning and generalizing over policies is likely easier and\nmore straightforward than learning value functions. For instance, do | care\nwhether cell 1000 is 0.000! or 0.00014 or anything else, i the action is obviously\nleft? Allocating resources For accurately estimating value functions is unlikely\n\nto yield any advantages over discovering the pattern over actions.\n\nA final advantage to mention is that because policies are parameterized with continuous\nvalues, the action probabilities change smoothly as a function of the learned parameters.\nTherefore, policy-based methods often have better convergence properties. As you remember\nfrom previous chapters, value-based methods are prone to oscillations and even divergence.\nOne of the reasons for this is that tiny changes in value-function space may imply significant\nchanges in action space. A significant difference in actions can create entirely unusual new\ntrajectories, and therefore create instabilities.\n\nIn value-based methods, we use an aggressive operator to change the value function; we\ntake the maximum over Q-value estimates. In policy-based methods, we instead follow the\ngradient with respect to stochastic policies, which only progressively and smoothly changes\nthe actions. If you directly follow the gradient of the policy, you\u2019re guaranteed convergence\nto, at least, a local optimum.\n\n| Speak PYTHON\nStochastic policy for discrete action spaces 1/2\nclass FCDAP(nn.Module) : <\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014++ 0) This class, FCDAP stands for Fully\n\ndef init (self, connected diserete-action policy.\ninput_dim, 4 @ The parameters allow you\noutput_dim, to specify a fully connected\nhidden_dims=(32, 32), architecture, activation function, and\ninit_std=1, weight and bias max magnitude.\n\nactivation_fc=F.relu):\nsuper(FCDAP, self). init ()\n\nself.activation_fc = activation fc eates all .\nself.input_layer = nn.Linear ( Sirst hi we POCA\ninput_dim, hidden_dims[0]) se\nself.hidden layers = nn.ModuleList () @) Then, it creates\nfor i in range(len(hidden dims) ~-1): connections across\nhidden layer = nn.Linear ( all hidden layers.\n\nhidden _dims[i], hidden_dims[i+1])\nself.hidden layers.append (hidden layer)\n-\u2014 \u00a9 Last, it connects the Final\nself.output_layer = nn.Linear( hidden layer to the output\nhidden_dims[-1], output_dim) nodes, creating the output layer.\n\n@) Here we have the method that takes care of the forward functionality.\n\ndef forward(self, state):\nx = state\nif not isinstance(x, torch.Tensor):\n[ x = torch.tensor(x, dtype=torch.float32)\nx = X.unsqueeze (0)\n\u00a9 First, we make sure the state is of the type of variable and shape we expect before we\ncan pass it through the network.\n[| setieton ttn maedtene Se npg encen rman\nx = self.activation_fc(self.input_layer (x) )\n@) Then, we pass the output of the First activation through the sequence of hidden\nlayers and respective activations.\nfor hidden layer in self.hidden_ layers:\nx = self.activation_fc(hidden_layer (x) )\nGo) Finally, we obtain the output, which\nreturn self.output_layer (x) are logits, preferences over actions.\n\n| Speak PyTHoN\nStochastic policy for discrete action spaces 2/2\n\nreturn self.output_layer (x) 4\u2014\u2014\u2014 Gi) This line repeats the last line\n\non the previous page.\ndef full pass(self, state) : \u20ac4 (a) Here we do the full forward pass. This\n\nis a handy function to obtain probabilities,\nactions, and everything needed for training.\nlig logits = self.forward(state)\nG3) The forward pass returns the logits, the preferences over actions.\ndist = torch.distributions.Categorical (logits=logits)\nG4) Next, we sample the action from the probability distribution. pf\naction = dist.sample()\nGs) Then, caleulate the log probability oF that action and format it for training.\nL, logpa = dist.log prob (action) .unsqueeze (-1) _]\nGe) Here we calculate the entropy of the policy.\njl. entropy = dist.entropy() .unsqueeze (-1)\n(7) And in here, For stats, we determine whether the policy selected was exploratory or not.\nis_exploratory = action != np.argmax( \\\nlogits.detach () .numpy () )\n\nG8) Finally, we return an action that can be directly passed into the\nenvironment, the Flag indicating whether the action was exploratory,\nthe log probability of the action, and the entropy of the policy.\n\nreturn action.item(), is exploratory.item(), \\\n\nnT logpa, entropy\n. (9) This is a helper Function for\ndef select_action(self, state): when we only need sampled action.\nlogits = self.forward(state)\ndist = torch.distributions.Categorical (logits=logits)\naction = dist.sample()\n\nreturn action.item()\n\n@o) And this one is For selecting the\ndef select_greedy action(self, state):\n\nlogits = self.forward(state)\nreturn np.argmax (logits.detach() .numpy() )\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.7,
                        "section_name": "Learning policies directly",
                        "section_path": "./screenshots-images-2/chapter_11/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_7/0e3a76f6-47ef-4e07-8b47-dc4b0626952a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning policies directly\n\nOne of the main advantages of optimizing policies directly is that, well, it\u2019s the right objective.\nWe learn a policy that optimizes the value function directly, without learning a value func-\ntion, and without taking into account the dynamics of the environment. How is this possible?\nLet me show you.\n\nShow Me tHe Matu\nDeriving the policy gradient\n\u00a9 First, let's bring back a simplified version of the\nobjective equation a couple of pages ago. > J(0) a Eso~Po [exe (so)|\n@) We know what we want is to Find the gradient\nwith respect to that performance metric. -\u2014\u2014\u2014> VgJ(0) = Vg Ex,~p, |v [\u00bb 'no(80)|\n@ To simplify notation, let's use tau as a variable\nrepresenting the full trajectory. -\u2014> T = Sy, Ap, Ri, S1,...,.S7-1, Ar-1, Rr, Sr\n@) This way we can abuse notation\nand use the @ function to obtain the +> G(r) = Ry + yRot,..., +77 | Rr\nreturn of the full trajectory.\nS) we can also get the probability of a trajectory.\n@ This is the probability of the initial states, then the action, then the transition, and so on\nuntil we have the product of all the probabilities that make the trajectory likely. eel!\nP(T|\\79) = Po(So)m(Ao|S0;8)P(S1, Ri|S0, Ao)..-P(Sr, Rr|Sr\u2014-1, Ar-1\n@ after alll that notation change,\nwe can say that the objective is this. -\u2014\u2014> Vo E,~ x, [a(n] = Vo Eso~po [ene (so)\n@ Next, let\u2019s look at a way for estimating\n\n)\ncetundtongeaenenmacnr i? VoEx [f(2)] = Es Sie\n\n(9) with that i\n\nsubttatevelues ana gt eet V0 Emu [G(7)] = Erany [Vo log (r|n0)G\n\n(0) Notice the dependence on the\n\nprobability of the trajectory,\nGD IF we substitute the probability of trajectory, take the logarithm, turn products into the\nsum, and differentiate with respect to theta, all dependence on the transition function is\na ee ee |\n\nT\nVoEr~ne [em] = Exvmy bs Vo log x(At| 54; noc]\n\nt=0\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.8,
                        "section_name": "Reducing the variance of the policy gradient",
                        "section_path": "./screenshots-images-2/chapter_11/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_8/eadf0e35-36b0-4b66-ac16-859b19469f41.png",
                            "./screenshots-images-2/chapter_11/section_8/375f8493-bc09-4a42-9eb2-76ceb9da7e6c.png",
                            "./screenshots-images-2/chapter_11/section_8/f811818a-0f87-4f2d-bf35-e0aa69133aee.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Reducing the variance of the policy gradient\n\nIt\u2019s useful to have a way to compute the policy gradient without knowing anything about the\nenvironment\u2019s transition function. This algorithm increases the log probability of all actions\nin a trajectory, proportional to the goodness of the full return. In other words, we first collect\na full trajectory and calculate the full discounted return, and then use that score to weight the\nlog probabilities of every action taken in that trajectory: A, A,,,...\u00bb Ay}.\n\nLet's use only rewards that are a consequence of actions\n\n@ This is somewhat counterintuitive\nbecause we're increasing the likelihood of\naction A, in the same proportion as action\nA, even if the return after A, is greater\nthan the return after A, We Know we\ncan't go back in time and current actions\naren't responsible for past reward. Wwe\ncan do something about that.\n\n& SHow Me tHE Matu\n\nReducing the variance of the policy gradient\n@ This is the gradient we try to estimate in the REINFORCE algorithm coming up next.\n\nT\nL Vo (0) = Exe S > Gilr)Vo log 77 (At|S1)\nt=0\n\n@) all this @) Then, for each step To anduce at vae at\nsays is that _in the trajectory, we the score to weight the log\nwe sample a calculate the return probability of the action\ntrajectory. From that step. taken at that time step.\n\nA Bir oF History\n0001\nIntroduction of the REINFORCE algorithm\nRonald J. Williams introduced the REINFORCE family of algorithms in 1992 in a paper titled\n\n\u201cSimple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.\u201d\nIn 1986, he coauthored a paper with Geoffrey Hinton et al. called \u201cLearning representa-\ntions by back-propagating errors,\u201d triggering growth in artificial neural network (ANN)\n\nresearch at the time.\n\nI Speak PYTHON\n\nREINFORCE 1/2\nclass REINFORCE(): \u00a2\u2014\u2014\u20144 (@) Thisis the ReINFORCE algorithm. When you see the\nSone <7, that means code was removed for simplicity. Go\n\n+o the chapter\u2019s Notebook for the complete code.\ndef optimize model (self):\nT = len(self.rewards)\n\ndiscounts = np.logspace(0, T, num=T, base=self.gamma,\nendpoint=False)\n\n(@) First, we calculate the discounts as with all monte Carlo methods. The logspace Function with\nthese parameters returns the series of per time step gammas; for example, Li, 0.99, 0.9801, . . . J.\n\nQC\u201d returns = np.array(\n[np.sum(discounts[:T-t] * self.rewards[t:]) \\\n(2) Next, we ealeulate the = -\u2014\u2014* gor + in range(T) ])\n\nCEG . @) To emphasize, this is the returns for every time step in the episode,\nreturns for all time steps. en ; .\nfrom the initial state at time step 0, to one before the terminal T-1.\n(S) Notice here that we're using the mathematically correct policy-gradient update, which isn\u2019t\nwhat you commonly find out there. The extra discount assumes that we're trying to optimize the\nexpect discounted return from the initial state, so returns later in an episode get discounted.\nSook\n\n[ policy loss = -(discounts * returns * \\\n\nself.logpas) .mean()\n@) This is policy loss; it\u2019s the log probability of the actions selected weighted by the returns\nobtained after that action was selected Notice that because PyTorch does gradient descent by\ndefault and the performance is something we want to maximize, we use the negative mean of\nthe performance to Flip the Function. Think about it as doing gradient ascent on the performance.\nAlso, we account for discounted policy gradients, so we multiply the returns by the discounts.\n\n@ In these three steps, we\nself.policy optimizer.zero_ grad() First zero the gradients in the\n\npolicy loss.backward() optimizer, then do a backward\nself.policy optimizer.step() Pass, and then step in the\n\nJ 72 Ths Funetion obtains an action to be passed to the\nenvironment and all variables required For training.\ndef interaction step(self, state, env):\n\naction, is_exploratory, logpa, _ = \\\n\nself.policy model.full pass (state)\nnew_state, reward, is terminal, _ = env.step (action)\nSq 008\n\nreturn new state, is terminal\n\n1 Speak PYTHON\nREINFORCE 2/2\n\nclass REINFORCE(): \u00a2\u2014\u2014\u2014+ @) Still exploring functions of the REINFORCE class\n<.2->\nSJ 4) The train method is the entry point For training the agent.\ndef train(self, make_env_fn, make env_kargs, seed, gamma,\nmax_minutes, max_episodes, goal _ mean 100 reward) :\nCc for episode in range(1, max_episodes + 1):\n\nGD We begin by looping through the episodes.\nstate, is_ terminal = env.reset(), False ll\n\nos <\u2014__ 71 @ Fr each new episode, we initialize the\nvariables needed for training and stats.\nself.logpas, self.rewards = [], [] |\nTy gor step in county er 0 the following For each Kime step\nfor step in count ():\n\n(4) First, we collect state, is terminal = \\\n=\nexperiences until we > self.interaction_step(state, env)\n\nhit a. terminal state.\nif is_terminal:\nL_5 break 12 Then, we run one optimization step with\nthe batch of all time steps in the episode.\nself.optimize model ()\n\ndef evaluate(self, eval policy model, Ge) anct er thing | t you to\neval_env, n_episodes=1, see is the way | select the policy\n\ngreedy=True) : during evaluation. instead of\nrs = [] selecting a greedy policy, | sample\nfor _ in range(n_ episodes) : ae eich = pcpumn hy\nSo 00t- ; policy. The correct thing to do\nfor _ in count(): here depends on the environment,\nbut sampling is the sate bet.\nif greedy:\n\na = eval_policy model.\\\nselect_greedy action(s)\n\nelse:\n\na = eval policy model.select_action(s)\ns, xr, d, _ = eval_env.step(a)\nSoo of\n\nreturn np.mean(rs), np.std(rs)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.9,
                        "section_name": "VPG: Learning a value function",
                        "section_path": "./screenshots-images-2/chapter_11/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_9/a24d7c15-20e9-4a1a-9b30-2ff09414f7ad.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "VPG: Learning a value function\n\nThe REINFORCE algorithm you learned about in the previous section works well in simple\nproblems, and it has convergence guarantees. But because we\u2019re using full Monte Carlo\nreturns for calculating the gradient, its variance is a problem. In this section, we discuss a few\napproaches for dealing with this variance in an algorithm called vanilla policy gradient or\nREINFORCE with baseline.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.1,
                        "section_name": "Further reducing the variance of the policy gradient",
                        "section_path": "./screenshots-images-2/chapter_11/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_10/86df813e-f815-45d1-bf38-d289b855f03e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Further reducing the variance of the policy gradient\n\nREINFORCE is a principled algorithm, but it has a high variance. You probably remember\nfrom the discussion in chapter 5 about Monte Carlo targets, but let\u2019s restate. The accumula-\ntion of random events along a trajectory, including the initial state sampled from the initial\nstate distribution\u2014transition function probabilities, but now in this chapter with stochastic\npolicies\u2014is the randomness that action selection adds to the mix. All this randomness is com-\npounded inside the return, making it a high-variance signal that's challenging to interpret.\n\nOne way for reducing the variance is to use partial returns instead of the full return for\nchanging the log probabilities of actions. We already implemented this improvement. But\nanother issue is that action log probabilities change in the proportion of the return. This\nmeans that, if we receive a significant positive return, the probabilities of the actions that led\nto that return are increased by a large margin. And if the return is of significant negative mag-\nnitude, then the probabilities are decreased by of large margin.\n\nHowever, imagine an environment such as the cart-pole, in which all rewards and returns\nare positive. In order to accurately separate okay actions from the best actions, we need a lot\nof data. The variance is, otherwise, hard to muffle. It would be handy if we could, instead of\nusing noisy returns, use something that allows us to differentiate the values of actions in the\nsame state. Recall?\n\nRerresH My Memory\n\nUsing estimated advantages in policy-gradient methods\n@ Remember the definition of the true\naction-advantage Function. -\u2014\u2014_> ax(s, a) = dn(S; a) a Un(s)\n@ We can say that the advantage function is\napproximately the following,\n\n~~ T-1\n\nT+ A(S;, At) \u00ae Rit yRepit..+y Rr \u2014 vn (Sz)\n(3) A not-too-bad estimate of it is the return\nG, minus the estimated expected return from A(S\u00a2, At) = G; = V(S;)\n\nthot state. We can use this easily.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.11,
                        "section_name": "Learning a value function",
                        "section_path": "./screenshots-images-2/chapter_11/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_11/67046b27-e13a-46b0-b28e-38a43fce962b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning a value function\n\nAs you see on the previous page, we can further reduce the variance of the policy gradient by\nusing an estimate of the action-advantage function, instead of the actual return. Using the\nadvantage somewhat centers scores around zero; better-than-average actions have a posi-\ntive score, worse-than-average, a negative score. The former decreases the probabilities, and\nthe latter increases them.\n\nWe're going to do exactly that. Let\u2019s now create two neural networks, one for learning the\npolicy, the other for learning a state-value function, V. Then, we use the state-value function\nand the return for calculating an estimate of the advantage function, as we see next.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.12,
                        "section_name": "Two neural networks, one for the policy, one for the value function",
                        "section_path": "./screenshots-images-2/chapter_11/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_12/fc7cd7a6-428e-4b4b-82a3-0796f9eceac5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Two neural networks, one for the policy, one for the value function\n\n@ The policy Policy network Value network = (3) The value network\nnetwork we use we use for the cart-pole\nfor the cart-pole environment is Four-\nenvironment is the node input as well,\nsame one We use in representing the state,\nREINFORCE: a Four- {) anda one-node output\nnode input layer, representing the value of\nand a two-node f thot state. This network\noutput layer. | O<\\\\ outputs the expected\nreturn from the input\n\nstate. More details soon.\n\nprovide more details\non the experiments\nlater.\n\nR b Witn an RL Accent\nREINFORCE, vanilla policy gradient, baselines, actor-critic\n\nSome of you with prior DRL exposure may be wondering, is this a so-called \u201cactor-critic\u201d? It's\nlearning a policy and a value-function, so it seems it should be. Unfortunately, this is one of\nthose concepts where the \u201cRL accent\u201d confuses newcomers. Here\u2019s why.\n\nFirst, according to one of the fathers of RL, Rich Sutton, policy-gradient methods approxi-\nmate the gradient of the performance measure, whether or not they learn an approximate\nvalue function. However, David Silver, one of the most prominent figures in DRL, and a former\nstudent of Sutton, disagrees. He says that policy-based methods don't additionally learn a\nvalue function, only actor-critic methods do. But, Sutton further explains that only methods\nthat learn the value function using bootstrapping should be called actor-critic, because it\u2019s\nbootstrapping that adds bias to the value function, and thus makes it a \u201ccritic.\u201d | like this dis-\ntinction; therefore, REINFORCE and VPG, as presented in this book, aren\u2019t considered actor-\ncritic methods. But beware of the lingo, it\u2019s not consistent.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.13,
                        "section_name": "Encouraging exploration",
                        "section_path": "./screenshots-images-2/chapter_11/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_13/47fd1a1b-fa62-4a2e-99a4-46b0ec5674c0.png",
                            "./screenshots-images-2/chapter_11/section_13/eafd191b-43a5-48bf-9236-3288a0c9b53e.png",
                            "./screenshots-images-2/chapter_11/section_13/4ef9d54d-f4fa-441a-950b-115d226cc30a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Encouraging exploration\n\nAnother essential improvement to policy-gradient methods is to add an entropy term to the\n\nloss function. We can interpret entropy in many different ways, from the amount of informa-\n\ntion one can gain by sampling from a distribution to the number of ways one can order a set.\nThe way I like to think of entropy is straight-\n\nforward. A uniform distribution, which has evenly Entropy contin on ss ureton\ndistributed samples, has high entropy, in fact, the \u2014\n\nhighest it can be. For instance, if you have two =\n\nsamples, and both can be drawn with a 50% Sy: SS\n\nchance, then the entropy is the highest it can\u201d wm\n\nbe for a two-sample set. If you have four samples, \u201c\n\neach with a 25% chance, the entropy is the \u201ctun\n\nsame, the highest it can be for a four-sample set. pay ten\n\nConversely, if you have two samples, and one\n\nhas a 100% chance and the other 0%, then the\n\nentropy is the lowest it can be, which is always zero. In PyTorch, the natural log is used for\ncalculating the entropy instead of the binary log. This is mostly because the natural log uses\nEuler\u2019s number, e, and makes math more \u201cnatural.\u201d Practically speaking, however, there\u2019s no\ndifference and the effects are the same. The entropy in the cart-pole environment, which has\ntwo actions, is between 0 and 0.6931.\n\nThe way to use entropy in policy-gradient methods is to add the negative weighted entropy\nto the loss function to encourage having evenly distributed actions. That way, a policy with\nevenly distributed actions, which yields the highest entropy, contributes to minimizing the\nloss. On the other hand, converging to a single action, which means entropy is zero, doesn\u2019t\nreduce the loss. In that case, the agent had better converge to the optimal action.\n\nShow Me THE Matu\n\nLosses to use for VPG\n@ This is the loss for the value function. It\u2019s simple, N 2\nthe mean squared monte Carlo error. +\u2014> .,,( Fd |(@ -V(534))\n@ The loss of \u2014\u2014-() The estimated =0\nthe policy is this. advantage G) Log probability of the (S) The weighted\n\n1 N action taken al entropy term\nL,(0) = aN. (6. \u2014 V(Si;0)) log (A; |S); 0) + 8H (x(S;:9))\n\n@ Negotive =0 t,\n\nmrweiatt \u00a31 co mann ovr the sarge entropy is good.\n\n| Speak PyTHoN\nState-value function neural network model\nclass FCV(nn.Module): \u00a2\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014_\u2014\u2014_\u2014_\u2014__; (i) This is the state-value\n\nfunction neural network. Hs\ndef init (self, similar to the @-Function\ninput_dim, network we used in the past.\nhidd dims= (32, 32\nidden_dims= (32,32) 1 \u20ac Ly ( \\otice | left handy\nactivation fc=F.relu):\nTO hyperparameters For you to\nsuper(FCV, self). init () around with, \u00a30 a9 ahead\nself.activation_fc = activation_fc ploy 2\u00b0\n\nand do so.\n(2) Here we create linear connections between\nthe input nodes and the first hidden layer.\nself.input_layer = nn.Linear(input_dim,\nhidden_dims[0])\n\n@ Here we create the connections\nbetween the hidden layers.\n\nself.hidden layers = nn.ModuleList ()\nfor i in range(len(hidden dims) -1) :\nhidden layer = nn.Linear(\nhidden _dims[i], hidden_dims[i+1])\nself.hidden_layers.append(hidden_layer)\n\nL, self.output_layer = nn.Linear( (Here we connect the last hidden\n\nhidden _dims[-1], 1) layer to the output layer, which has\nonly one node, representing the\n@) This is the forward-pass Function. value of the state.\ndef forward(self, state):\nx = state\nif not isinstance(x, torch.Tensor):\nx = torch.tensor(x, dtype=torch.float32)\nxX = X.unsqueeze (0)\n@ This is Formatting the input as we expect it.\nx = self.activation_fc(self.input_layer (x) ) \u00a9 doing\nfor hidden layer in self.hidden_ layers: 7 d\nx = self.activation fc(hidden layer (x) )\n\na) None only\n\nreturn self.output_layer (x)\n\n| Speak PYTHON\nVanilla policy gradient a.k.a. REINFORCE with baseline\n\nclass VPG(): \u00a2\u2014<4 @ This is the vP@ algorithm. | removed quite a bit of code, so if you\n\u201cSsn0% +\u2014\u2014F wont the full implementation, head to the chapter\u2019s Notebook.\n\ndef optimize model (self):\nT = len(self.rewards)\ndiscounts = np.logspace(0, T, num=T, base=self.gamma,\nendpoint=False)\nreturns = np.array(\n[np.sum(discounts[:T-t] * self.rewards[t:]) for t in range(T) ])\n\n@ very handy way For calculating the sum of discounted rewards from time step 0 to T\n) | want to emphasize that this loop is going through all steps from 0, then |, a, 3 all the\nway to the terminal state T, and calculating the return from that state, which is the sum\nof discounted rewards trom that state at time step t to the terminal state 7.\n\nvalue_error = returns - self.values\npolicy loss -(\ndiscounts * value error.detach() * self.logpas) .mean()\n\n@ First, calculate the value error; then use it to score the log probabilities of the actions. Then,\ndiscount these to be compatible with the discounted policy gradient. Then, use the negative mean.\n\nentropy loss = -self.entropies.mean()\n\nIrv loss = policy loss + \\\n\n\u00a9) Calculate the entropy, and self.entropy loss weight * entropy loss\n\nadd. a Fraction to the loss. \u00a9) Now, we optimize the\nself.policy optimizer.zero_ grad() policy. Zero the optimizer, do\nloss.backward() the backward pass, then clip\ntorch.nn.utils.clip grad_norm_( the gradients, if desired,\n\nself.policy model.parameters(),\nself.policy model _max_grad_norm)\n\nself.policy optimizer.step() \u00a24(@we step the optimizer.\n@ Last, we optimize the value-function neural network.\n\nvalue_loss = value_error.pow(2) .mul (0.5) .mean()\n\nself.value_optimizer.zero_ grad()\n\nvalue_loss.backward ()\n\ntorch.nn.utils.clip grad _norm_(\nself.value_model.parameters(),\nself.value_ model max_grad_norm)\n\nself.value_optimizer.step()\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.14,
                        "section_name": "A3C: Parallel policy updates",
                        "section_path": "./screenshots-images-2/chapter_11/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_14/ef296cd9-9eeb-4628-9841-ae012d6c66d4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "a an ia as\n\nA3C: Parallel policy updates\n\nVPG is a pretty robust method for simple problems; it is, for the most part, unbiased because\nit uses an unbiased target for learning both the policy and value function. That is, it uses\nMonte Carlo returns, which are complete actual returns experienced directly in the environ-\nment, without any bootstrapping. The only bias in the entire algorithm is because we use\nfunction approximation, which is inherently biased, but since the ANN is only a baseline used\nto reduce the variance of the actual return, little bias is introduced, if any at all.\n\nHowever, biased algorithms are necessarily a thing to avoid. Often, to reduce variance, we\nadd bias. An algorithm called asynchronous advantage actor-critic (A3C) does a couple of\nthings to further reduce variance. First, it uses n-step returns with bootstrapping to learn the\npolicy and value function, and second, it uses concurrent actors to generate a broad set of\nexperience samples in parallel. Let\u2019s get into the details.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.15,
                        "section_name": "Using actor-workers",
                        "section_path": "./screenshots-images-2/chapter_11/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_15/45b52e31-3fcf-4bde-9b5d-9b6b60c1b24b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using actor-workers\n\nOne of the main sources of variance in DRL algorithms is how correlated and non-stationary\nonline samples are. In value-based methods, we use a replay buffer to uniformly sample\nmini-batches of, for the most part, independent and identically distributed data. Unfortu-\nnately, using this experience-replay scheme for reducing variance is limited to off-policy\nmethods, because on-policy agents cannot reuse data generated by previous policies. In other\nwords, every optimization step requires a fresh batch of on-policy experiences.\n\nInstead of using a replay buffer, what we can do in on-policy methods such as the poli-\ncy-gradient algorithms we learn about in this chapter, is have multiple workers generating\nexperience in parallel and asynchronously updating the policy and value function. Having\nmultiple workers generating experience on multiple instances of the environment in parallel\ndecorrelates the data used for training and reduces the variance of the algorithm.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.16,
                        "section_name": "Asynchronous model updates",
                        "section_path": "./screenshots-images-2/chapter_11/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_16/63899708-dfc7-4f0c-a1ea-598c2a0d6e36.png",
                            "./screenshots-images-2/chapter_11/section_16/d643e484-44d4-4afb-9bf2-94ee7d916623.png",
                            "./screenshots-images-2/chapter_11/section_16/ae7ba3a4-766d-4c5c-9530-9692a0fb6dcc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "@ In 3c, we Asynchronous model updates @ after an\n\ncreate multiple experience batch\nworker-learners. /\u2014> Worker 1 Worker 2 Worker n is collected, each\nach of them c) @ worker updates\ncreates an \u00a2 e\u00b0 \u00a2 e\u00b0 . en the global model\ninstance of the asynchronously,\nenvironment and & without\ncoordination with\n\nvata E badass, GDED\n(Policy ) (Policy ) (v-function ) (Policy )\n\nThen, they reload\nonion ce neers NO SAA their copy of the\ngenerating Vee $1 adele and Heep\n\n. ot it.\n\nI SPEAK PYTHON\nA3C worker logic 1/2\n\nclass A3C(): 4\u00a2\u20144 @Thisis the aac agent.\n<...> 4\u2014\u2014\u2014 @) As usual, these are snippets. You know where to Find the\ning code.\ndef work(self, rank) : \u00a24 @) Thisis the work function each worker loops around\nin. The rank parameter is used as an ID For workers.\n\nL, local_seed = self.seed + rank G) see how we create a unique\n\nenv = self.make_env_fn( seed per worker. We want diverse\n**self.make_env_kargs, experiences.\nseed=local_ seed) (S) we create a.uniquely seeded\ntorch.manual_ seed (local_ seed)\nnp. random. seed (local seed) G) we also use that unique seed\nrandom. seed (local_seed) for PyTorch, NumPy and Python.\n\nnS = env.observation_space.shape[0)\n\nnA = env.action_space.n CO Handy veriabies\n\n\u00ae Here we create a. local policy model. See how we initialize its weights with the weights\nof a shared policy network. This network allow us to synchronize the agents periodically.\nlocal_policy model = self.policy model fn(nS, nA)\nlocal_policy model.load_state_dict (\nself.shared_ policy model.state_dict())\n\n(9) We do the same thing with the value model. Notice we don't need nA for output dimensions.\nlocal_value_model = self.value_model_fn (nS)\nlocal_value_model.load_state_dict (\n\nself.shared_value _model.state dict ())\n\nGo) We start the training loop, until the worker is signaled to get out of it.\n\nwhile not self.get_out_signal:\n\nCr? state, is_terminal = env.reset(), False\n\nGD The First thing is to reset the environment, and set the done or is_terminal flag to False.\nGa) As you see next, we use n-step returns For training the policy and value functions.\n\nL__, n_steps_ start = 0\n\nlogpas, entropies, rewards, values = [], [], [], []\n\n3) Let's continue\nfor step in count (start=1): or\n\n1 Speak PYTHON\nA3C worker logic 2/2\n\n+ for step in count (start=1):\n4) | removed eight spaces From the indentation to make it easier to read.\nGs) we are in the episode loop. The First thing is to collect a step of experience.\nstate, reward, is terminal, is truncated, \\\nis_exploratory = self.interaction_step(\nstate, env, local _ policy model,\nlocal_value_ model, logpas,\nentropies, rewards, values)\n\nGe) We collect n-steps maximum. If we hit a terminal state, we stop there.\n\nif is_terminal or step - n_steps_start ==\nself.max_n_ steps:\n\n| is_failure = is terminal and not is truncated\nGD We check if the time wrapper was triggered or this is a true terminal state.\n\nnext_value = 0 if is failure else \\\n\nlocal_value_model (state) .detach() .item()\n\n(8) If it\u2019s a Failure, then the value of the next state is 0; otherwise, we bootstrap.\n\n| rewards .append (next_value)\n\n9) Look! I'm being sneaky here and appending the next_value to the rewards. 6y doing this, the\noptimization code trom VPq remains largely the same, as you'll see soon. Make sure you see it.\n\nself.optimize model (\nlogpas, entropies, rewards, values,\nlocal_policy model, local value _model)\n\n0) Next we optimize the model. We dig into that Function shortly.\n\nlogpas, entropies, rewards, values = [], [], [], []\nn_steps_ start = step 4H G) we reset the variables after\nthe optimization step and continue.\nif is_terminal: \u00a2\u20144+ @a)and, if the state is terminal,\n\nbreak of course exit the episode\n<---> \u20ac-4 (3) There is a lot removed. =e\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.17,
                        "section_name": "Using n-step estimates",
                        "section_path": "./screenshots-images-2/chapter_11/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_17/1dd9282f-7ee3-4716-a22c-09e66c26c3af.png",
                            "./screenshots-images-2/chapter_11/section_17/a9c83854-155e-4988-aad2-f2c9e619e5c5.png",
                            "./screenshots-images-2/chapter_11/section_17/95d1be79-b01a-4ab8-88e5-12f46814fd62.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using n-step estimates\n\nOn the previous page, you notice that I append the value of the next state, whether terminal\nor not, to the reward sequence. That means that the reward variable contains all rewards from\nthe partial trajectory and the state-value estimate of that last state. We can also see this as\nhaving the partial return and the predicted remaining return in the same place. The partial\nreturn is the sequence of rewards, and the predicted remaining return is a single-number\nestimate. The only reason why this isn\u2019t a return is that it isn\u2019t a discounted sum, but we can\ntake care of that as well.\n\nYou should realize that this is an n-step return, which you learned about in chapter 5. We\ngo out for n-steps collecting rewards, and then bootstrap after that nth state, or before if\nwe land on a terminal state, whichever comes first.\n\nA3C takes advantage of the lower variance of n-step returns when compared to Monte\nCarlo returns. We use the value function also to predict the return used for updating the\npolicy. You remember that bootstrapping reduces variance, but it adds bias. Therefore,\nwe've added a critic to our policy-gradient algorithm. Welcome to the world of actor-critic\nmethods.\n\nShow Me tHe Matu\nUsing n-step bootstrapping estimates\n\n\u00a9 sefore we were using Full returns \u201c T A(S;, Ar; 6) = Ge \u2014 V(S;3. 0)\nfor our advantage estimates.\n(@ Now, we're using n-step returns\n\nA(S;, Ai; @) = Re + 7Rigi t.. +7\" Ren + *1V (Stang @) \u2014 V(S:; 0)\n\n@) we now use this n-step advantage estimate\nfor updating the action a\n\nL, (0) = ty c (St, At; 6) log 7( A, | St; 0 )+ amr a(5s0)\n\nn=0\n\n@) We also use the n-step return to improve the value Function estimate. Notice\nthe bootstrapping here. This is what makes the algorithm an actor-critic method.\n\nJ\n\nN\n1 : 2\nL,(\u00a2) = WV 5 l(n + Rep te $Y Regn +7 V (Sty ngs) \u2014 V(S\u00a2:6)) |\n\nn=0\n\nI Speak PYTHON\nA3C optimization step 1/2\nclass A3C(): 4\u00a2\u20144 @asc, optimization Function\n\n<.2->\ndef optimize model ( __]\n\nself, logpas, entropies, rewards, values,\nlocal_ policy model, local _value_ model):\n\n@) First get the length of the reward. Remember,\nT = len(rewards) rewards includes the bootstrapping value.\ndiscounts = np.logspace(0, T, num=T, base=self.gamma,\nendpoint=False)\n\n@) Next, we calculate all discounts up to n+.\n\nreturns = np.array(\n\n? [np.sum(discounts[:T-t] * rewards[t:]) for t in range(T) ])\n@) This now is the n-step predicted return.\n\ndiscounts = torch.FloatTensor (\ndiscounts [:-1]) .unsqueeze (1)\nreturns = torch.FloatTensor (returns[:-1]) .unsqueeze (1)\n\n(S) To continue, we need to remove the extra. elements and format the variables as expected,\nrc value_error = returns - values\n(@) Now, we calculate the value errors as the predicted return minus the estimated values.\npolicy loss = -(discounts * value _error.detach() * \\\nlogpas) .mean ()\n\nloss = policy loss + self.entropy loss weight * \\\n\n[ entropy loss = -entropies.mean()\nentropy loss\n\n@ we calculate the loss as before.\n\nself.shared policy optimizer.zero grad()\nloss .backward () +i @ Notice we now zero the shared policy\noptimizer, then calculate the loss.\ntorch.nn.utils.clip grad_norm_(\nlocal_policy model.parameters(),\nr self.policy model _max_grad_norm)\n\n(@) Then, clip the gradient magnitude.\nfor param, shared_param in zip( \u00a24 Go) Continue on the next page.\n\nI Speak PYTHON\nA3C optimization step 2/2\n\n| nd for param, shared param in zip(\n\ni) Okay, so check this out. What local_policy model.parameters(),\nwe're doing here is iterating over self.shared policy model.parameters ()) :\n\nall local and shared policy network, | \u2014\u2014\u2014\u2014t Ga) And what we\nparameters. want to do is copy\nif shared _param.grad is None: every gradient\nshared param. grad = param.grad $rom the local to\nthe shared model.\n\n(2) Once the gradients are copied into the shared optimizer, we run an optimization step.\ntT\u00bb self.shared policy optimizer.step()\n\n4) Immediately after, we load the shared model into the local model.\n\nLL, local_ policy model.load state dict (\nself.shared_ policy model.state dict())\n\nGs) Next, we do the same thing but with the state-value network. Calculate the loss.\n\nvalue_loss = value_error.pow(2) .mul (0.5) .mean()\n(ie) Zero the shared value optimizer.\n\nself.shared_value_optimizer.zero_grad()\nvalue_loss.backward() \u2014\u2014 Backpropagate\nthe gradients.\n68) Then, clip them.\ntorch.nn.utils.clip grad_norm_(\nlocal_value_model.parameters(),\nself.value model _max_grad_norm)\n\n9) Then, copy all the gradients from the local model to the shared model.\n\nfor param, shared param in zip(\nlocal_value_model.parameters(),\nself.shared_value_model.parameters ()):\nif shared _param.grad is None:\nshared param. grad = param.grad\n\nself.shared value optimizer.step() 4\u00a2\u20144 @o) Step the optimizer.\n\u2014\u2014 i) Finally, load the\nShared roti into the\nlocal_value_model.load_ state dict ( loca! variable.\nself.shared value model.state dict())\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.18,
                        "section_name": "Non-blocking model updates",
                        "section_path": "./screenshots-images-2/chapter_11/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_18/3da20da2-4f72-407b-8ba7-e51bf3229f3c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Non-blocking model updates\n\nOne of the most critical aspects of A3C is that its network updates are asynchronous and lock-\nfree. Having a shared model creates a tendency for competent software engineers to want a\nblocking mechanism to prevent workers from overwriting other updates. Interestingly, A3C\nuses an update style called a Hogwild!, which is shown to not only achieve a near-optimal rate\nof convergence but also outperform alternative schemes that use locking by an order of\nmagnitude.\n\n1 Speak PYTHON\n\nShared Adam optimizer\nclass SharedAdam(torch.optim.Adam): 44 @ we need to create an Adam (and\n\nSooo RMSprop in the Notebook) optimizer\nthat puts internal variables into\nfor group in self.param_groups: shared memory, Gladiy, PyTorch\nfor p in group['params']: makes this straightforward.\nstate = self.state[p] @) We only need to call the share_\nstate['step'] = 0 memory_ method on the variables\nstate['shared_step'] = \\  wemneed shared across workers.\n(@) more torch.zeros(1).share memory ()\nVariables state['exp_avg'] = \\ +\nthan shown torch.zeros like(p.data).share_ memory ()\n\nhere }+\u2014\u2014\u2014_-} <...\n| a (4) Then, override the step function\nso that we can manually inerement\ndef step(self, closure=None) : iabl oes -\nfor group in self.param groups: paee tines Se cee\n\nfor p in group['params']: Gakus . 3\nif p.grad is None: continue\nself.state[p] ['steps'] = \\\n\nself.state([p] ['shared_step'] .item()\nself.state[p] ['shared_step'] += 1\nsuper() .step(closure) \u00a2\u2014\u2014\u2014\u2014\u2014-+ G5) Last, call the parent's \u2018step.\u2019\n\nA Bit oF History\n0001 ;\n\nIntroduction of the asynchronous advantage actor-critic (A3C)\nVlad Mnih et al. introduced A3C in 2016 in a paper titled \u201cAsynchronous Methods for Deep\nReinforcement Learning.\" If you remember correctly, Vlad also introduced the DQN agent in\n\ntwo papers, one in 2013 and the other in 2015. While DQN ignited growth in DRL research\nin general, A3C directed lots of attention to actor-critic methods more precisely.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.19,
                        "section_name": "GAE: Robust advantage estimation",
                        "section_path": "./screenshots-images-2/chapter_11/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_19/e12eba3e-4b38-43b3-a0e1-515bf72da839.png",
                            "./screenshots-images-2/chapter_11/section_19/3bd4aa97-980e-4846-a928-35a35bde215d.png",
                            "./screenshots-images-2/chapter_11/section_19/c42131ea-b4b0-4436-ae82-1cd16ef4da9a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "GAE: Robust advantage estimation\n\nA3C uses n-step returns for reducing the variance of the targets. Still, as you probably remem-\nber from chapter 5, there\u2019s a more robust method that combines multiple n-step bootstrap-\nping targets in a single target, creating even more robust targets than a single n-step: the\n-target. Generalized advantage estimation (GAE) is analogous to the A-target in TD(A), but\nfor advantages.\n\nGeneralized advantage estimation\n\nGAE is not an agent on its own, but a way of estimating targets for the advantage function\nthat most actor-critic methods can leverage. More specifically, GAE uses an exponentially\nweighted combination of n-step action-advantage function targets, the same way the A-target\nis an exponentially weighted combination of n-step state-value function targets. This type of\ntarget, which we tune in the same way as the A-target, can substantially reduce the variance of\npolicy-gradient estimates at the cost of some bias.\n\n= SHow Me tHe MatH\nPossible policy-gradient estimators\n\n\u00ae In policy-gradient and actor-critic oo\nBOBO re OCT g=E b W,Vo log 7(Ai| St; 0|\n\ngradient of this Form.\nt=0 t\n\ni @ we can replace Psi for a number of expressions that\nestimate the score with different levels of variance and bias.\n\n@) This one is the (4) @ut as we did in \u00a9 As we did in vPG, we\ntotal return starting REINFORCE, we can start at can use a baseline, which\nfrom step 0, all the the current time step, and go in our case was the state-\nway to the end, +o the end of the episode. value function.\nT a T T\nY= Ver w= 7 Rey Y= as \u201cRy \u2014 b(St)\nt=0 t=t ti=t\n(&) In ASC, we used @\u00ae...or even the TD\nthe n-step advantage \u00a9 eu, we could also residual, which can be\nestimate, which is the use the true action- seen as a one-step\n\nWe = ax(S;, At) Wt = qr(St, At) 4 Ve = Re + vz (St41) \u2014 Uz (St)\n\nf\\ SHow Me tHe MatH\nGAE is a robust estimate of the advantage function\nAl (S:, At; @) = Re + WV (St41; 0) \u2014 V(S13 6) TF O a-step advantage\nAY(S,, Aes) = Re + Revs + PV (S:4218) \u2014 V(S:6) NEY\nA\u00ae(S;, Ati @) = Re + Rest +P Ripe + PV (S435 \u00a2) \u2014 VS 0)\n\nA\u201d(S;, Aus @) = Re + yReg + +\" Regn $971 V (Stanais d) \u2014 V(Si3 6)\n\n(@... which we can mix to make an estimate Ge iad\nanalogous to TD lambda, but For advantages. -p ACAP(A) (S$, At: @) = So (A)'5e41\n@) Similarly, o.lambda of i=0\n\ncciiatags oir tok ACAEOO (5), Ars d) = Re + WV (St413. 0) \u2014 V(St39)\n\na. lambda of | returns the Aen co ;\ninfinite-step advantage AGAB)(S1, At) = a) Resi \u2014 V(St; 4)\nestimate. __\u2014______ l=0\n\n& SHow Me tHe Matu\n\nPossible value targets\n\u00a9 Notice we can use several diferent targets to train the state- a\nt\u2019-t\n\nvalue function neural network used to calculate @Ae values. n= >07 Ry\n@ we could use the reward to go, a.k.o., monte Carlo returns. -\u2014\u2014> v=\n@ The n-step =RityRiat..+ \"Rant yUv(s ;\u00a2\n\nreoging Ut t> YAt+1 Y Attn +7 (Stin+1; 9)\nincluding the TD\n\nG) or the ene, as a TOVambdo) estimate He yr = ACAHOA(S,, At: ) + V(St; 6)\n\nA Bit oF History\n0001\nIntroduction of the generalized advantage estimations\n\nJohn Schulman et al. published a paper in 2015 titled \u201cHigh-dimensional Continuous Control\nUsing Generalized Advantage Estimation,\u201d in which he introduces GAE.\n\nJohn is a research scientist at OpenAl, and the lead inventor behind GAE, TRPO, and\nPPO, algorithms that you learn about in the next chapter. In 2018, John was recognized by\nInnovators Under 35 for creating these algorithms, which are to this date state of the art.\n\n| Speak PYTHON\nGAE\u2019s policy optimization step\n\nclass GAE(): _ _ :\n<...> + 00 THis is the @nt optimize model logic.\n\ndef optimize model (\nself, logpas, entropies, rewards, values,\nlocal_policy model, local value _ model):\n@ First, we create the discounted returns, the way we did with ASC.\nT = len(rewards)\ndiscounts = np.logspace(0, T, num=T, base=self.gamma,\n\nendpoint=False)\nreturns = np.array(\n\n{np.sum(discounts[:T-t] * rewards[t:]) for t in range(T) ])\n@) These two lines are creating, First, a NumPy array with all the state values, and second, an\narray with the Gamma*lambda)\u201ct. Note, lambda is often referred to as tau, too. I\u2019m using that.\nnp_values = values.view(-1) .data.numpy()\ntau_discounts = np.logspace(0, T-1, num=T-1,\nbase=self.gamma*self.tau, endpoint=False)\n@) This line creates an array of TD errors: Rt + gamma * value_t+ - value_t, for t=0 to T-\n\nL, advs = rewards[:-1] + self.gamma * \\\nnp_values[1:] - np_values[:-1]\n\n\u00a9 Here we create the GAEs, by multiplying the tau discounts times the TD errors.\n\nLH, gaes = np.array(\n\n[np.sum(tau_discounts[:T-1-t] * advs[t:]) for t in range(T-1)])\n<...> 4-4 @ we now use the gaes to calculate the policy loss.\n\npolicy loss = -(discounts * gaes.detach() * \\\n\nlogpas) .mean ()\nentropy loss = -entropies.mean()\n\nloss = policy loss + self.entropy loss weight * \\\nentropy loss\n@ And proceed as usual.\nvalue _error = returns - values\n\nvalue_loss = value_error.pow(2) .mul (0.5) .mean()\n+ Id\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.2,
                        "section_name": "A2C: Synchronous policy updates",
                        "section_path": "./screenshots-images-2/chapter_11/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_20/621a000e-90a7-46c3-a888-4421fbc402a5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A2C: Synchronous policy updates\n\nIn A3C, workers update the neural networks asynchronously. But, asynchronous workers\nmay not be what makes A3C such a high-performance algorithm. Advantage actor-critic\n(A2C) is a synchronous version of A3C, which despite the lower numbering order, was pro-\nposed after A3C and showed to perform comparably to A3C. In this section, we explore A2C,\nalong with a few other changes we can apply to policy-gradient methods.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.21,
                        "section_name": "Weight-sharing model",
                        "section_path": "./screenshots-images-2/chapter_11/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_21/5492f9a3-d564-4e9a-87e5-605b154bae1e.png",
                            "./screenshots-images-2/chapter_11/section_21/50f4acc9-ff4b-4827-b5aa-a246b8c9f1a0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Weight-sharing model\n\nOne change to our current algorithm is to use a single neural network for both the policy and\nthe value function. Sharing a model can be particularly beneficial when learning from images\nbecause feature extraction can be compute-intensive. However, model sharing can be chal-\nlenging due to the potentially different scales of the policy and value function updates.\n\nSharing weights between policy and value outputs\n\n4\u2014 Policy outputs <\u2014\u2014 (i) we can share a Sew layers of the\nnetwork in policy-gradient methods, too. The\nnetwork would look like the dueling network\nyou implemented in chapter 10 with outputs\n\n<\u2014 Value output the size of the action space and another\noutput For the state-value function.\n\n1 SPEAK PYTHON\n\nWeight-sharing actor-critic neural network model 1/2\n\nclass FCAC(nn.Module) : \u00a2\u2014\u2014\u00a5H4 @ This is the fully connected actor-eritic model.\ndef init (\nself, input_dim, output_dim,\nhidden_dims=(32,32), activation_fc=F.relu) :\n@) This is the network instantiation process. This is similar to the independent network model.\nsuper(FCAC, self). init ()\nself.activation_fc = activation fc\nself.input_layer = nn.Linear(input_dim, hidden dims([0])\nself.hidden layers = nn.ModuleList ()\nfor i in range(len(hidden dims) -1) :\nhidden layer = nn.Linear(\nhidden _dims[i], hidden_dims[i+1])\nself.hidden_layers.append(hidden_layer)\nself.value output_layer = nn.Linear( \u00a2\u2014\u2014-4+@) Contines...\n\n| Speak PYTHON\nWeight-sharing actor-critic neural network model 2/2\n\nself.value_output_layer = nn.Linear( @) okay, Here\u2019s where\nhidden _dims[-1], 1) it\u2019s built, both the\nself.policy output_layer = nn.Linear( value output and the\nhidden _dims[-1], output_dim) policy output connect\nto the last layer of\nthe hidden layers.\ndef forward(self, state):\nx = state \u2014H \u00a9 The forward\nif not isinstance(x, torch.Tensor): pass starts by\nx = torch.tensor(x, dtype=torch.float32) reshaping the\nif len(x.size()) == 1: input to match\nxX = X.unsqueeze (0) the expected\nx = self.activation_fc(self.input_layer(x)) variable type and\nfor hidden layer in self.hidden_layers: shape.\n\nx = self.activation_fc(hidden_layer (x) )\nreturn self.policy output_layer(x), \\\nself.value_output_layer (x) @ And notice how it\n\nand a value layers.\ndef full pass(self, state):\n\nlogits, value = self.forward(state)\n@thisis dist = torch.distributions.Categorical (logits=logits)\na handy action = dist.sample()\nSunction logpa = dist.log_ prob(action) .unsqueeze (-1)\n+o get log, entropy = dist.entropy() .unsqueeze (-1)\n\nprobabilities, action = action.item() if len(action) == 1 \\\n\nentropies, else action.data.numpy()\nandother is exploratory = action != np.argmax(\n\nvariables ot logits.detach().numpy(), axis=int(len(state) !=1))\n\nonce. -+\u2014}> return action, is exploratory, logpa, entropy, value\n\n@\u00ae This selects the action or actions For the given state or batch of states.\ndef select_action(self, state):\nlogits, _ = self.forward(state)\ndist = torch.distributions.Categorical (logits=logits)\naction = dist.sample()\naction = action.item() if len(action) == 1 \\\nelse action.data.numpy()\nreturn action\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.22,
                        "section_name": "Restoring order in policy updates",
                        "section_path": "./screenshots-images-2/chapter_11/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_22/984c0907-a4c2-42c7-88a9-a1a467fc53ac.png",
                            "./screenshots-images-2/chapter_11/section_22/e18e279a-78d3-4901-8228-9870ffe9cda1.png",
                            "./screenshots-images-2/chapter_11/section_22/14e609c8-087f-456b-bcc3-4a8714e07859.png",
                            "./screenshots-images-2/chapter_11/section_22/0292b75d-b8b8-4912-b738-b7a85c15bc93.png",
                            "./screenshots-images-2/chapter_11/section_22/b560696b-3c0b-4a8e-9f51-f098eef2a822.png",
                            "./screenshots-images-2/chapter_11/section_22/df317235-bd46-49cc-8de0-94300e78e5eb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Restoring order in policy updates\n\nUpdating the neural network in a Hogwild!-style can be chaotic, yet introducing a lock mech-\nanism lowers A3C performance considerably. In A2C, we move the workers from the agent\ndown to the environment. Instead of having multiple actor-learners, we have multiple actors\nwith a single learner. As it turns out, having workers rolling out experiences is where the gains\nare in policy-gradient methods.\n\nSynchronous model updates\n\n( in pac, we have a. sing le Worker 1 Worker 2 Worker n\n\nagent driving the interaction Ce - (@ The neural networks\nwith the environment. But, in now need to process\n\nthis case, the environment is \u201c, T a batches of dato. This\na.multi-process class that means in AAC We can take\ngathers samples from ( utirocersen ) +\u2014\u2014\u2014 advantage of @Pus, unlike\nmultiple environments ASC in which CPUs are the\not once.\n\n\u00a2 ) most important resource.\nOnt o>\n\n| Speak PyTHON\nMulti-process environment wrapper 1/2\n\nclass MultiprocessEnv (object) :\ndef init (self, make_env_fn,make_env_kargs,\n\nseed, n_workers) : @ This is the multi-\nself.make_env_fn = make_env_fn process environment\nself.make_env_kargs = make _env_kargs class, which creates\nself.seed = seed pipes to communicate\nself.n_workers = n_workers with the workers,\nand creates the\nself.pipes = [ workers themselves.\nmp.Pipe() for rank in range(self.n_ workers) |\n(@ Here we create the workers.\n\nself.workers = [\nmp. Process (target=self.work,\nargs=(rank, self.pipes[rank][1])) \\\nfor rank in range(self.n workers) ]\n@) Here we start them.\ntl\u00bb {w.start() for w in self.workers]\n\n| Speak PYTHON\nMulti-process environment wrapper 2/2\n(w.start() for w in self.workers] \u00a2\u2014\u20144 (4) Continuation\n\ndef work(self, rank, worker end):\nenv = self.make_env_fn( (S) workers First create the environment.\n**self.make env_kargs, seed=self.seed + rank) nF\n\nwhile True: \u00a2\u2014\u2014\u2014\u2014 (& \u00e9\u00a2et in this loop listening for commands.\ncmd, kwargs = worker end.recv()\n\nif cmd == 'reset':\n\nworker end.send(env.reset (**kwargs) )\n| elif cmd == 'step':\n\nworker end.send(env.step(**kwargs) )\n@ tach elif cmd == ' past_limit':\ncommand calls # Another way to check time limit truncation\nthe respective worker end.send(\\\nenv Function env._elapsed_steps >= env. _max episode steps)\n\nandsendsthe else:\n\nresponse back env.close(**kwargs)\nto the parent del env\n\nprocess. worker end.close()\nbreak\n@\u00ae) This is the main step Function, For instance. (9) when called, it broadcasts the\nL def step(self, actions): Ccommeanc anclarguments to Worlsers.\nassert len(actions) == self.n_ workers iE\n\n[self.send_msg(('step', {'action':actions [rank] }),rank)\\\nfor rank in range(self.n_ workers) |\n\nresults = [] ye 69) workers do their part\n\nfor rank in range(self.n_ workers) : and send back the data,\nparent_end, _ = self.pipes[rank] which is collected here.\no, r, d, _ = parent_end.recv() GD We automatically\n\nif d: reset on done.\nself.send_msg(('reset', {}), rank) 1\n\n\u00a9 = parent_end.recv()\n\nresults.append((o,\nnp.array(r, dtype=np.float),\n\nLast, appen stack the results\na a vereeae int np.array(d, dtype=np.float), _))\ntions, ewar dones,\n\nreturn \\ t\n[np.vstack(block) for block in np.array(results) .T]\n\n1 Speak PYTHON\nThe A2C train logic\n\nclass A2C():\ndef train(self, make_envs fn, make _env_fn,\nmake_env_kargs, seed, gamma, max_minutes,\nmax_episodes, goal_mean_100 reward):\n\u00a9 This is how we train with the multi-processor environment.\n\nenvs = self.make envs_fn(make_env_fn, \u00a2\u2014\u20144 (@) tere, see\n\nmake _env_kargs, self.seed, how to create,\nself.n_ workers) basically,\nSooo vectorized\nenvironments.\n\n@) Here we create a single model. This is the\nactor-critic model with policy and value outputs.\n\nself.ac_model = self.ac_model fn(nS, nA)\nself.ac_optimizer = self.ac_ optimizer fn(\nself.ac_model, self.ac_optimizer lr)\n\n[[5 weates - envesresety (4) Look, we reset the multi-processor\nstates = envs.reset() environment and get a stack of states back.\n\nfor step in count (start=1):\n\n\u2014\u2014\u2014 states, is terminals = \\\n\n(S) The main thing is we work, self.interaction_step(states, envs)\nwith stacks now.\nif is_terminals.sum() or \\\nstep - n_steps start == self.max_n_ steps:\n\npast_limits enforced = envs. past_limit()\nfailure = np.logical and(is terminals,\nnp.logical not (past_limits enforced) )\n\nnext_values = self.ac_model.evaluate state(\nstates) .detach().numpy() * (1 - failure)\n\n(&) But, ot its self.rewards.append(next_values)\ncore, everything self.values.append (torch. Tensor (next_values) )\nis the same. self.optimize model ()\n| self.logpas, self.entropies = [], []\nself.rewards, self.values = [], []\n\nn_steps_ start = step\n\n| Speak PYTHON\nThe A2C optimize-model logic\n\nclass A2C(): \u00a2\u2014\u2014\u2014\u2014\u2014\u2014\u2014++ 0 This is how we optimize the model in Aac.\ndef optimize model (self):\n\nT = len(self.rewards)\n\ndiscounts = np.logspace(0, T, num=T, base=self.gamma,\nendpoint=False)\n\nreturns = np.array(\n\n@ The main thing to [[np.sum(discounts[:T-t] * rewards[t:, w])\nnotice is now we work with | [> for t in range(T)|] \\\n\nmatrices with vectors of for w in range(self.n_ workers) ])\ntime steps per worker.\n\nnp_values = values.data.numpy ()\ntau_discounts = np.logspace(0, T-1, num=T-1,\nbase=self.gamma*self.tau, endpoint=False)\n[PC = rewards[:-1] + self.gamma * np values[1:] \\\n. _ - np _values[:-1]\n() Some operations work the same exact way, surprisingly. -\ngaes = np.array(\n{({np.sum(tau_discounts[:T-1-t] * advs[t:, w]) \\\nfor t in range(T-1) ]\nfor w in range(self.n_workers) |) \u20ac\u2014i @) And for some,\n\ndiscounted_gaes = discounts[:-1] * gaes we just need to\n) Look +\u2014\u2014 add. a loop to\nhow we value _error = returns - values include all workers.\nbuild a value_loss = value_error.pow(2) .mul (0.5) .mean()\nsingle loss policy loss = -(discounted_gaes.detach() * \\\n\nfunction. logpas) .mean ()\n\nentropy loss = -entropies.mean()\n\nloss = self.policy loss weight * policy loss + \\\nself.value_loss_weight * value_loss + \\\nself.entropy loss weight * entropy loss\n\nself.ac_optimizer.zero grad()\nloss .backward ()\ntorch.nn.utils.clip grad _norm_(\nself.ac_model.parameters(),\nself.ac_ model _max_grad_norm)\nself.ac_optimizer.step()\n\nHF \u00a9 Finally, we\noptimize a single\nneural network.\n\nIt\u2019s In THE DetaiLs\nRunning all policy-gradient methods in the CartPole-v1 environment\n\nTo demonstrate the policy-gradient algorithms, and to make comparison easier with the\nvalue-based methods explored in the previous chapters, | ran experiments with the same\nconfigurations as in the value-based method experiments. Here are the details:\n\nREINFORCE:\n+ Runsa policy network with 4-128-64-2 nodes, Adam optimizer, and Ir 0.0007.\n+ Trained at the end of each episode with Monte Carlo returns. No baseline.\nVPG (REINFORCE with Monte Carlo baseline):\n\n+ Same policy network as REINFORCE, but now we add an entropy term to the loss\nfunction with 0.001 weight, and clip the gradient norm to 1.\n\n+ Wenow learn a value function and use it as a baseline, not as a critic. This means\nMC returns are used without bootstrapping and the value function only reduces\nthe scale of the returns. The value function is learned with a 4-256-128-1 network,\nRMSprop optimizer, and a 0.001 learning rate. No gradient clippings, though it\u2019s\npossible.\n\nA3C:\n+ We train the policy and value networks the same exact way.\n\n+ We now bootstrap the returns every 50 steps maximum (or when landing on a\nterminal state). This is an actor-critic method.\n\n+ We use eight workers each with copies of the networks and doing Hogwild!\nupdates.\n\nGAE:\n+ Same exact hyperparameter as the rest of the algorithms.\n\n+ Main difference is GAE adds a tau hyperparameter to discount the advantages. We\nuse 0.95 for tau here. Notice that the agent style has the same n-step bootstrap-\nping logic, which might not make this a pure GAE implementation. Usually, you see\nbatches of full episodes being processed at once. It still performs pretty well.\n\nA2C:\n\n+ A2C does change most of the hyperparameters. To begin with, we have a single\nnetwork: 4-256-128-3 (2 and 1). Train with Adam, Ir of 0.002, gradient norm of 1.\n\n+ The policy is weighted at 1.0, value function at 0.6, entropy at 0.001.\n+ Wego for 10-step bootstrapping, eight workers, and a 0.95 tau.\nThese algorithms weren't tuned independently; I'm sure they could do even better.\n\nTatty it Up\nPolicy-gradient and actor-critic methods on the CartPole-v1 environment\n\n@ Van all policy-gradient algorithms in the cart-pole environment so that\nyou can more easily compare policy-based and value-based methods.\n\n@ One of the main\nMoving Avg Reward (Training) things to notice is\n400. REINFORCE how VPq is more\n200 SE sample efficient\nseo BE than more complex\nmethods, such as ASC\nor AAC. This is mostly\n\u00b0 because these two\nMoving Avg Reward (Evaluation) methods use multiple\nworkers, which\ninitially cost lots oF\ndata. to get only a bit\nof progress.\n@ eeinForce alone\nTotal Steps is too inefficient\nto be a practical\nAS pheoe:\n(4) However, in terms.\noF training time,\nyou can see how\nREINFORCE uses Few\nresources. Also, notice\nhow algorithms with\nworkers consume\nmuch more compute.\n\n\u00a9) interestingly,\n0 in terms of wall-\nWall-clock Time clock time, parallel\n. methods and\na averaging \u201cIO\nco seconds to solve\n; cart-pole vi! The\n\n288 8 &\n\nTraining Time\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.23,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_11/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_23/40029920-e0ef-4a5f-a1f5-96fbb56f2d00.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter, we surveyed policy-gradient and actor-critic methods. First, we set up the\nchapter with a few reasons to consider policy-gradient and actor-critic methods. You learned\nthat directly learning a policy is the true objective of reinforcement learning methods. You\nlearned that by learning policies, we could use stochastic policies, which can have better per-\nformance than value-based methods in partially observable environments. You learned that\neven though we typically learn stochastic policies, nothing prevents the neural network from\nlearning a deterministic policy.\n\nYou also learned about four algorithms. First, we studied REINFORCE and how it\u2019s a\nstraightforward way of improving a policy. In REINFORCE, we could use either the full\nreturn or the reward-to-go as the score for improving the policy.\n\nYou then learned about vanilla policy gradient, also known as REINFORCE with baseline.\nIn this algorithm, we learn a value function using Monte Carlo returns as targets. Then, we\nuse the value function as a baseline and not as a critic. We don\u2019t bootstrap in VPG; instead,\nwe use the reward-to-go, such as in REINFORCE, and subtract the learned value function to\nreduce the variance of the gradient. In other words, we use the advantage function as the\npolicy score.\n\nWe also studied the A3C algorithm. In A3C, we bootstrap the value function, both for\nlearning the value function and for scoring the policy. More specifically, we use n-step returns\nto improve the models. Additionally, we use multiple actor-learners that each roll out the\npolicy, evaluate the returns, and update the policy and value models using a Hogwild!\napproach. In other words, workers update lock-free models.\n\nWe then learned about GAE, and how this is a way for estimating advantages analogous to\nTD(A) and the A-return. GAE uses an exponentially weighted mixture of all n-step advan-\ntages for creating a more robust advantage estimate that can be easily tuned to use more\nbootstrapping and therefore bias, or actual returns and therefore variance.\n\nFinally, we learned about A2C and how removing the asynchronous part of A3C yields a\ncomparable algorithm without the need for implementing custom optimizers.\n\nBy now, you\n\n+ Understand the main differences between value-based, policy-based, policy-gradient,\nand actor-critic methods\n\n+ Can implement fundamental policy-gradient and actor-critic methods by yourself\n\n+ Can tune policy-gradient and actor-critic algorithms to pass a variety of\nenvironments\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 12,
                "chapter_name": "Advanced\nactor-critic methods",
                "chapter_path": "./screenshots-images-2/chapter_12",
                "sections": [
                    {
                        "section_id": 12.1,
                        "section_name": "Advanced\nactor-critic methods",
                        "section_path": "./screenshots-images-2/chapter_12/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_1/e2434636-d520-4ef2-9ec7-bb7e3e502c88.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the last chapter, you learned about a different, more direct, technique for solving deep\nreinforcement learning problems. You first were introduced to policy-gradient methods in\nwhich agents learn policies by approximating them directly. In pure policy-gradient meth-\nods, we don\u2019t use value functions as a proxy for finding policies, and in fact, we don\u2019t use\nvalue functions at all. We instead learn stochastic policies directly.\n\nHowever, you quickly noticed that value functions can still play an important role and\nmake policy-gradient methods better. And so you were introduced to actor-critic methods.\nIn these methods, the agent learns both a policy and a value function. With this approach,\nyou could use the strengths of one function approximation to mitigate the weaknesses of the\nother approximation. For instance, learning policy can be more straightforward in certain\nenvironments than learning a sufficiently accurate value function, because the relationships\nin action space may be more tightly related than the relationships of values. Still, even though\nknowing the values of states precisely can be more complicated, a rough approximation can\nbe useful for reducing the variance of the policy-gradient objective. As you explored in the\nprevious chapter, learning a value function and using it as a baseline or for calculating advan-\ntages can considerably reduce the variance of the targets used for policy-gradient updates.\nMoreover, reducing the variance often leads to faster learning.\n\nHowever, in the previous chapter, we focused on using the value function as a critic for\nupdating a stochastic policy. We used different targets for learning the value function and\nparallelized the workflows in a few different ways. However, algorithms used the learned value\nfunction in the same general way to train the policy, and the policy learned had the same\nproperties, because it was a stochastic policy. We scratched the surface of using a learned\npolicy and value function. In this chapter, we go deeper into the paradigm of actor-critic meth-\nods and train them in four different challenging environments: pendulum, hopper, cheetah,\nand lunar lander. As you soon see, in addition to being more challenging environments, most\nof these have a continuous action space, which we face for the first time, and it'll require using\nunique polices models.\n\nTo solve these environments, we first explore methods that learn deterministic policies;\nthat is, policies that, when presented with the same state, return the same action, the action\nthat\u2019s believed to be optimal. We also study a collection of improvements that make deter-\nministic policy-gradient algorithms one of the state-of-the-art approaches to date for solving\ndeep reinforcement learning problems. We then explore an actor-critic method that, instead\nof using the entropy in the loss function, directly uses the entropy in the value function equa-\ntion. In other words, it maximizes the return along with the long-term entropy of the policy.\nFinally, we close with an algorithm that allows for more stable policy improvement steps by\nrestraining the updates to the policy to small changes. Small changes in policies make policy-\ngradient methods show steady and often monotonic improvements in performance, allowing\nfor state-of-the-art performance in several DRL benchmarks.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.2,
                        "section_name": "DDPG: Approximating a deterministic policy",
                        "section_path": "./screenshots-images-2/chapter_12/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_2/1a09e3cf-4346-4f09-8e3d-71b46580984b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "DDPG: Approximating a deterministic policy\n\nIn this section, we explore an algorithm called deep deterministic policy gradient (DDPG).\nDDPG can be seen as an approximate DQN, or better yet, a DQN for continuous action\nspaces. DDPG uses many of the same techniques found in DQN: it uses a replay buffer to\ntrain an action-value function in an off-policy manner, and target networks to stabilize train-\ning. However, DDPG also trains a policy that approximates the optimal action. Because of\nthis, DDPG is a deterministic policy-gradient method restricted to continuous action spaces.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.3,
                        "section_name": "DDPG uses many tricks from DQN",
                        "section_path": "./screenshots-images-2/chapter_12/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_3/b99066af-dd81-4c26-a093-85ba04ca82ae.png",
                            "./screenshots-images-2/chapter_12/section_3/126ee127-e4c4-4c3b-882d-342ebf2a60d0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "DDPG uses many tricks from DQN\n\nStart by visualizing DDPG as an algorithm with the same architecture as DQN. The training\nprocess is similar: the agent collects experiences in an online manner and stores these online\nexperience samples into a replay buffer. On every step, the agent pulls out a mini-batch from\nthe replay buffer that is commonly sampled uniformly at random. The agent then uses this\nmini-batch to calculate a bootstrapped TD target and train a Q-function.\n\nThe main difference between DQN and DDPG is that while DQN uses the target Q-function\nfor getting the greedy action using an argmax, DDPG uses a target deterministic policy func-\ntion that is trained to approximate that greedy action. Instead of using the argmax of the\nQ-function of the next state to get the greedy action as we do in DQN, in DDPG, we directly\napproximate the best action in the next state using a policy function. Then, in both, we use\nthat action with the Q-function to get the max value.\n\n& SHow Me tHe Matu\nDQN vs. DDPG value function objectives\n\u00ae Recall this function. This is the OQN loss function for the Q-Function. It\u2019s straightforward.\n= 2\nLi(O1) = Evsay.e)~tecD) [(r + max Q(s',a's6-) \u2014 Q(s,a;64)) | 1\n\n@ we sample a mini- t @) Then, calculate the TD target using the\nbatch from the buffer 0, reward and the discounted maximum value oF\nuniformly ot random. the next state, according to the target network.\n\n[PEO = Benera rata [(r + 1Q(s\" argmax Q(s',0/30);07) ~ Q(s,a;04)) \"|\n\n4} neo, roca this reurite of the samne exact equation we change the max forthe argmasc\nLi(O1) = Eyaa.nayucDy |( + 1Q(s', w(8\"s 97): 6) \u2014 Q(s,056:))\u201d]\n\n(S) in DOP a, We also | (&) But, instead of the 1c mutearns the deterministic greedy\n\nsample the mini- argmax according to Q, action in the state in question. Also,\nbatch as in DQN. we learn a. policy, mu. notice phi is also a target network ).\n\n| Speak PYTHON\nDDPG's Q-function network\nclass FCQV(nn.Module): \u00a7=\u2014\u2014\u2014_ This is the\ndef init (self, Q-Ffunction network\ninput_dim, used in DOPE.\noutput_dim,\nhidden_dims=(32, 32),\nactivation _fc=F.relu):\nsuper(FCQV, self). init () @ Here we start the\nself.activation_ fc = activation_fc i architecture as usual.\n\nself.input_layer = nn.Linear(input_dim, hidden _dims[0])\nself.hidden layers = nn.ModuleList ()\nfor i in range(len(hidden_ dims) ~-1):\n\nin_dim = hidden _dims[i]\n\n(3) Here we have the First\nif i \u2014- 0: exception. We increase the\nee dimension of the first hidden\n\nin_dim += output_dim \\ by the + di ro\n\nhidden layer = nn.Linear(in_ dim, hidden dims[i+1])\nself.hidden layers.append (hidden layer)\n\nself.output_layer = nn.Linear(hidden_dims[-1], 1)\n\n<...> @) Notice the output of the network is a single node\nrepresenting the value of the state-action pair.\n\ndef forward(self, state, action): <\u2014\u2014H 5) The Forward pass\nx, u = self. format(state, action) starts as expected,\nx = self.activation_fc(self.input_layer (x) )\n\nfor i, hidden layer in enumerate (self.hidden layers):\n(@ but we concatenate the action to the\nstates right on the First hidden layer.\n\nif i == 0:\nx = torch.cat((x, u), dim=1)\n\n@ Then, continue\nas expected,\n\nx = self.activation_fc(hidden_ layer (x) )\n\n@ Finally, return the output. 7\nreturn self.output_layer (x)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.4,
                        "section_name": "Learning a deterministic policy",
                        "section_path": "./screenshots-images-2/chapter_12/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_4/49055a55-0b15-410d-88be-6ac90f585926.png",
                            "./screenshots-images-2/chapter_12/section_4/7c4e591a-6670-4cc8-8537-2a10d5513624.png",
                            "./screenshots-images-2/chapter_12/section_4/e68d0d70-8496-49bd-b567-4b20873b06db.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "ae SS eee ewe wey\n\nNow, the one thing we need to add to this algorithm to make it work is a policy network. We\nwant to train a network that can give us the optimal action in a given state. The network must\nbe differentiable with respect to the action. Therefore, the action must be continuous to make\nfor efficient gradient-based learning. The objective is simple; we can use the expected Q-value\nusing the policy network, mu. That is, the agent tries to find the action that maximizes this\nvalue. Notice that in practice, we use minimization techniques, and therefore minimize the\nnegative of this objective.\n\n= SHow Me tHe Matu\nDDPG's deterministic policy objective\n\n@ Learning the policy is straightforward as well; we maximize the expected value of the\n@-function using the state, and the policy's selected action for that state.\n\nJi(\u00a2i) = E,nu(p) [2%s, (s; d); 6)| @) and then query\n\nthe Q-function for\n@ For this we use the sampled @) Query the policy the Q-value.\nstates from the replay buffer. for the best action\nin those states.\n\nAlso notice that, in this case, we don\u2019t use target networks, but the online networks for both\nthe policy, which is the action selection portion, and the value function (the action evaluation\nportion). Additionally, given that we need to sample a mini-batch of states for training the\nvalue function, we can use these same states for training the policy network.\n\nA Bit oF History\n0001\nIntroduction of the DDPG algorithm\n\nDDPG was introduced in 2015 in a paper titled \u201cContinuous control with deep reinforcement\nlearning.\u201d The paper was authored by Timothy Lillicrap (et al.) while he was working at Google\nDeepMind as a research scientist. Since 2016, Tim has been working as a Staff Research\nScientist at Google DeepMind and as an Adjunct Professor at University College London.\n\nTim has contributed to several other DeepMind papers such as the A3C algorithm,\nAlphaGo, AlphaZero, Q-Prop, and Starcraft Il, to name a few. One of the most interesting facts\nis that Tim has a background in cognitive science and systems neuroscience, not a traditional\ncomputer science path into deep reinforcement learning.\n\n| Speak PYTHON\nDDPG's deterministic policy network\n\nclass FCDP(nn.Module) : \u2014\u2014 0 this is the poli\nCO tele _((ecutee, pelle prea\ninput_dim, kerministic policy,\n\naction_bounds,\n\nhidden_dims=(32, 32),\n\nactivation fc=F.relu, 4H @ Notice the activation of\n\nout_activation_fc=F.tanh) : the output layer is di?Serent\nsuper(FCDP, self). init () this time. We use the tanh\n\nactivation Function to\n\n: : , , squash the output to (1, D.\n\nself.activation_ fc = activation fc\n\nself.out_activation_fe = out_activation_ fc\ncr self.env_min, self.env_max = action_bounds\n\n(3) We need to get the minimum and maximum values of the actions, so that\nwe can rescale the network's output (-1, i) to the expected range.\nself.input_layer = nn.Linear(input_dim, hidden dims[0])\nself.hidden layers = nn.ModuleList () t\nfor i in range(len(hidden_ dims) ~-1): a\n\narchitectur:\nhidden_layer = nn.Linear (hidden dims[i], 7 antes \u00a9\nhidden _dims[i+1]) conte\nself. hidden_layers.append (hidden layer) states in,\nactions out.\n\nself.output_layer = nn.Linear (hidden _dims[-1],\nlen(self.env_max) ) _!\n\ndef forward(self, state): () The forward pass is also straightforward\nx = self. format (state)\n\nx = self.activation_fc(self.input_layer(x)) \u00a2\u20144@\u00ae Input\nfor hidden layer in self.hidden_layers: .\nx = self.activation_fc(hidden_layer (x) ) @ widden\nx = self.output_layer (x) cM \u2014o\u201c@ Output\n(9) Notice, however, that we activate the output using, the output activation Function.\nLH, x = self.out_activation_fc(x)\n\nGO) Also important is that we rescale the action from the -I to | range to the range specific to\nthe environment. The rescale_fn isn\u2019t shown here, but you can go to the Notebook for details.\n\nreturn self.rescale fn(x)\n\nI Speak PYTHON\nDDPG's model-optimization step\n\ndef optimize model(self, experiences) : \u00a2\u20141 @ The optimize_model\nfunction takes in a mini-\nstates, actions, rewards, \\ batch of experiences.\nnext_states, is terminals = experiences\nbatch_size = len(is terminals)\n\n@ With it, we calculate the targets using the predicted max value of the next state, coming\nfrom the actions according to the policy and the values according to the Q-Function.\n\nargmax_a_q sp = self.target_policy model (next_states)\nmax_a_q sp = self.target_ value model (next_states,\nargmax_a_q sp)\ntarget_q sa = rewards + self.gamma * max_a_q sp * \\\n(1 - is_terminals)\n) we then get the predictions, and calculate the error and the\nloss. Notice where we use the target and online networks.\nq_sa = self.online value model(states, actions)\ntd_error = q_ sa - target_q sa.detach()\nvalue_loss = td_error.pow(2) .mul(0.5) .mean()\n\nself.value_optimizer.zero_grad() \u00a2\u2014 @) The optimization step is\nvalue_loss.backward() like all other networks.\ntorch.nn.utils.clip grad_norm_(\nself.online value _model.parameters(),\nself.value_max_grad_norm)\nself.value_optimizer.step()\n\n) Next, we get the actions as predicted by the online policy for the states in the mini-\nbatch, then use those actions to get the value estimates using the online value network.\nargmax_a_q_s = self.online policy model (states)\nmax_a_q_s = self.online value model (states,\nargmax_a_q Ss)\npolicy loss = -max_a_q_s.mean() \u20ac4 @wnext, we get the policy loss.\n\n@ Finally, we zero the\nself.policy optimizer.zero_grad() optimizer, do the backward pass\npolicy loss.backward() on the loss, clip the gradients,\n\ntorch.nn.utils.clip grad_norm_( and step the optimizer.\nself.online policy model.parameters(),\nself.policy max_grad_norm)\n\nself.policy optimizer.step()\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.5,
                        "section_name": "Exploration with deterministic policies",
                        "section_path": "./screenshots-images-2/chapter_12/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_5/88337cab-f292-4415-8b3e-bd4329203dfb.png",
                            "./screenshots-images-2/chapter_12/section_5/c44a08bf-71b4-4d29-a7d2-968c3549d53b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Exploration with deterministic policies\n\nIn DDPG, we train deterministic greedy policies. In a perfect world, this type of policy takes\nin a state and returns the optimal action for that state. But, in an untrained policy, the actions\nreturned won't be accurate enough, yet still deterministic. As mentioned before, agents need\nto balance exploiting knowledge with exploring. But again, since the DDPG agent learns a\ndeterministic policy, it won\u2019t explore on-policy. Imagine the agent is stubborn and always\nselects the same actions. To deal with this issue, we must explore off-policy. And so in DDPG,\nwe inject Gaussian noise into the actions selected by the policy.\n\nYou've learned about exploration in multiple DRL agents. In NFQ, DQN, and so on, we\nuse exploration strategies based on Q-values. We get the values of actions in a given state\nusing the learned Q-function and explore based on those values. In REINFORCE, VPG, and\nso on, we use stochastic policies, and therefore, exploration is on-policy. That is, exploration\nis taken care of by the policy itself because it\u2019s stochastic; it has randomness. In DDPG, the\nagent explores by adding external noise to actions, using off-policy exploration strategies.\n\n| Speak PYTHON\nExploration in deterministic policy gradients\nclass NormalNoiseDecayStrategy(): \u00a2\u20141(@) This is the select_action\n\ndef select_action(self, model, function of the straJ\nstate, max_exploration=False) :\nif max_exploration: () To maximize exploration, we set the\nnoise scale = self-.high noise scale to the maximum action.\nelse:\n\nnoise scale = self.noise ratio * self.high\n4 @ otherwise, we seale the noise down.\nwith torch.no grad():\n(4) we get the greedy action = model (state) .cpu() .detach().data\ngreedy action\u00bb greedy action = greedy action.numpy() .squeeze()\nstraight from\nthe network,  () Next, we get the Gaussian noise for the action using the scale and 0 mean.\nnoise = np.random.normal (loc=0,\n@ Add the noise to the action, scale=noise scale,\nand clip it to be in range. size=len(self.high) )\nnoisy action = greedy action + noise\naction = np.clip(noisy action, self.low, self.high)\n@ Next, we update the noise ratio schedule. This could\nbe constant, or linear, exponential, and so on.\nself.noise ratio = self. noise ratio_update()\n\nreturn action \u2014\u2014\u2014 \u00a9 Last, return the action.\n\nConcrete Example\nThe pendulum environment\n\nThe Pendulum-v0 environment consists of an inverted pendulum that the\nagent needs to swing up, so it stays upright with the least effort possible. The\nstate-space is a vector of three variables (cos(theta), sin(theta), theta dot)\nindicating the cosine of the angle of the rod, the sine, and the angular speed.\nThe action space is a single continuous variable from -2 to 2, indicating\nthe joint effort. The joint is that black dot at the bottom of the rod. The action\nis the effort either clockwise or counterclockwise.\nThe reward function is an equation based on angle, speed, and effort. The\ngoal is to remain perfectly balanced upright with no effort. In such an ideal\ntime step, the agent receives 0 rewards, the best it can do. The highest cost\n(lowest reward) the agent can get is approximately -16 reward. The precise\nequation is -(theta\u20182 + 0.1*theta_dt42 + 0.001 *action\u20192).\nThis is a continuing task, so there\u2019s no terminal state. However, the\nenvironment times out after 200 steps, which serves the same purpose. The environment is\nconsidered unsolved, which means there's no target return. However, -150 is a reasonable\nthreshold to hit.\n\nTatty It Up\nDDPG in the pendulum environment\n\n\u00a9 On the right you Moving Avg Reward (Training)\nsee the results -250. ~~ DDPG\n\nof training DOPE\nuntil it reaches\n-ISO reward on the\nevaluation episodes. h ~19\u00b0\u00b0\nwe use Five seeds =| -1250\nhere, but the graph\nis truncated ot the Moving Avg Reward (Evaluation)\npoint where the\nFirst seed\u2019s episodes ~5\u00b0\u00b0\nend. As you can see,\na. g00d job quickly. \u2014 -1500\nPendulum is a\n\n. 2000\nsimple environment. 20 40 oe Oe 80 100 120\n\n500\n\n-750\n\n-1000\n\n\u00b0\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.6,
                        "section_name": "TD3: State-of-the-art improvements over DDPG",
                        "section_path": "./screenshots-images-2/chapter_12/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_6/0e62eb44-4c71-4014-8253-c169b47149c6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "TD3: State-of-the-art improvements over DDPG\n\nDDPG has been one of the state-of-the-art deep reinforcement learning methods for control\nfor several years. However, there have been improvements proposed that make a big differ-\nence in performance. In this section, we discuss a collection of improvements that together\nform a new algorithm called twin-delayed DDPG (TD3). TD3 introduces three main changes\nto the main DDPG algorithm. First, it adds a double learning technique, similar to what you\nlearned in double Q-learning and DDQN, but this time with a unique \u201ctwin\u201d network archi-\ntecture. Second, it adds noise, not only to the action passed into the environment but also to\nthe target actions, making the policy network more robust to approximation error. And,\nthird, it delays updates to the policy network, its target network, and the twin target network,\nso that the twin network updates more frequently.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.7,
                        "section_name": "Double learning in DDPG",
                        "section_path": "./screenshots-images-2/chapter_12/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_7/7398b3fc-44d2-44d3-83d3-6bc01cb50f62.png",
                            "./screenshots-images-2/chapter_12/section_7/bf83dcd0-b694-4181-a326-7b8e32976aba.png",
                            "./screenshots-images-2/chapter_12/section_7/da9d1372-41f2-492b-84e0-a76a57807c20.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Double learning in DDPG\n\nIn TD3, we use a particular kind of Q-function network with two separate streams that end\non two separate estimates of the state-action pair in question. For the most part, these two\nstreams are totally independent, so one can think about them as two separate networks.\nHowever, it'd make sense to share feature layers if the environment was image-based. That\nway CNN would extract common features and potentially learn faster. Nevertheless, sharing\nlayers is also usually harder to train, so this is something you'd have to experiment with and\ndecide by yourself.\n\nIn the following implementation, the two streams are completely separate, and the only\nthing being shared between these two networks is the optimizer. As you see in the twin net-\nwork loss function, we add up the losses for each of the networks and optimize both networks\non that joint loss.\n\n= SHow Me tHe Mato\n\nTwin target in TD3\n\u00ae The twin \" target AY\nCrests PIL) = Evsyayr.ey~eecd) | (TWIN \u2014 Q(s, 0; 62))\u201d|\nis the sum of Pe\nwees of cash ind Ji (8\u00b0) = E(s,0,r,8) t(D) [ (Twine \u2014 Q(s, a; 6?))\ncalculate the * = n,\u2014\nwe clot eee TWIN \"8 =r +9 min Qls!,u(s/s 6): 0\")\nthe two streams. This isn't 0, -\u2014<$\u00a3$\u00a3@ $<\ncomplete TD3 target. We'll add (3) But, notice how we use the target networks\nto it in a couple of pages. for both the policy and value networks.\n\n1 Speak PYTHON\nTD3\u2019s twin Q-network 1/2\n\nclass FCTOQV(nn.Module) : \u2014\u2014F 00 This is the Fully connected\ndef init (self, Twin Q-value network, This is\ninput_dim, what TD3 uses to approximate\noutput_dim, the Q-values, with the twin\nhidden_dims= (32,32), streams.\n\nactivation _fc=F.relu):\nsuper(FCTOV, self). init ()\nself.activation_fc = activation_fc\n\n@ Notice we have two input layers. Again, these streams are really two separate networks.\nself.input_layer_a = nn.Linear(input_dim + output_dim,\nhidden _dims[0])\n\nself.input_layer_b = nn.Linear(input_dim + output_dim,\n\nhidden _dims[0])\n@ Next, we create hidden layers for each of the streams.\n\nself.hidden layers a = nn.ModuleList()\n\nself.hidden layers b = nn.ModuleList()\n\nfor i in range(len(hidden dims) -1) :\nhid_a = nn.Linear (hidden _dims[i], hidden_dims[i+1])\nself. hidden_layers a -append (hid a)\nhid_b = nn.Linear (hidden _dims[i], hidden_dims[i+1])\nself.hidden_layers b.append(hid_b)\n\n@) And we end with two output layers, each with a single node representing the Q-value.\n\nnn.Linear (hidden _dims[-1], 1)\nnn.Linear (hidden _dims[-1], 1)\n\nself.output_layer a\nself.output_layer b\n\n\u00a9) we start the forward pass, formatting the inputs to match what the network expects.\ndef forward(self, state, action):\nx, u = self. format(state, action)\n\n@ Next, we concatenate the state and action and pass them through each stream.\n\nx = torch.cat((x, u), dim=1)\nxa = self.activation_fc(self.input_layer _a(x))\nxb = self.activation_fc(self.input_layer b(x))\n@ Continues . ..\nfor hidden layer_a, hidden_layer b in zip(\nself.hidden layers a, self.hidden layers b):\n\nI Speak PYTHON\nTD3\u2019s twin Q-network 2/2\n\n@ Here we pass through all the hidden layers and their respective activation function.\nfor hidden layer a, hidden layer b in zip(\nself.hidden_ layers a, self.hidden_ layers _b):\nxa self.activation_fc(hidden_layer_a(xa))\nxb = self.activation_fc(hidden_layer_b (xb) )\n\n@) Finally, we do a. pass through the output layers and return their direct output.\nxa = self.output_layer_a(xa)\nxb = self.output_layer_ b(xb)\n\nreturn xa, xb GO) This is the Forward pass through\nthe Qa stream. This is useful For\ngetting the values when calculating\ndef Qa(self, state, action): the targets to the policy updates.\n\nCc x, u = self. format(state, action)\n\nGD We format the inputs, and concatenate them before passing it through the a stream.\nLy, x = torch.cat((x, u), dim=1)\n\nxa = self.activation fc(self.input_layer _a(x))\nGa) Then pass through the \u201ca\u201d hidden layers . . -\n\nfor hidden _layer_a in self.hidden_ layers a:\n\nxa = self.activation_fc(hidden_ layer a(xa))\n\n2)... all the way through the output layer, as if we had only one network to begin with.\n\nreturn self.output_layer_a(xa)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.8,
                        "section_name": "Smoothing the targets used for policy updates",
                        "section_path": "./screenshots-images-2/chapter_12/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_8/7614074f-1cec-423b-91a4-a757ae6cc4a1.png",
                            "./screenshots-images-2/chapter_12/section_8/2b13c15a-4475-4abe-b6e7-500bd53de17e.png",
                            "./screenshots-images-2/chapter_12/section_8/5e226582-388e-4f34-b4a6-ecfab6415d8b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Smoothing the targets used for policy updates\n\nRemember that to improve exploration in DDPG, we inject Gaussian noise into the action\nused for the environment. In TD3, we take this concept further and add noise, not only to the\naction used for exploration, but also to the action used to calculate the targets.\n\nTraining the policy with noisy targets can be seen as a regularizer because now the network\nis forced to generalize over similar actions. This technique prevents the policy network from\nconverging to incorrect actions because, early on during training, Q-functions can prema-\nturely inaccurately value certain actions. The noise over the actions spreads that value over a\nmore inclusive range of actions than otherwise.\n\n= SHow Me tHE Matu\n\nTarget smoothing procedure\n@ Lets consider a. clamp function, which clamp(z,1,h) = max(min(z, h), 1)\nbasically \u201cclamps\u201d or \u201cclips\u201d a value x\nbetween a low 4 anda high A. _\u2014_\u2014_______________t\noO al smorth \u2014 Clamp((s\u2019; 6) + clamp(e, el), \u20ac-h), al, a_h))\n@ In TDS, we smooth the action by adding clipped Gaussian noise, &. We First sample &, and\nclamp it to be between a preset min and max for &. We add that clipped Gaussian noise to the\n\naction, and then clamp the action to be between the min and max allowable according to the\nenvironment. Finally, we use that smoothed action.\n\nTD3tarset =r+ ymin Q(s', qismooth. go\")\nn\n\n1 Speak PYTHON\n\nTD3\u2018s model-optimization step 1/2 @ To optimize the\ndef optimize model(self, experiences) : 4<\u2014\u2014 T03 models, we\nstates, actions, rewards, \\ take in a mini-bateh\nnext_states, is_terminals = experiences of experiences.\nbatch_size = len(is terminals)\n\n@) We First get the min and\nwith torch.no grad(): max of the environment.\noo env_min = self.target_policy model.env_min Al\n@) eet the env_max = self.target_policy model.env_max\n\nnoiseandscale a_ran = env_max - env_min\nit to the range a_noise = torch.randn like(actions) * \\\nof the actions. K\u2014> self.policy noise ratio * a_ran\n4) Get the noise clip min and max.\nn_min = env_min * self.policy noise clip ratio\nn_max = env_max * self.policy noise clip ratio\n$ Thery ot the noise. ise = torch.max (\ntorch.min(a_ noise, n_max), n_min)\n\n]) eet the action From the target policy model.\nI argmax_a_q sp = self.target_policy model (\nnext_states)\n@ Then, add. the noise to the action, and clip the action, too.\n| noisy argmax_a_q sp = argmax_a_q sp + a_noise\nnoisy argmax_a_q sp = torch.max(torch.min(\n\nnoisy argmax_a_q_ sp, env_max), env_min)\n\n| SPEAK PYTHON\nTD3\u2019s model-optimization step 2/2\n@weusethe noisy argmax_a_q sp = torch.max(torch.min(\n\nclamped noisy noisy argmax_a_q_ sp, env_max), env_min)\naction to get max_a q Sp _a, max_a_q sp_b = \\\n\nthe max value. -\u2014> self.target_value_ model (next_states,\n\n\u00a9) Recall we get the max value by getting the minimum noisy argmax_a_q_sp)\n\npredicted value between the two streams, and use it for the target.\nmax_a_q sp = torch.min(max_a_q sp a, max_a_q sp b)\ntarget_q sa = rewards + self.gamma * max_a_q sp * \\\nGio) Next, we get the predicted values coming from both of the (1 - is_terminals)\nstreams to calculate the errors and the joint loss.\nq_sa_a, q_sa_b = self.online value model (states,\nactions)\n\ntd_error_a = q_sa_a - target_q sa\ntd_error_b = q_sa_b - target_q sa\nvalue_loss\n\ntd_error_a.pow(2) .mul(0.5).mean() + \\\ntd_error_b.pow(2) .mul (0.5) .mean()\n\nGD Then, we do the\nself.value_optimizer.zero_ grad() standard backpropagation\nvalue_loss.backward() steps For the twin networks.\n\ntorch.nn.utils.clip grad_norm_(\nself.online value model.parameters(),\nself.value_max_grad_norm)\nself.value_optimizer.step()\n\na) Notice how we delay the policy updates here. | explain this a bit more on the next page.\nif np.sum(self.episode timestep) % \\ tL,\nself.train policy every steps == 0:\n\n3) The update is similar to DOPE, but using the single stream \u2018Qa.\u2019\ntT} argmax_a_q_s = self.online_ policy model (states)\nmax_a_q_s = self.online value _model.Qa(\n\n4) But, the loss is the same. states, argmax_a_q_s)\n\u2014u\u2014\u2014\u2014\u2014\u00bb policy loss = -max_a_q_s.mean()\n(is) Here are the policy optimization steps. The standard stuff.\nself.policy optimizer.zero grad()\npolicy loss.backward()\ntorch.nn.utils.clip grad _norm_(\nself.online policy model.parameters(),\nself.policy max_grad_norm)\nself.policy optimizer.step()\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.9,
                        "section_name": "Delaying updates",
                        "section_path": "./screenshots-images-2/chapter_12/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_9/f28f300a-99bb-4909-baf4-a67b88c15088.png",
                            "./screenshots-images-2/chapter_12/section_9/63b248df-56ef-4089-82fd-446a39d7b5cf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Delaying updates\n\nThe final improvement that TD3 applies over DDPG is delaying the updates to the policy\nnetwork and target networks so that the online Q-function updates at a higher rate than the\nrest. Delaying these networks is beneficial because often, the online Q-function changes shape\nabruptly early on in the training process. Slowing down the policy so that it updates after a\ncouple of value function updates allows the value function to settle into more accurate values\nbefore we let it guide the policy. The recommended delay for the policy and target networks\nis every other update to the online Q-function.\n\nThe other thing that you may notice in the policy updates is that we must use one of the\nstreams of the online value model for getting the estimated Q-value for the action coming\nfrom the policy. In TD3, we use one of the two streams, but the same stream every time.\n\n0001 A Bit oF History\nIntroduction of the TD3 agent\n\nTD3 was introduced by Scott Fujimoto et al. in 2018 in a paper titled \u201cAddressing Function\nApproximation Error in Actor-Critic Methods.\u201d\n\nScott is a graduate student at McGill University working on a PhD in computer science and\nsupervised by Prof. David Meger and Prof. Doina Precup.\n\nConcrete EXAMPLE\nThe hopper environment\n\nThe hopper environment we use is an open source version of the MuJoCo and Roboschool\nHopper environments, powered by the Bullet Physics engine. MuJoCo is a physics engine\nwith a variety of models and tasks. While MuJoCo is widely used in DRL research, it requires a\nlicense. If you aren\u2019t a student, it can cost you a couple thousand dollars. Roboschool was an\nattempt by OpenAl to create open source versions of MuJoCo environments, but it was dis-\ncontinued in favor of Bullet. Bullet Physics is an open source project with many of the same\nenvironments found in MuJoCo.\n\nThe HopperBulletEnv-v0 environment features a vector\nwith 15 continuous variables as an unbounded observation\nspace, representing the different joints of the hopper robot.\nIt features a vector of three continuous variables bounded\nbetween -1 and 1 and representing actions for the thigh, leg,\nand foot joints. Note that a single action is a vector with three\nelements at once. The task of the agent is to move the hop-\nper forward, and the reward function reinforces that, also\npromoting minimal energy cost.\n\n\nIt\u2019s in THE DeTaiLs\nTraining TD3 in the hopper environment\n\nIf you head to the chapter\u2019s Notebook, you may notice that we train the agent until it reaches\na 1,500 mean reward for 100 consecutive episodes. In reality, the recommended threshold\nis 2,500. However, because we train using five different seeds, and each training run takes\nabout an hour, | thought to reduce the time it takes to complete the Notebook by merely\nreducing the threshold. Even at 1,500, the hopper does a decent job of moving forward, as\nyou can see on the GIFs in the Notebook.\n\nNow, you must know that all the book's implementations take a long time because they\nexecute one evaluation episode after every episode. Evaluating performance on every epi-\nsode isn\u2019t necessary and is likely overkill for most purposes. For our purposes, it\u2019s okay, but if\nyou want to reuse the code, | recommend you remove that logic and instead check evalua-\ntion performance once every 10-100 or so episodes.\n\nAlso, take a look at the implementation details. The book's TD3 optimizes the policy and\nthe value networks separately. If you want to train using CNNs, for instance, you may want to\nshare the convolutions and optimize all at once. But again, that would require much tuning.\n\nTatty it Up\nTD3 in the hopper environment\n@ Td3 does well\nin the hopper Moving Avg Reward (Training)\nenvironment, even 200 ~~~ TD3\nthough this is a.\nchallenging one. a)\n\nYou can see how 100\n\nMoving Avg Reward (Evaluation)\n\nenjoy the GIFs. In\nparticular, take a.\nlook at the progress\nof the agent. It\u2019s\nfun to see the \u00b0 200 400\nprogression of the\n\nperformance.\n\n800 1000 1200\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.1,
                        "section_name": "SAC: Maximizing the expected return and entropy",
                        "section_path": "./screenshots-images-2/chapter_12/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_10/ce5f232f-8493-413a-96b1-0be046b2c279.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "SAC: Maximizing the expected return and entropy\n\nThe previous two algorithms, DDPG and TD3, are off-policy methods that train a determin-\nistic policy. Recall, off-policy means that the method uses experiences generated by a behav-\nior policy that\u2019s different from the policy optimized. In the cases of DDPG and TD3, they\nboth use a replay buffer that contains experiences generated by several previous policies.\nAlso, because the policy being optimized is deterministic, meaning that it returns the same\naction every time it\u2019s queried, they both use off-policy exploration strategies. On our imple-\nmentation, they both use Gaussian noise injection to the action vectors going into the\nenvironment.\n\nTo put it into perspective, the agents that you learned about in the previous chapter learn\non-policy. Remember, they train stochastic policies, which by themselves introduce random-\nness and, therefore, exploration. To promote randomness in stochastic policies, we add an\nentropy term to the loss function.\n\nIn this section, we discuss an algorithm called soft actor-critic (SAC), which is a hybrid\nbetween these two paradigms. SAC is an off-policy algorithm similar to DDPG and TD3, but\nit trains a stochastic policy as in REINFORCE, A3C, GAE, and A2C instead of a deterministic\npolicy, as in DDPG and TD3.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.11,
                        "section_name": "Adding the entropy to the Bellman equations",
                        "section_path": "./screenshots-images-2/chapter_12/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_11/c65a81ac-d7ab-4ee9-9860-197b2042aaca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adding the entropy to the Bellman equations\n\nThe most crucial characteristic of SAC is that the entropy of the stochastic policy becomes\npart of the value function that the agent attempts to maximize. As you see in this sectiovn,\njointly maximizing the expected total reward and the expected total entropy naturally encour-\nages behavior that\u2019s as diverse as possible while still maximizing the expected return.\n\n= SHow Me tHE MatH\nThe agent needs to also maximize the entropy\n\n\u00a9 in SAC, we define @ We're going to add. up the\nthe action-value reward and the discounted value\nSunction as follows. of the next state-action pair.\n\nIx(s, a) a E,, s'~P(s,a),a'~x(s') [r + 1(an(s',0\u2019) +aH (x(-Is')))|\n\nt\n\n@ Here's the expectation @) However, we add the entropy of the\nover the reward, next policy at the next state. Alpha tunes the\nstate, and next action. importance we give to the entropy term.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.12,
                        "section_name": "Learning the action-value function",
                        "section_path": "./screenshots-images-2/chapter_12/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_12/64e07876-adba-4b07-beb0-7b66efa6621a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning the action-value function\n\nIn practice, SAC learns the value function in a way similar to TD3. That is, we use two net-\nworks approximating the Q-function and take the minimum estimate for most calculations.\nA few differences, however, are that, with SAC, independently optimizing each Q-function\nyields better results, which is what we do. Second, we add the entropy term to the target val-\nues. And last, we don\u2019t use the target action smoothing directly as we did in TD3. Other than\nthat, the pattern is the same as in TD3.\n\n= SHow Me THE MatH\nAction-value function target (we train doing MSE on this target)\n\n\u00a9 This is the target (@) We grab the reward @)... minimum value oF\nwe use on SAC. plus the discounted . . . the next state-action pair.\nSACI! =r + 7[ min Q(s', a; 0%) \u2014 alog r(a\u2019|s'; 0)|\nn\n(4) Notice the current policy ee @ pnd subtract the\nprovides the next actions. target networks. weighte log probability,\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.13,
                        "section_name": "Learning the policy",
                        "section_path": "./screenshots-images-2/chapter_12/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_13/ac5361af-28b1-4925-9d8a-99564fd8aed2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning the policy\n\nThis time for learning the stochastic policy, we use a squashed Gaussian policy that, in the\nforward pass, outputs the mean and standard deviation. Then we can use those to sample\nfrom that distribution, squash the values with a hyperbolic tangent function tanh, and then\nrescale the values to the range expected by the environment.\n\nFor training the policy, we use the reparameterization trick. This \u201ctrick\u201d consists of moving\nthe stochasticity out of the network and into an input. This way, the network is deterministic,\nand we can train it without problems. This trick is straightforwardly implemented in PyTorch,\nas you see next.\n\n= SHow Me tHE Matu\nPolicy objective (we train minimizing the negative of this objective)\n\u00a9 This is the objective () Notice we sample the state from the\nof the policy. buffer, but the action From the policy.\nJx(\u00a2) = Esnu(D),a~n [ min Q(s,4;0\") \u2014 alog r(a|s; )|\n\n@) we want the value minus the weighted @ That means we want to minimize the\nlog probability to be as high as possible. negative of what\u2019s inside brackets.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.14,
                        "section_name": "Automatically tuning the entropy coefficient",
                        "section_path": "./screenshots-images-2/chapter_12/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_14/42bd4067-aa76-4c4e-aa0c-030c0b00a6c1.png",
                            "./screenshots-images-2/chapter_12/section_14/f3c23b41-8887-4679-bd15-15b37cbc234b.png",
                            "./screenshots-images-2/chapter_12/section_14/520c8b50-003d-49fd-8c9b-fd967d54b4a4.png",
                            "./screenshots-images-2/chapter_12/section_14/2ab65694-4b1c-45f2-8c20-14fd4b0217c6.png",
                            "./screenshots-images-2/chapter_12/section_14/bb0f5943-48b2-4d5b-a01c-5c1daae1c537.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Automatically tuning the entropy coefficient\n\nThe cherry on the cake of SAC is that alpha, which is the entropy coefficient, can be tuned\nautomatically. SAC employs gradient-based optimization of alpha toward a heuristic expected\nentropy. The recommended target entropy is based on the shape of the action space; more\nspecifically, the negative of the vector product of the action shape. Using this target entropy,\nwe can automatically optimize alpha so that there\u2019s virtually no hyperparameter to tune\nrelated to regulating the entropy term.\n\n= SHow Me tHE MatH\nAlpha objective function (we train minimizing the negative of this objective)\n\u00ae This is the @ same as with the policy, we get the state\nobjective For alpha. from the buffer, and the action from the policy.\n\nJ (a) = Esn.u(d),am a(x + log (a|s;@))\n\n@\u00ae we want both the weighted 4, which is the target entropy | 4) -- - which means we\nheuristic, and the log probability to be as high as possible . . . race the negetve\nthis.\n\nss)\n\n| Speak PyTHoN\nSAC Gaussian policy 1/2\nclass FCGP(nn.Module): \u00a2\u2014\u2014\u2014\u2014\u2014+ \u00a9 This is the Gaussian policy that\ndef init (self, we use in SAC.\n<...>\nself.input_layer = nn.Linear(input_dim, + @ we start\nhidden_dims[0]) everything the\nself.hidden_layers = nn.ModuleList () same way os\nfor i in range(len(hidden dims) -1) : other policy\nhidden_layer = nn.Linear(hidden_dims[i], networks:\nhidden _dims[i+1]) input, to\nself hidden layers.append(hidden layer) \u00a2+4 hidden layers.\n\nof the action and the other the log standard deviation.\n\nself.output_layer mean = nn.Linear (hidden dims[-1],\nlen(self.env_max) )\n\n[ steesinantne er sent eatn\n\nself.output_layer_log std = nn.Linear(\nhidden_dims[-1],\nlen (self.env_max) )\n\nI Speak PYTHON\nSAC Gaussian policy 2/2\n\nin self.output_layer log std = nn.Linear(\nhidden _dims[-1],\nG) same line to help you keep the Flow of the code len (self.env_max) )\n\u00a9 Here we calculate +, the target entropy heuristic.\nL_, self.target_entropy = -np.prod(self.env_max.shape)\nrc self.logalpha = torch.zeros(1,\n@ Next, we create a variable, initialize to zero, requires _grad=True,\nand create an optimizer to optimize the log, alpha. device=self.device)\nself.alpha_ optimizer = optim.Adam([self.logalpha],\nlr=entropy lr)\n@ The forward Function is what we'd expect. ~\ndef forward(self, state):\nx = self. format(state) \u00a2\u2014\u2014\u2014\u2014\u2014+ @\u00ae) we format the\nx = self.activation_fc(self.input_layer (x) ) input variables, and\nfor hidden_layer in self.hidden_layers: pass them through\nx = self.activation_fc(hidden_layer (x) ) the whole network.\nx_mean = self.output_layer_ mean (x)\nx_log_std = self.output_layer log std(x) aes\n\nx_log_std = torch.clamp(x_log std, Senha\n_ _ _ > . to &, to control\nself.log std _min,\nnl a the std range to\nself.log std_max) r aA\nreturn x_mean, x_log std \u00a2\u2014\u2014\u2014\u2014x\nGo) And return the values.\ndef full pass(self, state, epsilon=le-6): \u00a2\u20144 GW inthe full\nmean, log std = self.forward(state) pass, we get the\nGa) Get a Normal distribution with those values. mean and log std,\n\npi_s = Normal(mean, log _std.exp())\n(3) rsample here does the reparameterization trick.\npre_tanh_action = pi_s.rsample()\no> tanh_action = torch.tanh(pre_tanh_action)\n(4) Then we squash the action to be in range -1, |.\nrc action = self.rescale_fn(tanh_action)\nGs) Then, rescale to be the environment expected range.\nlog_prob = pi_s.log prob(pre tanh_action) - torch.log(\n| (1 - tanh_action.pow(2)).clamp(0, 1) + epsilon)\nGe) We also need to rescale the log probability and the mean.\nlog_prob = log prob.sum(dim=1, keepdim=True)\nreturn action, log prob, self.rescale fn(\ntorch. tanh (mean) )\n\n| Speak PyTHoN\nSAC optimization step 1/2\n\ndef optimize model(self, experiences) : \u00a2\u20144 @ Thisis the\nstates, actions, rewards, \\ optimization step in SAC.\nnext_states, is terminals = experiences\nbatch_size = len(is terminals)\n\n@) First, get the experiences\nfrom the mini-batch.\ncurrent_actions, \\\nlogpi_s, _ = self.policy model.full pass (states)\n\n@) Next, we get the current actions, a-hat, and log probabilities of state s.\ntarget_alpha = (logpi_s + \\\nself.policy model.target_entropy) .detach()\nalpha_loss = -(self.policy model.logalpha * \\\ntarget_alpha) .mean()\n\n@ Here, we calculate the loss of alpha, and here we step alphas optimizer.\n\nself.policy model.alpha_optimizer.zero_ grad()\nalpha_loss.backward()\nself.policy model.alpha_optimizer.step()\n\n\u00a9 This is how we get the current value of alpha.\nL, alpha = self.policy model.logalpha.exp()\nG) In these lines, we get the Q-values using the online models, and a-hat.\n\ncurrent_q sa_a = self.online value model _a(\n\nstates, current_actions)\ncurrent_q_ sa_b = self.online value model _b(\n\nstates, current_actions)\n\n@ Then, we use the minimum Q-value estimates.\n\ncurrent_q sa = torch.min(current_q sa_a,\ncurrent_q_ sa_b)\n\n@\u00ae) Here, we calculate the policy loss using that minimum Q-value estimate.\nlL policy loss = (alpha * logpi_s - current_q sa) .mean()\n@) On the next page, we calculate the Q-functions loss.\n\nap, logpi_sp, _ = self.policy model.full pass (\nnext_states)\n\nI Speak PYTHON\n\nSAC optimization step 2/2\nco @ logpi_sp, _ = self.policy model.full pass (\nGo) To calculate the value loss, we get the predicted next action. next_states)\n\nGD using the target value models, we calculate the Q-value estimate of the next state-action pair.\n\nq_spap_a = self.target_value model a(next_states, ap)\nq_spap_b = self.target_value_ model b(next_states, ap)\n=o d_spap = torch.min(q spap a, q_spap b) - \\\n\nGa) Get the minimum Q-value estimate, and factor in the entropy. alpha * logpi_sp\n(3) This is how we calculate the target, using the reward plus the\ndiscounted minimum value of the next state along with the entropy,\n\ntarget_q sa = (rewards + self.gamma * \\\nq_spap * (1 - is terminals) ) .detach()\n\nG4) Here we get the predicted values of the state-action pair using the online model.\nq_sa_a = self.online value model _a(states, actions)\nq_sa_b = self.online value model b(states, actions)\nqa_loss = (q_sa_a - target_q sa) .pow(2) .mul (0.5) .mean()\n\n[ qb_loss = (q_sa_b - target_q sa) .pow(2) .mul (0.5) .mean()\n\nGs) Calculate the loss and optimize each Q-function separately. First, a:\nself.value_ optimizer a.zero grad() qT\nqa_loss.backward ()\ntorch.nn.utils.clip grad_norm_(\n\nself.online value model _a.parameters(),\nself.value_max_grad_norm)\nself.value_ optimizer _a.step()\n\nGe) Then, b:\nself.value_optimizer b.zero grad()\nqb_loss.backward()\ntorch.nn.utils.clip grad_norm_(\n\nself.online value model _b.parameters(),\nself.value_max_grad_norm)\nself.value_ optimizer b.step()\n7 Finally, the policy:\nae self.policy optimizer.zero grad()\npolicy loss.backward()\ntorch.nn.utils.clip grad _norm_(\nself.policy model.parameters(),\nself.policy max_grad_norm)\nself.policy optimizer.step()\n\n0001 A Bit oF History\nIntroduction of the SAC agent\n\nSAC was introduced by Tuomas Haarnoja in 2018 in a paper titled \u201cSoft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor.\u201d At the time of pub-\nlication, Tuomas was a graduate student at Berkeley working on a PhD in computer science\nunder the supervision of Prof. Pieter Abbeel and Prof. Sergey Levine, and a research intern at\nGoogle. Since 2019, Tuomas is a research scientist at Google DeepMind.\n\nConcrete Example\nThe cheetah environment\n\nThe HalfCheetahBulletEnv-v0 environment features a vector\nwith 26 continuous variables for the observation space, repre-\nsenting the joints of the robot. It features a vector of 6 contin-\nuous variables bounded between -1 and 1, representing the\nactions. The task of the agent is to move the cheetah forward,\nand as with the hopper, the reward function reinforces that\nalso, promoting minimal energy cost.\n\nae\n\n=\n\nTay it Up\nSAC on the cheetah environment\n\n@ sac does sooo Moving Avg Reward (Training)\nthe cheetah\n\nenvironment. In\nonly ~300-00\nepisodes, it learns\nto control the robot. -1000\nNotice that this\nenvironment has a\nrecommended\nreward threshold 1000\n\noF 3,000, but at\n\n4,000 the agent \u00b0\n\ndoes sufficiently\n\nWell. Also, it already oe\n\ntokes a few hours. 0 so 100\nto train.\n\nMoving Avg Reward (Evaluation)\n\n150 200 250 300\nEpisodes\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.15,
                        "section_name": "PPO: Restricting optimization steps",
                        "section_path": "./screenshots-images-2/chapter_12/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_15/6f30b830-c739-492d-84f7-fdae198026ab.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "PPO: Restricting optimization steps\n\nIn this section, we introduce an actor-critic algorithm called proximal policy optimization\n(PPO). Think of PPO as an algorithm with the same underlying architecture as A2C. PPO can\nreuse much of the code developed for A2C. That is, we can roll out using multiple environ-\nments in parallel, aggregate the experiences into mini-batches, use a critic to get GAE esti-\nmates, and train the actor and critic in a way similar to training in A2C.\n\nThe critical innovation in PPO is a surrogate objective function that allows an on-policy\nalgorithm to perform multiple gradient steps on the same mini-batch of experiences. As you\nlearned in the previous chapter, A2C, being an on-policy method, cannot reuse experiences\nfor the optimization steps. In general, on-policy methods need to discard experience samples\nimmediately after stepping the optimizer.\n\nHowever, PPO introduces a clipped objective function that prevents the policy from get-\nting too different after an optimization step. By optimizing the policy conservatively, we not\nonly prevent performance collapse due to the innate high variance of on-policy policy gradi-\nent methods but also can reuse mini-batches of experiences and perform multiple optimiza-\ntion steps per mini-batch. The ability to reuse experiences makes PPO a more sample-efficient\nmethod than other on-policy methods, such as those you learned about in the previous\nchapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.16,
                        "section_name": "Using the same actor-critic architecture as A2C",
                        "section_path": "./screenshots-images-2/chapter_12/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_16/240246bb-4204-4841-bea6-68def86669bd.png",
                            "./screenshots-images-2/chapter_12/section_16/3d631c4f-94b0-40e8-b3b9-a0091b341bc0.png",
                            "./screenshots-images-2/chapter_12/section_16/466dbf7b-9729-45a0-971e-7b3df0c24033.png",
                            "./screenshots-images-2/chapter_12/section_16/1d643052-fd4c-4dee-a073-1117bf670114.png",
                            "./screenshots-images-2/chapter_12/section_16/b824b264-753a-46dc-86bf-528ae3be0c12.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Using the same actor-critic architecture as A2C\n\nThink of PPO as an improvement to A2C. What I mean by that is that even though in this\nchapter we have learned about DDPG, TD3, and SAC, and all these algorithms have com-\nmonality. PPO should not be confused as an improvement to SAC. TD3 is a direct improve-\nment to DDPG. SAC was developed concurrently with TD3. However, the SAC author\npublished a second version of the SAC paper shortly after the first one, which includes several\nof the features of TD3. While SAC isn\u2019t a direct improvement to TD3, it does share several\nfeatures. PPO, however, is an improvement to A2C, and we reuse part of the A2C code. More\nspecifically, we sample parallel environments to gather the mini-batches of data and use GAE\nfor policy targets.\n\n0001 A Bit oF History\n\nIntroduction of the PPO agent\nPPO was introduced by John Schulman et al. in 2017 in a paper titled \u201cProximal Policy\nOptimization Algorithms.\u201d John is a Research Scientist, a cofounding member, and the co-lead\nof the reinforcement learning team at OpenAl. He received his PhD in computer science from\nBerkeley, advised by Pieter Abbeel.\n\nBatching experiences\n\nOne of the features of PPO that A2C didn\u2019t have is that with PPO, we can reuse experience\nsamples. To deal with this, we could gather large trajectory batches, as in NFQ, and \u201cfit\u201d the\nmodel to the data, optimizing it over and over again. However, a better approach is to create\na replay buffer and sample a large mini-batch from it on every optimization step. That gives\nthe effect of stochasticity on each mini-batch because samples aren\u2019t always the same, yet we\nlikely reuse all samples in the long term.\n\n| Speak PyTHON\nEpisode replay buffer 1/4\n\nclass EpisodeBuffer(): +\u2014\u2014H 0) This is the Fill of the Episodesuffer class.\ndef fill(self, envs, policy model, value model):\n\nstates = envs.reset() + 1\n\nwe_shape = (n_workers, self.max_episode_ steps) (@) voriables\n\nworker rewards = np.zeros(shape=we_shape, to keep\ndtype=np. float32) worker\n\nworker exploratory = np.zeros(shape=we_shape, _ information\n\ndtype=np .bool) grouped\nworker steps = np.zeros(shape=(n_workers),\ndtype=np.uint16)\n\nworker seconds = np.array([time.time(),] * n_workers,\n\ndtype=np. float64)\n\nbuffer full False I \u2018\nu r u = Fals \"\n\n= loop to Fill up the buffer.\nwhile not buffer full and \\\n\nlen(self.episode_steps[self.episode_steps>0]) < \\\nself.max_episodes/2:\nwith torch.no_grad(): @) we start by getting the current\nactions, logpas, \\ actions, log probabilities, and stats.\nare exploratory = policy model.np pass (states)\nvalues = value model (states)\n\u00a9 we pass the actions to the environments and get the experiences.\n\nnext_states, rewards, terminals, \\\ninfos = envs.step (actions)\n\n@ Then, self.states_mem[(self.current_ep_idxs,\nSOUS worker steps] = states\nnp! self.actions mem[self.current_ep_idxs,\ninto the worker steps] = actions\nreplay self.logpas_mem[self.current_ep idxs,\nbuffer. -\u2014> ~ 7.\n\nworker steps] = logpas\n\nI Speak PYTHON\n\nEpisode replay buffer 2/4 -\n@ Same line. Also, |\nself.logpas_mem[{self.current_ep idxs, removed spaces to make\nworker steps] = logpas it easier to read.\n\n(@\u00ae we create these two variables for each worker. Remember, workers are inside environments.\n\nworker exploratory([np.arange(self.n workers),\nworker steps] = are exploratory\nworker rewards (np.arange(self.n_ workers),\nworker steps] = rewards\n\n@ Here we manually truncate episodes that go For too many steps.\nfor w_idx in range(self.n_ workers) :\nif worker _steps[w_idx] + 1 == self.max_episode_steps:\nterminals[(w_idx] = 1\ninfos [(w_idx] ['TimeLimit.truncated'] = True\nGO) we check For terminal states and preprocess them.\n\nL& if terminals.sum():\n\nGD) we\nidx_terminals = np.flatnonzero (terminals) bootstrap\nnext_values = np.zeros(shape=(n_workers) ) iwhe\ntruncated = self. truncated_fn(infos) terminal\n\nif truncated.sum(): state was\nidx_truncated = np.flatnonzero (truncated) truncated.\nwith torch.no grad():\nnext_values[idx truncated] = value_model(\\\nnext_states[idx truncated] ) .cpu() .numpy()\n\nstates = next_states 4+\u2014 (a) we update the states variable\nworker steps += 1 \u00a2\u2014\u2014\u2014\u2014\\i and increase the step count.\n\n(3) Here we process the\nif terminals.sum() : workers if we have terminals.\nnew_states = envs.reset (ranks=idx_ terminals)\nstates[idx terminals] = new_states\n\nfor w_idx in range(self.n_ workers) :\n\nif w_idx not in idx terminals: (14) We process each\ncontinue a terminal worker one\nat a time.\n\ne_idx = self.current_ep_idxs[w_idx]\n\nI Speak PYTHON\nEpisode replay buffer 3/4\n\n+ e_idx = self.current_ep idxs[w_idx] L, (is) Further removed spaces\n\nT = worker steps[w_idx]\nself.episode steps[e idx]\n\n(ie) Here we collect statistics to\n\ndisplay and analyze after the Fact.\nself.episode reward[e_ idx] = worker _rewards([w_idx, :T] .sum()\nself.episode exploration[e idx] = worker _exploratory[\\\nw_idx, :T].mean()\nself.episode seconds[e_ idx] = time.time() - \\\nworker seconds [w_idx]\n(7D We append the bootstrapping value to the reward vector. Calculate the predicted returns.\n\nep rewards = np.concatenate((worker rewards[(w_idx, :T],\n[next_values[w_idx]]))\n\nep discounts = self.discounts[:T+1]\nep_returns = np.array(\\\n[np.sum(ep discounts[:T+1-t] * ep _rewards[t:]) \\\nfor t in range(T) |)\nself.returns mem(e_ idx, :T] = ep_returns\nGe) Here we get the predicted values, and also append the bootstrapping value to the vector.\nep states = self.states mem[e idx, :T]\nwith torch.no grad():\nep_ values = torch.cat((value_model(ep states),\ntorch.tensor (\\\n[next_values[w_idx]],\ndevice=value_model.device,\ndtype=torch. float32) ))\n\n9) Here we calculate the generalized advantage\nestimators, and save them into the buffer.\n\nnp_ ep values = ep_values.view(-1) .cpu() .numpy()\nep tau_discounts = self.tau_discounts[:T]\ndeltas = ep_rewards[:-1] + self.gamma * \\\nnp_ ep values[1:] - np_ep values[:-1]\ngaes = np.array(\\\n[np.sum(self.tau_discounts[:T-t] * deltas[t:]) \\\nfor t in range(T) |)\n\nself.gaes mem[e idx, :T] = gaes\n\n(@0) And start\nworker exploratory[(w_idx, :] = 0 resetting all\nworker rewards[w_idx, :] = 0 worker variables\nworker steps[w_idx] = 0 to process the\n\nworker _seconds[w_idx] = time.time() next episode.\n\n| Speak PYTHON\nEpisode replay buffer 4/4\n\no\u2014_ worker_seconds[w_idx] = time.time()\n@D same line, indentation edited again\nnew_ep_id = max(self.current_ep idxs) + 1\nif new_ep_id >= self.max_episodes:\n(a) Check which episode buffer full = True (25) 19 weer isn't full, we set the\nis next in queue and break break J id ot the new episode to the worker.\niF you have too many,\nself.current_ep_idxs[w_idx] = new_ep_id\n(4) If we're in these lines, it means the episode is full, so we process the memory For sampling.\nep_idxs = self.episode_ steps > 0\nep_t = self.episode steps[ep_idxs]\n\nSs) Because we initialize the whole buffer at once, we need to remove from the\nmemory everything that isn\u2019t a number, in the episode and the steps dimensions.\n\nself.states_ mem = [row[:ep t[i]] for i, \\\n\nrow in enumerate(self.states_mem[ep_idxs]) ]\nself.states mem = np.concatenate(self.states mem)\nself.actions_ mem = [row[:ep t[i]] for i, \\\n\nrow in enumerate(self.actions mem[{ep idxs]) ]\nself.actions_mem np.concatenate(self.actions_ mem)\nself.returns mem = [row[:ep t[i]] for i, \\\n\nrow in enumerate(self.returns mem[ep_idxs]) ]\nself.returns mem = torch.tensor(np.concatenate (\\\n\nself.returns mem), device=value_model.device)\n\nself.gaes_mem = [row[:ep t[i]] for i, \\\n\nrow in enumerate (self.gaes_mem[ep_idxs] ) |\nself.gaes_mem = torch.tensor (np.concatenate (\\\n\nself.gaes_ mem), device=value_model.device)\nself.logpas mem = [row[:ep t[i]] for i, \\\n\nrow in enumerate (self.logpas mem[ep_idxs]) ]\nself.logpas mem = torch.tensor(np.concatenate(\\\n\nself.logpas_mem), device=value_model.device)\n\n(b) Finally, we extract the statistics to display.\n\nL ep_r = self.episode reward[ep_ idxs]\nep x self.episode exploration[ep idxs]\nep_s self.episode seconds[ep_idxs]\n\nreturn ep t, ep_r, ep_x, \u20acp_S \u00a2\u2014\u2014\u2014\u2014\u20144 (a7) Andreturn the stats.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.17,
                        "section_name": "Clipping the policy updates",
                        "section_path": "./screenshots-images-2/chapter_12/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_17/427cf610-1d96-48a8-b905-60913c2076eb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Clipping the policy updates\n\nThe main issue with the regular policy gradient is that even a small change in parameter space\ncan lead to a big difference in performance. The discrepancy between parameter space and\nperformance is why we need to use small learning rates in policy-gradient methods, and even\nso, the variance of these methods can still be too large. The whole point of clipped PPO is to\nput a limit on the objective such that on each training step, the policy is only allowed to be so\nfar away. Intuitively, you can think of this clipped objective as a coach preventing overreact-\ning to outcomes. Did the team get a good score last night with a new tactic? Great, but don\u2019t\nexaggerate. Don\u2019t throw away a whole season of results for a new result. Instead, keep improv-\n\ning a little bit at a time.\n\n& SHow Me THE MatH\n\nClipped policy objective @) Next, we calculate the ratio\n\u00a9 For the policy objective, we First extract the between the new and old policy, and\nstates, actions, and @Aes from the buffer. use it for the objective.\nt\n\u2014\u2014 . | tlalsid) cae ma\\s;o) GAE\nI(G, 0) = Eps, AeA) tu(D(s-)) {min [zee ie clamp (aes yt 61+ ja \\\n\n\u2014\n\n@) We want to use the minimum (@)...and the clipped-ratio\nbetween the weighted GAE ... version of the same objective.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.18,
                        "section_name": "Clipping the value function updates",
                        "section_path": "./screenshots-images-2/chapter_12/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_18/ba81cf41-da6c-42cd-b0d8-d5bde53f913d.png",
                            "./screenshots-images-2/chapter_12/section_18/49a070e8-d82c-40ea-a7e6-5f39c848b082.png",
                            "./screenshots-images-2/chapter_12/section_18/3b67cec4-ea52-4c42-b9c8-0d301a51f499.png",
                            "./screenshots-images-2/chapter_12/section_18/f2217a77-749b-4183-a124-8c67850a644d.png",
                            "./screenshots-images-2/chapter_12/section_18/fa73d7dd-b7e8-4bc7-bd10-68da4cf5cf7f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Clipping the value function updates\n\nWe can apply a similar clipping strategy to the value function with the same core concept: let\nthe changes in parameter space change the Q-values only this much, but not more. As you\ncan tell, this clipping technique keeps the variance of the things we care about smooth,\nwhether changes in parameter space are smooth or not. We don\u2019t necessarily need small\nchanges in parameter space; however, we'd like level changes in performance and values.\n\n= SHow Me tHe Matu\nClipped value loss\n\n@ For the value Function, we also sample from\nthe replay buffer. is the return, vthe value.\n\nat\n(0,0s)=Erea.aivjnultoe>) {res le -V(s;6),G\u2014- (v + clamp (Vis 6) -V,-6, \u2018)) \\\n\n() Notice, we take the maximum Li co To estimate this through sampling, we\nmagnitude of the two errors. do MSE on the path that the max chooses.\n\n@ Look how we First move\nthe predicted values, then clip the\ndifference and shift it back.\n\n\nI Speak PYTHON\n\nPPO optimization step 1/3\ndef optimize model (self) : <\u2014__ 1 O now, let\u2019s look at those\n~ two equations in code.\n@ First, extract the full batch of experiences from the buffer.\nT states, actions, returns, \\\ngaes, logpas = self.episode buffer.get_stacks()\n\n@ eet the values before we start optimizing the models.\ntT, values = self.value_ model (states) .detach ()\n\n4) eet the gaes and normalize the batch.\n\nL_, gaes = (gaes - gaes.mean()) / (gaes.std() + EPS)\nn_samples = len(actions)\n\n\u00a9 Now, start optimizing the policy First for at most the preset epochs.\n\nfor i in range(self.policy optimization epochs) :\n\n@ We sub-sample from the full batch 0. mini-batch.\nbatch_size = int(self.policy sample ratio * \\\nn_samples)\nbatch_idxs = np.random.choice(n_samples,\nbatch_size,\nreplace=False)\n@ extract the mini-batch using the randomly sampled indices.\nstates batch = states[batch_idxs]\nL actions batch = actions[batch_idxs]\ngaes_ batch = gaes[batch_idxs]\nlogpas batch = logpas[batch_idxs]\n\n(\u00ae We use the online model to get the predictions.\n\nlogpas pred, entropies pred = \\\nself.policy model.get_predictions( \\\nstates batch, actions_batch)\n\n(9) Here we calculate the ratios: log probabilities to ratio oF probabilities.\nlL, ratios = (logpas pred - logpas batch) .exp()\ni id pi_obj gaes_ batch * ratios\nGO) Then, calculate the objective and the clipped objective. ry\n\npi_obj_clipped = gaes_ batch * ratios.clamp( \\\n1.0 - self.policy clip range,\n1.0 + self.policy clip range)\n\n1 Speak PYTHON\nPPO optimization step 2/3\n\npi_obj_clipped = gaes_ batch * ratios.clamp( \\\n1.0 - self.policy clip range,\n1.0 + self.policy clip range)\nGD We calculate the loss using the negative of the minimum of the objectives.\ntU\u2014-\u00bb policy loss = -torch.min(pi_ obj,\npi_obj_clipped) .mean ()\nGa) Also, we calculate the entropy loss, and weight it accordi\ntT\u00bb entropy loss = -entropies pred.mean() * \\\nself.entropy loss weight\n3) Zero the optimizing and start training. ~ ~\nself.policy optimizer.zero_grad()\n(policy loss + entropy loss) .backward()\ntorch.nn.utils.clip grad_norm_( \\\nself.policy model.parameters(),\nself.policy model _max_grad_norm)\nself.policy optimizer.step()\n\n4) after stepping the optimizer, we do this nice trick of ensuring we only\noptimize again if the new policy is within bounds of the original policy.\nwith torch.no grad():\nlogpas pred all, _ = \\\nself.policy model.get_predictions (states,\nactions)\n\nGs) Here we calculate the ki-divergence of the two policies.\nkl = (logpas - logpas pred _ all) .mean()\n\nGe) And break out of the training loop if it\u2019s greater than a. stopping condition.\n\nif kl.item() > self.policy stopping kl:\nbreak\n\nG7 Here, we start doing similar updates to the value Function.\n\nal for i in range(self.value optimization epochs) :\nbatch_size = int(self.value_sample ratio * \\\nn_samples)\nG8) We grab the mini-batch From the full batch, as with the policy.\ntLH_, batch_idxs = np.random.choice(n_samples,\nbatch_size,\nreplace=False)\nstates batch = states([batch_idxs]\n\nI Speak PYTHON\nPPO optimization step 3/3\n\nstates batch = states[batch_idxs]\nreturns batch = returns([batch_idxs]\nvalues batch = values[batch_idxs]\n\n9) @et the predicted values according to the model, and calculate the standard loss.\nvalues pred = self.value_model(states batch)\nv_loss = (values _pred - returns batch) .pow(2)\n\nGO) Here we calculate the clipped predicted values.\n\nL_, values pred clipped = values batch + \\\n(values pred - values_batch).clamp( \\\n-self.value_clip range,\nself.value_clip range)\n\ni) Then, calculate the clipped loss.\n\nL_, v_loss_clipped = (values _pred_clipped - \\\nreturns batch) .pow(2)\n\nGa) we use the MSE of the maximum between the standard and clipped loss.\n\nL_, value_loss = torch.max(\\\n\nv_loss, v_loss_clipped) .mul(0.5) .mean ()\n2) Finally, we zero the optimizer, backpropagate the loss, clip the gradient, and step.\n\nself.value_optimizer.zero_grad()\n\nvalue_loss.backward ()\n\ntorch.nn.utils.clip grad_norm_( \\\nself.value_model.parameters(),\nself.value_ model _max_grad_norm)\n\nself.value_optimizer.step()\n\n@4) We can do something similar to early stopping, but with the value Function.\n\nwith torch.no_grad():\nvalues _pred_all = self.value_model (states)\n\n@s) Basically we check for the mse of the predicted values of the new and old policies.\n\nmse = (values - values pred _ all) .pow(2)\nmse mse.mul (0.5) .mean()\nif mse.item() > self.value_stopping_mse:\n\nbreak\n\nConcrete EXAMPLE\nThe LunarLander environment\n\nUnlike all the other environments we have explored in\nthis chapter, the LunarLander environment features a\ndiscrete action space. Algorithms, such as DDPG and\nTD3, only work with continuous action environments,\nwhether single-variable, such as pendulum, or a vec-\ntor, such as in hopper and cheetah. Agents such as\nDON only work in discrete action-space environments,\nsuch as the cart-pole. Actor-critic methods such as\nA2C and PPO have a big plus, which is that you can use\nstochastic policy models that are compatible with vir-\ntually any action space.\n\nIn this environment, the agent needs to select one out of four possible actions on every\nstep. That is 0 for do nothing; or 1 for fire the left engine; or 2 for fire the main engine; or 3 for\nfire the right engine. The observation space is a vector with eight elements, representing the\ncoordinates, angles, velocities, and whether its legs touch the ground. The reward function is\nbased on distance from the landing pad and fuel consumption. The reward threshold for\nsolving the environment is 200, and the time step limit is 1,000.\n\nTatty it Up\n\nPPO in the LunarLander environment\n\n@ The Moving Avg Reward (Training)\n\nenvironment 200 PPO an\nisn't a difficult 100 ee\nenvironment, and \u00b0 cee \u2014_\u2014\u2014\nPPO, being a great eoeement\n\nalgorithm, solves -100 | ___e\n\nit in 10 minutes or <200\n\nSo. You may notice\nthe curves arent Moving Avg Reward (Evaluation)\n\ncontinuous. This \u201c \u2014\u2014\nis because in this \u00b0 es \u2014\u2014\u2014\u2014_-\nalgorithm, we only -200 ) _\u2014 i\n\nrun an evaluation -400:\u00ab=Oo\u20ac=E\n\nstep after each ~600 d\n\nepisode batch ~e00\n\ncollection. 0 100 200 300 400 500 600\nEpisodes\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.19,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_12/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_19/593271b0-6ff8-4ba4-a7b4-09abc9a252dc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter, we surveyed the state-of-the-art actor-critic and deep reinforcement learning\nmethods in general. You first learned about DDPG methods, in which a deterministic policy\nis learned. Because these methods learn deterministic policies, they use off-policy exploration\nstrategies and update equations. For instance, with DDPG and TD3, we inject Gaussian noise\ninto the action-selection process, allowing deterministic policies to become exploratory.\n\nIn addition, you learned that TD3 improves DDPG with three key adjustments. First, TD3\nuses a double-learning technique similar to that of DDQN, in which we \u201ccross-validate\u201d the\nestimates coming out of the value function by using a twin Q-network. Second, TD3, in addi-\ntion to adding Gaussian noise to the action passed into the environment, also adds Gaussian\nnoise to target actions, to ensure the policy does not learn actions based on bogus Q-value\nestimates. Third, TD3 delays the updates to the policy network, so that the value networks get\nbetter estimates before we use them to change the policy.\n\nWe then explored an entropy-maximization method called SAC, which consists of maxi-\nmizing a joint objective of the value function and policy entropy, which intuitively translates\ninto getting the most reward with the most diverse policy. The SAC agent, similar to DDPG\nand TD3, learns in an off-policy way, which means these agents can reuse experiences to\nimprove policies. However, unlike DDPG and TD3, SAC learns a stochastic policy, which\nimplies exploration can be on-policy and embedded in the learned policy.\n\nFinally, we explored an algorithm called PPO, which is a more direct descendant of A2C,\nbeing an on-policy learning method that also uses an on-policy exploration strategy. However,\nbecause of a clipped objective that makes PPO improve the learned policy more conserva-\ntively, PPO is able to reuse past experiences for its policy-improvement steps.\n\nIn the next chapter, we review several of the research areas surrounding DRL that are\npushing the edge ofa field that many call artificial general intelligence (AGI). AGI is an oppor-\ntunity to understand human intelligence by recreating it. Physicist Richard Feynman said,\n\u201cWhat I cannot create, I don\u2019t understand.\u201d Wouldn\u2019t it be nice to understand intelligence?\n\nBy now, you\n\n+ Understand more advanced actor-critic algorithms and relevant tricks\n\n+ Can implement state-of-the-art deep reinforcement learning methods and perhaps\ndevise improvements to these algorithms that you can share with others\n\n+ Can apply state-of-the-art deep reinforcement learning algorithms to a variety of\nenvironments, hopefully even environments of your own\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 13,
                "chapter_name": "Toward artificial\ngeneral intelligence",
                "chapter_path": "./screenshots-images-2/chapter_13",
                "sections": [
                    {
                        "section_id": 13.1,
                        "section_name": "Toward artificial\ngeneral intelligence",
                        "section_path": "./screenshots-images-2/chapter_13/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_1/8cc53611-e9d5-419b-8ad1-0331b2587b12.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In this book, we have surveyed a wide range of decision-making algorithms and reinforcement\nlearning agents; from the planning methods that you learned about in chapter 3 to the state-\nof-the-art deep reinforcement learning agents that we covered in the previous chapter. The\nfocus of this book is to teach the ins and outs of the algorithms; however, there\u2019s more to DRL\nthan what we covered in this book, and I want you to have some direction going forward.\n\nI designed this chapter to hit a couple of points. In the first section, we recap the entire\nbook. I'd like you to zoom out and look at the big picture again. I want you to see what you\u2019ve\nlearned so that you can choose for yourself where to go next. I also mention several of the\nnotable types of agents that I couldn\u2019t cover before I ran out of pages. But, know that while\nthere are more types of algorithms, what you learned in this book covers the foundational\nmethods and concepts.\n\nAfter going over what was covered and what wasn\u2019t, I introduce several of the more\nadvanced research areas in DRL that may lead to the eventual creation of artificial general\nintelligence (AGI). I know AGI is a hot topic, and lots of people use it in a deceiving way.\nBeing such an exciting and controversial topic, people use it to get attention. Don\u2019t give your\nenergy to those folks; don\u2019t be misled; don\u2019t get distracted. Instead, focus on what matters,\nwhat\u2019s right in front of you. And make progress toward your goals, whatever they may be.\n\nI do believe humans can create AGI because we'll keep trying forever. Understanding intel-\nligence and automating tasks is a quest we\u2019ve been longing for and working on for centuries,\nand that\u2019s never going to change. We try to understand intelligence through philosophy and\nthe understanding of the self. We look for answers about intelligence through introspection.\nI would argue that most AI researchers are part-time philosophers themselves. They use what\nthey\u2019ve learned in reinforcement learning to better themselves and the other way around, too.\n\nAlso, humans love automation; that\u2019s what intelligence has allowed us to do. We\u2019re going\nto continue trying to automate life, and we'll get there. Now, while we can argue whether AGI\nis the beginning of human-like robots that overtake the world, today, we still cannot train a\nsingle agent to play all Atari games at a super-human level. That is, a single trained agent\ncannot play all games, though a single general-purpose algorithm can be trained inde-\npendently. But, we should be cautious when considering AGI.\n\nTo close the chapter and the book, I provide ideas for you going forward. I receive many\nquestions regarding applying DRL to custom problems, environments of your own. I do this\nfor a living on my full-time job, so I can share my two cents as to how to go about it. I also give\ncareer advice for those interested, and a parting message. It\u2019s one more chapter: let\u2019s do this.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.2,
                        "section_name": "What was covered and what notably wasn\u2019t?",
                        "section_path": "./screenshots-images-2/chapter_13/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_2/85127f6e-a8e5-4dfd-9e2d-9599f6781b2a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What was covered and what notably wasn\u2019t?\n\nThis book covers most of the foundations of deep reinforcement learning, from MDPs and\ntheir inner workings to state-of-the-art actor-critic algorithms and how to train them in\ncomplex environments. Deep reinforcement learning is an active research field in which\nnew algorithms are published every month. The field is advancing at a rapid pace, and it\u2019s\nunfortunately not possible to provide high-quality explanations for everything there is in a\nsingle book.\n\nThankfully, most of the things left out are advanced concepts that aren\u2019t required in most\napplications. That doesn\u2019t mean that they aren\u2019t relevant; I highly recommend you continue\nthe journey of learning DRL. You can count on me to help along the way; I\u2019m easy to find. For\nnow, though, of the things left out of this book, I only consider two essential; they're model-\nbased deep reinforcement learning methods and derivative-free optimization methods.\n\nIn this section, we quickly review the algorithms and methods that you learned about in\nthis book and touch on these two essential methods that were notably missing.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.3,
                        "section_name": "Comparison of different algorithmic\napproaches to deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_3/d58e31b3-fedf-4f46-b42e-a9a8d761c998.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Comparison of different algorithmic\napproaches to deep reinforcement learning\n\nLess sample efficiency More sample efficiency\nLess computationally expensive More computationally expensive\nLess direct learning More direct learning\nMore direct use of learned function Less direct use of learned function\n\nPolicy-based Value-based Model-based\n\nL, @ In this book you learned about all these algorithmic approaches to deep\nreinforcement learning: we covered value-based, policy-based, and actor-\ncritic methods in depth. And later in this chapter, | introduce model-based\nand derivative-free methods to paint the Full picture For you.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.4,
                        "section_name": "Markov decision processes",
                        "section_path": "./screenshots-images-2/chapter_13/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_4/4c14ee1c-96ee-4a9a-aef9-a8bd696ce584.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Markov decision processes\n\nThe first two chapters were an introduction to the field of reinforcement learning and to the\nway we describe the problems we're trying to solve. MDPs are an essential concept to have in\nmind, and even if they look simple and limited, they\u2019re powerful. There\u2019s much more we\ncould have explored in this area. The thing I want you to take from these concepts is the abil-\nity to think of problems as MDPs. Practice this yourself. Think about a problem, and break it\ndown into states, observations, actions, and all the components that would make that prob-\nlem an MDP.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.5,
                        "section_name": "The transition function of the frozen lake environment",
                        "section_path": "./screenshots-images-2/chapter_13/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_5/71603f6f-ac6f-435f-bfe9-4234743c46a7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The transition function of the frozen lake environment\n\n@ Remember what MOPs look like. -\u2014\u2014}_\n\nBp\n= 4) 4\n\nset of states S,\nhere O-Is, a set\n\nof actions A, the (2) In the case of\ntransition function the frozen lake\nT, reward signal environment,\n\n2, the initial state which is an\ndistribution S,, the episodic task,\ndiscount factor there are\ngamma, and the terminal states,\nhorizon #.\n\nhere S, 7, ll, la, IS.\n\nYou'll notice that even though it seems the world is non-stationary and non-Markovian, we\ncan transform some of the things that make it seem that way, and then see the world as an\nMDP. Do the probability distributions of the real world change, or is it that we don\u2019t have\nenough data to determine the actual distributions? Does the future depend on past states, or\nis the state space so high-dimensional that we can\u2019t conceive all history of the world being\npart of a single state? Again, as an exercise, think of problems and try to fit them into this\nMDP framework. It may turn out to be useful if you want to apply DRL to your problems.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.6,
                        "section_name": "Planning methods",
                        "section_path": "./screenshots-images-2/chapter_13/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_6/161b642a-dec2-4886-8ede-3ed27ae7475a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Planning methods\n\nIn the third chapter, we discussed methods that help you find optimal policies of problems\nthat have MDPs available. These methods, such as value iteration and policy iteration, itera-\ntively compute the optimal value functions, which in turn allow extracting optimal policies\nquickly. Policies are nothing but universal plans\u2014a plan for every situation.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.7,
                        "section_name": "Policy evaluation on the always-left policy on the SWF environment",
                        "section_path": "./screenshots-images-2/chapter_13/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_7/31af0f6e-5698-4aff-bbf4-3fa7f7be4767.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Policy evaluation on the always-left policy on the SWF environment\n\nSTART\nlel olFl lel,\n1 2 3 4 5\n\n@ Here we calculate the Q@\u2014Ffunction after a. full-state sweep.\n\n1st Iteration\n\n(@ see how \u2018START\neven ofter 0.0 0.0/0.0 o0}00 aolo0 o.0}017 0.56\nthe first 1 2 3 4 5 7\n\niteration the 2nd Iteration\n\ngreedy policy (2) Recall that value\nover the iteration does just\n@-function 6} that. It improved a.\nwas already we truncated policy\n\na different evaluation.\n\nand better 104th Iteration\n\nlieu! START\npolicy: 0.0 0.0/0.0 0.0}0.01 0.01]0.03 0.04/0.24 06:\n1 2 3 4 5\n\n() This is the fully converged state-value function for the\naluays-lef policy. This is the output of policy evaluation.\n\nThe two most important takeaways from this section are first. These algorithms isolate\nsequential decision-making problems. There\u2019s no uncertainty because they require the MDP,\nand there\u2019s no complexity because they only work for discrete state and action spaces. Second,\nthere\u2019s a general pattern to see here; we\u2019re interested in evaluating behaviors, perhaps as much\nas we\u2019re interested in improving them. This realization was something I didn\u2019t get for some\ntime. To me, improving, optimizing sounded more interesting, so policy evaluation methods\ndidn\u2019t get my attention. But you later understand that if you get evaluation right, improving\nis a piece of cake. The main challenge is usually evaluating policies accurately and precisely.\nBut, if you have the MDP, you calculate these values correctly and straightforwardly.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.8,
                        "section_name": "Bandit methods",
                        "section_path": "./screenshots-images-2/chapter_13/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_8/6b0c23d2-df9f-4616-96b6-7566ab75abf3.png",
                            "./screenshots-images-2/chapter_13/section_8/9f238df6-56de-4217-b4b7-2ce9b690e0f5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Bandit methods\n\nThe fourth chapter was about learning from evaluative feedback. In this case, we learned\nabout the uncertainty aspect of reinforcement learning by taking the MDP away. We hide the\nMDP, but make the MDP super simple; a single-state single-step horizon MDP, in which the\nchallenge is to find the optimal action or action distribution in the fewest number of epi-\nsodes; that is, minimizing total regret.\n\nIn chapter 4, you learned more effective ways for\ndealing with the exploration-exploitation trade-off\n\nWe studied several different exploration strategies and tested them in a couple of bandit envi-\nronments. But at the end of the day, my goal for that chapter was to show you that uncer-\ntainty on its own creates a challenge worth studying separately. There are a few great books\n\nabout this topic, and if you\u2019re interested\nin it, you should pursue that path; it\u2019s\na reasonable path that needs much\nattention.\n\nThe right nugget to get out of this\nchapter and into reinforcement learn-\ning is that reinforcement learning is\nchallenging because we don\u2019t have\naccess to the MDP, as in the planning\nmethods in chapter 3. Not having the\nMDP creates uncertainty, and we can\nonly solve uncertainty with explora-\ntion. Exploration strategies are the rea-\nson why our agents can learn on their\nown, by trial-and-error learning, and\nit\u2019s what makes this field exciting.\n\n10-armed Gaussian bandits\n\n@) @ut the\nreward paid\nvaries. \\t\u2019s\nsampled from\n& Gaussian\ndistribution.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.9,
                        "section_name": "Tabular reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_9/34d91887-a1c5-4a1d-9561-ef029c1dda7f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tabular reinforcement learning\n\nChapters 5, 6, and 7 are all about mixing the sequential and uncertain aspects of reinforce-\nment learning. Sequential decision-making problems under uncertainty are at the core of\nreinforcement learning when presented in a way that can be more easily studied; that is, with-\nout the complexity of large and high-dimensional state or action spaces.\n\nChapter 5 was about evaluating policies, chapter 6 was about optimizing policies, and\nchapter 7 was about advanced techniques for evaluating and optimizing policies. To me, this\nis the core of reinforcement learning, and learning these concepts well helps you understand\ndeep reinforcement learning more quickly. Don\u2019t think of DRL as something separate from\ntabular reinforcement learning; that\u2019s the wrong thinking. Complexity is only one dimen-\nsion of the problem, but it\u2019s the same exact problem. You often see top, deep reinforcement\nlearning-research labs releasing papers solving problems in discrete state and action spaces.\nThere\u2019s no shame in that. That\u2019s often the smart approach, and you should have that in mind\nwhen you experiment. Don\u2019t start with the highest-complexity problem; instead, isolate, then\ntackle, and finally, increase complexity.\n\nIn these three chapters, we covered a wide variety of algorithms. We covered evaluation\nmethods such as first-visit and every-visit Monte Carlo prediction, temporal-difference pre-\ndiction, n-step TD, and TD(A). We also covered control methods such as first-visit and every-\nvisit Monte Carlo control, SARSA, Q-learning, double Q-learning, and more advanced\nmethods, such as SARSA(A) and Q(A) both with replacing and also with accumulating traces.\nWe also covered model-based approaches, such as Dyna-Q and trajectory sampling.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.1,
                        "section_name": "Deep reinforcement learning is part of\nthe larger field of reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_10/adaa875d-2249-4ac5-a34b-8943f6b08233.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep reinforcement learning is part of\nthe larger field of reinforcement learning\n\n@) Remember that\n\nlearning also has\nprediction and\ncontrol methods.\nDQN, For instance,\nis a control method,\nwhile the critic in\nASC is a prediction\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.11,
                        "section_name": "Value-based deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_11/6716b5f8-8867-4450-b445-879a30a1b69a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Value-based deep reinforcement learning\n\nChapters 8, 9, and 10 are all about the nuances of value-based deep reinforcement learning\nmethods. We touched on neural fitted Q-iteration (NFQ), deep Q-networks (DQN), double\ndeep Q-networks (DDQN), dueling architecture in DDQN (dueling DDQN), and prioritized\nexperience replay (PER). We started with DQN and added improvements to this baseline\nmethod one at a time. We tested all algorithms in the cart-pole environment.\n\nThere are many more improvements that one can implement to this baseline algorithm,\nand I recommend you try that. Check out an algorithm called Rainbow, and implement some\nof the improvements to DQN not in this book. Create a blog post about it and share it with\nthe world. Techniques you learn when implementing value-based deep reinforcement learn-\ning methods are essential to other deep reinforcement learning approaches, including learning\ncritics in actor-critic methods. There are a many improvements and techniques to discover.\nKeep playing around with these methods.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.12,
                        "section_name": "Policy-based and actor-critic deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_12/2572b513-9cae-4659-ab13-d6f31cdce2c1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Policy-based and actor-critic deep reinforcement learning\n\nChapter 11 was an introduction to policy-based and actor-critic methods. Policy-based was a\nnew approach to reinforcement learning at that point in the book, so we introduced the con-\ncepts in a straightforward algorithm known as REINFORCE, which only parameterizes the\npolicy. For this, we approximated the policy directly and didn\u2019t use any value function at all.\nThe signal that we use to optimize the policy in REINFORCE is the Monte Carlo return, the\nactual returns experienced by the agent during an episode.\n\nWe then explored an algorithm that learns a value function to reduce the variance of the\nMC return. We called this algorithm vanilla policy gradient (VPG). The name is somewhat\narbitrary, and perhaps a better name would have been REINFORCE with Baseline. Nevertheless,\nit\u2019s important to note that this algorithm, even though it learns a value function, is not an\nactor-critic method because it uses the value function as a baseline and not as a critic. The\ncrucial insight here is that we don\u2019t use the value function for bootstrapping purposes, and\nbecause we also train the value-function model using MC returns, there\u2019s minimal bias in it.\nThe only bias in the algorithm is the bias introduced by the neural network, nothing else.\n\nThen, we covered more advanced actor-critic methods that do use bootstrapping. A3C,\nwhich uses n-step returns; GAE, which is a form of lambda return for policy updates; and\nA2C, which uses synchronous updates to the policy. Overall, these are state-of-the-art meth-\nods, and you should know that they\u2019re reliable methods that are still widely used. One of the\nmain advantages and unique characteristics of A3C, for instance, is that it only needs CPUs,\nand can train faster than other methods, if you lack a GPU.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.13,
                        "section_name": "Advanced actor-critic techniques",
                        "section_path": "./screenshots-images-2/chapter_13/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_13/7f78a855-6144-4518-a1e1-9e7d0e7899ca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Advanced actor-critic techniques\n\nEven though A3C, GAE, and A2C, are actor-critic methods, they don\u2019t use the critic in unique\nways. In chapter 12, we explored methods that do. For instance, many people consider DDPG\nand TD3 actor-critic methods, but they fit better as value-based methods for continuous\naction spaces. If you look at the way A3C uses the actor and the critic, for instance, you find\nsubstantial differences in DDPG. Regardless, DDPG and TD3 are state-of-the-art methods,\nand whether actor-critic or not, it doesn\u2019t make much of a difference when solving a problem.\nThe main caveat is that these two methods can only solve continuous action-space environ-\nments. They could be high-dimensional action spaces, but the actions must be continuous.\nOther methods, such as A3C, can solve both continuous and discrete action spaces.\n\nSAC is an animal of its own. The only reason why it follows after DDPG and TD3 is because\nSAC uses many of the same techniques as DDPG and TD3. But the unique characteristic of\nSAC is that it\u2019s an entropy-maximization method. The value function maximizes not only\nthe return but also the entropy of the policy. These kinds of methods are promising, and I\nwouldn\u2019t be surprised to see new state-of-the-art methods that derive from SAC.\n\nFinally, we looked at another exciting kind of actor-critic method with PPO. PPO is an\nactor-critic method, and you probably notice that because we reused much of the code from\nA3C. The critical insight with PPO is the policy update step. In short, PPO improves the pol-\nicy a bit at a time; we make sure the policy doesn\u2019t change too much with an update. You can\nthink of it as a conservative policy-optimization method. PPO can be easily applied to both\ncontinuous and discrete action spaces, and PPO is behind some of the most exciting results\nin DRL, such as OpenAI Five, for instance.\n\nWe covered many great methods throughout these chapters, but more importantly, we\ncovered the foundational methods that allow you to understand the field going forward.\nMany of the algorithms out there derive from the algorithms we covered in this book, with a\nfew exceptions, namely, model-based deep reinforcement learning methods, and derivative-\nfree optimization methods. In the next two sections, I give insights into what these methods\nare so that you can continue your journey exploring deep reinforcement learning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.14,
                        "section_name": "DRL algorithms in this book",
                        "section_path": "./screenshots-images-2/chapter_13/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_14/dc8ad05b-9609-4542-ad6a-5ac91adfc142.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "DRL algorithms in this book\n\nREINFORCE\nVPG DDPG\nA3C TD3\nDQN Dueling DDQN GAE SAC\n! NFQ DDQN PER A2C PPO |\n8 9 10 11 12\ntT\n\nValue-based methods Policy-based and actor-critic methods\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.15,
                        "section_name": "Model-based deep reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_15/ec3f853b-1711-4a7f-af39-e8409a012326.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Model-based deep reinforcement learning\n\nIn chapter 7, you learned about model-based reinforcement learning methods such as\nDyna-Q and trajectory sampling. Model-based deep reinforcement learning is, at its core,\nwhat you'd expect; the use of deep learning techniques for learning the transition, the reward\nfunction, or both, and then using that for decision making. As with the methods you learned\nabout in chapter 7, one of the significant advantages of model-based deep reinforcement\nlearning is sample efficiency; model-based methods are the most sample efficient in rein-\nforcement learning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.16,
                        "section_name": "Model-based reinforcement\nlearning algorithms to have in mind",
                        "section_path": "./screenshots-images-2/chapter_13/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_16/7fe214f2-4019-4276-849b-2c17ec0024e4.png",
                            "./screenshots-images-2/chapter_13/section_16/49d274b1-9615-4543-8c0c-d1ca55139eb4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Model-based reinforcement\nlearning algorithms to have in mind\n\n@ Some of the\n\nithms that we\nlearn about, and\nothers for the Dyna-Q\ncase Trajectory sampling\nPrioritized sweeping\nRTDP\n\n(@ Some model-\nbased deep\nreinforcement\nlearning methods\nto have in mind\n\nIn addition to sample efficiency, another inherent advantage of using model-based methods\nis transferability. Learning a model of the dynamics of the world can help you achieve differ-\nent related tasks. For instance, if you train an agent to control a robotic arm to reach an\nobject, a model-based agent that learns how the environment reacts to the agent\u2019s attempts to\nmove toward the object might more easily learn to pick up that object in a later task. Notice\nthat, in this case, learning a model of the reward function isn\u2019t useful for transfer. However,\nlearning how the environment reacts to its motion commands is transferable knowledge that\ncan allow the accomplishment of other tasks. Last time I checked, the laws of physics hadn\u2019t\n\nbeen updated for hundreds of years\u2014talk about a slow-moving field!\n\nA couple of other pluses worth mentioning follow. First, learning a model is often a\nsupervised-learning task, which is much more stable and well-behaved than reinforcement\nlearning. Second, if we have an accurate model of the environment, we can use theoretically\ngrounded algorithms for planning, such as trajectory optimization, model-predictive con-\ntrol, or even heuristic search algorithms, such as Monte Carlo tree search. Last, by learning a\nmodel, we make better use of experiences overall because we extract the most information\nfrom the environment, which means more possibilities for better decisions.\n\nBut it isn\u2019t all roses; model-based learning is also challenging. There are a few disadvantages\nto have in mind when using model-based methods. First, learning a model of the dynamics\nof an environment, in addition to a policy, a value function, or both, is more computationally\nexpensive. And if you were to learn only a model of the dynamics, then the compounding of\nmodel error from the model would make your algorithm impractical.\n\nNot all aspects of the dynamics are directly beneficial to the policy. We covered this issue\nwhen arguing for learning a policy directly instead of learning a value function. Imagine a\npouring task; if you need first to learn fluid dynamics, the viscosity of fluids, and fluid flow\nwhen you only want to pick up a cup and pour, then we\u2019re overcomplicating the task. Trying\nto learn a model of the environment is more complicated than learning the policy directly.\n\nIt\u2019s essential to recall that deep learning models are data hungry. As you know, to get the\nbest out of a deep neural network, you need lots of data, and this is a challenge for model-\nbased deep reinforcement learning methods. The problem compounds with the fact that it\u2019s\nalso hard to estimate model uncertainty in neural networks. And so, given that a neural net-\nwork tries to generalize, regardless of model uncertainty, you can end up with long-term\npredictions that are total garbage.\n\nThis issue makes the argument that model-based methods are the most sample efficient\nquestionable because you may end up needing more data to learn a useful model than the\ndata you need for learning a good policy under model-free methods. However, if you have\nthat model, or acquire the model independently of the task, then you can reuse that model for\nother tasks. Additionally, if you were to use \u201cshallow\u201d models, such as Gaussian processes, or\nGaussian mixture models, then we\u2019re back at square one, having model-based methods as the\nmost sample efficient.\n\nI'd like you to move from this section, knowing that it isn\u2019t about model-based versus\nmodel-free. And even though you can combine model-based and model-free methods and\nget attractive solutions, at the end of the day, engineering isn\u2019t about that either, the same way\nthat it isn\u2019t a matter of value-based versus policy-based, and it also isn't actor-critic. You\ndon\u2019t want to use a hammer when you need a screwdriver. My job is to describe what each\ntype of algorithm is suitable for, but it\u2019s up to you to use that knowledge the right way. Of\ncourse, explore, have fun, that matters, but, when it\u2019s time to solve a problem, pick wisely.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.17,
                        "section_name": "Derivative-free optimization methods",
                        "section_path": "./screenshots-images-2/chapter_13/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_17/bc24de85-6088-405c-bdf0-75375c6abc99.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Derivative-free optimization methods\n\nDeep learning is the use of multi-layered function approximators to learn a function. A tra-\nditional deep learning use case goes as follows. First, we create a parametric model that mir-\nrors a function of interest. Then, we define an objective function to know how wrong the\nmodel is at any given time. Next, we iteratively optimize the model by calculating where to\nmove the parameters, using backpropagation. And finally, we update the parameters, using\ngradient descent.\n\nBackpropagation and gradient descent are practical algorithms for optimizing neural\nnetworks. These methods are valuable for finding the lowest or highest point of a function\nwithin a given range; for instance, a local optimum of the loss or objective function. But,\ninterestingly, they aren\u2019t the only way of optimizing parametric models, such as deep neural\nnetworks, and, more importantly, they aren\u2019t always the most effective.\n\nDerivative-free optimization, such as genetic algorithms or evolution strategies, is a differ-\nent model-optimization technique that has gotten attention from the deep reinforcement\nlearning community in recent years. Derivative-free methods, which are also known as\ngradient-free, black-box, and zeroth-order methods, don\u2019t require derivatives and can be\nuseful in situations in which gradient-based optimization methods suffer. Gradient-based\noptimization methods suffer when optimizing discrete, discontinuous, or multi-model func-\ntions, for instance.\n\nDerivative-free methods can be useful and straightforward in many cases. Even randomly\nperturbing the weights of a neural network, if given enough compute, can get the job done.\nThe main advantage of derivative-free methods is that they can optimize an arbitrary\nfunction. They don\u2019t need gradients to work. Another advantage is that these methods are\nstraightforward to parallelize. It\u2019s not uncommon to hear hundreds or thousands of CPUs\nused with derivative-free methods. On the flip side, it\u2019s good that they\u2019re easy to parallelize\nbecause they\u2019re sample inefficient. Being black-box optimization methods, they don\u2019t exploit\nthe structure of the reinforcement learning problem. They ignore the sequential nature of\nreinforcement learning problems, which can otherwise give valuable information to optimi-\nzation methods.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.18,
                        "section_name": "Derivative-free optimization methods",
                        "section_path": "./screenshots-images-2/chapter_13/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_18/a5ccc9d2-c0d0-4f1c-b2a9-07e8045746e7.png",
                            "./screenshots-images-2/chapter_13/section_18/673572ad-6cbe-405a-85da-6b7e3cbde15b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Derivative-free methods are an extreme case\n\nPolicy-based Value-based Model-based\n\n\n(DP) Micuet\u2019s Anatocy\n@\n\nHow derivative-free methods work\n\nTo get an intuitive sense of how gradient-based and gradient-free methods compare, imag-\nine for a second the game of Hot and Cold. Yeah, that game kids play in which one kid, the\nhunter, is supposed to find a hidden object of interest, while the rest of the kids, who know\nwhere the object is, yell \u201ccold\u201d if the hunter is far from the object, and \u201chot\u201d if the hunter is\nclose to it. In this analogy, the location of the hunter is the parameters of the neural network.\nThe hidden object of interest is the global optimum, which is either the lowest value of the\nloss function to minimize or the highest value of the objective function to maximize. The\nintent is to optimize the distance between the hunter and the object. For this, in the game,\nyou use the kids yelling \u201ccold\u201d or \u201chot\u201d to optimize the hunter's position.\n\nHere's when this analogy gets interesting. Imagine you have kids who, in addition to yell-\ning \u201ccold\u201d or \u201chot,\u201d get louder as the hunter gets closer to the object. You know, kids get\nexcited too quickly and can\u2019t keep a secret. As you hear them go from saying \u201ccold\u201d softly to\ngetting louder every second, you know, as the hunter, you're walking in the right direction.\nThe distance can be minimized using that \u201cgradient\u201d information. The use of this information\nfor getting to the object of interest is what gradient-based methods do. If the information\ncomes in a continuous form, meaning the kids yell a couple of times per second, and get\nlouder or softer and go from yelling \u201ccold\u201d to yelling \u201chot\u201d with distance, then you can use the\nincrease or decrease in the magnitude of the information, which is the gradient, to get to the\nobject. Great!\n\nOn the other hand, imagine the kids yelling the information are mean, or maybe just not\nperfect. Imagine they give discontinuous information. For instance, they may not be allowed\nto say anything while the hunter is in certain areas. They go from \u201csoftly cold\u201d to nothing for a\nwhile to \u201csoftly cold\u201d again. Or perhaps, imagine the object is right behind the middle of a\nlong wall. Even if the hunter gets close to the object, the hunter won't be able to reach the\nobject with gradient information. The hunter would be close to the object, so the right thing\nto yell is\u201chot,\u201d but in reality, the object is out of reach behind a wall. In all these cases, perhaps\na gradient-based optimization approach isn\u2019t the best strategy, and gradient-free methods,\neven if moving randomly, may be better for finding the object.\n\nA gradient-free method approach could be as simple as that. The hunter would pick a\nrandom place to go and ignore \u201cgradient\u201d information while getting there, then check with\nthe yelling kids, and then try another random position. After getting an idea of a few random\npositions, say, 10, the hunter would take the top 3 and try random variations from those 3\nthat are apparently better locations. In this case, gradient information isn\u2019t useful.\n\nTrust me, this analogy can keep going, but I'm going to stop it there. The bottom line is\ngradient-based and gradient-free methods are only strategies for reaching a point of interest.\nThe effectiveness of these strategies depends on the problem at hand.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.19,
                        "section_name": "More advanced concepts toward AGI",
                        "section_path": "./screenshots-images-2/chapter_13/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_19/c74537d9-b238-4f69-9873-737e4f9f38c7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "More advanced concepts toward AGI\n\nIn the previous section, we reviewed the foundational concepts of deep reinforcement learn-\ning that are covered in this book and touched on the two essential types of methods that we\ndidn\u2019t cover in depth. But, as I mentioned before, there are still many advanced concepts that,\neven though not required for an introduction to deep reinforcement learning, are crucial\nfor devising artificial general intelligence (AGI), which is the ultimate goal for most AI\nresearchers.\n\nIn this section, we start by going one step deeper into AGI and argue for some of the traits\nAl agents need to tackle tasks requiring more-general intelligence. I explain at a high level\nwhat these traits are and their intent so that you can continue your journey studying AI and\nperhaps one day contribute to one of these cutting-edge research fields.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.2,
                        "section_name": "What is AGI, again?",
                        "section_path": "./screenshots-images-2/chapter_13/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_20/bb23c43b-b84d-45f4-9bbd-6f8678730cc9.png",
                            "./screenshots-images-2/chapter_13/section_20/c4469cc8-e049-4dae-8b8f-85a77769a953.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is AGI, again?\n\nIn this book, you\u2019ve seen many examples of AI agents that seem impressive at first sight. The\nfact that the same computer program can learn to solve a wide variety of tasks is remarkable.\nMoreover, after you move on to more complex environments, it\u2019s easy to get carried away\nby these results: AlphaZero learns to play chess, Go, and Shogi. OpenAl Five defeats human\nteams at the game of Dota2. AlphaStar beats a top professional player at the game of\nStarCraft II. These are compelling general-purpose algorithms. But do these general-purpose\nalgorithms show any sign of general intelligence? First of all, what is general intelligence?\n\nGeneral intelligence is the ability to combine various cognitive abilities to solve new prob-\nlems. For artificial general intelligence (AGI), we then expect a computer program to show\ngeneral intelligence. Okay. Now, let\u2019s ask the following question: are any of the algorithms\npresented in this book, or even state-of-the-art methods such as AlphaZero, OpenAl Five,\nand AlphaStar, examples of artificial general intelligence? Well, it\u2019s not clear, but I'd say no.\n\nYou see, on the one hand, many of these algorithms can use \u201cmultiple cognitive abilities,\u201d\nincluding perception and learning for solving a new task, say, playing Pong. If we stick to our\ndefinition, the fact that the algorithm uses multiple cognitive abilities to solve new problems\nis a plus. However, one of the most dissatisfying parts of these algorithms is that none of these\ntrained agents are good at solving new problems unless you train them, which most of the\ntime requires millions of samples before you get any impressive results. In other words, if you\ntrain a DQN agent to play Pong from pixels, that trained agent, which can be at superhuman\nlevel at Pong, has no clue about how to play a decent game of Breakout and has to train for\nmillions of frames before it shows any skill.\n\nHumans don\u2019t have this problem. If you learn to play Pong, I\u2019m pretty sure you can pick up\nBreakout in like two seconds. Both games have the same task of hitting a ball with a paddle. On\nthe other hand, even the AlphaZero agent, a computer program with the most impressive skills\nof all time at multiple fundamentally different board games, and that can beat professional\nplayers who dedicate their lives at these games, but will never do your laundry.\n\nCertain AI researchers say their goal is to create Al systems that perceive, learn, think, and\neven feel emotions like humans do. Machines that learn, think, feel, and perhaps even look\nlike people, are most definitely an exciting thought. Other researchers have a more practical\napproach; they don\u2019t necessarily want an AI that thinks like humans unless thinking like a\nhuman is a requirement for making a good lunch. And perhaps emotions are what make a\ngreat cook, who knows. The point is that while some folks want AGI to delegate, to stop doing\nmundane tasks, other folks have a more philosophical goal. Creating AGI could be a path to\nunderstanding intelligence itself, to understanding the self, and that on its own would be a\nremarkable accomplishment for humanity.\n\nEither way, every AI researcher would agree that, regardless of the end goal, we still need\nAl algorithms that display more general and transferable skills. There are many traits that Al\nsystems likely require before they can do more human-like tasks, such as doing the laundry,\ncooking lunch, or washing the dishes. Interestingly, it\u2019s those mundane tasks that are the\nmost difficult to solve for AI. Let\u2019s review several of the research areas that are currently push-\ning the frontier on making deep reinforcement learning and artificial intelligence show signs\nof general intelligence.\n\nThe following sections introduce several of the concepts that you may want to explore\nfurther as you keep learning advanced deep reinforcement learning techniques that get the\nfield of AI closer to human-level intelligence. I spend only a few sentences so that you\u2019re\naware of as many as possible. I intend to show you the door, not what\u2019s inside it. It\u2019s for you\nto decide which door to open.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.21,
                        "section_name": "Workforce revolutions",
                        "section_path": "./screenshots-images-2/chapter_13/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_21/b5c291da-f629-4c5b-a2e9-590d89a9e0e5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Workforce revolutions\n\n@ many great Al researchers believe\nwe're within 20-40 years From AGI.\n\nHow would the world look then?\nMechanical\nengine\n- Personal Artificial\nElectricit\ny computer general\n\n| | meters\n\ntt\n\n1750 1800 1850 1900 1950 2000 2050\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.22,
                        "section_name": "Advanced exploration strategies",
                        "section_path": "./screenshots-images-2/chapter_13/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_22/10a31ce8-39e0-47fe-ae2e-09dccbb1e812.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Advanced exploration strategies\n\nOne area of research that\u2019s showing exciting results has to do with the reward function.\nThroughout this book, you\u2019ve seen agents that learn from the reward signal, but interestingly,\nthere\u2019s recent research that shows agents can learn without any reward at all. Learning from\nthings other than rewards is an exciting thought, and it may be essential for developing\nhuman-like intelligence. If you observe a baby learn, there\u2019s much unsupervised and self-\nsupervised learning going on. Sure, at one point in their lives, we reward our children. You\nknow you get an A, you get B; your salary is x, yours is y. But agents aren\u2019t always after the\nrewards we put along their way. What is the reward function of life? Is it career success? Is it\nto have children? It\u2019s not clear.\n\nNow, removing the reward function from the reinforcement learning problem can be a bit\nscary. If we\u2019re not defining the reward function for the agent to maximize, how do we make\nsure their goals align with ours? How do we make artificial general intelligence that\u2019s suitable\nfor the goals of humankind? Maybe it\u2019s the case that, to create human-like intelligence, we\nneed to give agents the freedom to choose their destiny. Either way, to me, this is one of the\ncritical research areas to pursue.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.23,
                        "section_name": "Inverse reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_23/45800785-8710-489e-b4e1-adfbe9ee6d49.png",
                            "./screenshots-images-2/chapter_13/section_23/f7a53a25-9f40-446c-8fc5-0e795abbe9c4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Inverse reinforcement learning\n\nThere are other ways to learn behavior without a reward function, and even though we often\nprefer a reward function, learning to imitate a human first can help learn policies with fewer\nsamples. There are a few related fields to look for here. Behavioral cloning is the application of\nsupervised learning techniques to learn a policy from demonstrations, often from a human.\nAs the name suggests, there\u2019s no reasoning going on here, merely generalization. A related\nfield, called inverse reinforcement learning, consists of inferring the reward function from\ndemonstrations. In this case, we\u2019re not merely copying the behavior, but we\u2019re learning the\nintentions of another agent. Inferring intentions can be a powerful tool for multiple goals. For\ninstance, in multi-agent reinforcement learning, for both adversarial and cooperative set-\ntings, knowing what other agents are after can be useful information. If we know what an\nagent wants to do, and what it wants to do goes against our goals, we can devise strategies for\nstopping it before it\u2019s too late.\n\nBut, inverse reinforcement learning allows agents to learn new policies. Learning the reward\nfunction from another agent, such as a human, and learning a policy from this learned reward\nfunction is a technique often referred to as apprenticeship learning. One interesting point to\nconsider when learning about inverse reinforcement learning is that the reward function is\noften more succinct than the optimal policy. Attempting to learn the reward function can\nmake sense in multiple cases. Techniques that learn policies from demonstrations are also\ncalled imitation learning, often whether a reward function is inferred before the policy or\nstraight behavioral cloning. A frequent use case for imitation learning is the initialization of\nagents to a good enough policy. For instance, if an agent has to learn from random behavior,\n\nit could take a long time before it learns a good policy. The idea is that imitating a human, even\nif suboptimal, may lead to optimal policies with fewer interactions with the environment.\nHowever, this isn\u2019t always the case, and policies pretrained with demonstrations by humans\nmay introduce unwanted bias and prevent agents from finding optimal policies.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.24,
                        "section_name": "Transfer learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_24/8a5b8c24-3534-4b42-9ca8-582d9a06a5b8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Transfer learning\n\nYou probably notice that the agent trained on an environment, in general, cannot be trans-\nferred to new environments. Reinforcement learning algorithms are general purpose in the\nsense that the same agent can be trained in different environments, but they don\u2019t have\ngeneral intelligence, and what they learn cannot be straightforwardly transferred to new\nenvironments.\n\nTransfer learning is an area of research that looks at ways of transferring knowledge from\na set of environments to a new environment. One approach, for instance, that may be intui-\ntive to you if you have a deep learning background, is what\u2019s called fine-tuning. Similar to\nreusing the weights of a pretrained network in supervised learning, agents trained in related\nenvironments can reuse the features learned by the convolution layers on a different task. If\nthe environments are related, such as Atari games, for instance, several of the features may be\ntransferable. In certain environments, even policies can be transferred.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.25,
                        "section_name": "Sim-to-real transfer learning task\nis acommon need in the real world",
                        "section_path": "./screenshots-images-2/chapter_13/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_25/8886cd4b-51e6-455c-93cc-5f29782ca781.png",
                            "./screenshots-images-2/chapter_13/section_25/fb7a3e9b-91b1-4505-9493-21cc745fefa4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Sim-to-real transfer learning task\nis a common need in the real world\n\nDomain randomization in Better generalization at\n[ , simulation at training time test time in the real world\n\n@ many people\nthink you need ff 2\nsimulation to\ntransfer an ge 7\nfrom\nreal world, but real world looks\n\nthat\u2019s not true! like another\nvariation of the\nsimulation.\n(2) What works better is to have a Flexible simulator\nso that you can randomize the parameters during\ntraining, and the agent is forced to generalize better.\n\nThe general area of research on making agents learn more general skills is called transfer\nlearning. Another frequent use of transfer learning is to transfer policies learned in simulation\nto the real world. Sim-to-real transfer learning is a common need in robotics, in which train-\ning agents controlling robots can be tricky, costly, and dangerous. It\u2019s also not as scalable as\ntraining in simulation. A common need is to train an agent in simulation and then transfer\nthe policy to the real world. A common misconception is that simulations need to be high-\nfidelity and realistic for transferring agents from simulation to the real world. There\u2019s research\nsuggesting that it\u2019s the opposite. It\u2019s the variety, the diversity of observations, that makes\nagents more transferable. Techniques such as domain randomization are at the forefront of\nthis research area and show much promise.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.26,
                        "section_name": "Multi-task learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_26/e5615b0b-8001-46db-8300-46df8dec1e6e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Multi-task learning\n\nA related area of research, called multi-task learning, looks at transfer learning from a differ-\nent perspective. In multi-task learning, the goal is to train on multiple tasks, instead of one,\nand then transfer to a new task. In this case, model-based reinforcement learning approaches\ncome to mind. In robotics, for instance, learning a variety of tasks with the same robot can\nhelp the agent learn a robust model of the dynamics of the environment. The agent learns\nabout gravity, how to move toward the right, or left, and so on. Regardless of the tasks, the\nmodel of the dynamics learned can be transferred to a new task.\n\nMulti-task learning consists of training on mul-\ntiple related tasks and testing on a new one\n\n| Multiple related Better generalization\n\n\u00a9 multi-task tasks at training time at test time\n\nlearning is the\n\ntransfer of a\n\nin multiple\n\ntasks, either 7\n\nsimultaneously\n\nor sequentially, @) The idea is\n\nto another the agent\n\ntask. should perform\n\nee wert\n\n@ In this example, | use four different end either with no\neffectors, but in reality, the task doesn\u2019t need to or some\n\nbe too different. These could be related tasks. fine-tuning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.27,
                        "section_name": "Curriculum learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_27",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_27/4f8b06e4-9858-4d51-8eaa-90740ac21134.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Curriculum learning\n\nA common use-case scenario for multi-task learning is decomposing a task into multiple\ntasks sorted by difficulty level. In this case, the agent is put through a curriculum, learning\nmore complicated tasks progressively. Curriculum learning makes sense and can be useful\nwhen developing scenarios. If you need to create an environment for an agent to solve, it\noften makes sense for you to create the most straightforward scenario with a dense reward\nfunction. By doing this, your agent can quickly show progress toward learning the goal, and\nthis validates that your environment is working well. Then, you can increase the complexity\nand make the reward function more sparse. After you do this for a handful of scenarios, you\nnaturally create a curriculum that can be used by your agent. Then, you can train your agent\nin progressively more complex environments, and hopefully, have an agent reach the desired\nbehaviors more quickly.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.28,
                        "section_name": "Meta learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_28",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_28/e5b0a5cc-eb4b-493a-88c8-a3be977b0d9c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Meta learning\n\nAnother super exciting research area is called meta learning. If you think about it, we\u2019re\nhand-coding agents to learn many different tasks. At one point, we become the bottleneck. If\nwe could develop an agent that, instead of learning to solve a challenging task, learns to learn\nitself, we could remove humans from the equation; well, not quite, but take a step in that\ndirection. Learning to learn is an exciting approach to using experiences from learning mul-\ntiple tasks to get good at learning itself. It makes intuitive sense. Other exciting research paths\ncoming out of meta learning are automatically discovering neural network architectures and\noptimization methods. Keep an eye out for these.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.29,
                        "section_name": "Hierarchical reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_29",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_29/17d86ce3-cb9c-4ce8-855c-b699466515eb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Hierarchical reinforcement learning\n\nOften, we find ourselves developing environments that have problems with multiple hori-\nzons. For instance, if we want an agent to find the best high-level strategy, but give it only\nlow-level control commands for actions, then the agent needs to learn to go from low-level to\nhigh-level action space. Intuitively, there\u2019s a hierarchy in the policies for most agents. When\nI plan, I do so on a higher-level action space. I think about going to the store, not moving my\narms to get to the store. Hierarchical reinforcement learning enables agents to create a hierar-\nchy of actions internally to tackle long-horizon problems. Agents no longer reason about\nleft-right commands, but more about go here or there.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.3,
                        "section_name": "Multi-agent reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_13/section_30",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_30/405e0b12-56e1-4bf5-bb5b-6f2ea12384dd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Multi-agent reinforcement learning\n\nThe world wouldn\u2019t be as exciting without other agents. In multi-agent reinforcement learn-\ning, we look at techniques for having agents learn when there are multiple agents around.\nOne of the main issues that arise, when learning in multi-agent settings, is that as your agent\nlearns, other agents learn too, and therefore change their behavior. The problem is that this\nchange makes the observations non-stationary, because what your agent learns is outdated\nright after the other agents learn, and so learning becomes challenging.\n\nOne exciting approach to cooperative multi-agent reinforcement learning is to use actor-\ncritic methods in which the critic uses the full state information of all agents during training.\nThe advantage here is that your agents learn to cooperate through the critic, and we can then\nuse the policy during testing using a more realistic observation space. Sharing the full state\nmay seem unrealistic, but you can think about it as similar to how teams practice. During\npractice, everything is allowed. Say that you're a soccer player, and you can tell other agents\nyou intend to run on the wing when you make this move, and so on. You get to practice\nmoves with full information during training; then, you can only use your policy with limited\ninformation during testing.\n\nAnother appealing thought when looking into multi-agent reinforcement learning is that\nhierarchical reinforcement learning can be thought of as another case of multi-agent rein-\nforcement learning. How so? Think about multiple agents deciding on different horizons.\nThe multiple-horizon structure is similar to the way most companies do business. Folks at the\ntop plan higher-level goals for the next few years and other folks decide on how to get there\non a month-to-month and a day-to-day basis. The ones at the top set the goals for those on\nthe bottom. The whole system gets rewarded for the performance of all agents.\n\nOf course, multi-agent reinforcement learning isn\u2019t only for the cooperative case but also\nfor the adversarial case, which is perhaps the most exciting. Humans often see competition\nand adversaries as something inherently unfortunate, but multi-agent reinforcement learn-\ning suggests that our adversaries often are the best way to make ourselves better. Underlying\nmany recent reinforcement learning success stories are training techniques that include\nadversaries: either a previous version of the same agent, such as in self-play, to a whole tour-\nnament-like distribution of other agents that form after all the matches\u2014only the best agents\nsurvive. Adversaries often make us better, and for better or worse, they might be needed for\noptimal behavior.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.31,
                        "section_name": "Explainable Al, safety, fairness, and ethical standards",
                        "section_path": "./screenshots-images-2/chapter_13/section_31",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_31/34755adf-8f2d-48fa-8aee-d495bfb4710a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Explainable Al, safety, fairness, and ethical standards\n\nThere are a few other critical areas of research that, even though not directly a push for\nhuman-level intelligence, are fundamental for the successful development, deployment, and\nadoption of artificial intelligence solutions.\n\nExplainable artificial intelligence is an area of research that tries to create agents that are\nmore easily understandable by humans. The motives are apparent. A court of law can inter-\nrogate any person that breaks the law; however, machine learning models aren\u2019t designed to\nbe explainable. To ensure the fast adoption of AI solutions by society, researchers must inves-\ntigate ways to ease the problem of explainability. To be clear, I don\u2019t think this is a require-\nment. I prefer having an AI give me an accurate prediction in the stock market, whether it can\nexplain to me why or not. However, neither decision is straightforward. In life-or-death deci-\nsions involving humans, things become hairy quickly.\n\nSafety is another area of research that should get more attention. It\u2019s often the case that Als\nfail catastrophically in ways that are too obvious to humans. Also, Als are vulnerable to attacks\nthat humans aren\u2019t. We need to make sure that when Als are deployed, we know how the\nsystems react to a variety of situations. Als currently don\u2019t have a way to go through classical\nvalidation and verification (V&V) of software approaches, and this poses a significant chal-\nlenge for the adoption of Al.\n\nFairness is another crucial issue. We need to start thinking about who controls Als. If a\ncompany creates an AI to maximize profits at the expense of society, then what\u2019s the point of\nAl technologies? We already have something similar going on with advertising. Top compa-\nnies use AI to maximize gains through a form of manipulation. Should these companies be\nallowed to do this for profit? How about when Als get better and better? What\u2019s the purpose\nof this, destroy a human through manipulation? These are things that need to be seriously\nconsidered.\n\nFinally, AI ethical standards are another issue that has gotten recent attention with the\nMontreal Declaration for Responsible Development of Artificial Intelligence. These are 10\nethical principles for AI that serve the interests of society, and not merely for-profit compa-\nnies. These are several of the top fields to have in mind when you're ready to contribute.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.32,
                        "section_name": "What happens next?",
                        "section_path": "./screenshots-images-2/chapter_13/section_32",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_32/9579342f-e64b-469b-af25-0fbde9cc7523.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What happens next?\n\nWhile this section marks the end of this book, it should only mark the beginning or continu-\nation of your contributions to the field of Al and DRL. My intention with this book was, not\nonly to get you understanding the basics of DRL, but also to onboard you into this fantastic\ncommunity. You don\u2019t need much other than a commitment to continue the journey. There\nare many things you could do next, and in this section, I\u2019d like to give you ideas to get you\nstarted. Have in mind that the world is a choir needing a wide variety of voice types and tal-\nents; your job is to accept the talents given to you, develop them to the best of your abilities,\nand play your part with all you've got. While I can give you ideas, it\u2019s up to you what happens\nnext; the world needs and awaits your voice.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.33,
                        "section_name": "How to use DRL to solve custom problems",
                        "section_path": "./screenshots-images-2/chapter_13/section_33",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_33/e437a345-3886-4f9e-89a7-8415c0e1e0e0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "How to use DRL to solve custom problems\n\nThere\u2019s something super cool about RL algorithms that I want you to have in mind as you\nlearn about other types of agents. The fact is that most RL agents can solve any problem that\nyou choose, as long as you can represent the problem as a correct MDP, the way we discussed\nin chapter 2. When you ask yourself, \u201cWhat can X or Y algorithm solve?\u201d the answer is the\nsame problems other algorithms can solve. While in this book, we concentrate on a handful\nof algorithms, all the agents presented can solve many other environments with some hyper-\nparameter tuning. The need for solving custom environments is something many people\nwant but could take a whole other book to get right. My recommendation is to look at some\nof the examples available online. For instance, Atari environments use an emulator called\nStella in the backend. The environments pass images for observations and actions back and\nforth between the environment and the emulator. Likewise, MuJoCo and the Bullet Physics\nsimulation engine are the backends that drive continuous-control environments. Take a look\nat the way these environments work.\n\nPay attention to how observations are passed from the simulation to the environment and\nthen to the agent. Then, the actions selected by the agent are passed to the environment and\nthen to the simulation engine. This pattern is widespread, so if you want to create a custom\nenvironment, investigate how others have done it, and then do it yourself. Do you want to\ncreate an environment for an agent to learn to invest in the stock market? Think about which\nplatforms have an API that allows you to do that. Then, you can create different environ-\nments using the same API. One environment, for instance, can buy stocks, another buys\noptions, and so on. There are so many potential applications for state-of-the-art deep rein-\nforcement learning methods that it\u2019s a shame we have a limited number of quality environ-\nments at our disposal. Contributions in this area are undoubtedly welcome. If you want to\ncreate an environment and don\u2019t find it out there, consider investing the time to create your\nown and share it with the world.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.34,
                        "section_name": "Going forward",
                        "section_path": "./screenshots-images-2/chapter_13/section_34",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_34/59a5a5cf-ac80-4098-b35b-80085572845f.png",
                            "./screenshots-images-2/chapter_13/section_34/41f165ae-56df-499a-b4af-268189ba4203.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Going forward\n\nYou have learned a lot; there\u2019s no doubt about that. But, if you look at the big picture, there\u2019s\nso much more to learn. Now, if you zoom out even further, you realize that there\u2019s even more\nto discover; things nobody has learned before. You see that what the AI community is after is\nno easy feat; we\u2019re trying to understand how the mind works.\n\nThe fact is, even other fields, such as Psychology, Philosophy, Economics, Linguistic,\nOperations Research, Control Theory, and many more, are after the same goal, each from\ntheir perspective, and using their language. But the bottom line is that all of these fields would\nbenefit from understanding how the mind works, how humans make decisions, and how to\nhelp them make optimal decisions. Here are some ideas for us moving forward.\n\nFirst, find your motivation, your ambition, and focus. Certain people find the sole desire\nto explore; to discover facts about the mind is exciting. Others want to leave a better world\nbehind. Whatever your motivation, find it. Find your drive. If you aren\u2019t used to reading\nresearch papers, you won\u2019t enjoy them unless you know your motives. As you find your moti-\nvation and drive, you must remain calm, humble, and transparent; you need your drive to\nfocus and work hard toward your goals. Don\u2019t let your excitement get in your way. You must\nlearn to keep your motivation in your heart, yet move forward. Our ability to focus is in con-\nstant jeopardy from a myriad of readily available distractions. I\u2019m guaranteed to find new\nnotifications on my cell phone every 15 minutes. And we're trained to think that this is a good\nthing. It isn\u2019t. We must get back in control of our lives and be able to concentrate long and\nhard on something that interests us, that we love. Practice focus.\n\nSecond, balance learning and contributions and give yourself time to rest. What do you\nthink would happen if, for the next 30 days, I eat 5,000 calories a day and burn 1,000? What\nif I, instead, eat 1,000 and burn 5,000? How about I\u2019m an athlete and eat and burn 5,000, but\ntrain every day of the week? Right, all of those spell trouble to the body. The same happens\nwith the mind; certain people think they need to learn for years before they can do anything,\nso they read, watch videos, but don\u2019t do anything with it. Others think they no longer need to\nread any papers; after all, they\u2019ve already implemented a DQN agent and written a blog post\nabout it. They quickly become obsolete and lack the fuel to think. Some people get those two\nright, but never include the time to relax, and enjoy their family and reflect. That\u2019s the wrong\napproach. Find a way to balance what you take and what you give, and set time to rest. We\u2019re\nathletes of the mind, too much \u201cfat\u201d in your mind, too much information that you put in\nwithout purpose, and you become sluggish and too slow. Too many blog posts without doing\nany research, and you become outdated, repetitive, and dry. Not enough rest, and you won\u2019t\nplan well enough for the long term.\n\nAlso, know that you can\u2019t learn everything. Again, we\u2019re learning about the mind, and\nthere\u2019s much information about it out there. Be wise, and be picky about what you read.\nWho\u2019s the author? What\u2019s her background? Still, you can read it but have a higher sense of\nwhat you\u2019re doing. Try to give often. You should be able to explain things that you learn\nanother way. The quote, \u201cDon\u2019t reinvent the wheel,\u201d is misleading at best. It would be best if\nyou tried things on your own; that\u2019s vital. It\u2019s inevitable that if you explore, early on, you may\nfind yourself having a great idea, that you later realize has been worked on before. There\u2019s no\nshame in that; it\u2019s more critical for you to keep moving forward than to keep yourself waiting\nfor a eureka moment that solves world hunger. I heard Rich Sutton say something along the\nlines of \u201cthe obvious to you is your biggest contribution.\u201d But if you don\u2019t allow yourself to\n\u201creinvent the wheel,\u201d you run the risk of not sharing the \u201cobvious to you,\u201d thinking it\u2019s prob-\nably not worthy. I\u2019m not saying you need to publish a paper about this new algorithm you\nthought about, Q-learning. I\u2019m asking, please, don\u2019t let the fear of doing \u201cworthless\u201d work\nstop you from experimenting. The bottom line is to keep reading, keep exploring, keep con-\ntributing, and let it all sink in. It\u2019s a cycle; it\u2019s a flow, so keep it going.\n\nThird and last, embrace the process, lose yourself on the path. Your dreams are only a way\nto get you going, but it\u2019s in the going that you live your dreams. Get deep into it; don\u2019t just\nfollow what others are doing, and follow your interests. Think critically of your ideas, exper-\niment, gather data, try to understand the results, and detach yourself from the result. Don\u2019t\nbias your experiments, discover facts. If you lose yourself for long enough, you start to spe-\ncialize, which is good. This field is so vast that being great at everything isn\u2019t possible. However,\nif you follow your interests and intuition for long enough, you automatically spend more\ntime on certain things versus other things. Keep going. Some of us feel a need to stay in the\nknow, and once we start asking questions that have no answers, we feel the need to get back\nto shore. Don\u2019t be afraid to ask hard questions, and work on getting answers. There are no\ndumb questions; each question is a clue for solving the mystery. Keep asking questions. Keep\nplaying the game, and enjoy it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.35,
                        "section_name": "Get yourself out there! Now!",
                        "section_path": "./screenshots-images-2/chapter_13/section_35",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_35/7baf3d61-944f-4e90-89fd-23d6db31579d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Get yourself out there! Now!\n\nAssuming you just finished this book, make sure to get out there right away, think about how\nto put things together, and contribute something to this amazing community. How about\nwriting a blog post about several of the algorithms not covered in this book that you find\ninteresting? How about investigating some of the advanced concepts discussed in this chapter\nand sharing what you find? Write a blog post, create a video, and let the world know about it.\nBecome part of the movement; let\u2019s find out what intelligence is, let\u2019s build intelligent sys-\ntems, together. It\u2019s never the right time unless it\u2019s now.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.36,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_13/section_36",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_36/14989cf5-1b7d-4b29-9800-8c5374d57690.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nThat\u2019s it! You did it! That\u2019s a wrap for this book. The ball is in your court.\n\nIn the first chapter, I defined deep reinforcement learning as follows: \u201cDeep reinforcement\nlearning is a machine learning approach to artificial intelligence concerned with creating com-\nputer programs that can solve problems requiring intelligence. The distinct property of DRL\nprograms is learning through trial and error from feedback that\u2019s simultaneously sequential,\nevaluative, and sampled by leveraging powerful non-linear function approximation.\u201d\n\nI mentioned that success to me was that after you complete this book, you should be able\nto come back to this definition and understand it precisely. I said that you should be able to\ntell why I used the words that I used, and what each of these words means in the context of\ndeep reinforcement learning.\n\nDid I succeed? Do you intuitively now understand this definition? Now it\u2019s your turn to\nsend your reward to the agent behind this book. Was this project a\u20141, a 0, or a +1? Whatever\nyour comments, I learn, just like DRL agents, from feedback, and I look forward to reading\nyour review and what you have to say. For now, my part is complete.\n\nIn this final chapter, we reviewed everything the book teaches, and we also discussed the\ncore methods that we skipped, and some of the advanced concepts that could play a part in\nthe eventual creation of artificial general intelligence agents.\n\nAs a parting message, I\u2019d like to first thank you for the opportunity you gave me to share\nwith you my take on the field of deep reinforcement learning. I also want to encourage you to\nkeep going, to concentrate on the day-to-day, to think more about what you can do next,\nwith your current abilities, with your unique talents.\n\nBy now, you\n\n+ Intuitively understand what deep reinforcement learning is; you know the details of\nthe most critical deep reinforcement learning methods, from the most basic and\nfoundational to the state-of-the-art.\n\n+ Have a sense of where to go next because you understand how what we\u2019ve learned\nfits into the big picture of the fields of deep reinforcement learning and artificial\nintelligence.\n\n+ Are ready to show us what you've got, your unique talents, and interests. Go, make\nthe RL community proud. Now, it\u2019s your turn!\n",
                        "extracted-code": ""
                    }
                ]
            }
        ]
    }
}