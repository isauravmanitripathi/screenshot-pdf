{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "Intuition of\nartificial intelligence",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "What is artificial intelligence?",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/e76b4cff-b7eb-4c24-ba00-210b1c6ff3f5.png",
                            "./screenshots-images-2/chapter_1/section_1/444fcc99-d84d-4c28-8d43-1430a06af747.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Intelligence is a mystery\u2014a concept that has no agreed-upon definition.\nPhilosophers, psychologists, scientists, and engineers all have different\nopinions about what it is and how it emerges. We see intelligence in nature\naround us, such as groups of living creatures working together, and we see\nintelligence in the way that humans think and behave. In general, things\nthat are autonomous yet adaptive are considered to be intelligent.\nAutonomous means that something does not need to be provided constant\ninstructions; and adaptive means that it can change its behavior as the envi-\nronment or problem space changes. When we look at living organisms and\n\nmachines, we see that the core element for operation is data. Visuals that we see are data;\nsounds that we hear are data; measurements of the things around us are data. We con-\nsume data, process it all, and make decisions based on it; so a fundamental understand-\ning of the concepts surrounding data is important for understanding artificial intelligence\n(AI) algorithms.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.2,
                        "section_name": "Defining Al",
                        "section_path": "./screenshots-images-2/chapter_1/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_2/9d060082-343c-4ee8-b58d-6b9079e4f511.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Defining Al\n\nSome people argue that we don\u2019t understand what AI is because we struggle to define\nintelligence itself. Salvador Dali believed that ambition is an attribute of intelligence; he\nsaid, \u201cIntelligence without ambition is a bird without wings.\u201d Albert Einstein believed\nthat imagination is a big factor in intelligence; he said, \u201cThe true sign of intelligence is\nnot knowledge, but imagination.\u201d And Stephen Hawking said, \u201cIntelligence is the ability\nto adapt,\u201d which focuses on being able to adapt to changes in the world. These three great\nminds had different outlooks on intelligence. With no true definitive answer to intelli-\ngence yet, we at least know that we base our understanding of intelligence on humans as\nbeing the dominant (and most intelligent) species.\n\nFor the sake of our sanity, and to stick to the practical applications in this book, we\nwill loosely define AI as a synthetic system that exhibits \u201cintelligent\u201d behavior. Instead\nof trying to define something as AI or not AI, let\u2019s refer to the Al-likeness of it. Something\nmight exhibit some aspects of intelligence because it helps us solve hard problems and\nprovides value and utility. Usually, Al implementations that simulate vision, hearing,\nand other natural senses are seen to be AI-like. Solutions that are able to learn autono-\nmously while adapting to new data and environments are also seen to be Al-likeness.\n\nHere are some examples of things that exhibit Al-ness:\n\n+ Asystem that succeeds at playing many types of complex games\n+ Acancer tumor detection system\n\n+ Asystem that generates artwork based on little input\n\n+ Aself-driving car\n\nDouglas Hofstadter said, \u201cAI is whatever hasn\u2019t been done yet.\u201d In the examples just\nmentioned, a self-driving car may seem to be intelligent because it hasn\u2019t been perfected\nyet. Similarly, a computer that adds numbers was seen to be intelligent a while ago but is\ntaken for granted now.\n\nThe bottom line is that AJ is an ambiguous term that means different things to dif-\nferent people, industries, and disciplines. The algorithms in this book have been classi-\nfied as AI algorithms in the past or present; whether they enable a specific definition of\nAI or not doesn\u2019t really matter. What matters is that they are useful for solving hard\nproblems.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.3,
                        "section_name": "Understanding that data is core to Al algorithms",
                        "section_path": "./screenshots-images-2/chapter_1/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_3/fbd9f917-fd1d-4543-bc79-d9176707ff74.png",
                            "./screenshots-images-2/chapter_1/section_3/96f90160-0599-4db0-b5bc-71f04e1c0b62.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Understanding that data is core to Al algorithms\n\nData is the input to the wonderful algorithms that perform feats that almost appear to\nbe magic. With the incorrect choice of data, poorly represented data, or missing data,\nalgorithms perform poorly, so the outcome is only as good as the data provided. The\nworld is filled with data, and that data exists in forms we can\u2019t even sense. Data can rep-\nresent values that are measured numerically, such as the current temperature in the\nArctic, the number of fish in a pond, or your current age in days. All these examples\ninvolve capturing accurate numeric values based on facts. It\u2019s difficult to misinterpret\nthis data. The temperature at a specific location at a specific point in time is absolutely\ntrue and is not subject to any bias. This type of data is known as quantitative data.\n\nData can also represent values of observations, such as the smell of a flower or one\u2019s\nlevel of agreement with a politician\u2019s policies. This type of data is known as qualitative\ndata and is sometimes difficult to interpret because it\u2019s not an absolute truth, but a per-\nception of someone\u2019s truth. Figure 1.1 illustrates some examples of the quantitative and\nqualitative data around us.\n\nThe coordinates are\n463959775, 23.5838889.\n\nThe pasta tastes\ncreamy.\n\nThe temperature is\n24 degrees Celsius.\n\n+\n\u20ac, The flower smells\nsweet.\n\nFigure 1.1 Examples of data around us\n\nData is raw facts about things, so recordings of it usually have no bias. In the real world,\nhowever, data is collected, recorded, and related by people based on a specific context\nwith a specific understanding of how the data may be used. The act of constructing\nmeaningful insights to answer questions based on data is creating information.\nFurthermore, the act of utilizing information with experiences and consciously applying\nit creates knowledge. This is partly what we try to simulate with AI algorithms.\n\nFigure 1.2 shows how quantitative and qualitative data can be interpreted. Standard-\nized instruments such as clocks, calculators, and scales are usually used to measure\n\nquantitative data, whereas our senses of smell, sound, taste, touch, and sight, as well as\nour opinionated thoughts, are usually used to create qualitative data.\n\nQuantitative Qualitative\n\nInstruments\n\n- Creamy texture\n\n~ 358 ml velume cup - Streng taste with\n\nahint of checotate\n\n- 91\u00b0C in temperature.\n\n- 226 grams in welght Cotfee io golden\n\n~ Porcelain cup brewn in color\n\n~ Beans from Africa\n\nCup Is white in color\n\n= Smells rich\n\nCappuccine example\n\nFigure 1.2 Qualitative data versus quantitative data\n\nData, information, and knowledge can be interpreted differently by different people,\nbased on their level of understanding of that domain and their outlook on the world, and\nthis fact has consequences for the quality of solutions\u2014making the scientific aspect of\ncreating technology hugely important. By following repeatable scientific processes to cap-\nture data, conduct experiments, and accurately report findings, we can ensure more accu-\nrate results and better solutions to problems when processing data with algorithms.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.4,
                        "section_name": "Viewing algorithms as instructions in recipes",
                        "section_path": "./screenshots-images-2/chapter_1/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_4/258ea3c1-1e01-42fe-9750-822b89cf183f.png",
                            "./screenshots-images-2/chapter_1/section_4/14a6b604-c54e-41a3-a968-74b88866c5be.png",
                            "./screenshots-images-2/chapter_1/section_4/46f9a57d-8f99-45dc-bd16-99d5e6a83ffe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Viewing algorithms as instructions in recipes\n\nWe now have a loose definition of AI and an understanding of the importance of data.\nBecause we will be exploring several AI algorithms throughout this book, it is useful to\nunderstand exactly what an algorithm is. An algorithm is a set of instructions and rules\nprovided as a specification to accomplish a specific goal. Algorithms typically accept\ninputs, and after several finite steps in which the algorithm progresses through varying\nstates, an output is produced.\n\nEven something as simple as reading a book can be represented as an algorithm. Here\u2019s\nan example of the steps involved in reading this book:\n\n1, Find the book Grokking Artificial Intelligence Algorithms.\n2. Open the book.\n\n3. While unread pages remain,\na. Read page.\nb. Turn to next page.\nc. Think about what you have learned.\n4. Think about how you can apply your learnings in the real world.\n\nAn algorithm can be viewed as a recipe, as seen in figure 1.3. Given some ingredients and\ntools as inputs, and instructions for creating a specific dish, a meal is the output.\n\nPita bread algorithm\n\n20mi\n\n( olive oll Fg yeast VA 500g flour\n\n+\n\ni S)\n\nCover the deugh Ss Bake in even on 1\nfor 20 minutes. high f S\n\nFinctes. ww\n\nFigure 1.3 An example showing that an algorithm is like a recipe\n\nAlgorithms are used for many different solutions. For example, we can enable live video\nchat across the world through compression algorithms, and we can navigate cities\nthrough map applications that use real-time routing algorithms. Even a simple \u201cHello\nWorld\u201d program has many algorithms at play to translate the human-readable program-\nming language into machine code and execute the instructions on the hardware. You\ncan find algorithms everywhere if you look closely enough.\n\nTo illustrate something more closely related to the algorithms in this book, figure 1.4\nshows a number-guessing-game algorithm represented as a flow chart. The computer\ngenerates a random number in a given range, and the player attempts to guess that num-\nber. Notice that the algorithm has discrete steps that perform an action or determine a\ndecision before moving to the next operation.\n\nlet\n\nr=\nrandem\n\n(min, max)\n\nIs\nrequalg\n2\n\nreturn\n\u201cWrong\u201d\n\nreturn\n\u201cWinner\u201d\n\nFigure 1.4 A number-guessing-game algorithm flow chart\n\nGiven our understanding of technology, data, intelligence, and algorithms: AI algo-\nrithms are sets of instructions that use data to create systems that exhibit intelligent\nbehavior and solve hard problems.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.5,
                        "section_name": "A brief history of artificial intelligence",
                        "section_path": "./screenshots-images-2/chapter_1/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_5/3764a7be-769f-478c-9915-117af6e2a3f6.png",
                            "./screenshots-images-2/chapter_1/section_5/afa23187-2e92-4132-9dd7-e0163aaf9d5d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A brief history of artificial intelligence\n\nA brief look back at the strides made in AI is useful for understanding that old tech-\nniques and new ideas can be harnessed to solve problems in innovative ways. Al is not a\nnew idea. History is filled with myths of mechanical men and autonomous \u201cthinking\u201d\nmachines. Looking back, we find that we\u2019re standing on the shoulders of giants. Perhaps\nwe ourselves can contribute to the pool of knowledge in a small way.\n\nLooking at past developments highlights the importance of understanding the funda-\nmentals of AJ; algorithms from decades ago are critical in many modern AI implemen-\ntations. This book starts with fundamental algorithms that help build the intuition of\nproblem-solving and gradually moves to more interesting and modern approaches.\n\nFigure 1.5 isn\u2019t an exhaustive list of achievements in AI\u2014it is simply a small set of\nexamples. History is filled with many more breakthroughs!\n\n1950s\n\n- The term \u201cartificial intelligence\u201d is coined.\n\n- Concept of artificial neural network is\nintreduced.\n\n- Model of the Perceptronis invented\n\n- LISP programming language is invented.\n\nAl\n\nCoined\n\n1960s\n\n- ML medels for prediction are introduced.\n- Unimate robot works on a car assembly line.\n~ Shakey the robot has natural movement\nand preblem-solving abilities.\n- Paper highlighting the flaws ef Perceptrons\ncreates deudt about the concept.\n\n1970s\n- BKG wins at backgammon (with luck).\n~ Evolutionary algorithms are pepularized.\n- Freddy the rebot is able to use\nvisual perception.\n~ Proleg programming language is invented.\n\n1980s\n\n- LISP machines for expert systems.\n\n- Hope for neural networks via the\nintreduction ef backprepaga tion.\n\n- Swarm intelligence is pepularized.\n\n1990s\n\n- TD-Gammen shews the power of\nreinforcement learning.\n\n- Experiments with autenemeus cars.\n\n~ IBM\u2019s Deep Blue becomes a chess champien.\n\n- Rise of internet bots and search.\n\n2000s\n\n- Game of checkers is solved.\n\n- Face recognition with neural networks.\n\n- IBM\u2019s Watson wins at Jeopardy.\n\n- XBox Kinect\u2019s advanced motion detection.\n- Smart voice assistants by tech giants.\n\n- Google\u2019s Alpha Go becomes a Go champion.\n- Al-specific hardware and leT devices.\n\n~ Tumor detection better than decters.\n\n- Self-driving cars.\n\nFigure 1.5 The evolution of Al\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.6,
                        "section_name": "Problem types and problem-solving\nparadigms",
                        "section_path": "./screenshots-images-2/chapter_1/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_6/b3168c53-73a4-4191-969a-f29a3d733d0e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Al algorithms are powerful, but they are not silver bullets that can solve any problem.\nBut what are problems? This section looks at different types of problems that we usually\nexperience in computer science, showing how we can start gaining intuition about them.\nThis intuition can help us identify these problems in the real world and guide the choice\nof algorithms used in the solution.\n\nSeveral terms in computer science and Al are used to describe problems. Problems are\nclassified based on the context and the goal.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.7,
                        "section_name": "Search problems: Find a path to a solution",
                        "section_path": "./screenshots-images-2/chapter_1/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_7/e0c94c1b-dc58-4920-b0fd-e3d33977a63c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Search problems: Find a path to a solution\n\nA search problem involves a situation that has multiple possible solutions, each of which\nrepresents a sequence of steps (path) toward a goal. Some solutions contain overlapping\nsubsets of paths; some are better than others; and some are cheaper to achieve than oth-\ners. A \u201cbetter\u201d solution is determined by the specific problem at hand; a \u201ccheaper\u201d solu-\ntion means computationally cheaper to execute. An example is determining the shortest\npath between cities on a map. Many routes may be available, with different distances and\ntraffic conditions, but some routes are better than others. Many AI algorithms are based\non the concept of searching a solution space.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.8,
                        "section_name": "Optimization problems: Find a good solution",
                        "section_path": "./screenshots-images-2/chapter_1/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_8/1fe1ca77-780c-4851-bd91-f5a4d4618cdf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optimization problems: Find a good solution\n\nAn optimization problem involves a situation in which there are a vast number of valid\nsolutions and the absolute-best solution is difficult to find. Optimization problems usu-\nally have an enormous number of possibilities, each of which differs in how well it solves\nthe problem. An example is packing luggage in the trunk of a car in such a way as to\nmaximize the use of space. Many combinations are available, and if the trunk is packed\neffectively, more luggage can fit in it.\n\nLocal best versus global best\n\nBecause optimization problems have many solutions, and because these solutions\nexist at different points in the search space, the concept of local bests and global\nbests comes into play. A local best solution is the best solution within a specific area\nin the search space, and a global best is the best solution in the entire search space.\n\nUsually, there are many local best solutions and one global best solution. Consider\nsearching for the best restaurant, for example. You may find the best restaurant in\nyour local area, but it may not necessarily be the best restaurant in the country or\nthe best restaurant in the world.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.9,
                        "section_name": "Prediction and classification problems: Learn from patterns in data",
                        "section_path": "./screenshots-images-2/chapter_1/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_9/c55facfa-1747-442f-8758-57e428a489c7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Prediction and classification problems: Learn from patterns in data\n\nPrediction problems are problems in which we have data about something and want to try\nto find patterns. For example, we might have data about different vehicles and their\nengine sizes, as well as each vehicle\u2019s fuel consumption. Can we predict the fuel con-\nsumption of a new model of vehicle, given its engine size? If there\u2019s a correlation in the\ndata between engine sizes and fuel consumption, this prediction is possible.\n\nClassification problems are similar to prediction problems, but instead of trying to\nfind an exact prediction such as fuel consumption, we try to find a category of some-\nthing based on its features. Given the dimensions of a vehicle, its engine size, and the\nnumber of seats, can we predict whether that vehicle is a motorcycle, sedan, or sport-\nutility vehicle? Classification problems require finding patterns in the data that group\nexamples into categories. Interpolation is an important concept when finding patterns\nin data because we estimate new data points based on the known data.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.1,
                        "section_name": "Clustering problems: Identify patterns in data",
                        "section_path": "./screenshots-images-2/chapter_1/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_10/50804e00-fb08-4671-b989-2b91bfa81844.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Clustering problems: Identify patterns in data\n\nClustering problems include scenarios in which trends and relationships are uncovered\nfrom data. Different aspects of the data are used to group examples in different ways.\nGiven cost and location data about restaurants, for example, we may find that younger\npeople tend to frequent locations where the food is cheaper.\n\nClustering aims to find relationships in data even when a precise question is not being\nasked. This approach is also useful for gaining a better understanding of data to inform\nwhat you might be able to do with it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.11,
                        "section_name": "Deterministic models: Same result each time it\u2019s calculated",
                        "section_path": "./screenshots-images-2/chapter_1/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_11/a9b576f6-ea40-4ab2-9ffb-df9bbc7769e2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deterministic models: Same result each time it\u2019s calculated\n\nDeterministic models are models that, given a specific input, return a consistent output.\nGiven the time as noon in a specific city, for example, we can always expect there to be\ndaylight; and given the time as midnight, we can always expect darkness. Obviously this\nsimple example doesn\u2019t take into account the unusual daylight durations near the poles\nof the planet.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.12,
                        "section_name": "Stochastic/probabilistic models:\nPotentially different result each time it\u2019s calculated",
                        "section_path": "./screenshots-images-2/chapter_1/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_12/6d90c45c-ba99-4db3-9493-25610e85d02e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stochastic/probabilistic models:\nPotentially different result each time it\u2019s calculated\n\nProbabilistic models are models that, given a specific input, return an outcome from a set\nof possible outcomes. Probabilistic models usually have an element of controlled ran-\ndomness that contributes to the possible set of outcomes. Given the time as noon, for\nexample, we can expect the weather to be sunny, cloudy, or rainy; there is no fixed\nweather at this time.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.13,
                        "section_name": "Intuition of artificial intelligence concepts",
                        "section_path": "./screenshots-images-2/chapter_1/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_13/ebd84ebf-48c4-4740-9a2b-2c98732be2b6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Intuition of artificial intelligence concepts\n\nAl is a hot topic, as are machine learning and deep learning. Trying to make sense of\nthese different but similar concepts can be a daunting experience. Additionally, within\nthe domain of AI, distinctions exist among different levels of intelligence.\n\nIn this section, we demystify some of these concepts. The section is also a road map of\nthe topics covered throughout this book.\n\nLet\u2019s dive into the different levels of AI, introduced with figure 1.6.\n\nSuper intelligence\n\nUnknown\n\nGeneral intelligence\n\nHumans\n\nNarrow intelligence Pe\nPeng-playing program\n\nMap-reuting program\nFraud-detection program\n\nFigure 1.6 Levels of Al\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.14,
                        "section_name": "Narrow intelligence: Specific-purpose solutions",
                        "section_path": "./screenshots-images-2/chapter_1/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_14/fb425d08-b938-4f14-a643-8851a2f4e729.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Narrow intelligence: Specific-purpose solutions\n\nNarrow intelligence systems solve problems in a specific context or domain. These sys-\ntems usually cannot solve a problem in one context and apply that same understanding\nin another. A system developed to understand customer interactions and spending\nbehavior, for example, would not be capable of identifying cats in an image. Usually, for\nsomething to be effective in solving a problem, it needs to be quite specialized in the\ndomain of the problem, which makes it difficult to adapt to other problems.\n\nDifferent narrow intelligence systems can be combined in sensible ways to create\nsomething greater that seems to be more general in its intelligence. An example is a voice\nassistant. This system can understand natural language, which alone is a narrow problem,\nbut through integration with other narrow intelligence systems, such as web searches\nand music recommenders, it can exhibit qualities of general intelligence.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.15,
                        "section_name": "General intelligence: Humanlike solutions",
                        "section_path": "./screenshots-images-2/chapter_1/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_15/69a72ce2-ac97-4f3b-aa9b-8a32048fa13a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "General intelligence: Humanlike solutions\n\nGeneral intelligence is humanlike intelligence. As humans, we are able to learn from var-\nious experiences and interactions in the world and apply that understanding from one\nproblem to another. If you felt pain when touching something hot as a child, for exam-\nple, you can extrapolate and know that other things that are hot may have a chance of\nhurting you. General intelligence in humans, however, is more than just reasoning\nsomething like \u201cHot things may be harmful.\u201d General intelligence encompasses mem-\nory, spatial reasoning through visual inputs, use of knowledge, and more. Achieving\ngeneral intelligence in a machine seems to be an unlikely feat in the short term, but\nadvancements in quantum computing, data processing, and AI algorithms could make\nita reality in the future.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.16,
                        "section_name": "Super intelligence: The great unknown",
                        "section_path": "./screenshots-images-2/chapter_1/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_16/ee24a8f4-3e37-46a4-ac20-f98aaa53ca02.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Super intelligence: The great unknown\n\nSome ideas of super intelligence appear in science-fiction movies set in postapocalyptic\nworlds, in which all machines are connected, are able to reason about things beyond our\nunderstanding, and dominate humans. There are many philosophical differences about\nwhether humans could create something more intelligent than ourselves and, if we\ncould, whether we'd even know. Super intelligence is the great unknown, and for a long\ntime, any definitions will be speculation.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.17,
                        "section_name": "Old Al and new Al",
                        "section_path": "./screenshots-images-2/chapter_1/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_17/bc9c8ff8-2cf4-4864-97c5-664266eb811f.png",
                            "./screenshots-images-2/chapter_1/section_17/1c0fe351-188c-420c-9a55-a042a3b4b669.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Old Al and new Al\n\nSometimes, the notions of old AI and new AI are used. Old AI is often understood as\nbeing systems in which people encoded the rules that cause an algorithm to exhibit intel-\nligent behavior\u2014via in-depth knowledge of the problem or by trial and error. An exam-\nple of old AI is a person manually creating a decision tree and the rules and options in\nthe entire decision tree. New AI aims to create algorithms and models that learn from\ndata and create their own rules that perform as accurately as, or better than, human-\ncreated rules. The difference is that the latter may find important patterns in the data\nthat a person may never find or that would take a person much longer to find. Search\nalgorithms are often seen as old AI, but a robust understanding of them is useful in\nlearning more complex approaches. This book aims to introduce the most popular AI\nalgorithms and gradually build on each concept. Figure 1.7 illustrates the relationship\nbetween some of the different concepts within artificial intelligence.\n\nArtificial\nintelligence\n\nMachine\nlearning\n\nlearning\n\nFigure 1.7 Categorization of concepts within Al\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.18,
                        "section_name": "Search algorithms",
                        "section_path": "./screenshots-images-2/chapter_1/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_18/8a5f6b95-023b-4b14-b429-53c379dab243.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Search algorithms\n\nSearch algorithms are useful for solving problems in which several actions are required to\nachieve a goal, such as finding a path through a maze or determining the best move to\nmake in a game. Search algorithms evaluate future states and attempt to find the opti-\nmal path to the most valuable goal. Typically, we have too many possible solutions to\nbrute-force each one. Even small search spaces could result in thousands of hours of\ncomputing to find the best solution. Search algorithms provide smart ways to evaluate\nthe search space. Search algorithms are used in online search engines, map routing\napplications, and even game-playing agents.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.19,
                        "section_name": "Biology-inspired algorithms",
                        "section_path": "./screenshots-images-2/chapter_1/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_19/b1903a50-e2b3-4d5e-8715-e2868e91f963.png",
                            "./screenshots-images-2/chapter_1/section_19/4526d249-ae5e-4fb8-87af-736f98de6b19.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Biology-inspired algorithms\nWhen we look at the world around us, we notice incredible things in various creatures,\nplants, and other living organisms. Examples include the cooperation of ants in gather-\ning food, the flocking of birds when migrating, estimating how brains work, and the\nevolution of different organisms to produce stronger offspring. By observing and learn-\ning from various phenomena, we've gained knowledge of how these organic systems\noperate and of how simple rules can result in emergent intelligent behavior. Some of\nthese phenomena have inspired algorithms that are useful in AI, such as evolutionary\nalgorithms and swarm intelligence algorithms.\n\nEvolutionary algorithms are inspired by the theory of evolution defined by Charles\nDarwin. The concept is that a population reproduces to create new individuals and that\nthrough this process, the mixture of genes and mutation produces individuals that\n\nperform better than their ancestors. Swarm intelligence is a group of seemingly \u201cdumb\u201d\nindividuals exhibiting intelligent behavior. Ant-colony optimization and particle-swarm\noptimization are two popular algorithms that we will be exploring in this book.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.2,
                        "section_name": "Machine learning algorithms",
                        "section_path": "./screenshots-images-2/chapter_1/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_20/8eaa982d-2682-42e5-b95b-86c56b6816de.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Machine learning algorithms\n\nMachine learning takes a statistical approach to training models to learn from data. The\numbrella of machine learning has a variety of algorithms that can be harnessed to\nimprove understanding of relationships in data, to make decisions, and to make predic-\ntions based on that data.\n\nThere are three main approaches in machine learning:\n\n+ Supervised learning means training models with algorithms when the training\ndata has known outcomes for a question being asked, such as determining the\ntype of fruit if we have a set of data that includes the weight, color, texture, and\nfruit label for each example.\n\n+ Unsupervised learning uncovers hidden relationships and structures within the\ndata that guide us in asking the dataset relevant questions. It may find patterns\nin properties of similar fruits and group them accordingly, which can inform the\nexact questions we want to ask the data. These core concepts and algorithms\nhelps us create a foundation for exploring advanced algorithms in the future.\n\n+ Reinforcement learning is inspired by behavioral psychology. In short, it describes\nrewarding an individual if a useful action was performed and penalizing that\nindividual if an unfavorable action was performed. To explore a human example,\nwhen a child achieves good results on their report card, they are usually\nrewarded, but poor performance sometimes results in punishment, reinforcing\nthe behavior of achieving good results. Reinforcement learning is useful for\nexploring how computer programs or robots interact with dynamic\nenvironments. An example is a robot that is tasked to open doors; it is penalized\nwhen it doesn\u2019t open a door and rewarded when it does. Over time, after many\nattempts, the robot \u201clearns\u201d the sequence of actions required to open a door.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.21,
                        "section_name": "Deep learning algorithms",
                        "section_path": "./screenshots-images-2/chapter_1/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_21/438b7973-fd2e-4124-b2e6-e57880355bab.png",
                            "./screenshots-images-2/chapter_1/section_21/33466a6b-dca1-402b-aeb2-409ceb93f447.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep learning algorithms\n\nDeep learning, which stems from machine learning, is a broader family of approaches\nand algorithms that are used to achieve narrow intelligence and strive toward general\nintelligence. Deep learning usually implies that the approach is attempting to solve a\nproblem in a more general way like spatial reasoning, or it is being applied to problems\nthat require more generalization such as computer vision and speech recognition.\nGeneral problems are things humans are good at solving. For example, we can match\nvisual patterns in almost any context. Deep learning also concerns itself with super-\nvised learning, unsupervised learning, and reinforcement learning. Deep learning\napproaches usually employ many layers of artificial neural networks. By leveraging\n\ndifferent layers of intelligent components, each layer solves specialized problems;\ntogether, the layers solve complex problems toward a greater goal. Identifying any object\nin an image, for example, is a general problem, but it can be broken into understanding\ncolor, recognizing shapes of objects, and identifying relationships among objects to\nachieve a goal.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.22,
                        "section_name": "Uses for artificial intelligence algorithms",
                        "section_path": "./screenshots-images-2/chapter_1/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_22/770782ec-c254-4a84-9e76-ecae41159885.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Uses for artificial intelligence algorithms\n\nThe uses for AI techniques are potentially endless. Where there are data and problems to\nsolve, there are potential applications of AI. Given the ever-changing environment, the\nevolution of interactions among humans, and the changes in what people and industries\ndemand, AI can be applied in innovative ways to solve real-world problems. This section\ndescribes the application of AI in various industries.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.23,
                        "section_name": "Agriculture: Optimal plant growth",
                        "section_path": "./screenshots-images-2/chapter_1/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_23/4c3a09ab-7687-4471-bfa1-53b481b97ae8.png",
                            "./screenshots-images-2/chapter_1/section_23/4fe780eb-d0a6-4318-a23c-ff5a7abcd6af.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Agriculture: Optimal plant growth\n\nOne of the most important industries that sustain human life is agriculture. We need to\nbe able to grow quality crops for mass consumption economically. Many farmers grow\ncrops on a commercial scale to enable us to purchase fruit and vegetables at stores con-\nveniently. Crops grow differently based on the type of crop, the nutrients in the soil, the\nwater content of the soil, the bacteria in the water, and the weather conditions in the\narea, among other things. The goal is to grow as much high-quality produce as possi-\nble within a season, because specific crops generally grow well only during specific\nseasons.\n\nFarmers and other agriculture organizations have captured data about their farms\nand crops over the years. Using that data, we can leverage machines to find patterns and\nrelationships among the variables in the crop-growing process and identify the factors\nthat contribute most to successful growth. Furthermore, with modern digital sensors,\nwe can record weather conditions, soil attributes, water conditions, and crop growth in\nreal time. This data, combined with intelligent algorithms, can enable real-time recom-\nmendations and adjustments for optimal growth (figure 1.8).\n\nWeather Determine:\n\nWhat te grow\n\nPlant type When te grow\nSeil content Irrigation needs\nWater content Fertilizer needs\n\nFigure 1.8 Using data to optimize crop farming\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.24,
                        "section_name": "Banking: Fraud detection",
                        "section_path": "./screenshots-images-2/chapter_1/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_24/2194e1e7-4690-457f-9c92-ed73029a45d8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Banking: Fraud detection\n\nThe need for banking became obvious when we had to find a common consistent cur-\nrency for trading goods and services. Banks have changed over the years to offer differ-\nent options for storing money, investing money, and making payments. One thing that\nhasn\u2019t changed over time is people finding creative ways to cheat the system. One of the\nbiggest problems\u2014not only in banking but also in most financial institutions, such as\ninsurance companies\u2014is fraud. Fraud occurs when someone is dishonest or does some-\nthing illegal to acquire something for themselves. Fraud usually happens when loopholes\nin a process are exploited or a scam fools someone into divulging information. Because\nthe financial-services industry is highly connected through the internet and personal\ndevices, more transactions happen electronically over a computer network than in per-\nson, with physical money. With the vast amounts of transaction data available, we can,\nin real-time, find patterns of transactions specific to an individual\u2019s spending behavior\nthat may be out of the ordinary. This data helps save financial institutions enormous\namounts of money and protects unsuspecting consumers from being robbed.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.25,
                        "section_name": "Cybersecurity: Attack detection and handling",
                        "section_path": "./screenshots-images-2/chapter_1/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_25/a6731773-e8b6-48d7-958c-eef7da66ff3e.png",
                            "./screenshots-images-2/chapter_1/section_25/b7f3c7ed-17c5-4120-a8c2-ade9f009246a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Cybersecurity: Attack detection and handling\n\nOne of the interesting side effects of the internet boom is cybersecurity. We send and\nreceive sensitive information over the internet all the time\u2014instant messages, credit-\ncard details, emails, and other important confidential information that could be\n\nmisused if it fell into the wrong hands. Thousands of servers across the globe receive\ndata, process it, and store it. Attackers attempt to compromise these systems to gain\naccess to the data, devices, or even facilities.\n\nBy using AI, we can identify and block potential attacks on servers. Some large inter-\nnet companies store data about how specific individuals interact with their service,\nincluding their device IDs, geolocations, and usage behavior; when unusual behavior is\ndetected, security measures limit access. Some internet companies can also block and\nredirect malicious traffic during a distributed denial of service (DDoS) attack, which\ninvolves overloading a service with bogus requests in an attempt to bring it down or\nprevent access by authentic users. These unauthentic requests can be identified and\nrerouted to minimize the impact of the attack by understanding the users\u2019 usage data,\nthe systems, and the network.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.26,
                        "section_name": "Health care: Diagnosis of patients",
                        "section_path": "./screenshots-images-2/chapter_1/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_26/151214bc-6a53-4ef9-b5e1-a31450562551.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Health care: Diagnosis of patients\n\nHealth care has been a constant concern throughout human history. We need access to\ndiagnosis and treatment of different ailments in different locations in varying windows\nof time before a problem becomes more severe or even fatal. When we look at the diag-\nnosis of a patient, we may look at the vast amounts of knowledge recorded about the\nhuman body, known problems, experience in dealing with these problems, and a myriad\nof scans of the body. Traditionally, doctors were required to analyze images of scans to\ndetect the presence of tumors, but this approach resulted in the detection of only the\nlargest, most advanced tumors. Advances in deep learning have improved the detection\nof tumors in images generated by scans. Now doctors can detect cancer earlier, which\nmeans that a patient can get the required treatment in time and have a higher chance of\nrecovery.\n\nFurthermore, AI can be used to find patterns in symptoms, ailments, hereditary\ngenes, geographic locations, and the like. We could potentially know that someone has a\nhigh probability of developing a specific ailment and be prepared to manage that ailment\nbefore it develops. Figure 1.9 illustrates feature recognition of a brain scan using deep\nlearning.\n\nBrain scan Brain scan with feature recegnition\n\nFigure 1.9 Using machine learning for feature recognition in brain scans\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.27,
                        "section_name": "Logistics: Routing and optimization",
                        "section_path": "./screenshots-images-2/chapter_1/section_27",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_27/b028005e-7cf7-40e3-ae9b-857f929b5905.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Logistics: Routing and optimization\n\nThe logistics industry is a huge market of different types of vehicles delivering differ-\nent types of goods to different locations, with different demands and deadlines. Imagine\nthe complexity in a large e-commerce site\u2019s delivery planning. Whether the delivera-\nbles are consumer goods, construction equipment, parts for machinery, or fuel, the sys-\ntem aims to be as optimal as possible to ensure that demand is met and costs are\nminimized.\n\nYou may have heard of the traveling-salesperson problem: a salesperson needs to visit\nseveral locations to complete their job, and the aim is to find the shortest distance to\naccomplish this task. Logistics problems are similar but usually immensely more com-\nplex due to the changing environment of the real world. Through AI, we can find\noptimal routes between locations in terms of time and distance. Furthermore, we can\nfind the best routes based on traffic patterns, construction blockages, and even road\ntypes based on the vehicle being used. Additionally, we can compute the best way to pack\neach vehicle and what to pack in each vehicle in such a way that each delivery is\noptimized.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.28,
                        "section_name": "Telecoms: Optimizing networks",
                        "section_path": "./screenshots-images-2/chapter_1/section_28",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_28/6c932ea4-d437-473e-96be-4a277f75662c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Telecoms: Optimizing networks\n\nThe telecommunications industry has played a huge role in connecting the world. These\ncompanies lay expensive infrastructure of cables, towers, and satellites to create a net-\nwork that many consumers and organizations can use to communicate via the internet\nor private networks. Operating this equipment is expensive, so optimization of a net-\nwork allows for more connections, which allows more people to access high-speed con-\nnections. AI can be used to monitor behavior on a network and optimize routing.\nAdditionally, these networks record requests and responses; this data can be used to\noptimize the networks based on known load from certain individuals, areas, and spe-\ncific local networks. The network data can also be instrumental for understanding where\npeople are and who they are, which is useful for city planning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.29,
                        "section_name": "Games: Creating Al agents",
                        "section_path": "./screenshots-images-2/chapter_1/section_29",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_29/41533c58-458f-4c99-a6f1-9732a24af430.png",
                            "./screenshots-images-2/chapter_1/section_29/a6066816-db99-4cb7-a682-232038c21730.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Games: Creating Al agents\n\nSince home and personal computers first became widely available, games have been a\nselling point for computer systems. Games were popular very early in the history of per-\nsonal computers. If we think back, we may remember arcade machines, television con-\nsoles, and personal computers with gaming capabilities. The games of chess, backgammon,\nand others have been dominated by AI machines. If the complexity of a game is low\nenough, a computer can potentially find all possibilities and make a decision based on\nthat knowledge faster than a human can. Recently, a computer was able to defeat himan\nchampions in the strategic game, Go. Go has simple rules for territory control but has\nhuge complexity in terms of the decisions that need to be made for a winning scenario.\nAcomputer can\u2019t generate all possibilities for beating the best human players because the\nsearch space is so large; instead, it calls for a more-general algorithm that can \u201cthink\u201d\n\nabstractly, strategize, and plan moves toward a goal. That algorithm has already been\ninvented and has succeeded in defeating world champions. It has also been adapted to\nother applications, such as playing Atari games and modern multiplayer games. This\nsystem is called Alpha Go.\n\nSeveral research organizations have developed AI systems that are capable of playing\nhighly complex games better than human players and teams. The goal of this work is to\ncreate general approaches that can adapt to different contexts. At face value, these\ngame-playing AI algorithms may seem unimportant, but the consequence of developing\nthese systems is that the approach can be applied effectively in other important problem\nspaces. Figure 1.10 illustrates how a reinforcement learning algorithm can learn to play a\nclassic video game like Mario.\n\nFigure 1.10 Using neural networks to learn how to play games\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.3,
                        "section_name": "Art: Creating masterpieces",
                        "section_path": "./screenshots-images-2/chapter_1/section_30",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_30/fd55ce57-55c3-41ec-96bb-a7b32402550e.png",
                            "./screenshots-images-2/chapter_1/section_30/a1e7ad31-f7c6-4f33-aa2a-d2d0d131bd28.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Art: Creating masterpieces\n\nUnique, interesting artists have created beautiful paintings. Each artist has their own\nway of expressing the world around them. We also have amazing music compositions\nthat are appreciated by the masses. In both cases, the quality of the art cannot be mea-\nsured quantitatively; rather, it is measured qualitatively (by how much people enjoy the\npiece). The factors involved are difficult to understand and capture; the concept is driven\nby emotion.\n\nMany research projects aim to build AI that generates art. The concept involves gen-\neralization. An algorithm would need to have a broad and general understanding of the\nsubject to create something that fits those parameters. A Van Gogh AI, for example,\nwould need to understand all of Van Gogh\u2019s work and extract the style and \u201cfeel\u201d so that\nit can apply that data to other images. The same thinking can be applied to extracting\nhidden patterns in areas such as health care, cybersecurity, and finance.\n\nNow that we have abstract intuition about what Al is, the categorization of themes within\nit, the problems it aims to solve, and some use cases, we will be diving into one of the\noldest and simplest forms of mimicking intelligence: search algorithms. Search algo-\nrithms provide a good grounding in some concepts that are employed by other, more\nsophisticated AI algorithms explored throughout this book.\n\nSUMMARY OF INTUITION OF ARTIFICIAL INTELLIGENCE\n\nAlis difficult to define. There is ne clear consensus.\nLook at implementations as being Al-like things that exhibit intelligence.\n\nMany disciplines are encempassed in Al.\n\nArtificial\nintelligence\n\nMachine\nlearning\n\nlearning\n\nAlimplementations almost always have room for error. Be cautious\nabout the consequences of this.\n\nQuality and preparation of data\n\nZo, aes is impertant.\n\nAI has many uses and applications. Apply your mind!\n\nBrain soan with feature reoegnitien\n\nBrain scan\n\nBe responsible when developing technology.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 2,
                "chapter_name": "Search\nfundamentals",
                "chapter_path": "./screenshots-images-2/chapter_2",
                "sections": [
                    {
                        "section_id": 2.1,
                        "section_name": "What are planning and searching?",
                        "section_path": "./screenshots-images-2/chapter_2/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_1/ea4e0f40-f0be-48fb-a97c-2cb6a55e5e94.png",
                            "./screenshots-images-2/chapter_2/section_1/9e548b9d-57f3-4107-a5ec-c33c6e51eb75.png",
                            "./screenshots-images-2/chapter_2/section_1/3028affd-9d1c-4f33-a44b-c26d39c413e1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "When we think about what makes us intelligent, the ability to plan before\ncarrying out actions is a prominent attribute. Before embarking on a trip to\na different country, before starting a new project, before writing functions\nin code, planning happens. Planning happens at different levels of detail in\ndifferent contexts to strive for the best possible outcome when carrying out\nthe tasks involved in accomplishing goals (figure 2.1).\n\nd\nd\n\nOriginal plan Ad justed plan\n\nFigure 2.1 Example of how plans change in projects\n\nPlans rarely work out perfectly in the way we envision at the start of an endeavor. We\nlive in a world in which environments are constantly changing, so it is impossible to\naccount for all the variables and unknowns along the way. Regardless of the plan we\nstarted with, we almost always deviate due to changes in the problem space. We need to\n(again) make a new plan from our current point going forward, if after we take more\nsteps, unexpected events occur that require another iteration of planning to meet the\ngoals. As a result, the final plan that is carried out is usually different from the origi-\nnal one.\n\nSearching is a way to guide planning by creating steps in a plan. When we plan a trip,\nfor example, we search for routes to take, evaluate the stops along the way and what they\noffer, and search for accommodations and activities that align with our liking and bud-\nget. Depending on the results of these searches, the plan changes.\n\nSuppose that we have settled on a trip to the beach, which is 500 kilometers away, with\ntwo stops: one at a petting zoo and one at a pizza restaurant. We will sleep at a lodge close\nto the beach on arrival and partake in three activities. The trip to the destination will\ntake approximately 8 hours. We\u2019re also taking a shortcut private road after the restau-\nrant, but it\u2019s open only until 2:00.\n\nWe start the trip, and everything is going according to plan. We stop at the petting zoo\nand see some wonderful animals. We drive on and start getting hungry; it\u2019s time for the\nstop at the restaurant. But to our surprise, the restaurant recently went out of business.\nWe need to adjust our plan and find another place to eat, which involves searching for a\nclose-by establishment of our liking and adjusting our plan.\n\nAfter driving around for a while, we find a restaurant, enjoy a pizza, and get back on\nthe road. Upon approaching the shortcut private road, we realize that it\u2019s 2:20. The road\nis closed; yet again, we need to adjust our plan. We search for a detour and find that it\nwill add 120 kilometers to our drive, and we will need to find accommodations for the\nnight at a different lodge before we even get to the beach. We search for a place to sleep\n\nand plot out our new route. Due to lost time, we can partake in only two activities at\nthe destination. The plan has been adjusted heavily through searching for different\noptions that satisfy each new situation, but we end up having a great adventure en route\nto the beach.\n\nThis example shows how search is used for planning and influences planning toward\ndesirable outcomes. As the environment changes, our goals may change slightly, and our\npath to them inevitably needs to be adjusted (figure 2.2). Adjustments in plans can\nalmost never be anticipated and need to be made as required.\n\nOriginal plan Adjusted plan\n\nSTART\n\nPIT STOP\nSLEEP STOP\n\novpoe\n\nLy\n'\n1\n1\n1\n'\n|\n1\n1\n1\n|\n1\n|\n1\n|\nI\n!\n| ACTIVITY\n'\n\n!\n\nFigure 2.2 Original plan versus adjusted plan for a road trip\n\nSearching involves evaluating future states toward a goal with the aim of finding an\noptimal path of states until the goal is reached. This chapter centers on different\napproaches to searching depending on different types of problems. Searching is an old\nbut powerful tool for developing intelligent algorithms to solve problems.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.2,
                        "section_name": "Cost of computation:\nThe reason for smart algorithms",
                        "section_path": "./screenshots-images-2/chapter_2/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_2/5d5228a6-a18c-4f08-8cf3-d800833cee3f.png",
                            "./screenshots-images-2/chapter_2/section_2/21c8de84-a52a-4a52-b33e-5cdb0415eb54.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Cost of computation:\nThe reason for smart algorithms\n\nIn programming, functions consist of operations, and due to the way that traditional\ncomputers work, different functions use different amounts of processing time. The more\ncomputation required, the more expensive the function is. Big O notation is used to\ndescribe the complexity of a function or algorithm. Big O notation models the number\nof operations required as the input size increases. Here are some examples and associ-\nated complexities:\n\n+ A single operation that prints Hello World\u2014This operation is a single\noperation, so the cost of computation is O(1).\n\n+ A function that iterates over a list and prints each item in the list\u2014The number of\noperations is dependent on the number of items in the list. The cost is O(n).\n\n+ A function that compares every item in a list with every item in another list\u2014This\noperation costs O(n?).\n\nFigure 2.3 depicts different costs of algorithms. Algorithms that require operations to\nexplore as the size of the input increases are the worst-performing; algorithms that require\na more constant number of operations as the number of inputs increases are better.\n\n= a 2\n\n3 O(n!) 0(2\") O(n\u2019) Bad\n2\n\ns\n\n.\n\n3 O(n log n)\n\n.\n\n.\n\n\u00b0\n\n5 Okay\ns O(n)\n\n3\n\n\u20ac\n\n8\n\n2 O(log n) Good\n\u00b0\n\nE 0(1)\n\nSize of input data (n)\n\nFigure 2.3 Big O complexity\n\nUnderstanding that different algorithms have different computation costs is import-\nant because addressing this is the entire purpose of intelligent algorithms that solve\nproblems well and quickly. Theoretically, we can solve almost any problem by brute-\nforcing every possible option until we find the best one, but in reality, the computation\ncould take hours or even years, which makes it infeasible for real-world scenarios.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.3,
                        "section_name": "Problems applicable to searching algorithms",
                        "section_path": "./screenshots-images-2/chapter_2/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_3/aa2acc08-366b-4ff5-a296-24b283242e3a.png",
                            "./screenshots-images-2/chapter_2/section_3/eb9fa42e-ceb0-41cb-9a2f-d512b6012f99.png",
                            "./screenshots-images-2/chapter_2/section_3/20dda344-fe29-433a-adc7-6f82da62bc39.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Problems applicable to searching algorithms\n\nAlmost any problem that requires a series of decisions to be made can be solved with\nsearch algorithms. Depending on the problem and the size of the search space, different\nalgorithms may be employed to help solve it. Depending on the search algorithm\nselected and the configuration used, the optimal solution or a best available solution\nmay be found. In other words, a good solution will be found, but it might not necessarily\nbe the best solution. When we speak about a \u201cgood solution\u201d or \u201coptimal solution,\u201d we\nare referring to the performance of the solution in addressing the problem at hand.\n\nOne scenario in which search algorithms are useful is being stuck in a maze and\nattempting to find the shortest path to a goal. Suppose that we\u2019re in a square maze con-\nsisting of an area of 10 blocks by 10 blocks (figure 2.4). There exists a goal that we want\nto reach and barriers that we cannot step into. The objective is to find a path to the goal\nwhile avoiding barriers with as few steps as possible by moving north, south, east, or\nwest. In this example, the player cannot move diagonally.\n\nbd $ prayer\n\nFigure 2.4 An example of the maze problem\n\nHow can we find the shortest path to the goal while avoiding barriers? By evaluating\nthe problem as a human, we can try each possibility and count the moves. Using trial and\nerror, we can find the paths that are the shortest, given that this maze is relatively small.\n\nUsing the example maze, figure 2.5 depicts some possible paths to reach the goal,\nalthough note that we don\u2019t reach the goal in option 1.\n\n\u00ae\n\u201c eg\n\ntu.\n\nt\n0\nJ\n>>\n\n@ \u00ae\nx ve\ni, 1,\n1, t,\nt t\n1, 1,\n1 1,\ni: 1,\n\nFigure 2.5 Examples of possible paths to the maze problem\n\nBy looking at the maze and counting blocks in different directions, we can find several\nsolutions to the problem. Five attempts have been made to find four successful solutions\nout of an unknown number of solutions. It will take exhaustive effort to attempt to com-\npute all possible solutions by hand:\n\n+ Attempt 1 is not a valid solution. It took 4 actions, and the goal was not found.\n+ Attempt 2 is a valid solution, taking 17 actions to find the goal.\n+ Attempt 3 is a valid solution, taking 23 actions to find the goal.\n+ Attempt 4 isa valid solution, taking 17 actions to find the goal.\n\n+ Attempt 5 is the best valid solution, taking 15 actions to find the goal. Although\nthis attempt is the best one, it was found by chance.\n\nIf the maze were a lot larger, like the one in figure 2.6, it would take an immense amount\nof time to compute the best possible path manually. Search algorithms can help.\n\nFigure 2.6 A large example of the maze problem\n\nOur power as humans is to perceive a problem visually, understand it, and find solutions\ngiven the parameters. As humans, we understand and interpret data and information\nin an abstract way. A computer cannot yet understand generalized information in the\nnatural form that we do. The problem space needs to be represented in a form that is\napplicable to computation and can be processed with search algorithms.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.4,
                        "section_name": "Representing state: Creating a framework\nto represent problem spaces and solutions",
                        "section_path": "./screenshots-images-2/chapter_2/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_4/c4c768dc-bebd-4f3c-8b5b-a2d99e40db65.png",
                            "./screenshots-images-2/chapter_2/section_4/a3778bae-4ff6-426b-b2b4-e598c9ac5c18.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Representing state: Creating a framework\nto represent problem spaces and solutions\n\nWhen representing data and information in a way that a computer can understand, we\nneed to encode it logically so that it can be understood objectively. Although the data\nwill be encoded subjectively by the person who performs the task, there should be a con-\ncise, consistent way to represent it.\n\nLet\u2019s clarify the difference between data and information. Data is raw facts about\nsomething, and information is an interpretation of those facts that provides insight about\nthe data in the specific domain. Information requires context and processing of data to\nprovide meaning. As an example, each individual distance traveled in the maze example\nis data, and the sum of the total distance traveled is information. Depending on the per-\nspective, level of detail, and desired outcome, classifying something as data or informa-\ntion can be subjective to the context and person or team (figure 2.7).\n\nDATA\n\nx ; nce INFORMATION\n\nFigure 2.7 Data versus information\n\nData structures are concepts in computer science used to represent data in a way that\nis suitable for efficient processing by algorithms. A data structure is an abstract data type\nconsisting of data and operations organized in a specific way. The data structure we use\nis influenced by the context of the problem and the desired goal.\n\nAn example of a data structure is an array, which is simply a collection of data.\nDifferent types of arrays have different properties that make them efficient for different\npurposes. Depending on the programming language used, an array could allow each\nvalue to be of a different type or require each value to be the same type, or the array may\ndisallow duplicate values. These different types of arrays usually have different names.\nThe features and constraints of different data structures also enable more efficient com-\nputation (figure 2.8).\n\nARRAY STACK QUEUE LINKEDLIST GRAPH TREE\n\nFigure 2.8 Data structures used with algorithms\n\nOther data structures are useful in planning and searching. Trees and graphs are ideal\nfor representing data in a way that search algorithms can use.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.5,
                        "section_name": "Graphs: Representing search problems and solutions",
                        "section_path": "./screenshots-images-2/chapter_2/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_5/a2082687-c24e-457a-a7eb-4ede610f7a7b.png",
                            "./screenshots-images-2/chapter_2/section_5/c1d576a2-fc85-4c32-8b3d-3e9b7ff8f816.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Graphs: Representing search problems and solutions\n\nA graph is a data structure containing several states with connections among them.\nEach state in a graph is called a node (or sometimes a vertex), and a connection between\ntwo states is called an edge. Graphs are derived from graph theory in mathematics\nand used to model relationships among objects. Graphs are useful data structures that\nare easy for humans to understand, due to the ease of representing them visually as well\nas to their strong logical nature, which is ideal for processing via various algorithms\n(figure 2.9).\n\nedge (E)\n\nEo a are\nre]\n\nLe | id |\n\nV ={a, b,c, d, e}\n\nE = {ab, ac, ad, bd, be, cd, de}\n\nFigure 2.9 The notation used to represent graphs\n\nFigure 2.10 is a graph of the trip to the beach discussed in the first section of this chapter.\nEach stop is a node on the graph; each edge between nodes represent points traveled\nbetween; and the weights on each edge indicate the distance traveled.\n\nstart\n\nit Stor\nactivity \u00ae \u00b0\n\nactivity\n\n20\n415 So\n\nsleep stop pit step\n\nsleep step 120 70\n\nFigure 2.10 The example road trip represented as a graph\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.6,
                        "section_name": "Representing a graph as a concrete data structure",
                        "section_path": "./screenshots-images-2/chapter_2/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_6/9bbb8a22-10d8-4b17-82e5-73d89185f840.png",
                            "./screenshots-images-2/chapter_2/section_6/8ed005e6-3c58-422d-9d85-9ac94a87f962.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Representing a graph as a concrete data structure\n\nA graph can be represented in several ways for efficient processing by algorithms. At its\ncore, a graph can be represented by an array of arrays that indicates relationships among\nnodes, as shown in figure 2.11. It is sometimes useful to have another array that simply\nlists all nodes in the graph so that the distinct nodes do not need to be inferred from the\nrelationships.\n\n3 | co [ [a,\no t\n[e | 6 | [a,\n\nFigure 2.11 Representing a graph as an array of arrays\n\nOther representations of graphs include an incidence matrix, an adjacency matrix, and\nan adjacency list. By looking at the names of these representations, you see that the adja-\ncency of nodes in a graph is important. An adjacent node is a node that is connected\ndirectly to another node.\n\nEXERCISE: REPRESENT A GRAPH AS A MATRIX\nHow would you represent the following graph using edge arrays?\n\nSOLUTION: REPRESENT A GRAPH AS A MATRIX\n\na bedefgh\n@o01400%1 0 0\n\n000060014 14 0\n\n100100 0 0\n@eo10%140%1 0\n@0@0601400 0 4\n1410000 0 0\n01060100 0 @\n\nror\nOF OFT QOS\nGSHioSTDOS\nRispearpunsyenrusresteerean\n\n0000140 0 0\n\nAd jacency matrix\n\nArray of edges\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.7,
                        "section_name": "Trees: The concrete structures used to represent search solutions",
                        "section_path": "./screenshots-images-2/chapter_2/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_7/57c4425c-b01c-4dcc-a5d9-a828f901e05a.png",
                            "./screenshots-images-2/chapter_2/section_7/579b16dd-d428-48d8-8d82-cfc812e8ffff.png",
                            "./screenshots-images-2/chapter_2/section_7/92b45500-80fe-4388-ad8e-d93322116119.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Trees: The concrete structures used to represent search solutions\n\nA tree is a popular data structure that simulates a hierarchy of values or objects. A hier-\narchy is an arrangement of things in which a single object is related to several other\nobjects below it. A tree is a connected acyclic graph\u2014every node has an edge to another\nnode, and no cycles exist.\n\nIna tree, the value or object represented at a specific point is called a node. Trees typ-\nically have a single root node with zero or more child nodes that could contain subtrees.\nLet\u2019s take a deep breath and jump into some terminology. When a node has connected\nnodes, the root node is called the parent. You can apply this thinking recursively. A child\nnode may have its own child nodes, which may also contain subtrees. Each child node\nhas a single parent node. A node without any children is a leaf node.\n\nTrees also have a total height. The level of specific nodes is called a depth.\n\nThe terminology used to relate family members is heavily used in working with\ntrees. Keep this analogy in mind, as it will help you connect the concepts in the tree data\nstructure. Note that in figure 2.12, the height and depth are indexed from 0 from the\nroot node.\n\na root node\n4 children/neighbers and Parent of 4\nof the root nede a eage\n\n\u2122\n\ndepth: 2\n\nheight: 4\n\n}\n\nleaf nodes\n\nFigure 2.12 The main attributes of a tree\n\nThe topmost node in a tree is called the root node. A node directly connected to one or\nmore other nodes is called a parent node. The nodes connected to a parent node are\ncalled child nodes or neighbors. Nodes connected to the same parent node are called sib-\nlings. A connection between two nodes is called an edge.\n\nA path is a sequence of nodes and edges connecting nodes that are not directly con-\nnected. A node connected to another node by following a path away from the root node\nis called a descendent, and a node connected to another node by following a path toward\nthe root node is called an ancestor. A node with no children is called a leaf node. The term\ndegree is used to describe the number of children a node has; therefore, a leaf node has\ndegree zero.\n\nFigure 2.13 represents a path from the start point to the goal for the maze problem.\nThis path contains nine nodes that represent different moves being made in the maze.\n\nFigure 2.13 A solution to the maze problem represented as a tree\n\nTrees are the fundamental data structure for search algorithms, which we will be diving\ninto next. Sorting algorithms are also useful in solving certain problems and computing\nsolutions more efficiently. If you're interested in learning more about sorting algorithms,\ntake a look at Grokking Algorithms (Manning Publications).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.8,
                        "section_name": "Uninformed search:\nLooking blindly for solutions",
                        "section_path": "./screenshots-images-2/chapter_2/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_8/9e8c08fd-5c62-402d-9dc9-c99dd7aa0963.png",
                            "./screenshots-images-2/chapter_2/section_8/b0239e34-3d89-4d87-8883-8ca0a3afa691.png",
                            "./screenshots-images-2/chapter_2/section_8/3a3fc87f-c8af-47c6-babd-ea14eb56c717.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Uninformed search:\nLooking blindly for solutions\n\nUninformed search is also known as unguided search, blind search, or brute-force search.\nUninformed search algorithms have no additional information about the domain of the\nproblem apart from the representation of the problem, which is usually a tree.\n\nThink about exploring things you want to learn. Some people might look at a\nwide breadth of different topics and learn the basics of each, whereas other people\nmight choose one narrow topic and explore its subtopics in depth. This is what breadth-\nfirst search (BFS) and depth-first search (DFS) involve, respectively. Depth-first search\nexplores a specific path from the start until it finds a goal at the utmost depth. Breadth-\nfirst search explores all options at a specific depth before moving to options deeper in\nthe tree.\n\nConsider the maze scenario (figure 2.14). In attempting to find an optimal path to the\ngoal, assume the following simple constraint to prevent getting stuck in an endless loop\n\nand prevent cycles in our tree: the player cannot move into a block that they have previously\noccupied. Because uninformed algorithms attempt every possible option at every node,\ncreating a cycle will cause the algorithm to fail catastrophically.\n\n2) @\n*\n\nx\na\n\nve f >O-w|\n\nMe fOr\nSf\nx\nve f Ox\nFigure 2.14 The constraint for the maze problem\n\nThis constraint prevents cycles in the path to the goal in our scenario. But this constraint\nwill introduce problems if, in a different maze with different constraints or rules, moving\ninto a previously occupied block more than once is required for the optimal solution.\n\nIn figure 2.15, all possible paths in the tree are represented to highlight the different\noptions available. This tree contains seven paths that lead to the goal and one path that\nresults in an invalid solution, given the constraint of not moving to previously occupied\nblocks. It\u2019s important to understand that in this small maze, representing all the possi-\nbilities is feasible. The entire point of search algorithms, however, is to search or generate\nthese trees iteratively, because generating the entire tree of possibilities up front is ineffi-\ncient due to being computationally expensive.\n\nIt is also important to note that the term visiting is used to indicate different things.\nThe player visits blocks in the maze. The algorithm also visits nodes in the tree. The\n\norder of choices will influence the order of nodes being visited in the tree. In the maze\nexample, the priority order of movement is north, south, east, and then west.\n\n\nDepth\n\n10\n\n11\n\n12\n\nFigure 2.15 All possible movement options represented as a tree\n\nNow that we understand the ideas behind trees and the maze example, let\u2019s explore how\nsearch algorithms can generate trees that seek out paths to the goal.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.9,
                        "section_name": "Breadth-first search:\nLooking wide before looking deep",
                        "section_path": "./screenshots-images-2/chapter_2/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_9/d805261c-3107-4cb2-a49c-c71f07cfa5c1.png",
                            "./screenshots-images-2/chapter_2/section_9/19a6dd17-80e9-4ad2-9809-a811748a018d.png",
                            "./screenshots-images-2/chapter_2/section_9/21c1368f-912d-429d-a05a-f4dcf853f687.png",
                            "./screenshots-images-2/chapter_2/section_9/1a79a848-034b-4d83-bd48-9bb024bf1219.png",
                            "./screenshots-images-2/chapter_2/section_9/79afa8fb-c0f2-4e61-b36e-c999f049b4f5.png",
                            "./screenshots-images-2/chapter_2/section_9/c74a2cb6-dae1-4aef-aab8-65ef681ac8dd.png",
                            "./screenshots-images-2/chapter_2/section_9/5f07ef80-2574-4c6f-a786-1dde9fa3b357.png",
                            "./screenshots-images-2/chapter_2/section_9/3dcaf345-5b02-4853-9dcb-cdb28110356d.png",
                            "./screenshots-images-2/chapter_2/section_9/a4f20060-c369-4bc6-a593-a5c751c31d0e.png",
                            "./screenshots-images-2/chapter_2/section_9/ed4b2e9c-e905-40b1-83d0-5aa7ea9da81a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Breadth-first search:\nLooking wide before looking deep\n\nBreadth-first search is an algorithm used to traverse or generate a tree. This algorithm\nstarts at a specific node, called the root, and explores every node at that depth before\nexploring the next depth of nodes. It essentially visits all children of nodes at a specific\ndepth before visiting the next depth of child until it finds a goal leaf node.\n\nThe breadth-first search algorithm is best implemented by using a first-in, first-out\nqueue in which the current depths of nodes are processed, and their children are queued\nto be processed later. This order of processing is exactly what we require when imple-\nmenting this algorithm.\n\nFigure 2.16 is a flow chart describing the sequence of steps involved in the breadth-\nfirst search algorithm.\n4\n\nReturn\n\u201cNe path\nte goal\u201d\n\nDequeve\nnede as\ncurrent\nnede.\n\nMark reot\nnode as\nvisited.\n\nEnqueve\nreot nede.\n\nSet current\n\nMark Is Get next\nnode as neighber neighbor neighbor of\nparent of as visited. visited current\n\nneighbor. 2 nede.\n\n10\n\nCurrent\n\nhas next\n\nneighber,\n2\n\nIs goat\n\nEnqueve reached\n2\n\nneighbor.\n\nReturn\npath using\nneighber,\n\nFigure 2.16 Flow of the breadth-first search algorithm\n\nHere are some notes and additional remarks about each step in the process:\n\n1. Enqueue root node. The breadth-first search algorithm is best implemented with a\nqueue. Objects are processed in the sequence in which they are added to the\nqueue. This process is also known as first in, first out (FIFO). The first step is\nadding the root node to the queue. This node will represent the starting position\nof the player on the map.\n\n2. Mark root node as visited. Now that the root node has been added to the queue\nfor processing, it is marked as visited to prevent it from being revisited for no\nreason.\n\n3. Is queue empty? If the queue is empty (all nodes have been processed after many\niterations), and if no path has been returned in step 12 of the algorithm, there is\nno path to the goal. If there are still nodes in the queue, the algorithm can\ncontinue its search to find the goal.\n\n4. ReturnNo path to goal. This message is the one possible exit from the\nalgorithm if no path to the goal exists.\n\n5. Dequeue node as current node. By pulling the next object from the queue and\nsetting it as the current node of interest, we can explore its possibilities. When the\nalgorithm starts, the current node will be the root node.\n\n6. Get next neighbor of current node. This step involves getting the next possible\nmove in the maze from the current position by referencing the maze and\ndetermining whether a north, south, east, or west movement is possible.\n\n7. Is neighbor visited? If the current neighbor has not been visited, it hasn't been\nexplored yet and can be processed now.\n\n8. Mark neighbor as visited. This step indicates that this neighbor node has been\nvisited.\n\n9. Set current node as parent of neighbor. Set the origin node as the parent of the\ncurrent neighbor. This step is important for tracing the path from the current\nneighbor to the root node. From a map perspective, the origin is the position that\nthe player moved from, and the current neighbor is the position that the player\nmoved to.\n\n10. Enqueue neighbor. The neighbor node is queued for its children to be explored\nlater. This queuing mechanism allows nodes from each depth to be processed in\nthat order.\n\n11. Is goal reached? This step determines whether the current neighbor contains the\ngoal that the algorithm is searching for.\n\n12. Return path using neighbor. By referencing the parent of the neighbor node, then\nthe parent of that node, and so on, the path from the goal to the root will be\ndescribed. The root node will be a node without a parent.\n\n13. Current has next neighbor? If the current node has more possible moves to make\nin the maze, jump to step 6 for that move.\n\nLet\u2019s walk through what that would look like in a simple tree. Notice that as the tree is\nexplored and nodes are added to the FIFO queue, the nodes are processed in the order\ndesired by leveraging the queue (figures 2.17 and 2.18).\n\nVisit the first\nchild nede, B.\n\nSequence of\nprocessing the queve\n\nVisit the next\nchild nede, C.\n\no]e]>|\n\nVisit the next\nchild nede, D.\n\n[s[e[\u00b0[>\n\nFigure 2.17 The sequence of tree processing using breadth-first search (part 1)\n\nVisit the final Sequence of\n\nnode on this processing the queve\n\ndepth, E.\nA\nB\nc\nD\nE\n\nRQ\n\nVisit the first\n\nchild of the as\n\nfirst neighbor B\n\nof A. This is F, c\n\nfirst child of B.\nD\nE\nF\n\nVisit the next A\n\nchild ef B. This\n\nis G. 8\nCc\nD\nE\nF\nG\n\nFigure 2.18 The sequence of tree processing using breadth-first search ( part 2)\n\nEXERCISE: DETERMINE THE PATH TO THE SOLUTION\nWhat would be the order of visits using breadth-first search for the following tree?\n\nSOLUTION: DETERMINE THE PATH TO THE SOLUTION\n\nBreadth-first search order:\nA,B, E,D,C,L, F,N, 1, 0,0, M,G, J, P,H,K, @\n\n\nIn the maze example, the algorithm needs to understand the current position of the\nplayer in the maze, evaluate all possible choices for movement, and repeat that logic for\neach choice of movement made until the goal is reached. By doing this, the algorithm\ngenerates a tree with a single path to the goal.\n\nIt is important to understand that the processes of visiting nodes in a tree is used to\ngenerate nodes in a tree. We are simply finding related nodes through a mechanism.\n\nEach path to the goal consists of a series of moves to reach the goal. The number of\nmoves in the path is the distance to reach the goal for that path, which we will call the\ncost. The number of moves also equals the number of nodes visited in the path, from the\nroot node to the leaf node that contains the goal. The algorithm moves down the tree\ndepth by depth until it finds a goal; then it returns the first path that got it to the goal as\nthe solution. There may be a more optimal path to the goal, but because breadth-first\nsearch is uninformed, it is not guaranteed to find that path.\n\nNOTE In the maze example, all search algorithms used terminate when\nthey\u2019ve found a solution to the goal. It is possible to allow these algorithms to\nfind multiple solutions with a small tweak to each algorithm, but the best use\ncases for search algorithms find a single goal, as it is often too expensive to\nexplore the entire tree of possibilities.\n\nFigure 2.19 shows the generation of a tree using movements in the maze. Because the tree\nis generated using breadth-first search, each depth is generated to completion before\nlooking at the next depth (figure 2.20).\n\ni Depth\n% 1\nWd)\n%\nJ\n3 Depth\n% 1\na\n%\nJ >\nJ\nz Depth\n* 1\n2\nin|\nvit\nI Depth\nbs 1\n2\n% 1 3\n| og\nJets\nT Depth\n* 1\nt 2\nA t 3\nt ani 4\nook aha 5\n\nFigure 2.19 Maze movement tree generation using breadth-first search\n\nDepth\n\n\u00bb >\u00bb >\n\nt\n\n<>t0\nown er oO FF ON\n\n\u00a9\n\n10\n\n11\n\n12\n\nFigure 2.20 Nodes visited in the entire tree after breadth-first search\n\nAs mentioned previously, the breadth-first search algorithm uses a queue to generate a\ntree one depth at a time. Having a structure to store visited nodes is critical to prevent\ngetting stuck in cyclic loops; and setting the parent of each node is important for deter-\nmining a path from the starting point in the maze to the goal:\n\nrun_bfs(maze, current_point, visited_points):\nlet gq equal a new queue\npush current_point tog\nmark current_point as visited\nwhile q is not empty:\npop g and let current_point equal the returned point\nadd available cells north, east, south, and west toa list neighbors\nfor each neighbor in neighbors:\nif neighbor is not visited:\nset neighbor parent as current_point\nmark neighbor as visited\npush neighbor tog\nif value at neighbor is the goal:\nreturn path using neighbor\n\nreturn \u201cNo path to goal\u201d\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.1,
                        "section_name": "Depth-first search:\nLooking deep before looking wide",
                        "section_path": "./screenshots-images-2/chapter_2/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_10/ef3d6708-be9e-4559-bc53-cbc954495538.png",
                            "./screenshots-images-2/chapter_2/section_10/b904e293-cf73-4554-a167-2ab4b791368f.png",
                            "./screenshots-images-2/chapter_2/section_10/36065954-a695-4c72-a73d-3d0b81935f9b.png",
                            "./screenshots-images-2/chapter_2/section_10/4118662d-507e-4101-9ead-675e7166b7fc.png",
                            "./screenshots-images-2/chapter_2/section_10/ad9fa569-b7c3-49e3-a8cf-9e85e1959b42.png",
                            "./screenshots-images-2/chapter_2/section_10/58eb591c-d9e4-45bc-ad35-e785ba432db8.png",
                            "./screenshots-images-2/chapter_2/section_10/93e90c7f-74d6-400e-b251-6738cd1b337f.png",
                            "./screenshots-images-2/chapter_2/section_10/6ba2480f-957f-4ab5-8e44-d1c591eff232.png",
                            "./screenshots-images-2/chapter_2/section_10/b1f39a76-7d17-4eb7-95f5-bb96d762d75f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Depth-first search:\nLooking deep before looking wide\n\nDepth-first search is another algorithm used to traverse a tree or generate nodes and\npaths in a tree. This algorithm starts at a specific node and explores paths of con-\nnected nodes of the first child, doing this recursively until it reaches the farthest leaf\nnode before backtracking and exploring other paths to leaf nodes via other child nodes\nthat have been visited. Figure 2.21 illustrates the general flow of the depth-first search\nalgorithm.\n\nPop node\nfrom stack\nas current\nnode.\n\nAdd reet\nnode to\nstack.\n\n11\n\nAdd Set current\nneighbor node a8 \u00a2 has next\nto stack. rrelghber. elghbor\n\nReturn\npath using\ncurrent\nnede.\n\nFigure 2.21 Flow of the depth-first search algorithm\n\nLet\u2019s walk through the flow of the depth-first search algorithm:\n\n1.\n\nAdd root node to stack. The depth-first search algorithm can be implemented\nby using a stack in which the last object added is processed first. This process\nis known as last in, first out (LIFO). The first step is adding the root node to\nthe stack.\n\n. Is stack empty? If the stack is empty and no path has been returned in step 8 of\n\nthe algorithm, there is no path to the goal. If there are still nodes in the stack, the\nalgorithm can continue its search to find the goal.\n\n3. ReturnNo path to goal. This return is the one possible exit from the\nalgorithm if no path to the goal exists.\n\n4. Pop node from stack as current node. By pulling the next object from the stack and\nsetting it as the current node of interest, we can explore its possibilities.\n\n5. Is current node visited? If the current node has not been visited, it hasn\u2019t been\nexplored yet and can be processed now.\n\n6. Mark current node as visited. This step indicates that this node has been visited to\nprevent unnecessary repeat processing of it.\n\n7. Is goal reached? This step determines whether the current neighbor contains the\ngoal that the algorithm is searching for.\n\n8. Return path using current node. By referencing the parent of the current node,\nthen the parent of that node, and so on, the path from the goal to the root is\ndescribed. The root node will be a node without a parent.\n\n9. Current has next neighbor? If the current node has more possible moves to make\nin the maze, that move can be added to the stack to be processed. Otherwise, the\nalgorithm can jump to step 2, where the next object in the stack can be processed\nif the stack is not empty. The nature of the LIFO stack allows the algorithm to\nprocess all nodes to a leaf node depth before backtracking to visit other children\nof the root node.\n\n10. Set current node as parent of neighbor. Set the origin node as the parent of the\ncurrent neighbor. This step is important for tracing the path from the current\nneighbor to the root node. From a map perspective, the origin is the position that\nthe player moved from, and the current neighbor is the position that the player\nmoved to.\n\n11. Add neighbor to stack. The neighbor node is added to the stack for its children to\nbe explored later. Again, this stacking mechanism allows nodes to be processed to\nthe utmost depth before processing neighbors at shallow depths.\n\nFigures 2.22 and 2.23 explore how the LIFO stack is used to visit nodes in the order\ndesired by depth-first search. Notice that nodes get pushed onto and popped from the\nstack as the depths of the nodes visited progress. The term push describes adding objects\nto a stack, and the term pop describes removing the topmost object from the stack.\n\nSimilarly to\nbreadth-first\nsearch, visit\nthe first child\nof node A. This\nis B.\n\nSequence of\nprecessing the stack\n\nwa\n\nInstead of\nvisiting other\nchild nodes of A,\nthe first child\nof Bis visited \u2014\nin this case, F.\n\n[>|2 71\n\nAgain, the first\nchild of F is\nvisited. This is\nL.\n\n>[e[> IF]\n\nFigure 2.22 The sequence of tree processing using depth-first search (part 1)\n\nBecause Lisa\nleaf node (it\nhas ne children),\nthe algerithm\n\nbacktracks to\nvisit the next\n\nchild of F, which\n\nis M.\n\nBecause Misa\nleaf nede,the\nalgorithm\nbacktracks to\n\nvisit the next\nchild of B.\nBecause F has\nno unvisited\nchildren, this\nchild is G.\n\nFinally, because all\nchildren ef B\n\nhave been visited,\nthe algorithm\nbacktracks te the\nnext child ef A,\n\nwhich is C.\n\nRg\n\nSequence of\nprocessing the stack\n\nFigure 2.23 The sequence of tree processing using depth-first search (part 2)\n\nEXERCISE: DETERMINE THE PATH TO THE SOLUTION\nWhat would the order of visits be in depth-first search for the following tree?\n\nSOLUTION: DETERMINE THE PATH TO THE SOLUTION\n\nDepth-first search order:\n4,B,C,D,M,L,E,F,G,H,N,D,1,J,k, @, 0,P\n\nIt is important to understand that the order of children matters substantially when using\ndepth-first search, as the algorithm explores the first child until it finds leaf nodes before\nbacktracking.\n\nIn the maze example, the order of movement (north, south, east, and west) influences\nthe path to the goal that the algorithm finds. A change in order will result in a different\nsolution. The forks represented in figures 2.24 and 2.25 don\u2019t matter; what matters is the\norder of the movement choices in our maze example.\n\nDepth\n\nDepth\n\n<>40\n\nDepth\n\nx\nt\n\u00ab> i\nL\nt\n4,455\nweg a =f\ncevwoarune\n\nFigure 2.24 Maze movement tree generation using depth-first search\n\nDepth\n\na * GN\n\no\n\n10\n\n11\n\n12\n\nFigure 2.25 Nodes visited in the entire tree after depth-first search\n\nAlthough the depth-first search algorithm can be implemented with a recursive func-\ntion, we're looking at an implementation that is achieved with a stack to better repre-\nsent the order in which nodes are visited and processed. It is important to keep track\nof the visited points so that the same nodes do not get visited unnecessarily, creating\ncyclic loops:\n\nrun_dfs(maze, root_point, visited_points):\nlet s equal a new stack\nadd root_point tos\nwhile s is not empty\npop s and let current_point equal the returned point\nif current_point is not visited:\nmark current_point as visited\nif value at current_node is the goal:\nreturn path using current_point\n\nel\n\nadd available cells north, east, south, and west to a list neighbors\nfor each neighbor in neighbors:\n\nset neighbor parent as current_point\n\npush neighbor tos\n\nreturn \u201cNo path to goal\u201d\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.11,
                        "section_name": "Use cases for uninformed search algorithms",
                        "section_path": "./screenshots-images-2/chapter_2/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_11/56a21674-3aa9-40ae-8dba-8f4a70367fde.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Use cases for uninformed search algorithms\n\nUninformed search algorithms are versatile and useful in several real-world use cases,\nsuch as\n\n+ Finding paths between nodes in a network\u2014When two computers need to\ncommunicate over a network, the connection passes through many connected\ncomputers and devices. Search algorithms can be used to establish a path in that\nnetwork between two devices.\n\n+ Crawling web pages\u2014Web searches allow us to find information on the internet\nacross a vast number of web pages. To index these web pages, crawlers typically\nread the information on each page, as well as follow each link on that page\nrecursively. Search algorithms are useful for creating crawlers, metadata\nstructures, and relationships between content.\n\n+ Finding social network connections\u2014Social media applications contain many\npeople and their relationships. Bob may be friends with Alice, for example, but\nnot direct friends with John, so Bob and John are indirectly related via Alice. A\nsocial media application can suggest that Bob and John should become friends\nbecause they may know each other through their mutual friendship with Alice.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.12,
                        "section_name": "Optional: More about graph categories",
                        "section_path": "./screenshots-images-2/chapter_2/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_12/6e116870-ed4b-43b9-92c4-e61df68568d2.png",
                            "./screenshots-images-2/chapter_2/section_12/ed85b8b3-70b9-4e5f-96b5-53ad4dac89ca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optional: More about graph categories\n\nGraphs are useful for many computer science and mathematical problems, and due to\nthe nature of different types of graphs, different principles and algorithms may apply to\nspecific categories of graphs. A graph is categorized based on its overall structure, num-\nber of nodes, number of edges, and interconnectivity between nodes.\n\nThese categories of graphs are good to know about, as they are common and some-\ntimes referenced in search and other AI algorithms:\n\nUndirected graph\u2014No edges are directed. Relationships between two nodes are\nmutual. As with roads between cities, there are roads traveling in both directions.\n\nDirected graph\u2014Edges indicate direction. Relationships between two nodes are\nexplicit. As in a graph representing a child of a parent, the child cannot be the\nparent of its parent.\n\nDisconnected graph\u2014One or more nodes are not connected by any edges. As in a\ngraph representing physical contact between continents, some nodes are not\nconnected. Like continents, some are connected by land, and others are separated\nby oceans.\n\nAcyclic graph\u2014aA graph that contains no cycles. As with time as we know it, the\ngraph does not loop back to any point in the past (yet).\n\nComplete graph\u2014Every node is connected to every other node by an edge. As in\nthe lines of communication in a small team, everyone talks to everyone else to\ncollaborate.\n\nComplete bipartite graph\u2014aA vertex partition is a grouping of vertices. Given a\nvertex partition, every node from one partition is connected to every node of the\nother partition with edges. As at a cheese-tasting event, typically, every person\ntastes every type of cheese.\n\nWeighted graph\u2014A graph in which the edges between nodes have a weight. As in\nthe distance between cities, some cities are farther than others. The connections\n\u201cweigh\u201d more.\n\nIt is useful to understand the different types of graphs to best describe the problem and\nuse the most efficient algorithm for processing (figure 2.26). Some of these categories of\ngraphs are discussed in upcoming chapters, such as chapter 6 on ant colony optimization\nand chapter 8 on neural networks.\n\nUNDIRECTED\n\nNo edges are directed.\nRelationships between twe\nnedes are mutual.\n\nDIRECTED\n\nEdges indicate direction.\nRelationships between two\nnodes are explicit.\n\nDISC ONNECTED ACYCLIC\nOne er more nedes are not A graph that centains\nconnected by any edges. ne cycles.\n\n[2] [LP] [a [|\n[e] 4 | [\u00b0] E\n\nCOMPLETE COMPLETE BIPARTITE WEIGHTED\nEvery node Is connected Every node from one partition is A graph where the\nto every ether node by an connected te every nede of the edges between nedes\n\nedge. other partition. have a weight.\n\n<I\n\nD\n\nFigure 2.26 Types of graphs\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.13,
                        "section_name": "Optional: More ways to represent graphs",
                        "section_path": "./screenshots-images-2/chapter_2/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_13/76cb6f4b-aecb-425d-b2f2-c1b624303536.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optional: More ways to represent graphs\n\nDepending on the context, other encodings of graphs may be more efficient for processing\nor easier to work with, depending on the programming language and tools you're using.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.14,
                        "section_name": "Incidence matrix",
                        "section_path": "./screenshots-images-2/chapter_2/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_14/4eb9f627-3c92-4bfc-a60f-f0125bd2f7ad.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Incidence matrix\n\nAn incidence matrix uses a matrix in which the height is the number of nodes in the\ngraph and the width is the number of edges. Each row represents a node\u2019s relationship\nwith a specific edge. Ifa node is not connected by a specific edge, the value 0 is stored. If\na node is connected by a specific edge as the receiving node in the case of a directed\ngraph, the value -1 is stored. If a node is connected by a specific edge as an outgoing\nnode or connected in an undirected graph, the value 1 is stored. An incidence matrix\ncan be used to represent both directed and undirected graphs (figure 2.27).\n\ne1 e2 @3 04\n\na 1, @ 0]\nb 0, 2 o7},\nc 1, 1, |\nd \u00ae, 1, 1)\ne 0, 0 1] |\n\nFigure 2.27 Representing a graph as an incidence matrix\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.15,
                        "section_name": "Adjacency list",
                        "section_path": "./screenshots-images-2/chapter_2/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_15/2b6a13fc-e452-47a6-86eb-3ae5cb843a43.png",
                            "./screenshots-images-2/chapter_2/section_15/5b5c909f-4dda-4bb0-adfa-66562dfa9ddc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adjacency list\nAn adjacency list uses linked lists in which the size of the initial list is the number of\n\nnodes in the graph and each value represents the connected nodes for a specific node. An\nadjacency list can be used to represent both directed and undirected graphs (figure 2.28).\n\neo aoc pw\neeorrpr s/s\nees eric\neresr/|o\nFer eesla\nesress/s\n\n\u00bb\n\nD\n\n.\n\n@\n\nFigure 2.28 Representing a graph as an adjacency list\n\nGraphs are also interesting and useful data structures because they can easily be repre-\nsented as mathematical equations, which are the backing for all algorithms we use. You\n\ncan find more information about this topic throughout the book.\nSUMMARY OF SEARCH FUNDAMENTALS\n\nData structures are important to selve problems.\n\nSearch algorithms are useful in planning and finding solutions in some\n\nchanging envirenments.\n\nGraph and tree data structures are useful in Al.\n\nAamaworagnes A seneens\n\n= AY ma\n\nUninformed search is blind and can be computationally expensive. Using\nthe cerrect data structures helps.\n\nDepth-first search looks deep before looking wide. Breadth-first\nsearch looks wide before looking deep.\n\noaen\n\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 3,
                "chapter_name": "Intelligent\nsearch",
                "chapter_path": "./screenshots-images-2/chapter_3",
                "sections": [
                    {
                        "section_id": 3.1,
                        "section_name": "Defining heuristics:\nDesigning educated guesses",
                        "section_path": "./screenshots-images-2/chapter_3/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_1/bbe5e709-0906-45dd-b244-ca5b5c137df1.png",
                            "./screenshots-images-2/chapter_3/section_1/8fa63ed9-28b8-42c0-a99c-c90e2ee1d5cf.png",
                            "./screenshots-images-2/chapter_3/section_1/f7d4134b-4a18-4c77-8c1a-e777f520f0f7.png",
                            "./screenshots-images-2/chapter_3/section_1/35e651e8-2107-42f5-925f-9466d30cb8d4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Defining heuristics:\nDesigning educated guesses\n\nNow that we have an idea of how uninformed search algorithms work from\nchapter 2, we can explore how they can be improved by seeing more infor-\nmation about the problem. For this purpose, we use informed search.\nInformed search means that the algorithm has some context of the specific\nproblem being solved. Heuristics are a way to represent this context. Often\ndescribed as a rule of thumb, a heuristic is a rule or set of rules used to eval-\nuate a state. It can be used to define criteria that a state must satisfy or\n\nmeasure the performance of a specific state. A heuristic is used when a clear method of\nfinding an optimal solution is not possible. A heuristic can be interpreted as an educated\nguess in social terms and should be seen more as a guideline than as a scientific truth\nwith respect to the problem that is being solved.\n\nWhen you're ordering a pizza at a restaurant, for example, your heuristic of how good\nit is may be defined by the ingredients and type of base used. If you enjoy extra tomato\nsauce, extra cheese, mushrooms, and pineapple on a thick base with crunchy crust, a\npizza that includes more of these attributes will be more appealing to you and achieve\na better score for your heuristic. A pizza that contains fewer of those attributes will be\nless appealing to you and achieve a poorer score.\n\nAnother example is writing algorithms to solve a GPS routing problem. The heuristic\nmay be \u201cGood paths minimize time in traffic and minimize distance traveled\u201d or \u201cGood\npaths minimize toll fees and maximize good road conditions.\u201d A poor heuristic for a\nGPS routing program would minimize straight-line distance between two points. This\nheuristic might work for birds or planes, but in reality, we walk or drive; these methods\nof transport bind us to roads and paths between buildings and obstacles. Heuristics need\nto make sense for the context of use.\n\nTake the example of checking whether an uploaded audio clip is an audio clip in a\nlibrary of copyrighted content. Because audio clips are frequencies of sound, one way to\nachieve this goal is to search every time slice of the uploaded clip with every clip in the\nlibrary. This task will require an extreme amount of computation. A primitive start to\nbuilding a better search could be defining a heuristic that minimizes the difference of\ndistribution of frequencies between the two clips, as shown in figure 3.1. Notice that the\nfrequencies are identical apart from the time difference; they don\u2019t have differences in\ntheir frequency distributions. This solution may not be perfect, but it is a good start\ntoward a less-expensive algorithm.\n\nery 20K\n\nFigure 3.1 Comparison of two audio clips using frequency distribution\n\nHeuristics are context-specific, and a good heuristic can help optimize solutions sub-\nstantially. The maze scenario from chapter 2 will be adjusted to demonstrate the concept\nof creating heuristics by introducing an interesting dynamic. Instead of treating all\nmovements the same way and measuring better solutions purely by paths with fewer\nactions (shallow depth in the tree), movements in different directions now cost different\namounts to execute. There\u2019s been some strange shift in the gravity of our maze, and\nmoving north or south now costs five times as much as moving east or west (figure 3.2).\n\ni COST\n\nt\nrs J\n<\n>\n\nPF PP oO Oo\n\nFigure 3.2 Adjustments to the maze example: gravity\n\nIn the adjusted maze scenario, the factors influencing the best possible path to the goal\nare the number of actions taken and the sum of the cost for each action in a respec-\ntive path.\n\nIn figure 3.3, all possible paths in the tree are represented to highlight the options\navailable, indicating the costs of the respective actions. Again, this example demon-\nstrates the search space in the trivial maze scenario and does not often apply to real-life\nscenarios. The algorithm will be generating the tree as part of the search.\n\nDepth\n\na * @ WN\n\no\n\n10\n\n11\n\n12\n\nFigure 3.3 All possible movement options represented as a tree\n\nA heuristic for the maze problem can be defined as follows: \u201cGood paths minimize cost\nof movement and minimize total moves to reach the goal.\u201d This simple heuristic helps\nguide which nodes are visited because we are applying some domain knowledge to solve\nthe problem.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.2,
                        "section_name": "THOUGHT EXPERIMENT: GIVEN THE FOLLOWING SCENARIO,\nWHAT HEURISTIC CAN YOU IMAGINE?",
                        "section_path": "./screenshots-images-2/chapter_3/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_2/17738578-0127-4d9e-a367-d151fb0ff9c5.png",
                            "./screenshots-images-2/chapter_3/section_2/3adfc703-eabe-43a4-aa84-a67bd71caec6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "THOUGHT EXPERIMENT: GIVEN THE FOLLOWING SCENARIO,\nWHAT HEURISTIC CAN YOU IMAGINE?\n\nSeveral miners specialize in different types of mining, including diamond,\ngold, and platinum. All the miners are productive in any mine, but they mine\nfaster in mines that align with their specialties. Several mines that can con-\ntain diamonds, gold, and platinum are spread across an area, and depots\nappear at different distances between mines. If the problem is to distribute\nminers to maximize their efficiency and reduce travel time, what could a\nheuristic be?\n\nTHOUGHT EXPERIMENT: POSSIBLE SOLUTION\n\nA sensible heuristic would include assigning each miner to a mine of their\nspecialty and tasking them with traveling to the depot closest to that mine.\nThis can also be interpreted as minimizing assigning miners to mines that\nare not their specialty and minimizing the distance traveled to depots.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.3,
                        "section_name": "Informed search: Looking for solutions\nwith guidance",
                        "section_path": "./screenshots-images-2/chapter_3/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_3/d72d930f-d601-4717-8f45-524b63295544.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Informed search: Looking for solutions\nwith guidance\n\nInformed search, also known as heuristic search, is an algorithm that uses both breadth-\nfirst search and depth-first search approaches combined with some intelligence. The\nsearch is guided by heuristics, given some predefined knowledge of the problem at hand.\n\nWe can employ several informed search algorithms, depending on the nature of the\nproblem, including Greedy Search (also known as Best-first Search). The most popular\nand useful informed search algorithm, however, is A*.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.4,
                        "section_name": "A* search",
                        "section_path": "./screenshots-images-2/chapter_3/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_4/50c0761b-d181-422f-b524-9e24606ca53c.png",
                            "./screenshots-images-2/chapter_3/section_4/127c2aeb-daf8-4377-90af-602656314b82.png",
                            "./screenshots-images-2/chapter_3/section_4/bfa30c0d-ac73-496f-9e7e-72054dc5da1a.png",
                            "./screenshots-images-2/chapter_3/section_4/286286ac-7523-475a-bc0b-2909c1d4bc41.png",
                            "./screenshots-images-2/chapter_3/section_4/f67a4844-1518-42c9-af05-b29c7857f7e4.png",
                            "./screenshots-images-2/chapter_3/section_4/f753457a-0058-4714-ad84-0c69a61d2900.png",
                            "./screenshots-images-2/chapter_3/section_4/9fae9f92-2733-49fe-84f6-8fa7bcbf82ae.png",
                            "./screenshots-images-2/chapter_3/section_4/8184ae58-b4d4-4352-a8bd-fe689167b942.png",
                            "./screenshots-images-2/chapter_3/section_4/e323e568-7c6b-4702-8784-17207a8dc32e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A* search\n\nA* search is pronounced \u201cA star search.\u201d The A* algorithm usually improves performance\nby estimating heuristics to minimize the cost of the next node visited.\n\nTotal cost is calculated with two metrics: the total distance from the start node to the\ncurrent node and the estimated cost of moving to a specific node by using a heuristic.\nWhen we are attempting to minimize cost, a lower value indicates a better-performing\nsolution (figure 3.4).\n\n\u00a3(n)=g(n) + h(n)\n\ng(n): cost of the path from the start node\nto noden\n\nh(n): the cost from the heuristic function\nfor noden\n\n\u00a3(n): the cost of the path from the start\nnode to node n plus the cost from the\n\nheuristic function for noden\n\nFigure 3.4 The function for the A* search algorithm\n\nThe following example of processing is an abstract example of how a tree is visited using\nheuristics to guide the search. The focus is on the heuristic calculations for the differen\nnodes in the tree.\n\nBreadth-first search visits all nodes on each depth before moving to the next depth\nDepth-first search visits all nodes down to the final depth before traversing back to th\nroot and visiting the next path. A* search is different, in that it does not have a pre\ndefined pattern to follow; nodes are visited in the order based on their heuristic costs\nNote that the algorithm does not know the costs of all nodes up front. Costs are calcu.\nlated as the tree is explored or generated, and each node visited is added to a stack, whick\nmeans nodes that cost more than nodes already visited are ignored, saving computatior\ntime (figures 3.5, 3.6, and 3.7).\n\nGivena tree\nrepresenting\nnodes and their\nheuristic\nsceres, A* will\nvisit the first\nchild with the\nlowest cost. In\nthis case, itis\nC, with a cost\nof 2.\n\nSequence of\nprocessing the stack\n\nWhen two nodes cost the same, the node whose score was\ncalculated first is selected.\n\nBecause E alse has\na cost of 2 andis\n\na child of A, it will\nbe the next nede\nvisited.\n\n7(8] 2%\n\nThen A* will visit\nthe lowest-cost\nnede from\nchildren of A er\nchildren ef nedes\nit has already\nvisited.\n\n>lolm|=\n\u00bb\n\nFigure 3.5 The sequence of tree processing using A* search (part 1)\n\nThe next lowest-\ncost nede is K,a\nchild of E.\n\nSequence of\nprocessing the stack\n\nS\n\n>[o[m|-|>\nnN GO oO\n\nThe next lowest-\ncost node is H,a\nchild of C.\n\nnnoag\n\n>lo|m]-|x x\n\nA direct child of\nAis visited,\nbecause it has\nthe lowest cost\nof children of A\nand children of\nall other nodes\nvisited.\n\nNodes that cost more than the current lowest-cost\npath to the solution can be ignored, because paths\nto a solution via those nodes will be more expensive.\n\nNNO OO YW\n\n> e[m[-|[-]= o\n\nFigure 3.6 The sequence of tree processing using A* search (part 2)\n\nReturn\n\u201cNo path\nto geal.\u201d\n\nPop nede\nfrom stack\nas current\nnode.\n\nAdd reot\nnode te\nstack.\n\nSet current\n\nCalculate\nnede as\ngnber. neighber.\n\nReturn\npath using\ncurrent\nnede.\n\nAdd\nneighber\nto stack.\n\nFigure 3.7 Flow for the A* search algorithm\n\nLet\u2019s walk through the flow of the A* search algorithm:\n\n1. Add root node to stack. The A* search algorithm can be implemented with a stack\nin which the last object added is processed first (last-in, first-out, or LIFO). The\nfirst step is adding the root node to the stack.\n\n2. Is stack empty? If the stack is empty, and no path has been returned in step 8 of\nthe algorithm, there is no path to the goal. If there are still nodes in the queue, the\nalgorithm can continue its search.\n\n3. ReturnNo path to goal. This step is the one possible exit from the\nalgorithm if no path to the goal exists.\n\n4. Pop node from stack as current node. By pulling the next object from the stack and\nsetting it as the current node of interest, we can explore its possibilities.\n\n5. Is current node visited? If the current node has not been visited, it hasn't been\nexplored yet and can be processed now.\n\n6. Mark current node as visited. This step indicates that this node has been visited to\nprevent unnecessary repeat processing.\n\n7. Is goal reached? This step determines whether the current neighbor contains the\ngoal that the algorithm is searching for.\n\n8. Return path using current node. By referencing the parent of the current node,\nthen the parent of that node, and so on, the path from the goal to the root is\ndescribed. The root node will be a node without a parent.\n\n9. Current has next neighbor? If the current node has more possible moves to make\nin the maze example, that move can be added to be processed. Otherwise, the\nalgorithm can jump to step 2, in which the next object in the stack can be\nprocessed if it is not empty. The nature of the LIFO stack allows the algorithm to\nprocess all nodes to a leaf-node depth before backtracking to visit other children\nof the root node.\n\n10. Sort stack by cost ascending. When the stack is sorted by the cost of each node in\nthe stack ascending, the lowest-cost node is processed next, allowing the cheapest\nnode always to be visited.\n\n11. Set current node as parent of neighbor. Set the origin node as the parent of the\ncurrent neighbor. This step is important for tracing the path from the current\nneighbor to the root node. From a map perspective, the origin is the position that\nthe player moved from, and the current neighbor is the position that the player\nmoved to.\n\n12. Calculate cost for neighbor. The cost function is critical for guiding the A*\nalgorithm. The cost is calculated by summing the distance from the root node\nwith the heuristic score for the next move. More-intelligent heuristics will directly\ninfluence the A* algorithm for better performance.\n\n13. Add neighbor to stack. The neighbor node is added to the stack for its children to\nbe explored later. Again, this stacking mechanism allows nodes to be processed to\nthe utmost depth before processing neighbors at shallow depths.\n\nSimilar to depth-first search, the order of child nodes influences the path selected, but\nless drastically. If two nodes have the same cost, the first node is visited before the second\n(figures 3.8, 3.9, and 3.10).\n\n> bY\u00bb\nett.\n\nFigure 3.8 The sequence of tree processing using A* search (part 1)\n\n\noy\n\nge \u00ab eS >\n\nx\nny\n\nFigure 3.10 Nodes visited in the entire tree after A* search\n\nDepth\n\na Ff GDN\n\no\n\n10\n\n11\n\n12\n\nNotice that there are several paths to the goal, but the A* algorithm finds a path to the\ngoal while minimizing the cost to achieve it, with fewer moves and cheaper move costs\nbased on north and south moves being more expensive.\n\nThe A* algorithm uses a similar approach to the depth-first search algorithm but inten-\ntionally targets nodes that are cheaper to visit. A stack is used to process the nodes, but\nthe stack is ordered by cost ascending every time a new calculation happens. This order\nensures that the object popped from the stack is always the cheapest, because the cheap-\nest is first in the stack after ordering:\n\nrun_astar(maze, root_point, visited_points):\nlet s equal a new stack\nadd root_point tos\nwhile s is not empty\npop s and let current_point equal the returned point\nif current_point is not visited:\nmark current_point as visited\nif value at current_node is the goal:\nreturn path using current_point\nelse:\nadd available cells north, east, south, and west to a list neighbors\nfor each neighbor in neighbors:\nset neighbor parent as current_point\nset neighbor cost as calculate_cost(current_point, neighbor)\npush neighbor tos\nsort s by cost ascending\n\nreturn \u201cNo path to goal\u201d\n\nThe functions for calculating the cost are critical to the operation of A* search. The cost\nfunction provides the information for the algorithm to seek the cheapest path. In our\nadjusted maze example, a higher cost is associated with moving up or down. If there is a\nproblem with the cost function, the algorithm may not work.\n\nThe following two functions describe how cost is calculated. The distance from the\nroot node is added to the cost of the next movement. Based on our hypothetical example,\nthe cost of moving north or south influences the total cost of visiting that node:\n\ncalculate_cost(origin, target):\nlet distance_to_root equal length of path from origin to target\nlet cost_to_move equal get_move_cost (origin, target)\n\nreturn distance_to_root + cost_to_move\n\nmove_cost(origin, target):\nif target is north or south of origin:\nreturn 5\nelse:\n\nreturn 1\n\nUninformed search algorithms such as breadth-first search and depth-first search\nexplore every possibility exhaustively and result in the optimal solution. A* search is a\ngood approach when a sensible heuristic can be created to guide the search. It computes\nmore efficiently than uninformed search algorithms, because it ignores nodes that cost\nmore than nodes already visited. If the heuristic is flawed, however, and doesn\u2019t make\nsense for the problem and context, poor solutions will be found instead of optimal ones.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.5,
                        "section_name": "Use cases for informed search algorithms",
                        "section_path": "./screenshots-images-2/chapter_3/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_5/09b90f53-fc00-4d9e-b282-122c72e26f16.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Use cases for informed search algorithms\n\nInformed search algorithms are versatile and useful for several real-world use cases in\nwhich heuristics can be defined, such as the following:\n\n+ Path finding for autonomous game characters in video games\u2014Game developers\noften use this algorithm to control the movement of enemy units in a game in\nwhich the goal is to find the human player within an environment.\n\n+ Parsing paragraphs in natural language processing (NLP)\u2014The meaning of a\nparagraph can be broken into a composition of phrases, which can be broken\ninto a composition of words of different types (like nouns and verbs), creating a\ntree structure that can be evaluated. Informed search can be useful in extracting\nmeaning.\n\n+  Telecommunications network routing\u2014Guided search algorithms can be used to\nfind the shortest paths for network traffic in telecommunications networks to\nimprove performance. Servers/network nodes and connections can be\nrepresented as searchable graphs of nodes and edges.\n\n+ Single-player games and puzzles\u2014Informed search algorithms can be used to\nsolve single-player games and puzzles such as the Rubik\u2019s Cube, because each\nmove is a decision in a tree of possibilities until the goal state is found.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.6,
                        "section_name": "Adversarial search: Looking for solutions\nin a changing environment",
                        "section_path": "./screenshots-images-2/chapter_3/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_6/079eedec-e268-4edf-9b53-7b7d9259e9ac.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adversarial search: Looking for solutions\nin a changing environment\n\nThe search example of the maze game involves a single actor: the player. The environ-\nment is affected only by the single player; thus, that player generates all possibilities. The\ngoal until now was to maximize the benefit for the player: choosing paths to the goal\nwith the shortest distance and cost.\n\nAdversarial search is characterized by opposition or conflict. Adversarial problems\nrequire us to anticipate, understand, and counteract the actions of the opponent in pur-\nsuit of a goal. Examples of adversarial problems include two-player turn-based games\nsuch as Tic-Tac-Toe and Connect Four. The players take turns for the opportunity to\nchange the state of the environment of the game to their favor. A set of rules dictates how\nthe environment may be changed and what the winning and end states are.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.7,
                        "section_name": "A simple adversarial problem",
                        "section_path": "./screenshots-images-2/chapter_3/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_7/d9f80bc6-f88c-46cd-af99-0b61525c1a78.png",
                            "./screenshots-images-2/chapter_3/section_7/0c69a918-dde1-4b45-910d-6d6ac7c5786b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Asimple adversarial problem\n\nThis section uses the game of Connect Four to explore adversarial problems. Connect\nFour (figure 3.11) is a game consisting of a grid in which players take turns dropping\ntokens into a specific column. The tokens in a specific column pile up, and any player\nwho manages to create four adjacent sequences of their tokens\u2014vertically, horizontally,\nor diagonally\u2014wins. If the grid is full, with no winner, the game results in a draw.\n\nPlayer @\n\nchooses slot 0.\n\nstarts and\nchooses slot 4.\n\nAfter a number\nof turns, the\nboard may leek\nsomething like\n\nPlayer\nchooses slot 4\nand wins with\nfour vertical\n\nthis...an tekens.\ntime for\n\nplay.\n\nIn anether Player [<>]\n\nscenario, the\nbeard may leek\nlike this, andit\u2019s\ntime for \u00ae to\nplay.\n\nchooses slot 2\nand wins with\nfour horizental\ntokens.\n\nIn this scenario,\nplayer\n\nchooses slot 2\nand wins with\nfour diagonal\ntokens.\n\nFigure 3.11 The game of Connect Four\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.8,
                        "section_name": "Min-max search: Simulate actions and choose the best future",
                        "section_path": "./screenshots-images-2/chapter_3/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_8/90c634b5-c4b0-413b-870c-abcb081f45cf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Min-max search: Simulate actions and choose the best future\n\nMin-max search aims to build a tree of possible outcomes based on moves that each\nplayer could make and favor paths that are advantageous to the agent while avoiding\npaths that are favorable to the opponent. To do so, this type of search simulates possible\nmoves and scores the state based on a heuristic after making the respective move. Min-\nmax search attempts to discover as many states in the future as possible; but due to\nmemory and computation limitations, discovering the entire game tree may not be real-\nistic, so it searches to a specified depth. Min-max search simulates the turns taken by\neach player, so the depth specified is directly linked to the number of turns between\nboth players. A depth of 4, for example, means that each player has had 2 turns. Player A\nmakes a move, player B makes a move, player A makes another move, and Player B\nmakes another move.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.9,
                        "section_name": "Heuristics",
                        "section_path": "./screenshots-images-2/chapter_3/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_9/1683c34b-eb13-4c52-8dda-abec075f5247.png",
                            "./screenshots-images-2/chapter_3/section_9/874a5441-0355-4725-883d-d7055c407252.png",
                            "./screenshots-images-2/chapter_3/section_9/cde665f1-bba5-4e91-a8ba-bbc638d35454.png",
                            "./screenshots-images-2/chapter_3/section_9/37c905b2-0c05-477c-9227-259562dcc30f.png",
                            "./screenshots-images-2/chapter_3/section_9/f54835cf-c154-4552-9185-e51cba3b06f6.png",
                            "./screenshots-images-2/chapter_3/section_9/cbe88798-dbe3-478f-bd50-fe066d829f31.png",
                            "./screenshots-images-2/chapter_3/section_9/28a28ac9-9a83-49b5-95a6-fd1206b24525.png",
                            "./screenshots-images-2/chapter_3/section_9/44283524-da39-4620-b593-9fae9756ce91.png",
                            "./screenshots-images-2/chapter_3/section_9/a8ef9c14-3701-4db6-9215-c2abda2c62b9.png",
                            "./screenshots-images-2/chapter_3/section_9/b16626a0-629b-4df1-b02f-16e54c82844f.png",
                            "./screenshots-images-2/chapter_3/section_9/9c6782ea-1e52-41d4-8b7f-c934ce507afa.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Heuristics\nThe min-max algorithm uses a heuristic score to make decisions. This score is defined\nby a crafted heuristic and is not learned by the algorithm. If we have a specific game\nstate, every possible valid outcome of a move from that state will be a child node in the\ngame tree.\n\nAssume that we have a heuristic that provides a score in which positive numbers are\nbetter than negative numbers. By simulating every possible valid move, the min-max\nsearch algorithm tries to minimize making moves where the opponent will have an\nadvantage or a winning state and maximize making moves that give the agent an advan-\ntage or a winning state.\n\nFigure 3.12 illustrates a min-max search tree. In this figure, the leaf nodes are the only\nnodes where the heuristic score is calculated, since these states indicate a winner or a\ndraw. The other nodes in the tree indicate states that are in progress. Starting at the\ndepth where the heuristic is calculated and moving upward, either the child with the\nminimum score or the child with the maximum score is chosen, depending on whose\nturn is next in the future simulated states. Starting at the top, the agent attempts to max-\nimize its score; and after each alternating turn, the intention changes, because the aim is\nto maximize the score for the agent and minimize the score for the opponent.\n\nAgent's\ntorn\n\nOppenent\u2019s\ntorn\nAgent's\n\u2018torn\nCalculate the\nscores of nodes\nata specified\ndepth or of\nleaf nodes.\nAgont\u2019s\n\u2018torn\nOppenent\u2019s\ntorn\nChoose the\nAgent's child nedes with\n\" the maximum\nscere.\n\u2018Agent's\nturn\nChoose the\nOppenent\u2019s child nedes with\ntorn the minimum\nscore.\nAgent's\nturn\nChoese the\nAgent\u2019s child nedes with\ntorn the maximum\nscere.\nCopenent\u2019s\nAgent's\ntorn\n\nFigure 3.12 The sequence of tree processing using min-max search\n\nEXERCISE: WHAT VALUES WOULD PROPAGATE IN THE FOLLOWING MIN-MAX TREE?\n\n\nBecause the min-max search algorithm simulates possible outcomes, in games that offer\na multitude of choices, the game tree explodes, and it quickly becomes too computation-\nally expensive to explore the entire tree. In the simple example of Connect Four played\non a5 x 4 block board, the number of possibilities already makes exploring the entire\ngame tree on every turn inefficient (figure 3.13).\n\nFigure 3.13 The explosion of possibilities while searching the game tree\n\nTo use min-max search in the Connect Four example, the algorithm essentially makes\nall possible moves from a current game state; then it determines all possible moves from\neach of those states until it finds the path that is most favorable. Game states that result\nin a win for the agent return a score of 10, and states that result in a win for the opponent\nreturn a score of -10. Min-max search tries to maximize the positive score for the agent\n(figures 3.14 and 3.15).\n\nScore: 10 Score: -10\n\nFigure 3.14 Scoring for the agent versus scoring for the opponent\n\nReturn the\nJcurrent scere\nand last move.\n\n=a game oti\n= MIN er\n~current depth\n\nFer\nSet dest_nas the result of\nrunning this algerlthm\nrecursively with parameters:\n\nrere\n\noe\n\ncorrent depth. 2\n\n12)\n\nIf current\nmode is MAX\n\nayes\n\nNe\nIe current\n\nmede MAX\n?\n\n10\n\nSimulate by\n\napplying meve\n\nto game state\n\u2018game_n.\n\nIp beot_n\nlese than\nKnown bes\n\n\u2014\n\nSet dest\nknown scers,\na8 -09,\n\n}\u2014__\u2014\nSet dest\n\nknown score\nas +00.\n\no\u2014\u2014\u2014\n\nCopy current\ngame state\n\n\u2018a8 game_n.\n\nSet knewn\ndest as\nbest_n.\n\nFigure 3.15 Flow for the min-max search algorithm\n\nHas next\nvalid meve\n2\n\nAlthough the flow chart for the min-max search algorithm looks complex due to its size,\nit really isn\u2019t. The number of conditions that check whether the current state is to maxi-\nmize or minimize causes the chart to bloat.\n\nLet\u2019s walk through the flow of the min-max search algorithm:\n\n1. Given a game state, whether the current mode is minimization or maximization,\nand a current depth, the algorithm can start. It is important to understand\nthe inputs for the algorithm, as the min-max search algorithm is recursive. A\nrecursive algorithm calls itself in one or more of its steps. It is important\nfor a recursive algorithm to have an exit condition to prevent it from calling\n\nitself forever.\n\n10.\n\nll.\n\nIs current an end state or depth is 0? This condition determines whether the\ncurrent state of the game is a terminal state or whether the desired depth has been\nreached. A terminal state is one in which one of the players has won or the game\nis a draw. A score of 10 represents a win for the agent, and a score of -10\nrepresents a win for the opponent, and a score of 0 indicates a draw. A depth is\nspecified, because traversing the entire tree of possibilities to all end states is\ncomputationally expensive and will likely take too long on the average computer.\nBy specifying a depth, the algorithm can look a few turns into the future to\ndetermine whether a terminal state exists.\n\nReturn the current score and last move. The score for the current state is returned\nif the current state is a terminal game state or if the specified depth has been\nreached.\n\nIs current mode MAX? If the current iteration of the algorithm is in the maximize\nstate, it tries to maximize the score for the agent.\n\nSet best known score as +o. If the current mode is to minimize the score, the best\nscore is set to positive infinity, because we know that the scores returned by the\ngame states will always be less. In actual implementation, a really large number is\nused rather than infinity.\n\nSet best known score as -co, If the current mode is to maximize the score, the best\nscore is set to negative infinity, because we know that the scores returned by the\ngame states will always be more. In actual implementation, a really large negative\nnumber is used rather than infinity.\n\nGet all possible moves, given current game state. This step specifies a list of possible\nmoves that can be made, given the current game state. As the game progresses,\nnot all moves available at the start may be available anymore. In the Connect Four\nexample, a column may be filled; therefore, a move selecting that column is\ninvalid.\n\nHas next valid move? If any possible moves have not been simulated yet and there\nare no more valid moves to make, the algorithm short-circuits to returning the\nbest move in that instance of the function call.\n\nCopy current game state as game_n. A copy of the current game state is required to\nperform simulations of possible future moves on it.\n\nSimulate by applying move to game state game_n. This step applies the current\nmove of interest to the copied game state.\n\nSet best_n as the result of running this algorithm recursively. Here\u2019s where recursion\ncomes into play. best_n is a variable used to store the next best move, and we're\nmaking the algorithm explore future possibilities from this move.\n\n12. If current mode is MAX? When the recursive call returns a best candidate, this\ncondition determines whether the current mode is to maximize the score.\n\n13. Is best_n less than known best? This step determines whether the algorithm has\nfound a better score than one previously found if the mode is to maximize the\nscore.\n\n14. Is best_n greater than known best? This step determines whether the algorithm has\nfound a better score than one previously found if the mode is to minimize the\nscore.\n\n15. Set known best as best_n. If the new best score is found, set the known best as that\nscore.\n\nGiven the Connect Four example at a specific state, the min-max search algorithm gen-\nerates the tree shown in figure 3.16. From the start state, every possible move is explored.\nThen each move from that state is explored until a terminal state is found\u2014either the\nboard is full or a player has won.\n\nMax\n\nMax\n\nFigure 3.16 A representation of the possible states in a Connect Four game\n\nThe highlighted nodes in figure 3.17 are terminal state nodes in which draws are scored\nas 0, losses are scored as -10, and wins are scored as 10. Because the algorithm aims to\nmaximize its score, a positive number is required, whereas opponent wins are scored\nwith a negative number.\n\nMax\n\nMin\n\nMax\n\nFigure 3.17 The possible end states in a Connect Four game\n\nWhen these scores are known, the min-max algorithm starts at the lowest depth and\nchooses the node whose score is the minimum value (figure 3.18).\n\nMax\n\nMin\n\nMax\n\nMin\n\nFigure 3.18 The possible scores for end states in a Connect Four game (part 1)\n\nThen, at the next depth, the algorithm chooses the node whose score is the maximum\nvalue (figure 3.19).\n\nMax\n\nMin\n\ncc\n\nMin\n\nFigure 3.19 The possible scores for end states in a Connect Four game (part 2)\n\nFinally, at the next depth, nodes whose score is the minimum are chosen, and the root\nnode chooses the maximum of the options. By following the nodes and score selected\nand intuitively applying ourselves to the problem, we see that the algorithm selects a path\nto a draw to avoid a loss. If the algorithm selects the path to the win, there is a high\nlikelihood of a loss in the next turn. The algorithm assumes that the opponent will\nalways make the smartest move to maximize their chance of winning (figure 3.20).\n\nMax\n\nFigure 3.20 The possible scores for end states in a Connect Four game (part 3)\n\nThe simplified tree in figure 3.21 represents the outcome of the min-max search algo-\nrithm for the given game state example.\n\nMax\n\nMax\n\nFigure 3.21 Simplified game tree with min-max scoring\n\nThe min-max search algorithm is implemented to be a recursive function. The function\nis provided with the current state, desired depth to search, minimization or maximiza-\ntion mode, and last move. The algorithm terminates by returning the best move and\nscore for every child at every depth in the tree. Comparing the code with the flow chart\nin figure 3.15, we notice that the tedious conditions of checking whether the current\nmode is maximizing or minimizing are not as apparent. In the pseudocode, 1 or -1 rep-\nresents the intention to maximize or minimize, respectively. By using some clever logic,\nthe best score, conditions, and switching states can be done via the principle of negative\nmultiplication, in which a negative number multiplied by another negative number\nresults in a positive. So if -1 indicates the opponent\u2019s turn, multiplying it by -1 results in\n1, which indicates the agent\u2019s turn. Then, for the next turn, 1 multiplied by -1 results\nin -1 to indicate the opponent\u2019s turn again:\n\nminmax(state, depth, min_or_max, last_move):\nlet current score equal state.get_score\nif current_score is not equal to 0 or state.is_full or depth is equal to @:\nreturn new Move(last_move, current_score)\nlet best_score equal to min_or_max multiplied by -\u00ab\nlet best_move =-1\nfor each possible choice (@ to 4 in a 5x4 board) as move:\nlet neighbor equal to a copy of state\nexecute current move on neighbor\nlet best_neighbor equal minmax (neighbor, depth -1,min_or_max * -1, move)\nif (best_neighbor .score is greater than best_score and min_or_max is MAX)\nor (best_neighbor.score is less than best_score and min_or_max is MIN):\nlet best_score = best_neighbor.score\nlet best_move = best_neighbor move\n\nreturn new Move(best_move, best_score)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.1,
                        "section_name": "Alpha-beta pruning: Optimize by exploring the sensible paths only",
                        "section_path": "./screenshots-images-2/chapter_3/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_10/2893a8f9-0572-4a86-9a31-05d42e94c863.png",
                            "./screenshots-images-2/chapter_3/section_10/7f9b7384-30f2-4ce4-b11a-db22aed5a25f.png",
                            "./screenshots-images-2/chapter_3/section_10/23a06579-9dcb-4fc9-93f6-02b8116f2a8f.png",
                            "./screenshots-images-2/chapter_3/section_10/bea003fd-5711-4ab9-ab46-ca646c90c611.png",
                            "./screenshots-images-2/chapter_3/section_10/c1282d1b-d604-4676-9b12-4a8860d24231.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Alpha-beta pruning: Optimize by exploring the sensible paths only\n\nAlpha-beta pruning is a technique used with the min-max search algorithm to short-cir-\ncuit exploring areas of the game tree that are known to produce poor solutions. This\ntechnique optimizes the min-max search algorithm to save computation, because insig-\nnificant paths are ignored. Because we know how the Connect Four example game tree\n\nexplodes, we clearly see that ignoring more paths will improve performance significantly\n(figure 3.22).\n\nChoose the child\n\nAgent\u2019s nedes with the\nturn maximum score.\nCheose the child\nOppenent\u2019s nedes with the\nturn minimum scere.\nIgnore nedes that\nAgent's are greater than\n\nturn\n\nor equal to the\nminimum scere.\n\nthe\nmaximum score.\n\nFigure 3.22 An example of alpha-beta pruning\n\nThe alpha-beta pruning algorithm works by storing the best score for the maximizing\nplayer and the best score for the minimizing player as alpha and beta, respectively.\nInitially, alpha is set as -c, and beta is set as e\u2014the worst score for each player. If the\nbest score of the minimizing player is less than the best score of the maximizing\nplayer, it is logical that other child paths of the nodes already visited would not affect the\nbest score.\n\nFigure 3.23 illustrates the changes made in the min-max search flow to accommodate\nthe optimization of alpha-beta pruning. The highlighted blocks are the additional steps\nin the min-max search algorithm flow.\n\nReturn the Setbest\njourrent cere knewn sere\nfans (ast rave, 5-08\n\nYe\na 2 +\nGiver\nanne state Is te\nWiner erent an end Is current ee\ncurrent see state or ace MAX re 200 corvestiane\nthie\u2019 * rent oa\n?\nfi\n'\n| Beebe nas the rent of\n1 ronning ts alge coy curren |, Yes\nL___] recursively with parameters game state\nae games\n42 a\\ \u201cis\nye\n{it current \u00b0\u00b0 \u00abHi deasn Set known\n\na\n\nGet beta as\nbeste\n\n22\nIs aleha \\ Yoo\n(greater than\nor equal +e\nbeta\n\nFigure 3.23 Flow for the min-max search algorithm with alpha-beta pruning\n\nThe following steps are additions to the min-max search algorithm. These conditions\nallow termination of exploration of paths when the best score found will not change the\noutcome:\n\n16.\n\n17.\n\n18.\n\n19,\n\n20.\n\n21.\n22.\n\nIs current mode MAX? Again, determine whether the algorithm is currently\nattempting to maximize or minimize the score.\n\nIs best_n greater than or equal to alpha? If the current mode is to maximize the\nscore and the current best score is greater than or equal to alpha, no better scores\nare contained in that node's children, allowing the algorithm to ignore that node.\n\nSet alpha as best_n. Set the variable alpha as best_n.\n\nIs alpha greater than or equal to beta? The score is as good as other scores found,\nand the rest of the exploration of that node can be ignored by breaking.\n\nIs best_n less than or equal to beta? If the current mode is to minimize the score\nand the current best score is less than or equal to beta, no better scores are\ncontained in that node\u2019s children, allowing the algorithm to ignore that node.\n\nSet beta as best_n. Set the variable beta as best_n.\n\nIs alpha greater than or equal to beta? The score is as good as other scores found,\nand the rest of the exploration of that node can be ignored by breaking.\n\nThe pseudocode for achieving alpha-beta pruning is largely the same as the code for\nmin-max search, with the addition of keeping track of the alpha and beta values and\nmaintaining those values as the tree is traversed. Note that when minimum(min) is\nselected the variable min_or_max is -1, and when maximum(max) is selected, the vari-\nable min_or_max is 1:\n\nph:\n\nminmax_ab_pruning(state, depth,min_or_max, last_move\nlet current score equal state.get_score\nif current_score is not equal to 0 or state.is_full or depth is equal to @:\nreturn new Move(last_move, current_score)\nlet best_score equal to min_or_max multiplied by -~\nlet best_move =-1\nfor each possible choice (@ to 4 in a 5x4 board) as move:\nlet neighbor equal to a copy of state\nexecute current move on neighbor\nlet best_neighbor equal\nminmax (neighbor, depth -1, min_or_max * -1, move iferpna, betay]\nif (best_neighbor.score is greater than best_score and min_or_max is MAX)\nor (best_neighbor.score is less than best_score and min_or_max is MIN):\n\nlet best_score = best_neighbor.score\n\nlet best_move = best_neighbor move\nif best_score >= alpha:\n\nalpha = best_score\nif best_score <= beta:\n\nbeta = best_score\n\nif alpha >= beta:\n\nbreak\n\nreturn new Move(best_move, best_score)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.11,
                        "section_name": "Use cases for adversarial search algorithms",
                        "section_path": "./screenshots-images-2/chapter_3/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_11/97239316-b931-4733-8153-0bf59d22e866.png",
                            "./screenshots-images-2/chapter_3/section_11/754b0cbe-3167-4085-87f2-2adbe81a0594.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Use cases for adversarial search algorithms\n\nInformed search algorithms are versatile and useful in real-world use cases such as the\nfollowing:\n\nCreating game-playing agents for turn-based games with perfect information\u2014In\nsome games, two or more players act on the same environment. There have been\nsuccessful implementations of chess, checkers, and other classic games. Games\nwith perfect information are games that do not have hidden information or\nrandom chance involved.\n\nCreating game-playing agents for turn-based games with imperfect information\u2014\nUnknown future options exist in these games, including games like poker and\nScrabble.\n\nAdversarial search and ant colony optimization (ACO) for route optimization\u2014\nAdversarial search is used in combination with the ACO algorithm (discussed in\nchapter 6) to optimize package-delivery routes in cities.\n\nSUMMARY OF INTELLIGENT SEARCH\n\nInformed search gives algorithms seme intelligence.\n\nHeuristics can be tricky to think up, but a geod heuristic is power ful in\nfinding selutions efficiently.\n\nA* search uses heuristics and the distance from the reet te find\n\nsolutions eptimally.\n\n\u00a3(n)=g(n)+h(n)\n\nAdverserial search such as min-max is useful when something else\n\naffects the envirenment.\n\nSS a\n\n16\n\nAlpha-beta pruning helps eptimize the min-max algerithm by\n\n| tes sen\nBs\n=\n\n[wh\n\neliminating undesirable paths.\n\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 4,
                "chapter_name": "Evolutionary\nalgorithms",
                "chapter_path": "./screenshots-images-2/chapter_4",
                "sections": [
                    {
                        "section_id": 4.1,
                        "section_name": "What is evolution?",
                        "section_path": "./screenshots-images-2/chapter_4/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_1/163193ff-7461-4f14-838e-19248633009d.png",
                            "./screenshots-images-2/chapter_4/section_1/022b2d99-3e85-4a7a-98a6-f415bd64e477.png",
                            "./screenshots-images-2/chapter_4/section_1/064f0bc1-1779-4401-aff3-2852d29cb2d2.png",
                            "./screenshots-images-2/chapter_4/section_1/70ae5eff-8898-4d02-8768-1f37282b228e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is evolution?\n\nWhen we look at the world around us, we sometimes wonder how every-\nthing we see and interact with came to be. One way to explain this is the\ntheory of evolution. The theory of evolution suggests that the living organ-\nisms that we see today did not suddenly exist that way, but evolved through\nmillions of years of subtle changes, with each generation adapting to its\nenvironment. This implies that the physical and cognitive characteristics of\neach living organism are a result of best fitting to its environment for sur-\nvival. Evolution suggests that organisms evolve through reproduction by\nproducing children of mixed genes from their parents. Given the fitness of\n\nthese individuals in their environment, stronger individuals have a higher likelihood of\nsurvival.\n\nWe often make the mistake of thinking that evolution is a linear process, with clear\nchanges in successors. In reality, evolution is far more chaotic, with divergence in a spe-\ncies. A multitude of variants of a species are created through reproduction and mixing of\ngenes. Noticeable differences in a species could take thousands of years to manifest and\nbe realized only by comparing the average individual in each of those time points.\nFigure 4.1 depicts actual evolution versus the commonly mistaken version of the evolu-\n\ntion of humans.\nP \u00a9 |@\n\n\u201cE\u2014' &p\n\naie |!\n\n\u2122~ /\n\ni >|\n, as a Time\ns\u2014\n\nSe\n\nan\nTime\n&\nPerceived evolution Actual evolution\n\nFigure 4.1 The idea of linear human evolution vs. actual human evolution\n\nCharles Darwin proposed a theory of evolution that centers on natural selection. Natural\nselection is the concept that stronger members of a population are more likely to survive\ndue to being more fit for their environment, which means they reproduce more and,\nthus, carry traits that are beneficial to survival to future generations\u2014that could poten-\ntially perform better than their ancestors.\n\nA classic example of evolution for adaption is the peppered moth. The peppered moth\nwas originally light in color, which made for good camouflage against predators as the\nmoth could blend in with light-colored surfaces in its environment. Only around 2% of\nthe moth population was darker in color. After the Industrial Revolution, around 95%\nof the species were of the darker color variant. One explanation is that the lighter-colored\nmoths could not blend in with as many surfaces anymore because pollution had dark-\nened surfaces; thus lighter-colored moths were eaten more by predators because those\nmoths were more visible. The darker moths had a greater advantage in blending in with\n\nthe darker surfaces, so they survived longer and reproduced more, and their genetic\ninformation was more widely spread to successors.\n\nAmong the peppered moths, the attribute that changed on a high level was the color\nof the moth. This property didn\u2019t just magically switch, however. For the change to hap-\npen, genes in moths with the darker color had to be carried to successors.\n\nIn other examples of natural evolution, we may see dramatic changes in more than\nsimply color between different individuals, but in actuality, these changes are influenced\nby lower-level genetic differences over many generations (figure 4.2).\n\nVb] sm .\n|\n\n)\nDark meth WA\n\n(\n(MN\nHMI .\n\nTime\n\nFigure 4.2 The evolution of the peppered moth\n\nEvolution encompasses the idea that in a population of a species, pairs of organisms\nreproduce. The offspring are a combination of the parent\u2019s genes, but small changes are\nmade in that offspring through a process called mutation. Then the offspring become\npart of the population. Not all members of a population live on, however. As we know,\ndisease, injury, and other factors cause individuals to die. Individuals that are more\nadaptive to the environment around them are more likely to live on, a situation that gave\nrise to the term survival of the fittest. Based on Darwinian evolution theory, a population\nhas the following attributes:\n\n+ Variety\u2014Individuals in the population have different genetic traits.\n+ Hereditary\u2014A child inherits genetic properties from its parents.\n\n+ Selection\u2014A mechanism that measures the fitness of individuals. Stronger\nindividuals have the highest likelihood of survival (survival of the fittest).\n\nThese properties imply that the following things happen during the process of evolution\n(figure 4.3):\n\n+ Reproduction\u2014Usually, two individuals in the population reproduce to create\noffspring.\n\n* Crossover and mutation\u2014The offspring created through reproduction contain a\nmix of their parents\u2019 genes and have slight random changes in their genetic code.\n\nx &\n\nMutation\n\nFigure 4.3 A simple example of reproduction and mutation\n\nIn summary, evolution is a marvelous and chaotic system that produces variations of life\nforms, some of which are better than others for specific things in specific environments.\nThis theory also applies to evolutionary algorithms; learnings from biological evolution\nare harnessed for finding optimal solutions to practical problems by generating diverse\nsolutions and converging on better-performing ones over many generations.\n\nThis chapter and chapter 5 are dedicated to exploring evolutionary algorithms, which\nare powerful but underrated approaches to solving hard problems. Evolutionary algo-\nrithms can be used in isolation or in conjunction with constructs such as neural net-\nworks. Having a solid grasp of this concept opens many possibilities for solving different\nnovel problems.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.2,
                        "section_name": "Problems applicable to\nevolutionary algorithms",
                        "section_path": "./screenshots-images-2/chapter_4/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_2/7bad86ca-09b1-4508-a7eb-70a029bd350e.png",
                            "./screenshots-images-2/chapter_4/section_2/95d78573-8387-4dcb-be13-ed7d94ab6c11.png",
                            "./screenshots-images-2/chapter_4/section_2/d49cacfc-2ed2-49a9-a229-db069aaa4f12.png",
                            "./screenshots-images-2/chapter_4/section_2/8a43fd98-1ee9-4325-95e1-30c3997afad7.png",
                            "./screenshots-images-2/chapter_4/section_2/71413291-9651-4118-80b2-ff712c1ea52b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Problems applicable to\nevolutionary algorithms\n\nEvolutionary algorithms aren\u2019t applicable to solving all problems, but they are powerful\nfor solving optimization problems in which the solution consists of a large number of\npermutations or choices. These problems typically consist of many valid solutions, with\nsome being more optimal than others.\n\nConsider the Knapsack Problem, a classic problem used in computer science to explore\nhow algorithms work and how efficient they are. In the Knapsack Problem, a knapsack\nhas a specific maximum weight that it can hold. Several items are available to be stored\nin the knapsack, and each item has a different weight and value. The goal is to fit as many\nitems into the knapsack as possible so that the total value is maximized and the total\nweight does not exceed the knapsack\u2019s limit. The physical size and dimensions of the\nitems are ignored in the simplest variation of the problem (figure 4.4).\n\n4\n3kg\n$4\n\nCapacity limit : 9kg\n\n2\n4\n4kg (<>) akg\n6\n8\n\nFkg\n$F\n\n$5 $1\n\n5kg\n$4\n\n4kg\n$3\n\n2kg Xl 3kg\n\n$5 $1\n\nFigure 4.4 A simple Knapsack Problem example\n\nAs a trivial example, given the specification of the problem in table 4.1, a knapsack can\nhold a total weight capacity of 9 kg, and it could contain any of the eight items of varying\n\nweight and value.\n\nTable 4.1 Knapsack weight capacity: 9 kg\n\nItem ID\n\nItem name\n\nWeight (kg)\n\nValue ($)\n\n1\n\nPearls\n\n2\n3\n4\nS\n6\n7\n8\n\nGold\n\nThis problem has 255 possible solutions, including the following (figure 4.5):\n\n+ Solution 1\u2014Include Item 1, Item 4, and Item 6. The total weight is 8 kg, and the\n\ntotal value is $8.\n\n* Solution 2\u2014Include Item 1, Item 3, and Item 7. The total weight is 9 kg, and the\n\ntotal value is $14.\n\n+ Solution 3\u2014Include Item 2, Item 3, and Item 6. The total weight is 15 kg, which\nexceeds the knapsack\u2019s capacity.\n\nFigure 4.5 The optimal solution for the simple Knapsack Problem example\n\nClearly, the solution with the most value is Solution 2. Don\u2019t concern yourself too much\nabout how the number of possibilities is calculated, but understand that the possibilities\nexplode as the number of potential items increases.\n\nAlthough this trivial example can be solved by hand, the Knapsack Problem could\nhave varying weight constraints, a varying number of items, and varying weights and\nvalues for each item, making it impossible to solve by hand as the variables grow larger.\nIt will also be computationally expensive to try to brute-force every combination of\nitems when the variables grow; thus, we look for algorithms that are efficient at finding\na desirable solution.\n\nNote that we qualify the best solution we can find as a desirable solution rather than\nthe optimal solution. Although some algorithms attempt to find the one true optimal\nsolution to the Knapsack Problem, an evolutionary algorithm attempts to find the opti-\nmal solution but is not guaranteed to find it. The algorithm will find a solution that is\nacceptable for the use case, however\u2014a subjective opinion of what an acceptable solu-\ntion is, based on the problem. For a mission-critical health system, for example, a\n\u201cgood enough\u201d solution may not cut it; but for a song-recommender system, it may be\nacceptable.\n\nNow consider the larger dataset (yes, a giant knapsack) in table 4.2, in which the num-\nber of items and varying weights and values makes the problem difficult to solve by\nhand. By understanding the complexity of this dataset, you can easily see why many\ncomputer science algorithms are measured by their performance in solving such prob-\nlems. Performance is defined as how well a specific solution solves a problem, not neces-\nsarily computational performance. In the Knapsack Problem, a solution that yields a\nhigher total value would be better-performing. Evolutionary algorithms provide one\nmethod of finding solutions to the Knapsack Problem.\n\nTable 4.2 Knapsack capacity: 6,404,180 kg\n\nItem ID Item name Weight (kg)\n\nBronze coin 225,790\n\neC\na\na\n\nKnife 323,618\n\nValue ($)\n\n471,010\n\n579,152\n\n902,698\n\n1,686,515\n\n1,056,157\n677,562\n\na sword 382,846\n\na\na\na\n\nVenom potion 730,061\n\n833,132\n\n376,418\n\n1,253,986\n\n1,853,562\n\n1,320,297\n\n1,301,637\n\n859,835\n\n1,677,534\n\nCrossbow 952,360 2,068,204\nYesteryear book 926,023 1,746,556\nZinc cup 978,724 2,100,851\n\nOne way to solve this problem is to use a brute-force approach. This approach involves\ncalculating every possible combination of items and determining the value of each com-\nbination that satisfies the knapsack\u2019s weight constraint until the best solution is\n\nencountered.\n\nFigure 4.6 shows some benchmark analytics for the brute-force approach. Note that\nthe computation is based on the hardware of an average personal computer.\n\nCombinations | 2426 = 67,108,864\nIterations | 2426 = 67,108,864\nAccuracy 100%\nCompute time ~# minutes\n\nFigure 4.6 Performance analytics of brute-forcing the Knapsack Problem\n\nKeep the Knapsack Problem in mind, as it will be used throughout this chapter as we\nattempt to understand, design, and develop a genetic algorithm to find acceptable solu-\ntions to this problem.\n\nNOTE A note about the term performance: From the perspective of an indi-\nvidual solution, performance is how well the solution solves the problem.\nFrom the perspective of the algorithm, performance may be how well a spe-\ncific configuration does in finding a solution. Finally, performance may mean\ncomputational cycles. Bear in mind that this term is used differently based on\nthe context.\n\nThe thinking behind using a genetic algorithm to solve the Knapsack Problem can be\napplied to a range of practical problems. If a logistics company wants to optimize the\npacking of trucks based on their destinations, for example, a genetic algorithm would be\nuseful. If that same company wanted to find the shortest route between several destina-\ntions, a genetic algorithm would be useful as well. If a factory refined items into raw\nmaterial via a conveyor-belt system, and the order of the items influenced productivity,\na genetic algorithm would be useful in determining that order.\n\nWhen we dive into the thinking, approach, and life cycle of the genetic algorithm, it\nshould become clear where this powerful algorithm can be applied, and perhaps you will\nthink of other uses in your work. It is important to keep in mind that a genetic algorithm\nis stochastic, which means that the output of the algorithm is likely to be different each\ntime it is run.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.3,
                        "section_name": "Genetic algorithm: Life cycle",
                        "section_path": "./screenshots-images-2/chapter_4/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_3/116f3459-cc70-444f-860b-74e1e7631e7d.png",
                            "./screenshots-images-2/chapter_4/section_3/5634b3a5-9135-4199-b8d6-5144096ec9b4.png",
                            "./screenshots-images-2/chapter_4/section_3/cc1d6be7-a30e-46a6-bfff-7c580db38172.png",
                            "./screenshots-images-2/chapter_4/section_3/72b25f31-e328-450f-91f3-e982869da285.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Genetic algorithm: Life cycle\n\nThe genetic algorithm is a specific algorithm in the family of evolutionary algorithms.\nEach algorithm works on the same premise of evolution but has small tweaks in the dif-\nferent parts of the life cycle to cater to different problems. We explore some of these\nparameters in chapter 5.\n\nGenetic algorithms are used to evaluate large search spaces for a good solution. It is\nimportant to note that a genetic algorithm is not guaranteed to find the absolute best\nsolution; it attempts to find the global best while avoiding local best solutions.\n\nA global best is the best possible solution, and a local best is a solution that is\nless optimal. Figure 4.7 represents the possible best solutions if the solution must be\nminimized\u2014that is, the smaller the value, the better. If the goal was to maximize a solu-\ntion, the larger the value, the better. Optimization algorithms like genetic algorithms\naim to incrementally find local best solutions in search of the global best solution.\n\nLocal best\n\nLocal best\nLocal best\n\nGlobal best\n\nFigure 4.7 Local best vs. global best\n\nCareful attention is needed when configuring the parameters of the algorithm so that it\nstrives for diversity in solutions at the start and gradually gravitates toward better solu-\ntions through each generation. At the start, potential solutions should vary widely in\nindividual genetic attributes. Without divergence at the start, the risk of getting stuck\nina local best increases (figure 4.8).\n\nGenerations\n\nFigure 4.8 Diversity to convergence\n\nThe configuration for a genetic algorithm is based on the problem space. Each problem\nhas a unique context and a different domain in which data is represented, and solutions\nare evaluated differently.\n\nThe general life cycle of a genetic algorithm is as follows:\n\nCreating a population\u2014Creating a random population of potential solutions.\n\nMeasuring the fitness of individuals in the population\u2014Determining how good a\nspecific solution is. This task is accomplished by using a fitness function that\nscores solutions to determine how good they are.\n\nSelecting parents based on their fitness\u2014Selecting pairs of parents that will\nreproduce offspring.\n\nReproducing individuals from parents\u2014Creating offspring from their parents by\nmixing genetic information and applying slight mutations to the offspring.\n\nPopulating the next generation\u2014Selecting individuals and offspring from the\npopulation that will survive to the next generation.\n\nSeveral steps are involved in implementing a genetic algorithm. These steps encompass\nthe stages of the algorithm life cycle (figure 4.9).\n\nSet\nalgerithm\nparameters]\n\nEncode\nsolution\nspace.\n\nMeasure\nfitness of\nindividuals,\n\nCreate\ninitial\npopulation.\n\nPopulate Measure\n\nSelect Reproduce\nnext fitness of\nparents. offspring. generation, individuals.\n\nFigure 4.9 Genetic algorithm life cycle\n\nWith the Knapsack Problem in mind, how would we use a genetic algorithm to find\nsolutions to the problem? The next section dives into the process.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.4,
                        "section_name": "Encoding the solution spaces",
                        "section_path": "./screenshots-images-2/chapter_4/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_4/b5c0acad-e58c-44ec-9612-130930907773.png",
                            "./screenshots-images-2/chapter_4/section_4/ad96338b-615b-4f7f-9c63-253e497c6f4e.png",
                            "./screenshots-images-2/chapter_4/section_4/7833e344-4802-4b1b-ba4e-3a874e7545db.png",
                            "./screenshots-images-2/chapter_4/section_4/2ab4bcc5-61fa-4eb0-ac05-4a5dbf5b9c81.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Encoding the solution spaces\n\nWhen we use a genetic algorithm, it is paramount to do the encoding step correctly,\nwhich requires careful design of the representation of possible states. The state is a data\nstructure with specific rules that represents possible solutions to a problem. Furthermore,\na collection of states forms a population (figure 4.10).\n\nSet\nalgorithm\nparameters.\n\nEncede\nsolution\nspace.\n|. The First step in the genetic\nalgorithm lifecycle is encoding the\nsolution space.\n\nMeasure\nfitness of\nindividuals.\n\nCreate\ninitial\npopulation.\n\n3\nPopulate\nnext\ngeneration.\n\nMeasure\nfitness of\nindividuals.\n\nSelect\nparents.\n\nRepreduce\noffspring.\n\nFigure 4.10 Encode the solution.\n\nTerminology\n\nWith respect to evolutionary algorithms, an individual candidate solution is called\na chromosome. A chromosome is made up of genes. The gene is the logical type for\nthe unit, and the allele is the actual value stored in that unit. A genotype is a repre-\n\nsentation of a solution, and a phenotype is a unique solution itself. Each chromo-\nsome always has the same number of genes. A collection of chromosomes forms a\npopulation (figure 4.11).\n\n\nChromosome\n\n_\u2014\u2014\u2014\n\nGene\n\nfo [a [a Jofofafifo|\nAllele A\n\n[a Jo [i [ofofijolo|\n\n(o [a a [ofa fifolo|\n\n(a Jo [a JoJo fii fo |\n\nFigure 4.11 Terminology of the data structures representing a population of solutions\n\nPopulation\n\nIn the Knapsack Problem, several items can be placed in the knapsack. A simple way to\ndescribe a possible solution that contains some items but not others is binary encod-\ning (figure 4.12). Binary encoding represents excluded items with 0s and included items\nwith 1s. If the value at gene index 3 is 1, for example, that item is marked to be included.\nThe complete binary string is always the same size: the number of items available for\nselection. Several alternative encoding schemes exist, however, and are described in\nchapter 5.\n\n3kg Kk #kg\n$4 $7\n\n1kg\n$1\n\nCapacity limit: Ykg\n\n4kg\n$3\n\n3kg\n$1\n\n12345678\n\n[afofifofefo]ifo] item inctuded?\n\n22zsegere2es\nBOSS <SaO\n2 Qe Ge 0\u201d\na) \u2018\n\nFigure 4.12 Binary-encoding the Knapsack Problem\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.5,
                        "section_name": "Binary encoding: Representing possible solutions with zeros and ones",
                        "section_path": "./screenshots-images-2/chapter_4/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_5/c2146e66-0465-4798-86a0-bc4dadcdee04.png",
                            "./screenshots-images-2/chapter_4/section_5/b92cc603-8277-413e-95c2-10d3e0fdebbe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Binary encoding: Representing possible solutions with zeros and ones\n\nBinary encoding represents a gene in terms of 0 or 1, so a chromosome is represented by\na string of binary bits. Binary encoding can be used in versatile ways to express the pres-\nence of a specific element or even encoding numeric values as binary numbers. The\nadvantage of binary encoding is that it is usually more performant due to the use of\nprimitive types. Using binary encoding places less demand on working memory, and\ndepending on the language used, binary operations are computationally faster. But crit-\nical thought must be used to ensure that the encoding makes sense for the respective\nproblem and represents potential solutions well; otherwise, the algorithm may perform\npoorly (figure 4.13).\n\n26\n\nte 12 12\n\n\u2018\nlo]\n\nSSSCSOHFSOSKYYHSOVNOOHL OY OESC+ BYE\n= = = c LL @ \u00b0 c ou \u00b0\u00b0\nSEs 288825 SESESSESEISsSH335\nLeOQ oF fa FS FZUSES aeegaay\n#e5ogzt2s 3 Seb SERS SECV BLE\ncy Sseort FIR oP Oz SLES ESL ON\nsBpLEPO s*c 28\u00a7ovdag2 3,586\n2 S e - \u00a7s & Exes \u00b0 rg\no ge 9 g \u00b0\u00b0 C5q S=KS\neo sa \\ + 2 > es\na> @ & y\na co ~\na\n\nFigure 4.13 Binary-encoding the larger dataset for the Knapsack Problem\n\nGiven the Knapsack Problem with a dataset that consists of 26 items of varying weight\nand value, a binary string can be used to represent the inclusion of each item. The result\nis a 26-character string in which for each index, 0 means that the respective item is\nexcluded and 1 means that the respective item is included.\n\nOther encoding schemes\u2014including real-value encoding, order encoding, and tree\nencoding\u2014are discussed in chapter 5.\n\nEXERCISE: WHAT IS A POSSIBLE ENCODING FOR THE FOLLOWING PROBLEM?\n\nSuppose we have the following sentence and want to find which words canbe\nexcluded or Included te maintain a meaningful phrase using a genetic algorithm:\n\nTHE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\n\nIncerrect phrases\n\nTHE BROWN JUMPS OVER\nQUICK FOX OVER THE\nTHE FOX THE LAZY\nCorrect phrases\nTHE QUICK FOX\nQUICK FOX JUMPS\nTHE BROWN FOX DOG\nTHE BROWN LAZY DOG\nTHE QUICK DOG\nQUICK OVER THE DOG\nTHE QUICK LAZY DOG *Punctuatien is excluded\n\nSOLUTION: WHAT IS A POSSIBLE ENCODING FOR THE FOLLOWING PROBLEM?\n\nBecause the number of possible words is always the same, and the words are\nalways in the same position, binary encoding can be used to describe which\nwords are included and which are excluded. The chromosome consists of 9\ngenes, each gene indicating a word in the phrase.\n\nTHE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\n\nCT Tt TT tT TT TT TT TI\n\nTHE BROWN JUMPS OVER\n{ \u00b0 | |_\u00a2 | pe PTef |\nTHE BROWN LAZY DOG\n\nfs Pe Ti: TefePTeTe TT: I\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.6,
                        "section_name": "Creating a population of solutions",
                        "section_path": "./screenshots-images-2/chapter_4/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_6/09426eca-219b-4a8d-acfb-9036f1bce09c.png",
                            "./screenshots-images-2/chapter_4/section_6/fd1ab393-e86d-4625-af69-78cd0772976a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Creating a population of solutions\n\nIn the beginning, the population was created. The first step in a genetic algorithm is\ninitializing random potential solutions to the problem at hand. In the process of initial-\nizing the population, although the chromosomes are generated randomly, the con-\nstraints of the problem must be taken into consideration, and the potential solutions\nshould be valid or assigned a terrible fitness score if they violate the constraints. Each\nindividual in the population may not solve the problem well, but the solution is valid. As\nmentioned in the earlier example of packing items into a knapsack, a solution that spec-\nifies packing the same item more than once should be an invalid solution and should not\nform part of the population of potential solutions (figure 4.14).\n\n1 2\n\n3. The next concrete step in the genetic\nalgorithm lifecycle is creating an initial\npopulation.\n\nGenerate some possible solutions!\n\nSet\nalgerithm\nparameters.\n\nEncede\nsolution\nspace.\n\nSetting the algorithm parameters does\nhappen before this step, but we will get\nto this a bit later once a general\nunderstanding of the life cycle has been\nestablished.\n\nMeasure\nfitness of\nindividuals.\n\nCreate\ninitial\npopulation.\n\nPopulate\nnext\ngeneration.\n\nMeasure\nfitness of\nindividuals.\n\nSelect\nparents.\n\nReproduce\noffspring.\n\nFigure 4.14 Create an initial population.\n\nGiven how the Knapsack Problem\u2019s solution state is represented, this implementation\nrandomly decides whether each item should be included in the bag. That said, only solu-\ntions that satisfy the weight-limit constraint should be considered. The problem with\nsimply moving from left to right and randomly choosing whether the item is included is\nthat it creates a bias toward the items on the left end of the chromosome. Similarly, if we\nstart from the right, we will be biased toward items on the right. One possible way to get\n\naround this is to generate an entire individual with random genes and then determine\nwhether the solution is valid and does not violate any constraints. Assigning a terrible\nscore to invalid solutions can solve this problem (figure 4.15).\n\nEbb Teleh fofofs fi fofs]: Jofofofof+fefo]fofi fz fof: ]\n1 1 4 i\ne\n\neff fefot pots fot fof:fifofofof: fof: fofofo}: fo fe fa\n\nPopulation size\n\nFigure 4.15 An example of a population of solutions\n\nTo generate an initial population of possible solutions, an empty array is created to hold\nthe individuals. Then, for each individual in the population, an empty array is created to\n\nhold the genes of the individual. Each gene is randomly set to 1 or 0, indicating whether\nthe item at that gene index is included:\n\ngenerate_initial_population (population_size, individual_size)\nlet population be an empty array\nfor individual in range \u00a9 to population size\nlet current_individual be an empty array\nfor gene in range 0 to individual size\nlet random_gene be @ or 1 randomly\nappend random_gene to current_individual\nappend current_individual to population\n\nreturn population\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.7,
                        "section_name": "Measuring fitness of individuals\nin a population",
                        "section_path": "./screenshots-images-2/chapter_4/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_7/320e9c46-cceb-4a55-bc30-1f678fe288c1.png",
                            "./screenshots-images-2/chapter_4/section_7/3dd54a0e-112d-4c5a-a1ea-dfd3907803f4.png",
                            "./screenshots-images-2/chapter_4/section_7/1ede6ecd-27a2-4e5c-9784-cc7a976d782b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Measuring fitness of individuals\nin a population\n\nWhen a population has been created, the fitness of each individual in the population\nneeds to be determined. Fitness defines how well a solution performs. The fitness func-\ntion is critical to the life cycle of a genetic algorithm. If the fitness of the individuals is\nmeasured incorrectly or in a way that does not attempt to strive for the optimal solution,\nthe selection process for parents of new individuals and new generations will be influ-\nenced; the algorithm will be flawed and cannot strive to find the best possible solution.\n\nFitness functions are similar to the heuristics that we explored in chapter 3. They are\nguidelines for finding good solutions (figure 4.16).\n\n1 2\n\nSet\nalgerithm\nparameters.\n\nEncode\nsolution\nspace.\n\n4. The next step is measuring the Fitness\noF individuals in the population.\n\nWhich solutions are performing well?\n\nMeasure\nfitness of\nindividuals.\n\nCreate\ninitial\npopulation.\n\nPopulate\nnext\ngeneration.\n\nMeasure\nfitness of\nindividuals.\n\nSelect\nparents.\n\nRepreduce\noffspring.\n\nFigure 4.16 Measure the fitness of individuals.\n\nIn our example, the solution attempts to maximize the value of the items in the knapsack\nwhile respecting the weight-limit constraints. The fitness function measures the total\nvalue of the items in the knapsack for each individual. The result is that individuals with\nhigher total values are more fit. Note that an invalid individual appears in figure 4.17, to\nhighlight that its fitness score would result in 0\u2014a terrible score, because it exceeds the\nweight capacity for this instance of the problem, which is 6,404,180.\n\nA 11,393,360\n8 PPEEPh ef fefofolifofefof: fifo]: Jo] ofofi JoJo fo] 10,866,684\n\u00a2 0 (overweigns\nD 10,715,475\n\n15 million\n\nFigure 4.17 Measuring the fitness of individuals\n\nDepending on the problem being solved, the result of the fitness function may be\nrequired to be minimized or maximized. In the Knapsack Problem, the contents of the\nknapsack can be maximized within constraints, or the empty space in the knapsack\ncould be minimized. The approach depends on the interpretation of the problem.\n\nTo calculate the fitness of an individual in the Knapsack Problem, the sums of the values\nof each item that the respective individual includes must be determined. This task is\naccomplished by setting the total value to 0 and then iterating over each gene to deter-\nmine whether the item it represents is included. If the item is included, the value of the\nitem represented by that gene is added to the total value. Similarly, the total weight is\ncalculated to ensure that the solution is valid. The concepts of calculating fitness and\nchecking constraints can be split for clearer separation of concerns:\n\nealculate_individual_fitne:\n\n(individual,\nknapsack_items,\nknapsack_max_weight)\nlet total_weight equal 0\nlet total_value equal @\nfor gene_index in range @ to length of individual\nlet current_bit equal individual(gene_index]\nif current bit equals1\nadd weight of knapsack_items(gene_index] to total_weight\nadd value of knapsack_items(gene_index]) to total value\nif total_weight is greater than knapsack_max_weight\nreturn value as 0 since it exceeds the weight constraint\n\nreturn total_value as individual fitness\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.8,
                        "section_name": "Selecting parents based on their fitness",
                        "section_path": "./screenshots-images-2/chapter_4/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_8/986308e6-8148-4b1a-b105-c1901d2fc1fa.png",
                            "./screenshots-images-2/chapter_4/section_8/4432bf1d-aba3-4e70-9859-abb8fe6938f9.png",
                            "./screenshots-images-2/chapter_4/section_8/477f9f12-33de-4b83-ab82-5979cb36a60e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Selecting parents based on their fitness\n\nThe next step in a genetic algorithm is selecting parents that will produce new individu-\nals. In Darwinian theory, the individuals that are more fit have a higher likelihood of\nreproduction than others because they typically live longer. Furthermore, these individ-\nuals contain desirable attributes for inheritance due to their superior performance in\ntheir environment. That said, some individuals are likely to reproduce even if they are\nnot the fittest in the entire group, and these individuals may contain strong traits even\nthough they are not strong in their entirety.\n\nEach individual has a calculated fitness that is used to determine the probability of it\nbeing selected to be a parent to a new individual. This attribute makes the genetic algo-\nrithm stochastic in nature (figure 4.18).\n\nSet\nalgerithm\nparameters.\n\nEncede\nsolution\nspace.\nS. The next step is selecting parents thot\n\nwill reproduce new individuals.\n\nWho will be the lucky candidates that\nwill spread their genes?\n\nMeasure\nfitness of\nindividuals.\n\nCreate\ninitial\npopulation.\n\nPopulate\nnext\ngeneration.\n\nMeasure\nfitness of\nindividuals.\n\nRepreduce\noffspring.\n\nFigure 4.18 Select parents.\n\nA popular technique in choosing parents based on their fitness is roulette-wheel selec-\ntion. This strategy gives different individuals portions of a wheel based on their fitness.\nThe wheel is \u201cspun,\u201d and an individual is selected. Higher fitness gives an individual a\nlarger slice of the wheel. This process is repeated until the desired number of parents is\nreached.\n\nBy calculating the probabilities of 16 individuals of varying fitness, the wheel allocates\na slice to each. Because many individuals perform similarly, there are many slices of\nsimilar size (figure 4.19).\n\nGlee Telots Ele Te Tol ToT ool [sTof of Jefe fafa)\n\nBaooonoos 12,965,165\n(ee Ts J: Jol Tz Jolt feToT2 [2 Je ToTolo]2 Teli [ofa Jz ]oJofo} 12,244,273\nTEE\n\nDOB EB BOO EOORBOROORBOOOOCR ES\nBBOOOROOBRERE BORE OBOOOROOOREE\n\nTE\n.\na o\n\nremrnmsog o>\n\nd\nK o\n\nL sais\nM ed\nW sane.760\nU S.90e.aie\nP $056,664\n\nFigure 4.19 Determining the probability of selection for each individual\n\nThe number of parents selected to be used for reproducing new offspring is determined\nby the intended total number of offspring required, which is determined by the desired\npopulation size for each generation. Two parents are selected, and offspring are created.\nThis process repeats with different parents selected (with a chance of the same individu-\nals being a parent more than once) until the desired number of offspring have been\ngenerated. Two parents can reproduce a single mixed child or two mixed children. This\nconcept will be made clearer later in this chapter. In our Knapsack Problem example, the\nindividuals with greater fitness are those that fill the bag with the most combined value\nwhile respecting the weight-limit constraint.\n\nPopulation models are ways to control the diversity of the population. Steady state\nand generational are two population models that have their own advantages and\ndisadvantages.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.9,
                        "section_name": "Steady state: Replacing a portion of the population each generation",
                        "section_path": "./screenshots-images-2/chapter_4/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_9/94b846de-5e14-4598-84a2-271b147cf60b.png",
                            "./screenshots-images-2/chapter_4/section_9/ad301ff9-cf35-4338-802a-d136308afcef.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Steady state: Replacing a portion of the population each generation\n\nThis high-level approach to population management is not an alternative to the other\nselection strategies, but a scheme that uses them. The idea is that the majority of the\npopulation is retained, and a small group of weaker individuals are removed and replaced\nwith new offspring. This process mimics the cycle of life and death, in which weaker\n\nindividuals die and new individuals are made through reproduction. If there were 100\nindividuals in the population, a portion of the population would be existing individuals,\nand a smaller portion would be new individuals created via reproduction. There may be\n80 individuals from the current generation and 20 new individuals.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.1,
                        "section_name": "Generational: Replacing the entire population each generation",
                        "section_path": "./screenshots-images-2/chapter_4/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_10/c03c708f-307e-46e9-88ae-d05f510c024c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generational: Replacing the entire population each generation\n\nThis high-level approach to population management is similar to the steady-state model\nbut is not an alternative to selection strategies. The generational model creates a number\nof offspring individuals equal to the population size and replaces the entire population\nwith the new offspring. If there were 100 individuals in the population, each generation\nwould result in 100 new individuals via reproduction. Steady state and generational are\noverarching ideas for designing the configuration of the algorithm.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.11,
                        "section_name": "Roulette wheel: Selecting parents and surviving individuals",
                        "section_path": "./screenshots-images-2/chapter_4/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_11/c46200b1-2e25-44b4-9502-4d4df44da2a6.png",
                            "./screenshots-images-2/chapter_4/section_11/8cf70cca-f9a2-456e-ab89-8a0923bfa809.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Roulette wheel: Selecting parents and surviving individuals\n\nChromosomes with higher fitness scores are more likely to be selected, but chromo-\nsomes with lower fitness scores still have a small chance of being selected. The term\nroulette-wheel selection comes from a roulette wheel at a casino, which is divided into\nslices. Typically, the wheel is spun, and a marble is released into the wheel. The selected\nslice is the one that the marble lands on when the wheel stops turning.\n\nIn this analogy, chromosomes are assigned to slices of the wheel. Chromosomes with\nhigher fitness scores have larger slices of the wheel, and chromosomes with lower fitness\nscores have smaller slices. A chromosome is selected randomly, much as a ball randomly\nlands on a slice.\n\nThis analogy is an example of probabilistic selection. Each individual has a chance of\nbeing selected, whether that chance is small or high. The chance of selection of individ-\nuals influences the diversity of the population and convergence rates mentioned earlier\nin this chapter. Figure 4.19, also earlier in this chapter, illustrates this concept.\n\nFirst, the probability of selection for each individual needs to be determined. This prob-\nability is calculated for each individual by dividing its fitness by the total fitness of the\npopulation. Roulette-wheel selection can be used. The \u201cwheel\u201d is \u201cspun\u201d until the desired\nnumber of individuals have been selected. For each selection, a random decimal number\nbetween 0 and 1 is calculated. If an individual\u2019s fitness is within that probability, it is\nselected. Other probabilistic approaches may be used to determine the probability of\neach individual, including standard deviation, in which an individual\u2019s value is com-\npared with the mean value of the group:\n\nset_probabilities_of_population (population)\nlet total_fitness equal the sum of fitness of the population\nfor individual in population\nlet the probability of_selection of individual...\n\nequal it\u2019s fitness/total_fitness\n\nroulette_wheel_selection (population, number_of_selections):\nlet possible_probabilities equal\nset_probabilities_of_population (population)\nlet slices equal empty array\nlet total equal\nfor i in range(@, number_of_selections):\nappend [i, total, total + possible_probabilities(i}]\nto slices\ntotal += possible_probabilities[i]\nlet spin equal random(0, 1)\nlet result equal (slice for slice in slices if slice[1] < spin <= slice(2)]\n\nreturn result\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.12,
                        "section_name": "Reproducing individuals from parents",
                        "section_path": "./screenshots-images-2/chapter_4/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_12/e2e0aabe-f36c-4b26-854b-2c0df581b061.png",
                            "./screenshots-images-2/chapter_4/section_12/05bb2088-779b-4e01-97a9-ccc628db3c87.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Reproducing individuals from parents\n\nWhen parents are selected, reproduction needs to happen to create new offspring from\nthe parents. Generally, two steps are related to creating children from two parents. The\nfirst concept is crossover, which means mixing part of the chromosome of the first parent\nwith part of the chromosome of the second parent, and vice versa. This process results in\ntwo offspring that contain inversed mixes of their parents. The second concept is muta-\ntion, which means randomly changing the offspring slightly to create variation in the\npopulation (figure 4.20).\n\nEncede Set\n\nluti algerithm - .\nSpace parameters. c. The next step is reproducing new\noffspring.\nHow do new candidate solutions get\ns made up?\n\nMeasure\nfitness of\nindividuals.\n\nCreate\ninitial\npopulation.\n\nPopulate\nnext\ngeneration.\n\nMeasure\nfitness of\nindividuals.\n\nSelect\nparents.\n\nRepreduce\noffspring.\n\nFigure 4.20 Reproduce offspring.\n\nCrossover\nCrossover involves mixing genes between two individuals to create one or more\noffspring individuals. Crossover is inspired by the concept of reproduction. The\n\noffspring individuals are parts of their parents, depending on the crossover strategy\nused. The crossover strategy is highly affected by the encoding used.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.13,
                        "section_name": "Single-point crossover: Inheriting one part from each parent",
                        "section_path": "./screenshots-images-2/chapter_4/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_13/37dbc2e0-6178-4cb3-9e9b-77810ccf247d.png",
                            "./screenshots-images-2/chapter_4/section_13/1822dd7f-06e9-4607-b0c1-b515947931b2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Single-point crossover: Inheriting one part from each parent\n\nOne point in the chromosome structure is selected. Then, by referencing the two parents\nin question, the first part of the first parent is used, and the second part of the second par-\nent is used. These two parts combined create a new offspring. A second offspring can be\nmade by using the first part of the second parent and the second part of the first parent.\n\nSingle-point crossover is applicable to binary encoding, order/permutation encod-\ning, and real-value encoding (figure 4.21). These encoding schemes are discussed in\nchapter 5.\n\nlee Eyelet folefef: fofif:fofofofi]: |: fofofefifofo]:)\n'\n'\nfofoh=]+TePsfo}ifofofof fof efol2]sfof: fof ofofifofofo}\n'\n\n\u2018\n\n(oT Efelet Jefe fi fofs Ti fofo[: [Jo]: Jol ofofifofea}\nH\n\nLofof: J+ Tet :folifofofol:ojofofofaf:]:Jofolo}ifofefs|\n\nFigure 4.21 Single-point crossover\n\nTo create two new offspring individuals, an empty array is created to hold the new indi-\nviduals. All genes from index 0 to the desired index of parent A are concatenated with all\ngenes from the desired index to the end of the chromosome of parent B, creating one\noffspring individual. The inverse creates the second offspring individual:\n\none_point_crossover (parent_a,parent_b, xover_point)\n\nlet children equal empty a: Y\n\nlet child_1 equal genes 0 to xover_point from parent_a plus...\n\u00abgenes xover_point to parent_b length from parent_b\n\nappend child_1 to children\nlet child_2equal genes @ to xover_point from parent_b plus...\n\u00abgenes xover_point to parent_a length from parent_a\n\nappend child_2to children\n\nreturn children\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.14,
                        "section_name": "Two-point crossover: Inheriting more parts from each parent",
                        "section_path": "./screenshots-images-2/chapter_4/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_14/ae001e97-50df-4090-81d6-f174b9e18348.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Two-point crossover: Inheriting more parts from each parent\n\nTwo points in the chromosome structure are selected; then, referencing the two parents\nin question, parts are chosen in an alternating manner to make a complete offspring\nindividual. This process is similar to single-point crossover, discussed earlier. To describe\nthe process completely, the offspring consist of the first part of the first parent, the sec-\nond part of the second parent, and the third part of the first parent. Think about two-\npoint crossover as splicing arrays to create new ones. Again, a second individual can be\nmade by using the inverse parts of each parent. Two-point crossover is applicable to\nbinary encoding and real-value encoding (figure 4.22).\n\n' 1\n(of: [:Jefofi fofofo fs Joli f fofofofs [i Js Jofofofi fools]\n1 '\n(ofoD [fof fofsfofofofifofofofsfifofsfofofofifofalo}\n1 7\n' \\\n' '\n' '\n\u2018 1\nfof +Jefofof:fofefo]\n\n\u2018\n(efeL- TF Tel:Jelefe Es Tel Tefet: Ts fof: Jef ofofifofe]o)\n\nFigure 4.22 Two-point crossover\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.15,
                        "section_name": "Uniform crossover: Inheriting many parts from each parent",
                        "section_path": "./screenshots-images-2/chapter_4/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_15/576ae22b-d3f1-4148-8e14-cd2c515323e9.png",
                            "./screenshots-images-2/chapter_4/section_15/7d8e7ae9-c9fd-42b4-ab6d-466995a616c4.png",
                            "./screenshots-images-2/chapter_4/section_15/675f1964-f5dc-4c1d-b7ad-edb185d862bb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Uniform crossover: Inheriting many parts from each parent\n\nUniform crossover is a step beyond two-point crossover. In uniform crossover, a mask is\ncreated that represents which genes from each parent will be used to generate the child\noffspring. The inverse process can be used to make a second offspring. The mask can be\ngenerated randomly each time offspring are created to maximize diversity. Generally\nspeaking, uniform crossover creates more-diverse individuals because the attributes of\nthe offspring are quite different compared with any of their parents. Uniform crossover\nis applicable to binary encoding and real-value encoding (figure 4.23).\n\nPEEEPE PEPE PEEP PPh rr errr eT\na, : /\nPPEEEEPREPEEPPEELPE PPh rrr\n\nFigure 4.23 Uniform crossover\n\n(Mutation\nMutation involves changing offspring individuals slightly to encourage diversity in\nthe population. Several approaches to mutation are used based on the nature of the\nproblem and the encoding method.\n\nOne parameter in mutation is the mutation rate\u2014the likelihood that an off-\nspring chromosome will be mutated. Similarly to living organisms, some chromo-\nsomes are mutated more than others; an offspring is not an exact combination of its\nparents\u2019 chromosomes but contains minor genetic differences. Mutation can be\ncritical to encouraging diversity in a population and preventing the algorithm from\ngetting stuck in local best solutions.\n\nA high mutation rate means that individuals have a high chance of being selected\nto be mutated or that genes in the chromosome of an individual have a high chance\nof being mutated, depending on the mutation strategy. High mutation means more\ndiversity, but too much diversity may result in the deterioration of good solutions.\n\n\nEXERCISE: WHAT OUTCOME WOULD UNIFORM CROSSOVER GENERATE\nFOR THESE CHROMOSOMES?\n\n' '\n(oT: PTefofi Tofofof: Jol: [:fofofofs]s [:fofofofifofefs|\n\n' i\n' 1\nQ 1]O]}e\n' '\nCoTeT=TTef:TefsJofofe] i fofofolz fs fe]: fofofofifefofo}\n1\n1 '\n\nSOLUTION: WHAT OUTCOME WOULD UNIFORM CROSSOVER GENERATE\nFOR THESE CHROMOSOMES?\n\n' 1\n\n(oT F:Jefots Tofofof: Jol: f+ Jofofofs ff: fofofo]ifofofs]\n. '\n' '\n\n(eTeT=[+Tef:To]sJofofe] :fofefolz}: fo]: fofofofifefofo}\n'\n'\n'\n' '\n1\n\n(oT: Te Jolefisfofefe]2fol :fofofof:]1]:]:fofofofifofol2)\n: :\n\n(efeL- Te Tel: fo} JeToTefsTTofote Tito: Tofofof1 JoJo fo]\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.16,
                        "section_name": "Bit-string mutation for binary encoding",
                        "section_path": "./screenshots-images-2/chapter_4/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_16/ace6ce9e-de1f-42d1-95a5-e9e7e6a35d45.png",
                            "./screenshots-images-2/chapter_4/section_16/a56b7a23-762a-476c-b442-d3f5ad0d341d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Bit-string mutation for binary encoding\n\nIn bit-string mutation, a gene in a binary-encoded chromosome is selected randomly\nand changed to another valid value (figure 4.24). Other mutation mechanisms are appli-\ncable when nonbinary encoding is used. The topic of mutation mechanisms will be\nexplored in chapter 5.\n\nFigure 4.24 Bit-string mutation\n\nTo mutate a single gene of an individual\u2019s chromosome, a random gene index is selected.\nIf that gene represents 1, change it to represent 0, and vice versa:\n\nmutate_individual (individual, chromosome_length)\nlet random_index equal a random number between 0 and chromosome_length\nif gene at index random_index of individual is equal to 1:\nlet gene at index random_index of individual equal 0\nelse:\nlet gene at index random_index of individual equal 1\n\nreturn individual\n\nFlip-bit mutation for binary encoding\n\nIn flip-bit mutation, all genes in a binary-encoded chromosome are inverted to the\nopposite value. Where there were 1s are 0s, and where there were 0s are 1s. This type of\nmutation could degrade good-performing solutions dramatically and usually is used\nwhen diversity needs to be introduced into the population constantly (figure 4.25).\n\n(eT ETeTop ToJofot Joli fs fofofofs fi]: Jofofofsfofofs|\n\nBOOBRBOREEOROOREEOOoOReeoeeo\n\nFigure 4.25 Flip-bit mutation\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.17,
                        "section_name": "Populating the next generation",
                        "section_path": "./screenshots-images-2/chapter_4/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_17/5f5e619f-7194-4e0a-8f93-5b969b358aa9.png",
                            "./screenshots-images-2/chapter_4/section_17/d135389b-7c1b-4cbb-a79b-d8cffd680e5b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Populating the next generation\n\nWhen the fitness of the individuals in the population has been measured and offspring\nhave been reproduced, the next step is selecting which individuals live on to the next\ngeneration. The size of the population is usually fixed, and because more individuals\nhave been introduced through reproduction, some individuals must die off and be\nremoved from the population.\n\nIt may seem like a good idea to take the top individuals that fit into the popula-\ntion size and eliminate the rest. This strategy, however, could create stagnation in the\ndiversity of individuals if the individuals that survive are similar in genetic makeup\n(figure 4.26).\n\nEncede Set\nsolution algorithm\nparameters.\n\nSpace. 1. The next step is populating the next\n\ngener \u2018ation.\n\nWhich lucky individuals get to live to the\n\nCreate Measure next generation?\nInitial fitness of\npopulation. individuals. |\n\nPopulate\nnext\ngeneration.\n\nMeasure\nfitness of\nindividuals.\n\nSelect\nparents.\n\nRepreduce\noffspring.\n\nFigure 4.26 Populate the next generation.\n\nThe selection strategies mentioned in this section can be used to determine the individ-\nuals that are selected to form part of the population for the next generation.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.18,
                        "section_name": "Exploration vs. exploitation",
                        "section_path": "./screenshots-images-2/chapter_4/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_18/5bb73ec8-8405-4b68-b77e-2618e1a73dc8.png",
                            "./screenshots-images-2/chapter_4/section_18/9c04ba86-552b-41ec-8586-28a4944cef52.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Exploration vs. exploitation\n\nRunning a genetic algorithm always involves striking a balance between exploration and\nexploitation. The ideal situation is one in which there is diversity in individuals and the\npopulation as a whole seeks out wildly different potential solutions in the search space;\nthen stronger local solution spaces are exploited to find the most desirable solution. The\nbeauty of this situation is that the algorithm explores as much of the search space as\npossible while exploiting strong solutions as individuals evolve (figure 4.27).\n\nEncede Set\nsolution algorithm\nspace. parameters. 8. The next step is measuring the\n\nperformance of the solutions in the\ncurrent generation.\n\nHow well do these solutions solve the\nproblem at hand?\n\nMeasure\nfitness of\nindividuals.\n\nCreate\ninitial\npopulation.\n\nPepulate\nnext\ngeneration.\n\nMeasure\nfitness of\nindividuals.\n\nSelect\nparents.\n\nReproduce\noffspring.\n\nFigure 4.27 Measure the fitness of individuals.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.19,
                        "section_name": "Stopping conditions",
                        "section_path": "./screenshots-images-2/chapter_4/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_19/e00de31f-9993-44da-b201-accf3ff107a3.png",
                            "./screenshots-images-2/chapter_4/section_19/e554f0e5-2af4-4a3a-8347-a2685750865b.png",
                            "./screenshots-images-2/chapter_4/section_19/085e8526-ddfa-48ba-8550-404e51abc44a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stopping conditions\n\nBecause a genetic algorithm is iterative in finding better solutions through each genera-\ntion, a stopping condition needs to be established; otherwise, the algorithm might run\nforever. A stopping condition is the condition that is met where the algorithm ends; the\nstrongest individual of the population at that generation is selected as the best solution.\n\nThe simplest stopping condition is a constant\u2014a constant value that indicates the\nnumber of generations for which the algorithm will run. Another approach is to stop\nwhen a certain fitness is achieved. This method is useful when a desired minimum fit-\nness is known but the solution is unknown.\n\nStagnation is a problem in evolutionary algorithms in which the population yields\nsolutions of similar strength for several generations. If a population stagnates, the likeli-\nhood of generating strong solutions in future generations is low. A stopping condition\ncould look at the change in the fitness of the best individual in each generation and, if\nthe fitness changes only marginally, choose to stop the algorithm.\n\nThe various steps of a genetic algorithm are used in a main function that outlines the life\ncycle in its entirety. The variable parameters include the population size, the number of\ngenerations for the algorithm to run, and the knapsack capacity for the fitness function,\nin addition to the variable crossover position and mutation rate for the crossover and\nmutation steps:\n\nrun_ga (population_size, number_of_generations, knapsack_capacity):\nlet best_global_fitness equal @\n\nlet global_population equal.\n\n-generate_initial_population(population_size)\n\nfor generation in range(number_of_generations):\nlet current_best_fitness equal...\n...calculate_population_fitness(global_ population, knapsack_capacity)\nif current_best_fitness is greater than best_global_fitness:\nlet best_global_fitness equal current_best_fitness\nlet the_chosen equal...\n---roulette_wheel_selection (global_population, population_size)\nlet the_children equal...\n..-reproduce_children(the_chosen)\nlet the_children equal...\n..mutate_children(the_children)\n\nlet global_population equal...\n\nrge_population_and_children(global_population, the_children)\n\nAs mentioned at the beginning of this chapter, the Knapsack Problem could be solved\nusing a brute-force approach, which requires more than 60 million combinations to be\ngenerated and analyzed. When comparing genetic algorithms that aim to solve the same\nproblem, we can see far more efficiency in computation if the parameters for exploration\nand exploitation are configured correctly. Remember, in some cases, a genetic algorithm\nproduces a \u201cgood enough\u201d solution that is not necessarily the best possible solution but\nis desirable. Again, using a genetic algorithm for a problem depends on the context\n(figure 4.28).\n\nBrute force Genetic algorithm\n\nIterations | 2426 = 67,108,864 10,000 - 100,000\nAccuracy 100% 100%\nCompute time ~# minutes ~3 secends\nBest value 13,692,884 13,692,884\n\nFigure 4.28 Brute-force performance vs. genetic algorithm performance\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.2,
                        "section_name": "Configuring the parameters of a\ngenetic algorithm",
                        "section_path": "./screenshots-images-2/chapter_4/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_20/a09884ae-fa12-4860-bc06-cb72234e7319.png",
                            "./screenshots-images-2/chapter_4/section_20/9235c696-c603-4613-ba54-9477f490cf44.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Configuring the parameters of a\ngenetic algorithm\n\nIn designing and configuring a genetic algorithm, several decisions need to be made that\ninfluence the performance of the algorithm. The performance concerns fall into two\nareas: the algorithm should strive to perform well in finding good solutions to the prob-\nlem, and the algorithm should perform efficiently from a computation perspective. It\nwould be pointless to design a genetic algorithm to solve a problem if the solution will be\nmore computationally expensive than other traditional techniques. The approach used\nin encoding, the fitness function used, and the other algorithmic parameters influence\nboth types of performances in achieving a good solution and computation. Here are\nsome parameters to consider:\n\nChromosome encoding\u2014The chromosome encoding method requires thought to\nensure that it is applicable to the problem and that the potential solutions strive for\nglobal maxima. The encoding scheme is at the heart of the success of the algorithm.\n\nPopulation size\u2014The population size is configurable. A larger population\nencourages more diversity in possible solutions. Larger populations, however,\nrequire more computation at each generation. Sometimes, a larger population\nbalances out the need for mutation, which results in diversity at the start but no\ndiversity during generations. A valid approach is to start with a smaller\npopulation and grow it based on performance.\n\nPopulation initialization\u2014Although the individuals in a population are\ninitialized randomly, ensuring that the solutions are valid is important for\noptimizing the computation of the genetic algorithm and initializing individuals\nwith the right constraints.\n\nNumber of offspring\u2014The number of offspring created in each generation can be\nconfigured. Given that after reproduction, part of the population is killed off to\n\nensure that the population size is fixed, more offspring means more diversity, but\nthere is a risk that good solutions will be killed off to accommodate those\noffspring. If the population is dynamic, the population size may change after\nevery generation, but this approach requires more parameters to configure and\ncontrol.\n\nParent selection method\u2014The selection method used to choose parents can be\nconfigured. The selection method must be based on the problem and the desired\nexplorability versus exploitability.\n\nCrossover method\u2014The crossover method is associated with the encoding\nmethod used but can be configured to encourage or discourage diversity in the\npopulation. The offspring individuals must still yield a valid solution.\n\nMutation rate\u2014The mutation rate is another configurable parameter that\ninduces more diversity in offspring and potential solutions. A higher mutation\nrate means more diversity, but too much diversity may deteriorate good-\nperforming individuals. The mutation rate can change over time to create more\ndiversity in earlier generations and less in later generations. This result can be\ndescribed as exploration at the start followed by exploitation.\n\nMutation method\u2014The mutation method is similar to the crossover method in\nthat it is dependent on the encoding method used. An important attribute of the\nmutation method is that it must still yield a valid solution after the modification\nor assigned a terrible fitness score.\n\nGeneration selection methods\u2014Much like the selection method used to choose\nparents, a generation selection method must choose the individuals that will\nsurvive the generation. Depending on the selection method used, the algorithm\nmay converge too quickly and stagnate or explore too long.\n\nStopping condition\u2014The stopping condition for the algorithm must make sense\nbased on the problem and desired outcome. Computational complexity and time\nare the main concerns for the stopping condition.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.21,
                        "section_name": "Use cases for evolutionary algorithms",
                        "section_path": "./screenshots-images-2/chapter_4/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_21/405295b7-dff4-4ec2-8984-7eedbde99024.png",
                            "./screenshots-images-2/chapter_4/section_21/bce051db-343e-45f2-b002-e71dda722784.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Use cases for evolutionary algorithms\n\nEvolutionary algorithms have a wide variety of uses. Some algorithms address isolated\nproblems; others combine evolutionary algorithms with other techniques to create novel\napproaches to solving difficult problems, such as the following:\n\n+ Predicting investor behavior in the stock market\u2014Consumers who invest make\ndecisions every day about whether to buy more of a specific stock, hold on to\nwhat they have, or sell stock. Sequences of these actions can be evolved and\n\nmapped to outcomes of an investor's portfolio. Financial institutions can use this\ninsight to proactively provide valuable customer service and guidance.\n\n+ Feature selection in machine learning\u2014Machine learning is discussed in chapter 8,\nbut a key aspect of machine learning is: given a number of features about something,\ndetermining what it is classified as. If we're looking at houses, we may find many\nattributes related to houses, such as age, building material, size, color, and location.\nBut to predict market value, perhaps only age, size, and location matter. A genetic\nalgorithm can uncover the isolated features that matter the most.\n\n* Code breaking and ciphers\u2014A cipher is a message encoded in a certain way to\nlook like something else and is often used to hide information. If the receiver\ndoes not know how to decipher the message, it cannot be understood.\nEvolutionary algorithms can generate many possibilities for changing the\nciphered message to uncover the original message.\n\nChapter 5 dives into advanced concepts of genetic algorithms that adapt them to differ-\nent problem spaces. We explore different techniques for encoding, crossover, mutation,\nand selection, as well as uncover effective alternatives.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 5,
                "chapter_name": "Advanced\nevolutionary approaches",
                "chapter_path": "./screenshots-images-2/chapter_5",
                "sections": [
                    {
                        "section_id": 5.1,
                        "section_name": "Evolutionary algorithm life cycle",
                        "section_path": "./screenshots-images-2/chapter_5/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_1/ff3b6a2d-1fc4-41f1-aecb-2c066608effb.png",
                            "./screenshots-images-2/chapter_5/section_1/27730543-60fb-4a0a-882e-962bbf2ffeac.png",
                            "./screenshots-images-2/chapter_5/section_1/63df6d4c-d9eb-475a-8015-680605f66a0f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Evolutionary algorithm life cycle\n\nThe general life cycle of a genetic algorithm is outlined in chapter 4. In this\nchapter, we consider other problems that may be suitable to be solved with\na genetic algorithm, why some of the approaches demonstrated thus far\nwon't work, and alternative approaches.\n\nAs a reminder, the general life cycle of a genetic algorithm is as follows:\n\n* Creating a population\u2014Creating a random population of potential\nsolutions.\n\n+ Measuring fitness of individuals in the population\u2014Determining how good a\nspecific solution is. This task is accomplished by using a fitness function that\nscores solutions to determine how good they are.\n\n+ Selecting parents based on their fitness\u2014Selecting pairs of parents that will\nreproduce offspring.\n\n+ Reproducing individuals from parents\u2014Creating offspring from their parents by\nmixing genetic information and applying slight mutations to the offspring.\n\n+ Populating the next generation\u2014Selecting individuals and offspring from the\npopulation that will survive to the next generation.\n\nKeep the life cycle flow (depicted in figure 5.1) in mind as we work through this\nchapter.\n\n1 2\n\nEncode Set\nsolution algerithm\njparameters|\n\nspace.\n\nMeasure\nfitness of\nindividuals.\n\nCreate\ninitial\npopulation.\n\nPopulate\nnext\ngeneration.\n\nMeasure\nfitness of\nindividuals. |\n\nSelect\nparents.\n\nReproduce\noffspring.\n\nFigure 5.1 Genetic algorithm life cycle\n\nThis chapter starts by exploring alternative selection strategies; these individual\napproaches can be generically swapped in and out for any genetic algorithm. Then it\nfollows three scenarios that are tweaks of the Knapsack Problem (chapter 4) to highlight\nthe utility of the alternative encoding, crossover, and mutation approaches (figure 5.2).\n\ni Pt\n$4 $7\nCapacity limit : 4kg\n\n1kg\n$4\n\n4kg\n$3\n\n3kg\n$1\n\nFigure 5.2 The example Knapsack Problem\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.2,
                        "section_name": "Alternative selection strategies",
                        "section_path": "./screenshots-images-2/chapter_5/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_2/7ed5f627-3a44-4d1b-994a-a05b00c85242.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Alternative selection strategies\n\nIn chapter 4, we explored one selection strategy: roulette-wheel selection, which is one of\nthe simplest methods for selecting individuals. The following three selection strategies\nhelp mitigate the problems of roulette-wheel selection; each has advantages and disad-\nvantages that affect the diversity of the population, which ultimately affects whether an\noptimal solution is found.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.3,
                        "section_name": "Rank selection: Even the playing field",
                        "section_path": "./screenshots-images-2/chapter_5/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_3/2efa5c89-b8a9-42e5-a73d-513d2457d20a.png",
                            "./screenshots-images-2/chapter_5/section_3/b73903a2-4cb4-4029-8928-c9837a3d69da.png",
                            "./screenshots-images-2/chapter_5/section_3/4c679737-b80a-47e6-8202-ab43e97c1856.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Rank selection: Even the playing field\n\nOne problem with roulette-wheel selection is the vast differences in the magnitude of\nfitness between chromosomes. This heavily biases the selection toward choosing indi-\nviduals with high fitness scores or giving poor-performing individuals a larger chance of\nselection than desired. This problem affects the diversity of the population. More diver-\nsity means more exploration of the search space, but it can also make finding optimal\nsolutions take too many generations.\n\nRank selection aims to solve this problem by ranking individuals based on their fit-\nness and then using each individual\u2019s rank as the value for calculating the size of its slice\non the wheel. In the Knapsack Problem, this value is a number between 1 and 16, because\nwe're choosing among 16 individuals. Although strong individuals are more likely to be\nselected and weaker ones are less likely to be selected even though they are average, each\nindividual has a fairer chance of being selected based on rank rather than exact fitness.\nWhen 16 individuals are ranked, the wheel looks slightly different from roulette-wheel\nselection (figure 5.3).\n\n4 1\n8 2\nc 3\nD \u2018\n\u20ac GEbherbPPhbthhi eh TfohTelefotifofoe] =\nF GEPPREPPErER fe feoityelsfefefetifofefe} \u00ab\nG ?\nH Ebletehfolei hihi tleh fel: Jofefolefofeye] \u00ab\nI ,\n\u2019 ba\nk 4\nL bs\nM bd\nN \u201c\n9 s\nid 6\n\nFigure 5.3 Example of rank selection\n\nFigure 5.4 compares roulette-wheel selection and rank selection. It is clear that rank\nselection gives better-performing solutions a better chance of selection.\n\nKP a\nw \u00a9\n\nReulette-wheel\nselection == =  \u2014\u2014 selection\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.4,
                        "section_name": "Tournament selection: Let them fight",
                        "section_path": "./screenshots-images-2/chapter_5/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_4/e2f7f026-f1d5-46bf-bbdf-c54e433be877.png",
                            "./screenshots-images-2/chapter_5/section_4/c29d834b-26ac-4481-b2b8-fe4163c47760.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tournament selection: Let them fight\n\nTournament selection plays chromosomes against one other. Tournament selection ran-\ndomly chooses a set number of individuals from the population and places them in a\ngroup. This process is performed for a predetermined number of groups. The individual\nwith the highest fitness score in each respective group is selected. The larger the group,\nthe less diverse it is, because only one individual from each group is selected. As with\nrank selection, the actual fitness score of each individual is not the key factor in selecting\nindividuals globally.\n\nWhen 16 individuals are allocated to four groups, selecting only 1 individual from\neach group results in the choice of 4 of the strongest individuals from those groups. Then\nthe 4 winning individuals can be paired to reproduce (figure 5.5).\n\n-2rernmsoegoe\n\nvotzrErne\n\n33,207,038\n22,965,245\n12,346,873\n21139,363\n472,358\nrenner\nCREPEREEEPPEEP ETRE Pee eTeT Te) 10,042,441\n9,003,602\noas7s%\n9.670.106\n9an7see\nssa1m9\n9.924.936\n\nPER EEPP PEPE rrr rrrErzsIelsi) 1,768\n6\n6.056.664\n\n314\n\nFigure 5.5 Example of tournament selection\n\nQ\nSl a 3\n\nWinners\n\u00a2 A\n* E\nvot\n\u00b0 M\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.5,
                        "section_name": "Elitism selection: Choose only the best",
                        "section_path": "./screenshots-images-2/chapter_5/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_5/b121b8fe-3fbb-47fb-9aa5-cc1898dc47e5.png",
                            "./screenshots-images-2/chapter_5/section_5/33d72f8b-f353-43e5-8fe4-2ddd6ddf8d1a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Elitism selection: Choose only the best\n\nThe elitism approach selects the best individuals in the population. Elitism is useful for\nretaining strong-performing individuals and eliminating the risk that they will be lost\nthrough other selection methods. The disadvantage of elitism is that the population can\nfall into a local best solution space and never be diverse enough to find global bests.\n\nElitism is often used in conjunction with roulette-wheel selection, rank selection, and\ntournament selection. The idea is that several elite individuals are selected to reproduce,\nand the rest of the population is filled with individuals by means of one of the other\nselection strategies (figure 5.6).\n\nisaeans\n12,965,453\n12,044,873 \u00ab\n11,739,363\n12,711,289 \u00a9 Elitism Survivers\n23,611,967 \u00a9 A\nr0002,44n\n9,883,682 \u00ab\n9an7297 |\nsare\nsams\nsesame y\n1\nCET iz z it\nZ z iz\n2 2\n\nzamnmsog @ >\n\nretmrnvsago\n\n8,324,936 |\n8,018,768 |\n6,900,318 |\n6,0s6,66e\n\nvoz Er rem\n\nFigure 5.6 Example of elitism selection\n\nChapter 4 explores a problem in which including items in or excluding items from\nthe knapsack was important. A variety of problem spaces require a different encoding\nbecause binary encoding won't make sense. The following three sections describe these\nscenarios.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.6,
                        "section_name": "Real-value encoding:\nWorking with real numbers",
                        "section_path": "./screenshots-images-2/chapter_5/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_6/e6b9f119-51a9-4790-9092-fa673b0939f4.png",
                            "./screenshots-images-2/chapter_5/section_6/93a1ba4b-47d0-4156-af33-9c8f1ce265c8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Real-value encoding:\nWorking with real numbers\n\nConsider that the Knapsack Problem has changed slightly. The problem remains choos-\ning the most valuable items to fill the weight capacity of the knapsack. But the choice\ninvolves more than one unit of each item. As shown in table 5.1, the weights and values\nremain the same as the original dataset, but a quantity of each item is included. With this\nslight adjustment, a plethora of new solutions are possible, and one or more of those\nsolutions may be more optimal, because a specific item can be selected more than once.\nBinary encoding is a poor choice in this scenario. Real-value encoding is better suited to\nrepresenting the state of potential solutions.\n\nTable 5.1 Knapsack capacity: 6,404,180 kg\n\nJewel box\nKnife\n\nWeight (kg)\n32,252\n225,790\n468,\n489,494\n\nz\n\n265,590\n497,911\n\n823,576\n552,202\n323,618\n\nValue ($)\n471,010\n944,62\n962,094\n\n579,152\n\n902,698\n\n1,056,157\n677,562\n\n9\n\n6\n\nHelmet 800,493 1,686,515 10\n\n7\n3\n5\n\nLong sword\n\nOpal badge\n\nPearls\n\nRuby ring\nSilver bracelet\nTimepiece\n\nVenom potion\nWool scarf\nCrossbow\n\n382,846\n44,676\n\n610,876\n854,190\n671,123\n698,180\n\n730,061\n\n952,360\n\n833,132\n\n1,253,986\n\n1,853,562\n\n1,320,297\n\n1,301,637\n\n859,835\n\n1,677,534\n\n1,528,646\n\n1,827,477\n\n2,068,204\n\n13\n\nNecklace 169,738 376,418 8\n\n9\n\n17\n\n7\n\n9\n\nYesteryear book\nZinc cup\n\n926,023\n\n1,746,556\n\n978,724\n\n2,100,851\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.7,
                        "section_name": "Real-value encoding at its core",
                        "section_path": "./screenshots-images-2/chapter_5/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_7/f4b21819-86d2-472f-9944-e52421e8a7b2.png",
                            "./screenshots-images-2/chapter_5/section_7/879bc65b-8d11-4085-874f-6e3d6a47c072.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Real-value encoding at its core\n\nReal-value encoding represents a gene in terms of numeric values, strings, or symbols,\nand expresses potential solutions in the natural state respective to the problem. This\nencoding is used when potential solutions contain continuous values that cannot be\nencoded easily with binary encoding. As an example, because more than one item is\navailable to be carried in the knapsack, each item index cannot indicate only whether the\nitem is included; it must indicate the quantity of that item in the knapsack (figure 5.7).\n\n18 11 12 43 14 15 16 17 18 28 20 21 22 23 24 25 26\n\n\u00bb\na\n\nLS\nfe] +\na\n[S|\n\na\noT\n\n& Ink\n\n$ Axo\n@p Jewel Box\n\n@ Bronze Coin\n# Knife\n\n\u00ae Mask\n\n9 Necklace\n\n+ Opal Badge\n\n@ Fossil\n# Long Sword\n\n\u00a9 Gold Cein\n@ Helmet [o]-\n\nws Crewn\n\n& Diamond Statue\n\u00a9 Pearls\n\n@ Quiver\n\n@ Ruby Ring\n\n6 Silver Bracelet\nt& Uniform\n\n& Venom Petion\n2% Crossbow\n\n@ Yesteryear Book\n@ Zink Cup\n\n@ Timepiece\n2 Wool Scarf\n\nwy Emerald Belt\n\nFigure 5.7 Example of real-value encoding\n\nBecause the encoding scheme has been changed, new crossover and mutation options\nbecome available. The crossover approaches discussed for binary encoding are still valid\noptions to real-value encoding, but mutation should be approached differently.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.8,
                        "section_name": "Arithmetic crossover: Reproduce with math",
                        "section_path": "./screenshots-images-2/chapter_5/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_8/9f838bd6-98cf-465d-92d6-a5b28bd6469f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Arithmetic crossover: Reproduce with math\n\nArithmetic crossover involves an arithmetic operation to be computed by using each par-\nent as variables in the expression. The result of applying an arithmetic operation using\nboth parents is the new offspring. When we use this strategy with binary encoding, it is\nimportant to ensure that the result of the operation is still a valid chromosome. Arithmetic\ncrossover is applicable to binary encoding and real-value encoding (figure 5.8).\n\nNOTE Be wary: this approach can create very diverse offspring, which can be\nproblematic.\n\nee bTeleh fofefo fs foi]: Jofofols fifi fofofo]ifofof:]\n\na\n\n(ofop T+ fofsfofsfefofo]:fofofofz fifo: fof ofo]ifofofo}\n\nfe [2 JoTs [2 ]4]: fof: Jofofofi]+]2]+Jofzfofofolofofofofi}\n\nFigure 5.8 Example of arithmetic crossover\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.9,
                        "section_name": "Boundary mutation",
                        "section_path": "./screenshots-images-2/chapter_5/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_9/1e7ba72f-47dc-47df-9a2e-e6daf26f3dc7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Boundary mutation\n\nIn boundary mutation, a gene randomly selected from a real-value encoded chromo-\nsome is set randomly to a lower bound value or upper bound value. Given 26 genes in a\nchromosome, a random index is selected, and the value is set to either a minimum value\nora maximum value. In figure 5.9, the original value happens to be 0 and will be adjusted\nto 6, which is the maximum for that item. The minimum and maximum can be the\nsame for all indexes or set uniquely for each index if knowledge of the problem informs\nthe decision. This approach attempts to evaluate the impact of individual genes on the\n\nchromosome.\n\nMin: @\n\n\\] wee\n1 2 3 4 5 6 * 8B 4 40 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26\n\na\ngo\nUs]\n\n(2 Ts ToTe [effete fe [2 Jofol?fofotof: f=]: Jofefo]2]\n\ncc rw =z o4w~M TM \u00aet. Or cy} \u201cwa\n\nSESESsBsssseLssssesFPsseEstzss\n<SOSTHSSELMESEHSYaGstae sl SES gge\n|eeSPve vs sK Gea Ps es sEarare\nSeet Fry, Sies, eS SN\n\nsdzE\u00b0O 8 *\u00a7 a0vaeaGgF,SSES\n\n5 22 0 \u00a7 3 oF SEEN SeK ED\n\nEa Se $e S@*s\n\n9 Ba = 2\n\na> a \u00ae& \u00a9\n\na @ ~\n\na\n\nFigure 5.9 Example of boundary mutation\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.1,
                        "section_name": "Arithmetic mutation",
                        "section_path": "./screenshots-images-2/chapter_5/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_10/c0582821-1186-4340-b606-3ef9315271f0.png",
                            "./screenshots-images-2/chapter_5/section_10/77c41180-9271-4b2a-9e71-d5d2bd67c2f0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Arithmetic mutation\n\nIn arithmetic mutation, a randomly selected gene in a real-value-encoded chromosome\nis changed by adding or subtracting a small number. Note that although the example in\n\nfigure 5.10 includes whole numbers, the numbers could be decimal numbers, including\nfractions.\n\n13 14 15 16 17 48 19 20 21 22 23 24 25 26\n\n+1\n\n12\n\nDace ee add\n\ndno yuIz @\n\n400g 4B@K1934820K \u00a9\nMOQSSO45\nj4Bog joom A\nUG1}0g WoUsA |\nwie sun W\neoeidoun, \u00a9\n}2)2084g U2AIIS 8\nBury Xqng \u00a9\n4eanyn 2\n\n$)4e24g O\n\na6peg 1edg &\nBOB)XOON A\n\n1SEN @\n\npuomg 6ue7\nayy\n\nxog }e@mMec &\nMUS\n\nyowleH W\n\nule pen \u00a9\nnsseg\n\n19g pilesewg &\nenzetS puoweig\nuMeID FA\n\nujop ezue1g &\nexy \u00a5\n\nFigure 5.10 Example of arithmetic mutation\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.11,
                        "section_name": "Order encoding: Working with sequences",
                        "section_path": "./screenshots-images-2/chapter_5/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_11/86c3280b-9feb-4abe-a0e6-cc56ce4059c8.png",
                            "./screenshots-images-2/chapter_5/section_11/b6fd7a1a-8d7a-474e-8bd7-96e21e764090.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Order encoding: Working with sequences\n\nWe still have the same items as in the Knapsack Problem. We won't be determining the\nitems that will fit into a knapsack; instead, all the items need to be processed ina refinery\nin which each item is broken down to extract its source material. Perhaps the gold coin,\nsilver bracelet, and other items are smelted to extract only the source compounds. In this\nscenario, items are not selected to be included, but all are included.\n\nTo make things interesting, the refinery requires a steady rate of extraction, given the\nextraction time and the value of the item. It\u2019s assumed that the value of the refined mate-\nrial is more or less the same as the value of the item. The problem becomes an ordering\nproblem. In what order should the items be processed to maintain a constant rate of\nvalue? Table 5.2 describes the items with their respective extraction times.\n\nTable 5.2 Factory value per hour: 600,000\n\nHelmet\nJewel box\nLong sword\nNecklace\nOpal badge\nPearls\nRuby ring\n\nSilver bracelet\nTimepiece\n\n3\n\nVenom potion\nWool scarf\nCrossbow\n\nYesteryear book\nZinc cup\n\nN\nire\n\nWeight (kg)\n32,252\n225,790\n468,\n489,494\n\nz\n\n265,590\n497,911\n800,493\n823,576\n552,202\n\n382,846\n44,676\n169,738\n610,876\n854,190\n671,123\n698,180\n\nValue ($)\n\n68,674\n\n78,\n579,152\n\n902,698\n\n1,686,515\n\n1,688,691\n\n1,056,157\n\n677,562\n\n833,132\n\n376,418\n\n1,253,986\n\n1,853,562\n\n1,320,297\n\n1,301,637\n\n859,835\n\n1,677,534\n1,528,646\n1,827,477\n\n2,068,204\n\n2,100,851\n\nExtraction time\n\nN N a > N\na So So 3S 3\n\n\u00a9\n\ns\n\nOO\n\nN\n\nS\nMe\na Ss\n\no\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.12,
                        "section_name": "Importance of the fitness function",
                        "section_path": "./screenshots-images-2/chapter_5/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_12/0195fb41-d86c-4caa-be93-2dbb18df27f2.png",
                            "./screenshots-images-2/chapter_5/section_12/dc475a44-6d5c-413e-bcc8-12e5f8f2577c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Importance of the fitness function\n\nWith the change in the Knapsack Problem to the Refinery Problem, a key difference is\nthe measurement of successful solutions. Because the factory requires a constant mini-\nmum rate of value per hour, the accuracy of the fitness function used becomes para-\nmount to finding optimal solutions. In the Knapsack Problem, the fitness of a solution\nis trivial to compute, as it involves only two things: ensuring that the knapsack\u2019s weight\n\nlimit is respected and summing the selected items\u2019 value. In the Refinery Problem, the\nfitness function must calculate the rate of value provided, given the extraction time for\neach item as well as the value of each item. This calculation is more complex, and an\nerror in the logic of this fitness function directly influences the quality of solutions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.13,
                        "section_name": "Order encoding at its core",
                        "section_path": "./screenshots-images-2/chapter_5/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_13/ce8287d0-42ce-4594-8a6a-f67b6f6bd421.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Order encoding at its core\n\nOrder encoding, also known as permutation encoding, represents a chromosome as a\nsequence of elements. Order encoding usually requires all elements to be present in the\nchromosome, which implies that corrections might need to be made when performing\ncrossover and mutation to ensure that no elements are missing or duplicated. Figure 5.11\ndepicts how a chromosome represents the order of processing of the available items.\n\n\u00bb\u201d cv 2 =e \u00a9 cS\nSSTSSELSRSstPsse Es eass sles\nBeas ee SE<SSSeLSESZ ESE RSS\nBSeVMoH Se, \u00a5LHO AZ \u201cZH *H LZ oPvy\n24,2 - ot HSssd \u00aey.se eee wf o 8\nSzP5aaise SSa%y S) se S00=\n\ns8&i.o \u00a7 gcs 4 ere c\nSEES 5\u201d o K Soa 9\n\naus * ~@ >9S\n\n=.3 \\ o 8\n5 30 \u00b0 &\nqo ~\n\ng\n\nFigure 5.11 Example of order encoding\n\nAnother example in which order encoding is sensible is representing potential solutions\nto route optimization problems. Given a certain number of destinations, each of which\nmust be visited at least once while minimizing the total distance traveled, the route can\nbe represented as a string of the destinations in the order in which they are visited. We\nwill use this example when covering swarm intelligence in chapter 6.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.14,
                        "section_name": "Order mutation: Order/permutation encoding",
                        "section_path": "./screenshots-images-2/chapter_5/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_14/fa670157-ee9c-41e2-b302-fb2866536dcb.png",
                            "./screenshots-images-2/chapter_5/section_14/33a272c9-275d-4577-8f27-4c56e7ef868a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Order mutation: Order/permutation encoding\n\nIn order mutation, two randomly selected genes in an order-encoded chromosome swap\npositions, ensuring that all items remain in the chromosome while introducing diversity\n(figure 5.12).\n\n20R]}92N A\n\nUIeD PIP @\n$4824 O\n\nUuleD eZzuUGIg \u00a9\neoedeul, \u00a9\nUel}og WoUeA \u00a9\nery \u00a5\n\nMUS\n\nMOQSSO1D 3\nwes)\n\nesUy\n\nsey @\n\n4eainn @\n\nburg kqny &\n$4B0g oom GB\n00g UB2K1234929K @\ndno yuIZ @\n8804 &\n\nyowjeH WB\npuomg buoy\nUMGID fA\n\ne6peg jedo &\n32)20B1g UeA}Ig \u00a7\n219g presewg\n2nze3g puowelg \u00a9\nxog jomor @\n\nFigure 5.12 Example of order mutation\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.15,
                        "section_name": "Tree encoding: Working with hierarchies",
                        "section_path": "./screenshots-images-2/chapter_5/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_15/6ed84609-5fa1-44c2-a37b-1c4e6b38d2b4.png",
                            "./screenshots-images-2/chapter_5/section_15/b8bc47f6-1c4a-417b-b95a-1eb49a02dc3c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tree encoding: Working with hierarchies\n\nThe preceding sections show that binary encoding is useful for selecting items from a set,\nreal-value encoding is useful when real numbers are important to the solution, and order\nencoding is useful for determining priority and sequences. Suppose that the items in the\nKnapsack Problem are placed in packages to be shipped to homes around the town. Each\ndelivery wagon can hold a specific volume. The requirement is to determine the optimal\npositioning of packages to minimize empty space in each wagon (table 5.3).\n\nTable 5.3 Wagon capacity: 1000 wide x 1000 high\n\nItem ID Item name | Weight (kg) Value ($) Ww H\n1 me | 32252 | 6967 | 20 ~*| 0\n2 Bronze coin 225,790 471,010 10 10\n3 944,620 20\n4 Diamond 489,494 962,094 30 70\n\nstatue\n5 Emerald belt 35,384 78,344 30 20\n6 Fossil 265,590 579,152 15 15\n7 Gold coin 497,911 902,698 10 10\n8 Helmet 800,493 1,686,515 40 50\n9 Ink 823,576 1,688,691 5 10\n10 Jewel box 552,202 1,056,157 40 30\n12 Long sword 382,846 833,132 15 50\n13 Mask 44676 | 99,192 | 20 | 30\n14 Necklace 169,738 376,418 15 20\n15 Opal badge 610,876 1,253,986 5\n16 Pearls 854,190 1,853,562 10\n17 Quiver 671,123 1,320,297 30 70\n18 Ruby ring 698,180 1,301,637 5 10\n19 Silver 446,517 859,835 10 20\nbracelet\n20 Timepiece 909,620 1,677,534 15 20\n21 Uniform 904,818 1,910,501 30 40\n22 Venom 730,061 1,528,646 15 15\npotion\n23 Wool scarf 931,932 1,827,477 20 30\n24 Crossbow 952,360 2,068,204 50 70\n25 Yesteryear 926,023 1,746,556 25 30\nbook\n26 Zinc cup 978,724 2,100,851 15 25\n\nIn the interest of simplicity, suppose that the wagon\u2019s volume is a two-dimensional rect-\nangle and that the packages are rectangular rather than 3D boxes.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.16,
                        "section_name": "Tree encoding at its core",
                        "section_path": "./screenshots-images-2/chapter_5/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_16/a62d505b-7aca-4f46-87ad-65b6e3f2f141.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tree encoding at its core\n\nTree encoding represents a chromosome as a tree of elements. Tree encoding is versatile\nfor representing potential solutions in which the hierarchy of elements is important\nand/or required. Tree encoding can even represent functions, which consist of a tree of\nexpressions. As a result, tree encoding could be used to evolve program functions in\nwhich the function solves a specific problem; the solution may work but look bizarre.\n\nHere is an example in which tree encoding makes sense. We have a wagon with a spe-\ncific height and width, and a certain number of packages must fit in the wagon. The goal\nis to fit the packages in the wagon so that empty space is minimized. A tree-encoding\napproach would work well in representing potential solutions to this problem.\n\nIn figure 5.13, the root node, node A, represents the packing of the wagon from top to\nbottom. Node B represents all packages horizontally, similarly to node C and node D.\nNode E represents packages packed vertically in its slice of the wagon.\n\n~\u2014\u2014_\u20141m\u2014___>\n\nFigure 5.13 Example of a tree used to represent the Wagon Packing Problem\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.17,
                        "section_name": "Tree crossover: Inheriting portions of a tree",
                        "section_path": "./screenshots-images-2/chapter_5/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_17/6dde56dd-f12e-47f5-9fa2-e0959648709c.png",
                            "./screenshots-images-2/chapter_5/section_17/d1a003ec-e8b5-4ac9-973a-3ff9273ee584.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tree crossover: Inheriting portions of a tree\n\nTree crossover is similar to single-point crossover (chapter 4) in that a single point in the\ntree structure is selected and then the parts are exchanged and combined with copies of\nthe parent individuals to create an offspring individual. The inverse process can be used\nto make a second offspring. The resulting children must be verified to be valid solutions\nthat obey the constraints of the problem. More than one point can be used for crossover\nif using multiple points makes sense in solving the problem (figure 5.14).\n\nParentA\n\nParent B\n\nChild A\n\nChild B\n\nFigure 5.14 Example of tree crossover\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.18,
                        "section_name": "Change node mutation: Changing the value of a node",
                        "section_path": "./screenshots-images-2/chapter_5/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_18/2fc389db-c7b9-4f8c-a57e-35c1d05286ad.png",
                            "./screenshots-images-2/chapter_5/section_18/b13925b2-e7a7-4ff8-b4ac-fb0ae53f4734.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Change node mutation: Changing the value of a node\n\nIn change node mutation, a randomly selected node in a tree-encoded chromosome is\nchanged to a randomly selected valid object for that node. Given a tree representing an\norganization of items, we can change an item to another valid item (figure 5.15).\n\nMutate\n\nFigure 5.15 Change node mutation in a tree\n\nThis chapter and chapter 4 cover several encoding schemes, crossover schemes, and\nselection strategies. You could substitute your own approaches for these steps in your\ngenetic algorithms if doing so makes sense for the problem you're solving.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.19,
                        "section_name": "Common types of evolutionary algorithms",
                        "section_path": "./screenshots-images-2/chapter_5/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_19/7cf65f42-3fb9-4de7-9c95-6a585b0730ca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Common types of evolutionary algorithms\n\nThis chapter focuses on the life cycle and alternative approaches for a genetic algorithm.\nVariations of the algorithm can be useful for solving different problems. Now that we\nhave a grounding in how a genetic algorithm works, we'll look at these variations and\npossible use cases for them.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.2,
                        "section_name": "Genetic programming",
                        "section_path": "./screenshots-images-2/chapter_5/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_20/df3dcadf-d4c3-43d0-afb4-0bc749bc3530.png",
                            "./screenshots-images-2/chapter_5/section_20/16f5f67b-6b35-450c-b10a-ab4664f7526d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Genetic programming\n\nGenetic programming follows a process similar to that of genetic algorithms but is used\nprimarily to generate computer programs to solve problems. The process described in the\nprevious section also applies here. The fitness of potential solutions in a genetic program-\nming algorithm is how well the generated program solves a computational problem. With\n\nthis in mind, we see that the tree-encoding method would work well here, because most\ncomputer programs are graphs consisting of nodes that indicate operations and processes.\nThese trees of logic can be evolved, so the computer program will be evolved to solve a\nspecific problem. One thing to note: these computer programs usually evolve to look like\na mess of code that\u2019s difficult for people to understand and debug.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.21,
                        "section_name": "Evolutionary programming",
                        "section_path": "./screenshots-images-2/chapter_5/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_21/abf1e09c-89d9-48ca-bc3a-d3f435adfd87.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Evolutionary programming\n\nEvolutionary programming is similar to genetic programming, but the potential solu-\ntion is parameters for a predefined fixed computer program, not a generated computer\nprogram. If a program requires finely tuned inputs, and determining a good combina-\ntion of inputs is difficult, a genetic algorithm can be used to evolve these inputs. The\nfitness of potential solutions in an evolutionary programming algorithm is determined\nby how well the fixed computer program performs based on the parameters encoded in\nan individual. Perhaps an evolutionary programming approach could be used to find\ngood parameters for an artificial neural network (chapter 9).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.22,
                        "section_name": "Glossary of evolutionary algorithm terms",
                        "section_path": "./screenshots-images-2/chapter_5/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_22/7d82156d-1bb4-4b90-8e6a-29ff403fac9a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Glossary of evolutionary algorithm terms\n\nHere is a useful glossary of evolutionary algorithms terms for future research and\nlearning:\n\nAllele\u2014The value of a specific gene in a chromosome\nChromosome\u2014A collection of genes that represents a possible solution\nIndividual\u2014A single chromosome in a population\n\nPopulation\u2014A collection of individuals\n\nGenotype\u2014The artificial representation of the potential solution population in\nthe computation space\n\nPhenotype\u2014The actual representation of the potential solution population in the\nreal world\n\nGeneration\u2014A single iteration of the algorithm\n\nExploration\u2014The process of finding a variety of possible solutions, some of\nwhich may be good and some of which may be bad\n\nExploitation\u2014The process of honing in on good solutions and iteratively\nrefining them\n\nFitness function\u2014A particular type of objective function\n\nObjective function\u2014A function that attempts to maximize or minimize\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.23,
                        "section_name": "More use cases for evolutionary algorithms",
                        "section_path": "./screenshots-images-2/chapter_5/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_23/cf598b07-a72c-4da3-be4c-2bc4b8d771cf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "More use cases for evolutionary algorithms\n\nSome of the use cases for evolutionary algorithms are listed in chapter 4, but many more\nexist. The following use cases are particularly interesting because they use one or more\nof the concepts discussed in this chapter:\n\nAdjusting weights in artificial neural networks\u2014Atrtificial neural networks are\ndiscussed later, in chapter 9, but a key concept is adjusting weights in the network\nto learn patterns and relationships in data. Several mathematical techniques\nadjust weights, but evolutionary algorithms are more efficient alternatives in the\nright scenarios.\n\nElectronic circuit design\u2014Electronic circuits with the same components can be\ndesigned in many configurations. Some configurations are more efficient than\nothers. If two components that work together often are closer together, this\nconfiguration may improve efficiency. Evolutionary algorithms can be used to\nevolve different circuit configurations to find the most optimal design.\n\nMolecular structure simulation and design\u2014As in electronic circuit design,\ndifferent molecules behave differently and have their own advantages and\ndisadvantages. Evolutionary algorithms can be used to generate different\nmolecular structures to be simulated and studied to determine their behavioral\nproperties.\n\nNow that we\u2019ve been through the general genetic algorithm life cycle in chapter 4 and\nsome advanced approaches in this chapter, you should be equipped to apply evolutionary\nalgorithms in your contexts and solutions.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 6,
                "chapter_name": "Swarm intelligence:\nAnts",
                "chapter_path": "./screenshots-images-2/chapter_6",
                "sections": [
                    {
                        "section_id": 6.1,
                        "section_name": "What is swarm intelligence?",
                        "section_path": "./screenshots-images-2/chapter_6/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_1/bcdf66b9-c39f-4fae-ae0d-71b43a696fcd.png",
                            "./screenshots-images-2/chapter_6/section_1/5c451b73-7ad5-47ee-904c-97de6e075b34.png",
                            "./screenshots-images-2/chapter_6/section_1/046eca17-6a0d-436c-ab4b-7ea8937de530.png",
                            "./screenshots-images-2/chapter_6/section_1/f30580ad-d249-4835-9747-1c79d649f608.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is swarm intelligence?\n\nSwarm intelligence algorithms are a subset of evolutionary algorithms that\nwere discussed in chapter 5 and are also known as nature-inspired algo-\nrithms. As with the theory of evolution, the observation of the behavior of\nlife forms in nature is the inspiration for the concepts behind swarm intel-\nligence. When we observe the world around us, we see many life forms that\nare seemingly primitive and unintelligent as individuals, yet exhibit intelli-\ngent emergent behavior when acting in groups.\n\nAn example of these life forms is ants. A single ant can carry 10 to 50\ntimes its own body weight and run 700 times its body length per minute.\n\nThese are impressive qualities; however, when acting in a group, that single ant can\naccomplish much more. In a group, ants are able to build colonies; find and retrieve\nfood; and even warn other ants, show recognition to other ants, and use peer pressure\nto influence others in the colony. They achieve these tasks by means of pheromones\u2014\nessentially, perfumes that ants drop wherever they go. Other ants can sense these per-\nfumes and change their behavior based on them. Ants have access to between 10 and 20\ntypes of pheromones that can be used to communicate different intentions. Because\nindividual ants use pheromones to indicate their intentions and needs, we can observe\nemergent intelligent behavior in groups of ants.\n\nFigure 6.1 shows an example of ants working as a team to create a bridge between two\npoints to enable other ants to carry out tasks. These tasks may be to retrieve food or\nmaterials for their colony.\n\nFigure 6.1 A group of ants working together to cross a chasm\n\nAn experiment based on real-life harvesting ants showed that they always converged to\nthe shortest path between the nest and the food source. Figure 6.2 depicts the difference\nin the colony movement from the start to when ants have walked their paths and\nincreased the pheromone intensity on those paths. This outcome was observed in a clas-\nsical asymmetric bridge experiment with real ants. Notice that the ants converge to the\nshortest path after just eight minutes.\n\nAfter 4 minutes After 8 minutes\n\nFigure 6.2 Asymmetric bridge experiment\n\nAnt colony optimization (ACO) algorithms simulate the emergent behavior shown in\nthis experiment. In the case of finding the shortest path, the algorithm converges to a\nsimilar state, as observed with real ants.\n\nSwarm intelligence algorithms are useful for solving optimization problems when\nseveral constraints need to be met in a specific problem space and an absolute best solu-\ntion is difficult to find due to a vast number of possible solutions\u2014some better and\nsome worse. These problems represent the same class of problems that genetic algorithms\naim to solve; the choice of algorithm depends on how the problem can be represented\nand reasoned about. We dive into the technicalities of optimization problems in particle\nswarm optimization in chapter 7. Swarm intelligence is useful in several real-world con-\ntexts, some of which are represented in figure 6.3.\n\nDetermine groupings of\n\nDetermine the global\ndata with similar\n\noptimal value for selving\n\nfeatures. preblems.\nClustering Optimization\na\nintelligence\nuses\nRouting =m Scheduling\nDetermine efficient | = | Determining when\npaths between things sheuld happen for\ndestinations. an efficient eutceme.\n\nFigure 6.3 Problems addressed by swarm optimization\n\nGiven the general understanding of swarm intelligence in ants, the following sections\nexplore specific implementations that are inspired by these concepts. The ant colony\noptimization algorithm is inspired by the behavior of ants moving between destinations,\ndropping pheromones, and acting on pheromones that they come across. The emergent\nbehavior is ants converging to paths of least resistance.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.2,
                        "section_name": "Problems applicable to\nant colony optimization",
                        "section_path": "./screenshots-images-2/chapter_6/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_2/f892b125-2c42-4ee5-b2ec-5d054db1b12a.png",
                            "./screenshots-images-2/chapter_6/section_2/5cfff375-c52f-4112-b391-edf045fec716.png",
                            "./screenshots-images-2/chapter_6/section_2/67253738-21f0-4f27-a430-babf2bcee62d.png",
                            "./screenshots-images-2/chapter_6/section_2/fa4c61a0-7e61-47bc-af26-1ff792f7fd5b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Problems applicable to\nant colony optimization\n\nImagine that we are visiting a carnival that has many attractions to experience. Each\nattraction is located in a different area, with varying distances between attractions.\nBecause we don\u2019t feel like wasting time walking too much, we will attempt to find the\nshortest paths between all the attractions.\n\nFigure 6.4 illustrates the attractions at a small carnival and the distances between\nthem. Notice that taking different paths to the attractions involves different total lengths\nof travel.\n\n2@e\nBalleen\n\nFerris wheel\n\nThe attractions The paths between attractions\n\nFigure 6.4 Carnival attractions and paths between them\n\nThe figure shows six attractions to visit, with 15 paths between them. This example\nshould look familiar. This problem is represented by a fully connected graph, as described\nin chapter 2. The attractions are vertices or nodes, and the paths between attractions are\nedges. The following formula is used to calculate the number of edges in a fully con-\nnected graph. As the number of attractions gets larger, the number of edges explodes:\n\nn(n-1)/2\nAttractions have different distances between them. Figure 6.5 depicts the distance on each\n\npath between every attraction; it also shows a possible path between all attractions. Note that\nthe lines in figure 6.5 showing the distances between the attractions are not drawn to scale.\n\nTotal distance: 34\nDistances en the paths A pessible reute te all attractions\n\nFigure 6.5 Distances between attractions and a possible path\n\nIf we spend some time analyzing the distances between all the attractions, we will find\nthat figure 6.6 shows an optimal path between all the attractions. We visit the attractions\nin this sequence: swings, Ferris wheel, circus, carousel, balloons, and bumper cars.\n\nte\n\nY\n\nWe. A\n/ il\n\nDistances on the paths The best possible route to all attractions\n\n=\n\nFigure 6.6 Distances between attractions and an optimal path\n\nThe small dataset with six attractions is trivial to solve by hand, but if we increase the\nnumber of attractions to 15, the number of possibilities explodes (figure 6.7). Suppose\nthat the attractions are servers, and the paths are network connections. Smart algorithms\nare needed to solve these problems.\n\nFigure 6.7 A larger dataset of attractions and paths between them\n\nEXERCISE: FIND THE SHORTEST PATH IN THIS CARNIVAL CONFIGURATION BY HAND\n\nSOLUTION: FIND THE SHORTEST PATH IN THIS CARNIVAL CONFIGURATION BY HAND\n\n\u00a7o*\n\n814\n\n#\n84 1154\n72.\n\nOne way to solve this problem computationally is to attempt a brute-force approach:\nevery combination of tours (a tour is a sequence of visits in which every attraction is\nvisited once) of the attractions is generated and evaluated until the shortest total dis-\ntance is found. Again, this solution may seem to be a reasonable solution, but in a large\ndataset, this computation is expensive and time-consuming. A brute-force approach\nwith 48 attractions runs for tens of hours before finding an optimal solution.\n\n>\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.3,
                        "section_name": "Representing state: What do paths\nand ants look like?",
                        "section_path": "./screenshots-images-2/chapter_6/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_3/5266ff5a-d2e1-499a-8227-e030a95a25cc.png",
                            "./screenshots-images-2/chapter_6/section_3/3519a916-c8eb-49b5-9c37-1cb1ec0b0b8b.png",
                            "./screenshots-images-2/chapter_6/section_3/89a258a5-ed29-43b1-b937-a62f229ae45e.png",
                            "./screenshots-images-2/chapter_6/section_3/edf805ff-1bda-4270-9c1a-0e18e9b38731.png",
                            "./screenshots-images-2/chapter_6/section_3/a148deb6-025c-4555-b793-0f5860af44bd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Representing state: What do paths\nand ants look like?\n\nGiven the Carnival Problem, we need to represent the data of the problem in a way that\nis suitable to be processed by the ant colony optimization algorithm. Because we have\nseveral attractions and all the distances between them, we can use a distance matrix to\nrepresent the problem space accurately and simply.\n\nA distance matrix is a 2D array in which every index represents an entity; the related\nset is the distance between that entity and another entity. Similarly, each index in the list\ndenotes a unique entity. This matrix is similar to the adjacency matrix that we dived into\nin chapter 2 (figure 6.8 and table 6.1).\n\nCircus\n\nBalleen\n\n#r Careusel\n\nFigure 6.8 An example of the Carnival Problem\n\nTable 6.1 Distances between attractions\n\nBumper Ferris\n\nCircus Balloons\n\nFerris 4 5 7 6 3 0\nwheel\n\nThe distances between attractions can be represented as a distance matrix, an array of\narrays in which a reference to x, y in the array references the distance between attractions\nxand y. Notice that the distance between the same attraction will be 0 because it\u2019s in the\nsame position. This array can also be created programmatically by iterating through\ndata from a file and creating each element:\n\nlet attraction_distances equal\nC\n(0,8,7,4,6,4},\n(8,0,5,7,11,5],\n(7,5,0,9,6,7],\n(4,7,9,0,5,6),\n(6,11,6,5,0,3],\n(4,5,7,6,3,0],\n\nThe next element to represent is the ants. Ants move to different attractions and leave\npheromones behind. Ants also make a judgment about which attraction to visit next.\nFinally, ants have knowledge about their respective total distance traveled. Here are the\nbasic properties of an ant (figure 6.9):\n\n+ Memory\u2014In the ACO algorithm, this is the list of attractions already visited.\n+ Best fitness\u2014This is the shortest total distance traveled across all attractions.\n\n+ Action\u2014Choose the next destination to visit, and drop pheromones along the way.\n\nMemery\n\nBest fitness\n\nAction\n\nFigure 6.9 Properties of an ant\n\nPseudocode\n\nAlthough the abstract concept of an ant entails memory, best fitness, and action, specific\ndata and functions are required to solve the Carnival Problem. To encapsulate the logic\nfor an ant, we can use a class. When an instance of the ant class is initialized, an empty\narray is initialized to represent a list of attractions that the ant will visit. Furthermore, a\nrandom attraction will be selected to be the starting point for that specific ant:\n\nAnt(attraction_count):\nlet ant .visited_attractions equal an empty array\nappend a random number between @ and\n\n(attraction_count - 1) to ant -visited_attractions\n\nThe ant class also contains several functions used for ant movement. The visit_*\nfunctions are used to determine to which attraction the ant moves to next. The visit _\nattraction function generates a random chance of visiting a random attraction. In\nthis case, visit_random_attraction is called; otherwise, roulette_wheel_\nselection is used with a calculated list of probabilities. More details are coming up in\nthe next section:\n\nAnt functions:\nvisit_attraction(pheromone_trails)\nvisit_random_attraction()\nvisit_probabilistic_attraction(pheromone_trails)\nroulette_wheel_selection (probabilities)\n\nget_distance_traveled()\n\nLast, the get_distance_traveled function is used to calculate the total distance\ntraveled by a specific ant, using its list of visited attractions. This distance must be min-\nimized to find the shortest path and is used as the fitness for the ants:\n\nget_distance_travelled (ant):\nlet total_distance equal 0\nfor a in range(1, length of ant .visited_attractions):\ntotal_distance += distance between ant visited _attractions[a-1]and\n\nant .visited_attractions[a]\n\nreturn total\n\nThe final data structure to design is the concept of pheromone trails. Similarly to the\ndistances between attractions, pheromone intensity on each path can be represented as\na distance matrix, but instead of containing distances, the matrix contains pheromone\nintensities. In figure 6.10, thicker lines indicate more-intense pheromone trails. Table 6.2\ndescribes the pheromone trails between attractions.\n\nThe paths between attractions Possible pheromone intensity en paths\n\nFigure 6.10 Example pheromone intensity on paths\n\nTable 6.2 Pheromone intensity between attractions\n\nFerris\nwheel\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.4,
                        "section_name": "The ant colony optimization\nalgorithm life cycle",
                        "section_path": "./screenshots-images-2/chapter_6/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_4/59065a48-3a09-4569-9b84-9e74e8262744.png",
                            "./screenshots-images-2/chapter_6/section_4/bd3c919e-2c1d-4e2f-a63b-c50393d4a630.png",
                            "./screenshots-images-2/chapter_6/section_4/18ad24cc-d238-4a77-9012-9ce124c7d47d.png",
                            "./screenshots-images-2/chapter_6/section_4/0fa2225b-f940-498c-af00-91b18b2d2c09.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The ant colony optimization\nalgorithm life cycle\n\nNow that we understand the data structures required, we can dive into the workings of\nthe ant colony optimization algorithm. The approach in designing an ant colony optimi-\nzation algorithm is based on the problem space being addressed. Each problem has a\nunique context and a different domain in which data is represented, but the principles\nremain the same.\n\nThat said, let\u2019s look into how an ant colony optimization algorithm can be configured\nto solve the Carnival Problem. The general life cycle of such an algorithm is as follows:\n\nInitialize the pheromone trails. Create the concept of pheromone trails between\nattractions, and initialize their intensity values.\n\nSet up the population of ants. Create a population of ants in which each ant starts\nat a different attraction.\n\nChoose the next visit for each ant. Choose the next attraction to visit for each ant\nuntil each ant has visited all attractions once.\n\nUpdate the pheromone trails. Update the intensity of pheromone trails based on\nthe ants\u2019 movements on them, as well as factor in evaporation of pheromones.\n\nUpdate the best solution. Update the best solution, given the total distance\ncovered by each ant.\n\nDetermine the stopping criteria. The process of ants visiting attractions repeats for\nseveral iterations. One iteration is every ant visiting all attractions once. The\nstopping criterion determines the total number of iterations to run. More\niterations allow ants to make better decisions based on the pheromone trails.\n\nFigure 6.11 describes the general life cycle of the ant colony optimization algorithm.\n\nthe\nnext visit\nfor each ant\n\nSetup\npheromones.\n\nUpdate the\npheremone\ntrails.\n\nReturn the Update the\n\nbest best\nselution. solution.\n\nFigure 6.11 The ant colony optimization algorithm life cycle\n\nInit\nThe first step in the ant colony optimization algorithm is to initialize the pheromone\ntrails. Because no ants have walked on the paths between attractions yet, the pheromone\ntrails will be initialized to 1. When we set all pheromone trails to 1, no trail has any\nadvantage over the others. The important aspect is defining a reliable data structure to\ncontain the pheromone trails, which we look at next (figure 6.12).\n\nize the pheromone trails\n\nSet up the Cheese the\n\nSetup\n\npheromones. population of next visit ; 4\n\nants. for each ant.\n\n|. Trivial but important:\nmake sure the pheromone Update the\nintensity is set to |. pheremene\n\ntrails.\n\nReturn the Update the\nbest best\nselution. solution.\n\nFigure 6.12 Set up the pheromones.\n\nThis concept can be applied to other problems in which instead of distances between\nlocations, the pheromone intensity is defined by another heuristic.\n\nIn figure 6.13, the heuristic is the distance between two destinations.\n\nPheremenes initialize at 1\n\nFigure 6.13 Initialization of pheromones\n\nPseudocode\n\nSimilarly to the attraction distances, the pheromone trails can be represented by a dis-\ntance matrix, but referencing x, y in this array provides the pheromone intensity on the\npath between attractions x and y. The initial pheromone intensity on every path is ini-\ntialized to 1. Values for all paths should initialize with the same number to prevent\nbiasing any paths from the start:\n\nlet pheromone_trails equal\n(\n(2,1,1,1,1,1),\n(2,1,1,1,1,1),\n(2,1,1,1,2,11,\n(2,1,1,1,1,11,\n(2,1,1,1,1,11,\n(2.2,1,1,1,1)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.5,
                        "section_name": "Set up the population of ants",
                        "section_path": "./screenshots-images-2/chapter_6/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_5/f0389d71-e130-4bb7-bdb8-92f5d5a4bc68.png",
                            "./screenshots-images-2/chapter_6/section_5/1e0c8987-4b2a-4be7-986b-3700772bf57f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Set up the population of ants\n\nThe next step of the ACO algorithm is creating a population of ants that will move\nbetween the attractions and leave pheromone trails between them (figure 6.14).\n\nSet up the Cheese the\n\npopulation ef next visit 4\nants. for each ant,\n\na. Ants are assigned to\nstart ot randomly Update the\n\nnate pheremene\nselected destinations. trails.\n\nReturn the Update the\nbest best\nsolution. solution.\n\nFigure 6.14 Set up the population of ants.\n\nAnts will start at randomly assigned attractions (figure 6.15)\u2014at a random point in a\npotential sequence because the ant colony optimization algorithm can be applied to\nproblems in which actual distance doesn\u2019t exist. After touring all the destinations, ants\nare set to their respective starting points.\n\nRED,\n\nAnts starting at\nrandom attractiens\n\nHRD,\n\ni ka\u201d\nPE\n\nFigure 6.15 Ants start at random attractions.\n\nWe can adapt this principle to a different problem. In a task-scheduling problem, each\nant starts at a different task.\n\nPseudocode\n\nSetting up the colony of ants includes initializing several ants and appending them to a\nlist where they can be referenced later. Remember that the initialization function of the\nant class chooses a random attraction to start at:\n\nsetup_ants(attraction_count, number_of_ants_factor):\nlet number_of_ants equal round(attraction_count * number_of_ants_factor}\nlet ant_colony equal to an empty array\nfor i in range(@, number_of_ants):\nappend new Ant to ant_colony\n\nreturn ant_colony\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.6,
                        "section_name": "Choose the next visit for each ant",
                        "section_path": "./screenshots-images-2/chapter_6/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_6/3f2f769c-5a9d-48bc-9848-584da414399a.png",
                            "./screenshots-images-2/chapter_6/section_6/426f7e34-3918-4c4b-b28c-7a380c2ecff3.png",
                            "./screenshots-images-2/chapter_6/section_6/6dbab51c-d66d-40c4-9a87-27055853c2fb.png",
                            "./screenshots-images-2/chapter_6/section_6/f9b48fa2-bb4f-49c9-90da-520200001abd.png",
                            "./screenshots-images-2/chapter_6/section_6/39fa54d9-cc3f-4fc8-8e20-e8faa34717d5.png",
                            "./screenshots-images-2/chapter_6/section_6/757993db-ee4f-48f7-bae0-d95730eeec98.png",
                            "./screenshots-images-2/chapter_6/section_6/995acf84-08d4-47e4-ba33-9fc4de54c192.png",
                            "./screenshots-images-2/chapter_6/section_6/7bc4facf-1bb1-44a1-b752-5bc209c51338.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Choose the next visit for each ant\n\nAnts need to select the next attraction to visit. They visit new attractions until they have\nvisited all attractions once, which is called a tour. Ants choose the next destination based\non two factors (figure 6.16):\n\n+ Pheromone intensities\u2014The pheromone intensity on all available paths\n\n+ Heuristic value\u2014A result from a defined heuristic for all available paths, which is\nthe distance of the path between attractions in the carnival example\n\nSet up the Cheese the\npopulation of next visit\nants. for each ant.\n\nSet up\npheromenes.\n\n3. Ants each decide on\nthe next destination Update the\n\nto visit based on the pheromone\npheromone trails and waite.\n\nReturn the Update the\nbest best\nsolution. solution.\n\nFigure 6.16 Choose the next visit for each ant.\n\nAnts will not travel to destinations they have already visited. If an ant has already visited\nthe bumper cars, it will not travel to that attraction again in the current tour.\n\nThe stochastic nature of ants\nThe ant colony optimization algorithm has an element of randomness. The intention is\nto allow ants the possibility of exploring less-optimal immediate paths, which might\nresult in a better overall tour distance.\n\nFirst, an ant has a random probability of deciding to choose a random destination. We\ncould generate a random number between 0 and 1, and if the result is 0.1 or less, the ant\n\nwill decide to choose a random destination; this is a 10% chance of choosing a ran-\ndom destination. If an ant decides that it will choose a random destination, it needs to\nrandomly select a destination to visit, which is a random selection between all available\ndestinations.\n\nSelecting destination based on a heuristic\n\nWhen an ant faces the decision of choosing the next destination that is not random, it\ndetermines the pheromone intensity on that path and the heuristic value by using the\nfollowing formula:\n\n(pheromones on path x)* * (1 /heuristic for path x)?\n\nquleie ((pheromones on path nj? * (1 / heuristic for path n)\u201d)\ndestinations\n\nAfter it applies this function to every possible path toward its respective destination, the\nant selects the destination with the best overall value to travel to. Figure 6.17 illustrates\nthe possible paths from the circus with their respective distances and pheromone\nintensities.\n\nDistance: 8\nDistance: 4 Preremene: &\nPheramens a J\n\n-@-,\n3\natmo\nDistance: 7\nDistance: 6 Pheremene. >\npreremene: 8\n\nwh\n\nFigure 6.17 Example of possible paths from the circus\n\nLet\u2019s work through the formula to demystify the calculations that are happening and\nhow the results affect decision-making (figure 6.18).\n\n(pheromones on path x)* * (1 /heuristic for path x)\n\nT T\nPheromone influence Heuristic influence\n\nFigure 6.18 The pheromone influence and heuristic influence of the formula\n\nThe variables alpha (a) and beta (b) are used to give greater weight to either the phero-\nmone influence or the heuristic influence. These variables can be adjusted to balance the\nant\u2019s judgment between making a move based on what it knows versus pheromone trails,\nwhich represent what the colony knows about that path. These parameters are defined\nup front and are usually not adjusted while the algorithm runs.\n\nThe following example works through each path starting at the circus and calculates\nthe probabilities of moving to each respective attraction.\n\n* a (alpha) is set to 1.\n+ b (beta) is set to 2.\n\nBecause b is greater than a, the heuristic influence is favored in this example.\nLet\u2019s work through an example of the calculations used to determine the probability\nof choosing a specific path (figure 6.19).\n\n(pheromones on path x)* * (1 /heuristic for path xp\n\navatinsie (({pheromones on path n)* * (1 / heuristic for path n)?)\ndestinations\n\nApply this te each\n\na \u00bb,\n((pheromones on path x)\" * (1 / heuristic for path x)\u201d) attraction.\n\nFerrigwheel: 11* (1/4) = 0.688\nSwings: 8 * (1/6)7 = 0.222\nCarousel: 10 * (1/4)? = 0.625\nBumper cars: 7* (1/7)? = 0.143\nBalloons: 5* (1/8)* = 0.078\n\navuliubia (Pheromones on path n)** (1 / heuristic for pathn)\u201d) = 1.756 < Sum ef all\naencinations\n\nFerris wheel: 0.688 /1.756 = 0.392 Highest probability:\nSwings: 0.222 /1.756 = 0.126 39.2%\nCarousel: 0.625 /1.756 = 0.356 High prebability:\n\n+143 / 1.756 = 0.081 38.6%\n.078 / 1.756 = 0.044\n\nFigure 6.19 Probability calculations for paths\n\nBumper cars:\nBalloons:\n\n\nAfter applying this calculation, given all the available destinations, the ant is left with the\noptions shown in figure 6.20.\n\n39.2%\n\n0.081\n\n12.6%\n\nFigure 6.20 The final probability of each attraction being selected\n\nRemember that only the available paths are considered; these paths have not been\nexplored yet. Figure 6.21 illustrates the possible paths from the circus, excluding the\nFerris wheel, because it\u2019s been visited already. Figure 6.22 shows probability calculations\nfor paths.\n\nDistance: #\nPheremene: #\n\nem\n\nFigure 6.21 Example of possible paths from the circus, excluding visited attractions\n\n(pheromones on path x)* * (1 /heuristic for path x)\n\nauaaie ((pheromones on path n)* * (1 / heuristic for path n)\u201d)\n\ndestinations\n\nApply this te each\n\na \u00bb.\n((pheromones on path x)** (1 / heuristic for path x)\u201d) attraction,\n\nSwings: 8* (1/6)* = 0.222\nCarousel: 10 * (1/4)* = 0.625\nBumper cars: 7* (1/7)? 2143\nBalloons: 5* (1/8)? =0.078\n\nrelia ((pheromones on path n)\" * (1 / heuristic for pathn)\u201d) = 1.068 < Sum of all\n\ntions\n\nSwings: 0.222 / 1.068 = 0.208 . a\nCarousel: 0.625 / 1.068 = 0.585 < Highest prebabitity:\nBumper cars: 0.143 / 1.068 = 0.134 :\nBalloons: 0.078 / 1.068 = 0.073\n\nFigure 6.22 Probability calculations for paths\n\nThe ant\u2019s decision now looks like figure 6.23.\n\n73%\n73\n\u2122 ? sane X 20.8%\n\n0.134\n0.208\n\niit 0.585 lum\n\ntr mee\n\nFigure 6.23 The final probability of each attraction being selected\n\n\nThe pseudocode for calculating the probabilities of visiting the possible attractions is\nclosely aligned with the mathematical functions that we have worked through. Some\ninteresting aspects of this implementation include:\n\nDetermining the available attractions to visit\u2014Because the ant would have visited\nseveral attractions, it should not return to those attractions. The possible _\nattractions array stores this value by removing visited_attractions\nfrom the complete list of attractions: all_attractions.\n\nUsing three variables to store the outcome of the probability calculations\u2014\npossible indexes stores the attraction indexes; possible _\nprobabilities stores the probabilities for the respective index; and total_\nprobabilities stores the sum of all probabilities, which should equal 1 when\nthe function is complete. These three data structures could be represented by a\nclass for a cleaner code convention.\n\nvisit_probabilistic_attraction(pheromone_trails, attraction_count, ant\n\nalpha, beta):\n\nlet current_attraction equal ant .visited_attractions(-1]\nlet all_attractions equal range(@, attraction_count)\n\nlet possible_attractions equal all_attractions - ant .visited_attractions\nlet possible_indexes equal empty array\nlet possible_probabilities equal empty array\n\nlet total_probabilities equal @\n\nfor attraction in possible_attractions:\n\nappend attraction to possible_indexes\n\nlet pheromones_on_path equal\nmath.pow(pheromone_trails(current_attraction](attraction], alpha)\n\nlet heuristic_for_path equal\nmath .pow(1/attraction_distances(current_attraction][attraction], beta)\n\nlet probability equal pheromones_on_path * heuristic_for_path\n\nappend probability to possible_probabilities\n\nadd probability to total_probabilities\n\nlet possible_probabilities equal (probability / total_probabilities\n\nfor probability in possible_probabilities]\n\nreturn [possible_indexes, possible_probabilities]\n\nWe meet roulette-wheel selection again. The roulette-wheel selection function takes the\npossible probabilities and attraction indexes as input. It generates a list of slices, each of\nwhich includes the index of the attraction in element 0, the start of the slice in index 1,\nand the end of the slice in index 2. All slices contain a start and end between 0 and 1. A\nrandom number between 0 and 1 is generated, and the slices that it falls into is selected\nas the winner:\n\nroulette_wheel_selection (possible indexes, possible_probabilities,\npossible_attraction_count):\nlet slices equal empty array\nlet total equal \u00ae\nfor i in range(0, possible_attractions_count):\nappend (possible _indexes[i], total, total + possible_probabilities[i]]\nto slices\ntotal += possible_probabilities(i]\nlet spin equal random(0, 1)\nlet result equal (slice for slice in slices if slice(1]< spin <= slice(2])\n\nreturn result\n\nNow that we have probabilities of selecting the different attractions to visit, we will use\nroulette-wheel selection.\n\nTo recap, roulette-wheel selection (from chapters 3 and 4) gives different possibili-\nties portions of a wheel based on their fitness. Then the wheel is \u201cspun,\u201d and an individ-\nual is selected. A higher fitness gives an individual a larger slice of the wheel, as shown in\nfigure 6.23 earlier in this chapter. The process of choosing attractions and visiting them\ncontinues for every ant until each one has visited all the attractions once.\n\nEXERCISE: DETERMINE THE PROBABILITIES OF VISITING THE ATTRACTIONS\nWITH THE FOLLOWING INFORMATION\n\nDistance: 9\nPheromone: #\n\nDistance: 11\nPheromene: 9\n\n\u2014 aa\nTay Distance: 14 ea\neo Pheromone: 11\nCod\nalpha = 2\nbeta =3\n\nSOLUTION: DETERMINE THE PROBABILITIES OF VISITING THE ATTRACTIONS\nWITH THE FOLLOWING INFORMATION\n\n(pheromones on path x)* * (1 / heuristic for path x)?\n\navatiabae ({pheromones on path n)* * (1 / heuristic for path n)P)\n\ndestinations\n\n((pheromones on path x)* \u00ab (1 / heuristic for path x)\u201d)\n\n0.067\n-061\n0.044\n\nSwings: 77* (1/9)\u00b0\nBumper cars: 97* (1/11)?\nBalloons: 11\u00b0* (1/14)?\n\n'e ((pheromones on path n)* * (1 / heuristic for path n)\u201d) = @.172\nne\n\nSwings: 0.067 / 0.172 = 0.39\nBumper cars: 0.061 / 0.172 = 0.355\nBalloons: 0.044 / 0.172 = 0.256\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.7,
                        "section_name": "Update the pheromone trails",
                        "section_path": "./screenshots-images-2/chapter_6/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_7/ba2c6aa8-5b4e-445d-bd27-f0b60a05beb1.png",
                            "./screenshots-images-2/chapter_6/section_7/b7e81e3f-5d7d-4b75-8947-c63332f7ebff.png",
                            "./screenshots-images-2/chapter_6/section_7/bde32be5-1292-4e17-81b3-f3d33aa17f8f.png",
                            "./screenshots-images-2/chapter_6/section_7/558add06-c9c9-45e3-8635-63bb32ada263.png",
                            "./screenshots-images-2/chapter_6/section_7/04ecfeba-e645-4305-bcd7-dd8bfea57777.png",
                            "./screenshots-images-2/chapter_6/section_7/caf90d89-377c-4fb6-ab45-873ef0d8fe41.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Update the pheromone trails\n\nNow that the ants have completed a tour of all the attractions, they have all left pheromones\nbehind, which changes the pheromone trails between the attractions (figure 6.24).\n\nSet up the\nPopulation of next visit\nante. for each ant.\n\nSetup\npheromenes.\n\nS. After all ants have\ncompleted their\nrespective tours,\nevaporate pheromone\ntrails, and the update\nthem given the ants\u2019\nmovements.\n\nReturn the Update the\nbest best\nsolution. solution.\n\nFigure 6.24 Update the pheromone trails.\n\nTwo steps are involved in updating the pheromone trails: evaporation and depositing\nnew pheromones.\n\nUpdating pheromones due to evaporation\n\nThe concept of evaporation is also inspired by nature. Over time, the pheromone trails\nlose their intensity. Pheromones are updated by multiplying their respective current val-\nues by an evaporation factor\u2014a parameter that can be adjusted to tweak the perfor-\nmance of the algorithm in terms of exploration and exploitation. Figure 6.25 illustrates\nthe updated pheromone trails due to evaporation.\n\ne IX 6 e : Em\n7 as\nKe ap Ke a,\n4 an) 48 26 4\n? 38 \"\n5 7 25 7\n\u2018 Pr , \u2018 t \u00b0\n\nPheromenes en paths Pheremenes on paths after 50% evaperation\n\nFigure 6.25 Example of updating pheromone trails for evaporation\n\nUpdating pheromones based on ant tours\nPheromones are updated based on the ants that have moved along the paths. If more ants\nmove on a specific path, there will be more pheromones on that path.\n\nEach ant contributes its fitness value to the pheromones on every path it has\nmoved on. The effect is that ants with better solutions have a greater influence on the\nbest paths. Figure 6.26 illustrates the updated pheromone trails based on ant movements\non the paths.\n\n>\n\n5\n\nAth.\n2\n\nth .\n\nPheremenes on paths after evaporation\n\nJ\n\nAnt A total: 25\n\n1/25 = 0.04\n\n& 25s0e\n\neas\nBeene\n\n22\n\nF  veos ~\n\n4 25+20029 3.5 +004\nFresh\n\nPheromone addition after ant update\n\n45 2.624 4.029\n\nPheromones after ant update\n\nFigure 6.26 Pheromone updates based on ant movements\n\nEXERCISE: CALCULATE THE PHEROMONE UPDATE GIVEN THE\n\nFOLLOWING SCENARIO\n\u00b0\nSie a le /\n\\5\n\nCalculate 50%\n\n28 + evaperation\n\n\u201c\n\n.\nAR. hn\na 2\n\nPheromones on paths\n\n-\n\naoe\no\n\nAnt A path Ant B path\n\nSOLUTION: CALCULATE THE PHEROMONE UPDATE GIVEN THE\nFOLLOWING SCENARIO\n\nPheromones on paths Pheromones on paths after 50% evaperation\n\n2. e.\n:\n\nLPS h 004+ 4\naseobs Xissee Sh ees +0029 tas\n\n= Py\n\n(sore)\nth 0.75 + 0.029\n\nPheremone addition after ant update Pheremones after ant update\n\n\nThe update_pheromones function applies two important concepts to the phero-\nmone trails. First, the current pheromone intensity is evaporated based on the evapora-\ntion rate. If the evaporation rate is 0.5, for example, the intensity decreases by half. The\nsecond operation adds pheromones based on ant movements on that path. The amount\nof pheromones contributed by each ant is determined by the ant\u2019s fitness, which in this\ncase is each respective ant\u2019s total distance traveled:\nupdate_pheromones(evaporation_rate, pheromone_trails, attraction_count):\nfor x in range(@, attraction_count):\nfor y in range(0, attraction_count):\nlet pheromone_trails(x]{y] equal\npheromone_trails(x){y] * evaporation_rate\nfor ant in ant_colony:\n\npheromone_trails(x](y]+=1/ ant.get_distance_traveled()\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.8,
                        "section_name": "Update the best solution",
                        "section_path": "./screenshots-images-2/chapter_6/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_8/d203aeeb-513f-426a-94ea-0bc0b227cbd5.png",
                            "./screenshots-images-2/chapter_6/section_8/65f6e838-eaf3-4e0c-8fb1-8aa0acee447d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Update the best solution\nThe best solution is described by the sequence of attraction visits that has the lowest total\ndistance (figure 6.27).\n\nSetup\npheromones.\n\nReturn the\nbest\nselution.\n\nSet up the Cheese the\npopulation of next visit\nants. for each ant.\n\nUpdate the\npheromone\ntrails.\n\nUpdate the\n\nReached\n\nstopping\ncondi tion,\n\nFigure 6.27 Update the best solution.\n\n6. Looking at all\nthe ants in the\npopulation, choose\nthe best solution\nfrom the best ant.\n\nAfter an iteration, after every ant has completed a tour (a tour is complete when an ant\nvisits every attraction), the best ant in the colony must be determined. To make this\ndetermination, we find the ant that has the lowest total distance traveled and set it as the\nnew best ant in the colony:\nget_best (ant_population, previous_best_ant):\nlet best_ant equal previous_best_ant\nfor ant in ant_population:\nlet distance_traveled equal ant .get_distance_traveled()\nif distance_traveled < best_ant.best_distance:\nlet best_ant equal ant\n\nreturn best_ant\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.9,
                        "section_name": "Determine the stopping criteria",
                        "section_path": "./screenshots-images-2/chapter_6/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_9/ba87ec46-b8e7-49d7-8caf-270211135ea3.png",
                            "./screenshots-images-2/chapter_6/section_9/9d88bf5d-f55d-437a-82c7-a443e0d35add.png",
                            "./screenshots-images-2/chapter_6/section_9/2ca01acb-4223-433c-bb3b-233889593b73.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Determine the stopping criteria\n\nThe algorithm stops after several iterations: conceptually, the number of tours that the\ngroup of ants concludes. Ten iterations means that each ant does 10 tours; each ant would\nvisit each attraction once and do that 10 times (figure 6.28).\n\nChoose the\nnext visit\nfor each ant.\n\nSet up the\npopulation ef\nants.\n\nSetup\npheremenes.\n\n1. The algorithm can\u2019t continue\n\nindefinitely. ey creating\n\nstopping criteria, the algorithm Update the\nconverges to good solutions pheromone\nwithout unnecessary iterations. trails.\n\nUpdate the\nbest\nsolution.\n\nReturn the\nbest\nsolution.\n\nFigure 6.28 Reached stopping condition?\n\nThe stopping criteria for the ant colony optimization algorithm can differ based on the\ndomain of the problem being solved. In some cases, realistic limits are known, and when\nthey\u2019re unknown, the following options are available:\n+ Stop when a predefined number of iterations is reached. In this scenario, we define\na total number of iterations for which the algorithm will always run. If 100\niterations are defined, each ant completes 100 tours before the algorithm\nterminates.\n\n+ Stop when the best solution stagnates. In this scenario, the best solution after\neach iteration is compared with the previous best solution. If the solution\ndoesn\u2019t improve after a defined number of iterations, the algorithm terminates.\nIf iteration 20 resulted in a solution with fitness 100, and that iteration is\nrepeated up until iteration 30, it is likely (but not guaranteed) that no better\nsolution exists.\n\nThe solve function ties everything together and should give you a better idea of the\nsequence of operations and the overall life cycle of the algorithm. Notice that the algo-\nrithm runs for several defined total iterations. The ant colony is also initialized to its\nstarting point at the beginning of each iteration, and a new best ant is determined after\neach iteration:\n\nsolve(total_iterations, evaporation_rate, number_of_ants_factor,\nattraction_count):\nlet pheromone_trails equal setup_pheromones()\nlet best_ant equal Nothing\nfor i in range(\u00ae, total_iterations):\nlet ant_colony equal setup_ants(number_of_ants_factor)\nfor r in range(0, attraction_count -1):\nmove_ants(ant_colony)\nupdate_pheromones(evaporation_rate,\npheromone_trails,\nattraction_count)\n\nlet best_ant equal get_best (ant_colony)\n\nWe can tweak several parameters to alter the exploration and exploitation of the ant\ncolony optimization algorithm. These parameters influence how long the algorithm will\ntake to find a good solution. Some randomness is good for exploring. Balancing the\nweighting between heuristics and pheromones influences whether ants attempt a greedy\n\nsearch (when favoring heuristics) or trust pheromones more. The evaporation rate also\ninfluences this balance. The number of ants and the total number of iterations they have\ninfluences the quality of a solution. When we add more ants and more iterations, more\ncomputation is required. Based on the problem at hand, time to compute may influence\nthese parameters (figure 6.29):\n\nSet the probability of ants choesing a random attraction to visit (@.0 - 1.0) (0% - 100%)\nRANDOM_ATTRACTION_FACTOR = 0.3\n\nSet the weight fer pheremenes on path for selection by ants.\nALPHA = 4\n\nSet the weight fer heuristic of path fer selection by ants.\nBETA =7\n\nSet the percentage of ants in the colony based on the tetal number ef attractions.\n\nNUMBER_OF_ANTS_FACTOR = 0.5\n\nSet the number of tours ants must complete.\nTOTAL_ITERATIONS = 1000\n\nSet the rate of pheremene evaporation (0.0 - 1.0) (0% - 100%).\n\nEVAPORATION_RATE = 0.4\n\nFigure 6.29 Parameters that can be tweaked in the ant colony optimization algorithm\n\nNow you have insight into how ant colony optimization algorithms work and how they\ncan be used to solve the Carnival Problem. The following section describes some other\npossible use cases. Perhaps these examples may help you find uses for the algorithm in\nyour work.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.1,
                        "section_name": "Use cases for ant colony\noptimization algorithms",
                        "section_path": "./screenshots-images-2/chapter_6/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_10/ff0ea08a-ae60-463c-9ea3-f22de38d7ed2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Use cases for ant colony\noptimization algorithms\n\nAnt colony optimization algorithms are versatile and useful in several real-world appli-\ncations. These applications usually center on complex optimization problems such as the\nfollowing:\n\nRoute optimization\u2014Routing problems usually include several destinations that\nneed to be visited with several constraints. In a logistics example, perhaps the\ndistance between destinations, traffic conditions, types of packages being\ndelivered, and times of day are important constraints that need to be considered\nto optimize the operations of the business. Ant colony optimization algorithms\ncan be used to address this problem. The problem is similar to the carnival\nproblem explored in this chapter, but the heuristic function is likely to be more\ncomplex and context specific.\n\nJob scheduling\u2014Job scheduling is present in almost any industry. Nurse shifts are\nimportant to ensure that good health care can be provided. Computational jobs\non servers must be scheduled in an optimal manner to maximize the use of the\nhardware without waste. Ant colony optimization algorithms can be used to\nsolve these problems. Instead of looking at the entities that ants visit as locations,\nwe see that ants visit tasks in different sequences. The heuristic function includes\nconstraints and desired rules specific to the context of the jobs being scheduled.\nNurses, for example, need days off to prevent fatigue, and jobs with high\npriorities on a server should be favored.\n\nImage processing\u2014The ant colony optimization algorithm can be used for edge\ndetection in image processing. An image is composed of several adjacent pixels,\nand the ants move from pixel to pixel, leaving behind pheromone trails. Ants\ndrop stronger pheromones based on the pixel colors\u2019 intensity, resulting in\npheromone trails along the edges of objects containing the highest density of\npheromones. This algorithm essentially traces the outline of the image by\nperforming edge detection. The images may require preprocessing to decolorize\nthe image to grayscale so that the pixel-color values can be compared\nconsistently.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 7,
                "chapter_name": "Swarm intelligence:\nParticles",
                "chapter_path": "./screenshots-images-2/chapter_7",
                "sections": [
                    {
                        "section_id": 7.1,
                        "section_name": "What is particle swarm optimization?",
                        "section_path": "./screenshots-images-2/chapter_7/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_1/39702914-3ae5-4ade-afc0-4028e7ddde13.png",
                            "./screenshots-images-2/chapter_7/section_1/20b238c9-c8be-4eaa-9e5f-715820d86b94.png",
                            "./screenshots-images-2/chapter_7/section_1/5f25b1c3-66f2-4c62-a90f-1db173f0448d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is particle swarm optimization?\n\nParticle swarm optimization is another swarm algorithm. Swarm intelli-\ngence relies on emergent behavior of many individuals to solve difficult\nproblems as a collective. We saw in chapter 6 how ants can find the shortest\npaths between destinations through their use of pheromones.\n\nBird flocks are another ideal example of swarm intelligence in nature.\nWhen a single bird is flying, it might attempt several maneuvers and tech-\nniques to preserve energy, such as jumping and gliding through the air or\nleveraging wind currents to carry it in the direction in which it intends to\ntravel. This behavior indicates some primitive level of intelligence in a\n\nsingle individual. But birds also have the need to migrate during different seasons. In\nwinter, there is less availability of insects and other food. Suitable nesting locations also\nbecome scarce. Birds tend to flock to warmer areas to take advantage of better weather\nconditions, which improves their likelihood of survival. Migration is usually not a short\ntrip. It takes thousands of kilometers of movement to arrive at an area with suitable con-\nditions. When birds travel these long distances, they tend to flock. Birds flock because\nthere is strength in numbers when facing predators; additionally, it saves energy. The\nformation that we observe in bird flocks has several advantages. A large, strong bird will\ntake the lead, and when it flaps its wings, it creates uplift for the birds behind it. These\nbirds can fly while using significantly less energy. Flocks can change leaders if the direc-\ntion changes or if the leader becomes fatigued. When a specific bird moves out of forma-\ntion, it experiences more difficulty in flying via air resistance and corrects its movement\nto get back into formation. Figure 7.1 illustrates a bird flock formation; you may have\nseen something similar.\n\nFigure 7.1 An example bird flock formation\n\nCraig Reynolds developed a simulator program in 1987 to understand the attributes of\nemergent behavior in bird flocks and used the following rules to guide the group. These\nrules are extracted from observation of bird flocks:\n\n+ Alignment\u2014An individual should steer in the average heading of its neighbors to\nensure that the group travels in a similar direction.\n\n+ Cohesion\u2014An individual should move toward the average position of its\nneighbors to maintain the formation of the group.\n\n+ Separation\u2014An individual should avoid crowding or colliding with its neighbors\nto ensure that individuals do not collide, disrupting the group.\n\nAdditional rules are used in different variants of attempting to simulate swarm behavior.\nFigure 7.2 illustrates the behavior of an individual in different scenarios, as well as the\ndirection in which it is influenced to move to obey the respective rule. Adjusting move-\nment is a balance of these three principles shown in the figure.\n\nSeparation Alignment Cohesion\n\nFigure 7.2 Rules that guide a swarm\n\nParticle swarm optimization involves a group of individuals at different points in the\nsolution space, all using real-life swarm concepts to find an optimal solution in the\nspace. This chapter dives into the workings of the particle swarm optimization algo-\nrithm and shows how it can be used to solve problems. Imagine a swarm of bees that\nspreads out looking for flowers and gradually converges on an area that has the most\ndensity of flowers. As more bees find the flowers, more are attracted to the flowers. At its\ncore, this example is what particle swarm optimization entails (figure 7.3).\n\nFigure 7.3 A bee swarm converging on its goal\n\nOptimization problems have been mentioned in several chapters. Finding the optimal\npath through a maze, determining the optimal items for a knapsack, and finding the\noptimal path between attractions in a carnival are examples of optimization problems.\nWe worked through them without diving into the details behind them. From this chap-\nter on, however, a deeper understanding of optimization problems is important. The\nnext section works through some of the intuition to be able to spot optimization prob-\nlems when they occur.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.2,
                        "section_name": "Optimization problems: A slightly more\ntechnical perspective",
                        "section_path": "./screenshots-images-2/chapter_7/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_2/d87f582b-a719-4036-9c32-6d64333df54f.png",
                            "./screenshots-images-2/chapter_7/section_2/0c0ad70e-39ba-4e7a-bc0f-e03bf40916a9.png",
                            "./screenshots-images-2/chapter_7/section_2/939d1db6-d581-4c49-afa5-cc903a285c1b.png",
                            "./screenshots-images-2/chapter_7/section_2/6d0a5061-ed6d-4966-9296-4ca3346b8369.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optimization problems: A slightly more\ntechnical perspective\n\nSuppose that we have several peppers of different sizes. Usually, small peppers tend to be\n\nspicier than large peppers. If we plot all the peppers on a chart based on size and spici-\nness, it may look like figure 7.4.\n\nPepper spice\n\nPepper size\n\nFigure 7.4 Pepper spice vs. pepper size\n\nThe figure depicts the size of each pepper and how spicy it is. Now, by removing the\nimagery of the peppers, plotting the data points, and drawing a possible curve between\nthem, we are left with figure 7.5. If we had more peppers, we would have more data\npoints, and the curve would be more accurate.\n\nHot\n\nPepper spice\ne\n\nMild\n\ntom 200m 280m\n\nPepper size\n\nFigure 7.5 Pepper spice vs. pepper size trend\n\nThis example could potentially be an optimization problem. If we searched for a mini-\nmum from left to right, we would come across several points less than the previous\nones, but in the middle, we encounter one that is higher. Should we stop? If we did, we\nwould be missing the actual minimum, which is the last data point, known as the global\nminimum.\n\nThe trend line/curve that is approximated can be represented by a function, such as the\none shown in figure 7.6. This function can be interpreted as the spiciness of the pepper\nbeing equal to the result of this function where the size of the pepper is represented by x.\n\nf(x) = -(@ \u2014 4)(a \u2014 0.2)(a \u2014 2)(a \u2014 3) +5\n\nFigure 7.6 An example function for pepper spice vs. pepper size\n\nReal-world problems typically have thousands of data points, and the minimum output\nof the function is not as clear as this example. The search spaces are massive and difficult\nto solve by hand.\n\nNotice that we have used only two properties of the pepper to create the data points,\nwhich resulted in a simple curve. If we consider another property of the pepper, such as\ncolor, the representation of the data changes significantly. Now the chart has to be rep-\nresented in 3D, and the trend becomes a surface instead of a curve. A surface is like a\nwarped blanket in three dimensions (figure 7.7). This surface is also represented as a\nfunction but is more complex.\n\nFigure 7.7 Pepper spice vs. pepper size vs. pepper color\n\nFurthermore, a 3D search space could look fairly simple, like figure 7.7, or be so com-\nplex that attempting to inspect it visually to find the minimum would be almost impos-\nsible (figure 7.8).\n\nFigure 7.8 A function visualized in the 3D space as a plane\nFigure 7.9 shows the function that represents this plane.\n\nf(x,y) = \u2014 (y + 47) sin [5 + (y+47)| \u2014 asin ,/|x \u2014 (y+ 47)|\n\nFigure 7.9 The function that represents the surface in figure 7.8\n\nIt gets more interesting! We have looked at three attributes of a pepper: its size, its color,\nand how spicy it is. As a result, we're searching in three dimensions. What if we want to\ninclude the location of growth? This attribute would make it even more difficult to visu-\nalize and understand the data, because we are searching in four dimensions. If we add\nthe pepper\u2019s age and the amount of fertilizer used while growing it, we are left with a\nmassive search space in six dimensions, and we can\u2019t imagine what this search might\nlook like. This search too is represented by a function, but again, it is too complex and\ndifficult for a person to solve.\n\nParticle swarm optimization algorithms are particularly good at solving difficult\noptimization problems. Particles are distributed over the multidimensional search space\nand work together to find good maximums or minimums.\n\nParticle swarm optimization algorithms are particularly useful in the following\n\nscenarios:\n\n+ Large search spaces\u2014There are many data points and possibilities of\ncombinations.\n\n+ Search spaces with high dimensions\u2014There is complexity in high dimensions.\nMany dimensions of a problem are required to find a good solution.\n\nEXERCISE: HOW MANY DIMENSIONS WILL THE SEARCH SPACE FOR THE\nFOLLOWING SCENARIO BE?\n\nIn this scenario, we need to determine a good city to live in based on the aver-\nage minimum temperature during the year, because we don\u2019t like the cold. It\nis also important that the population be less than 700,000 people, because\ncrowded areas can be inconvenient. The average property price should be as\nlittle as possible, and the more trains in the city, the better.\n\nSOLUTION: HOW MANY DIMENSIONS WILL THE SEARCH SPACE FOR THE\nFOLLOWING SCENARIO BE?\n\nThe problem in this scenario consists of five dimensions:\n\nAverage temperature\nSize of population\nAverage price of property\nNumber of trains\n\nResult of these attributes, which will inform our decision\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.3,
                        "section_name": "Problems applicable to particle\nswarm optimization",
                        "section_path": "./screenshots-images-2/chapter_7/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_3/143109f6-62ca-450f-8a09-444753b024b5.png",
                            "./screenshots-images-2/chapter_7/section_3/6f50bf61-5c60-4637-bd84-4e45401ac14f.png",
                            "./screenshots-images-2/chapter_7/section_3/64b4b88d-3244-4346-b531-75ee886c6e4e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Problems applicable to particle\nswarm optimization\n\nImagine that we are developing a drone, and several materials are used to create its body\nand propeller wings (the blades that make it fly). Through many research trials, we have\nfound that different amounts of two specific materials yield different results in terms of\noptimal performance for lifting the drone and resisting strong winds. These two mate-\nrials are aluminum, for the chassis, and plastic, for the blades. Too much or too little of\neither material will result in a poor-performing drone. But several combinations yield\na good-performing drone, and only one combination results in an exceptionally well-\nperforming drone.\n\nFigure 7.10 illustrates the components made of plastic and the components made of\naluminum. The arrows illustrate the forces that influence the performance of the drone.\nIn simple terms, we want to find a good ratio of plastic to aluminum for a version of the\ndrone that reduces drag during lift and decreases wobble in the wind. So plastic and\naluminum are the inputs, and the output is the resulting stability of the drone. Let\u2019s\ndescribe ideal stability as reducing drag during liftoff and wobble in the wind.\n\n\u2014\u2014\u2014\u2014\u2014\u2014> _ Wobble in wind\nDrag during lift\nSS | Plastic\n\nAluminum\n\nGood performance = Lew drag and less wobble in the wind\n\nFigure 7.10 The drone optimization example\n\nPrecision in the ratio of aluminum and plastic is important, and the range of possibilities\nis large. In this scenario, researchers have found the function for the ratio of aluminum\nand plastic. We will use this function in a simulated virtual environment that tests the\ndrag and wobble to find the best values for each material before we manufacture another\nprototype drone. We also know that the maximum and minimum ratios for the materi-\nals are 10 and -10, respectively. This fitness function is similar to a heuristic.\n\nFigure 7.11 shows the fitness function for the ratio between aluminum (x) and plastic\n(y). The result is a performance score based on drag and wobble, given the input values\nfor xand y.\n\nf(x,y) = (a + 2y \u2014 7)? + (22 + y \u2014 5)?\n\nFigure 7.11 The example function for optimizing aluminum (x) and plastic (y)\n\nHow can we find the amount of aluminum and the amount of plastic required to create\na good drone? One possibility is to try every combination of values for aluminum and\nplastic until we find the best ratio of materials for our drone. Take a step back and imag-\nine the amount of computation required to find this ratio. We could conduct an\nalmost-infinite number of computations before finding a solution if we try every possi-\nble number. We need to compute the result for the items in table 7.1. Note that negative\nnumbers for aluminum and plastic are bizarre in reality; however, we're using them in\nthis example to demonstrate the fitness function used to optimize these values.\n\nTable 7.1 Possible values for aluminum and plastic compositions\n\n0.3623\n\nHow many parts aluminum? (x) How many parts plastic? (y)\n0.1 1.34\n-0.134 0.575\nAd 0.24\n1.1645 1.432\n-2.034 -0.65\n2.12 -0.874\n\n0.743 71.1645\n\n1.87\n\n1.75 -2.7756\n-10 > Aluminum 2 10 -10> Plastic = 10\n\nThis computation will go on for every possible number between the constraints and is\ncomputationally expensive, so it is realistically impossible to brute-force this problem. A\nbetter approach is needed.\n\nParticle swarm optimization provides a means to search a large search space without\nchecking every value in each dimension. In the drone problem, aluminum is one dimen-\nsion of the problem, plastic is the second dimension, and the resulting performance of\nthe drone is the third dimension.\n\nIn the next section, we determine the data structures required to represent a particle,\nincluding the data about the problem that it will contain.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.4,
                        "section_name": "Representing state:\nWhat do particles look like?",
                        "section_path": "./screenshots-images-2/chapter_7/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_4/00cf1cf7-2274-4daf-9fe2-66b857c83bd8.png",
                            "./screenshots-images-2/chapter_7/section_4/b57391ba-c45a-46ec-b902-6f0a61b17f06.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Representing state:\nWhat do particles look like?\n\nBecause particles move across the search space, the concept of a particle must be defined\n(figure 7.12).\n\nCurrent position\n\nBest pesitien\n\nVelocity\n\nFigure 7.12 Properties of a particle\n\nThe following represent the concept of a particle:\n+ Position\u2014The position of the particle in all dimensions\n+ Best position\u2014The best position found using the fitness function\n\n+ Velocity\u2014The current velocity of the particle\u2019s movement\n\nTo fulfill the three attributes of a particle, including position, best position, and velocity,\nthe following properties are required in a constructor of the particle for the various\noperations of the particle swarm optimization algorithm. Don\u2019t worry about the inertia,\ncognitive component, and social component right now; they will be explained in upcom-\ning sections:\n\nParticle(x,y, inertia, cognitive_constant, social_constant):\nlet particle.x equal to x\nlet particle.y equaltoy\nlet particle.fitness equal to infinity\nlet particle velocity equal to?\nlet particle .best_x equal tox\nlet particle.best_y equaltoy\nlet particle .best_fitness equal to infinity\nlet particle.inertia equal to inertia\nlet particle.cognitive_constant equal to cognitive_constant\n\nlet particle.social_constant equal to social_constant\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.5,
                        "section_name": "Particle swarm optimization life cycle",
                        "section_path": "./screenshots-images-2/chapter_7/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_5/2d5df370-6a53-4c00-bafd-a31bf625f4f5.png",
                            "./screenshots-images-2/chapter_7/section_5/70dec022-88d9-452c-a957-9eb9613ea7c1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Particle swarm optimization life cycle\n\nThe approach to designing a particle swarm optimization algorithm is based on the\nproblem space being addressed. Each problem has a unique context and a different\ndomain in which data is represented. Solutions to different problems are also measured\ndifferently. Let\u2019s dive into how a particle swarm optimization can be designed to solve\nthe drone construction problem.\n\nThe general life cycle of a particle swarm optimization algorithm is as follows\n(figure 7.13):\n\n1. Initialize the population of particles. Determine the number of particles to be used,\nand initialize each particle to a random position in the search space.\n\n2. Calculate the fitness of each particle. Given the position of each particle, determine\nthe fitness of that particle at that position.\n\n3. Update the position of each particle. Repetitively update the position of all the\nparticles, using principles of swarm intelligence. Particles will explore the search\nspace and then converge to good solutions.\n\n4. Determine the stopping criteria. Determine when the particles stop updating and\nthe algorithm stops.\n\nUpdate\npositions\nef particles.\n\nCalculate\nfitness of\nparticles.\n\nCaloulate\nfitness of\nparticles.\n\nSetup\nparticles.\n\nReturn\nbest\nsolution.\n\nFigure 7.13 The life cycle of a particle swarm optimization algorithm\n\nThe particle swarm optimization algorithm is fairly simple, but the details of step 3 are\nparticularly intricate. The following sections look at each step in isolation and uncover\nthe details that make the algorithm work.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.6,
                        "section_name": "Initialize the population of particles",
                        "section_path": "./screenshots-images-2/chapter_7/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_6/f754fb57-df9c-4e88-9d98-25215239ebb4.png",
                            "./screenshots-images-2/chapter_7/section_6/11d65ea2-b42a-481d-8877-259b94990eff.png",
                            "./screenshots-images-2/chapter_7/section_6/c7b43914-133f-49ca-89b2-c420349d21ae.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Initialize the population of particles\n\nThe algorithm starts by creating a specific number of particles, which will remain the\nsame for the lifetime of the algorithm (figure 7.14).\n\na 2 3 4\n\nUpdate\npositions\nef particles.\n\nCalovlate\nfitness of\nparticles.\n\nparticles.\n\nparticles.\n\n|. Create a swarm of particles with\neach particle\u2019s position initialized to a.\nrandom point, and set the particles\u2019\nvelocities to zero.\n\nFigure 7.14 Set up the particles.\n\nThe three factors that are important in initializing the particles are (figure 7.15):\n\nNumber of particles\u2014The number of particles influences computation. The more\nparticles that exist, the more computation is required. Additionally, more\nparticles will likely mean that converging on a global best solution will take\nlonger because more particles are attracted to their local best solutions. The\nconstraints of the problem also affect the number of particles. A larger search\nspace may need more particles to explore it. There could be as many as 1,000\nparticles or as few as 4. Usually, 50 to 100 particles produce good solutions\nwithout being too computationally expensive.\n\nStarting position for each particle\u2014The starting position for each particle should\nbe a random position in all the respective dimensions. It is important that the\nparticles are distributed evenly across the search space. If most of the particles\nare in a specific region of the search space, they will struggle to find solutions\noutside that area.\n\n+ Starting velocity for each particle\u2014The velocity of particles is initialized to 0\nbecause the particles have not been affected yet. A good analogy is that birds\nbegin takeoff for flight from a stationary position.\n\nxan? od\n\nFigure 7.15 A visualization of the initial positions of four particles in a 3D plane\n\nTable 7.2 describes the data encapsulated by each particle at the initialization step of the\nalgorithm. Notice that the velocity is 0; the current fitness and best fitness values are 0\nbecause they have not been calculated yet.\n\nTable 7.2 Data attributes for each particle\n\nCurrent | Current Best\naluminum | plastic Best plastic\n(x) ly) aluminum (x) | (y)\n\n\nThe method to generate a swarm consists of creating an empty list and appending new\nparticles to it. The key factors are:\n\n+ Ensuring that the number of particles is configurable.\n\n+ Ensuring that the random number generation is done uniformly; numbers are\ndistributed across the search space within the constraints. This implementation\ndepends on the features of the random number generator used.\n\n+ Ensuring that the constraints of the search space are specified: in this case, -10\nand 10 for both x and y of the particle.\n\ngenerate_swarm(number_of_particles):\nlet particles equal an empty list\nfor particle in range(number_of_particles):\nappend Particle (random (-10, 10), random (-10, 10), INERTIA,\nCOGNITIVE_CONSTANT, SOCIAL_CONSTANT) to particles\n\nreturn particles\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.7,
                        "section_name": "Calculate the fitness of each particle",
                        "section_path": "./screenshots-images-2/chapter_7/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_7/6f88113d-5fd4-43d2-89e2-e423ab05ce52.png",
                            "./screenshots-images-2/chapter_7/section_7/64b87c78-2f30-4052-8f28-7c3b8739feee.png",
                            "./screenshots-images-2/chapter_7/section_7/f9723ac7-2ec7-4d96-8440-454e678f8554.png",
                            "./screenshots-images-2/chapter_7/section_7/563862ae-7529-4339-a113-13894a67b907.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Calculate the fitness of each particle\nThe next step is calculating the fitness of each particle at its current position. The fitness\nof particles is calculated every time the entire swarm changes position (figure 7.16).\n\n1 2 3 4\n\nSet up Caloulate Update Calculate\nparticles. fitness of positions fltn\nparticles. of particles.\n\na. Determine how well each particle\nsolves the problem using the Fitness\nfunction.\n\nFigure 7.16 Calculate the fitness of the particles.\n\nIn the drone scenario, the scientists provided a function in which the result is the amount\nof drag and wobble given a specific number of aluminum and plastic components. This\nfunction is used as the fitness function in the particle swarm optimization algorithm in\nthis example (figure 7.17).\n\nf(a,y) = (a + 2y \u2014 7)? + (Qe +y\u20145)\n\nFigure 7.17 The example function for optimizing aluminum (x) and plastic (y)\n\nIf x is aluminum and y is plastic, the calculations in figure 7.18 can be made for each\nparticle to determine its fitness by substituting x and y for the values of aluminum and\nplastic.\n\n\u00a3(7,1) = (7+ 2(1)-7) + (2(7)+1-5)) =104\nf (-1,9) = (-1 + 2(9) - 7)*+ (2(-1)+9-5)? =104\n\u00a3(-10,1) = (-10 + 2(1) - y+ (2(-10)+1- 5) = 801\n\u00a3(-2,-5) = (-2 + 2(-5) - 7)'+ (2(-2)-5- 5)\" = 557\nFigure 7.18 Fitness calculations for each particle\nNow the table of particles represents the calculated fitness for each particle (table 7.3). It\nis also set as the best fitness for each particle because it is the only known fitness in the\nfirst iteration. After the first iteration, the best fitness for each particle is the best fitness\n\nin each specific particle\u2019s history.\n\nTable 7.3 Data attributes for each particle\n\nCurrent | Current Best Best\naluminum | plastic | Current | aluminum | plastic Best\nParticle | Velocity (x) y) fitness (x) y) fitness\n\n4 0 -2 5 365 2 5 365\n\n\nEXERCISE: WHAT WOULD THE FITNESS BE FOR THE FOLLOWING INPUTS\nGIVEN THE DRONE FITNESS FUNCTION?\n\nf(x,y) = (x + 2y \u2014 7)? + Qa +y\u20145)\n\nCurrent | Current Best\naluminum | plastic | Current plastic\nParticle (x) {y) fitness\n\nSOLUTION: WHAT WOULD THE FITNESS BE FOR THE FOLLOWING INPUTS\nGIVEN THE DRONE FITNESS FUNCTION?\n\n\u00a3(5,-3) = (5+ 2(-3) - 7) + (2(5)-3-5)* =68\n\u00a3(-6,-1) = (-6 + 2(-1) - 7)*+ (2(-6)-1-5)? =549\n\u00a3(7,3) = (7+ 2(3) - 7)'+ (2(7)+3-5) =180\n\u00a3(-1,9) = (-1 + 2(9) - 7)* (2(-1)+. 9-5)? =104\n\nThe fitness function is representing the mathematical function in code. Any math library\nwill contain the operations required, such as a power function and a square-root\nfunction:\n\ncaloulate_fitness(x,y):\nreturn power (x +2 *y - 7, 2)+ power(2* x+y - 5,2)\n\nThe function for updating the fitness of a particle is also trivial, in that it determines\nwhether the new fitness is better than a past best and then stores that information:\n\nupdate_fitness(x,y):\nlet particle.fitness equal the result of calculate_fitness(x,y)\nif particle.fitness is less than particle.best_fitness:\nlet particle .best_fitness equal particle.fitness\nlet particle .best_x equal x\n\nlet particle .best_y equal y\n\nThe function to determine the best particle in the swarm iterates through all particles,\n\nupdates their fitness based on their new positions, and finds the particle that yields the\nsmallest value for the fitness function. In this case, we are minimizing, so a smaller value\nis better:\n\nget_best (swarm):\nlet best_fitness equal infinity\nlet best_particle equal nothing\nfor particle in swarm:\nupdate fitness of particle\nif particle.fitness is less than best_fitness:\nlet best_fitness equal particle.fitness\nlet best_particle equal particle\n\nreturn best_particle\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.8,
                        "section_name": "Update the position of each particle",
                        "section_path": "./screenshots-images-2/chapter_7/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_8/f4fe93e7-e44d-47b9-8aeb-316d74201689.png",
                            "./screenshots-images-2/chapter_7/section_8/c5b47943-a314-49ff-a64b-f07aca83c9f8.png",
                            "./screenshots-images-2/chapter_7/section_8/6f2b4066-70c7-4d3d-92f5-ee0799bd0c1c.png",
                            "./screenshots-images-2/chapter_7/section_8/17866033-2600-4443-a7f2-3bd9d03567ca.png",
                            "./screenshots-images-2/chapter_7/section_8/d9267197-a56c-488a-8f3d-008595343ae1.png",
                            "./screenshots-images-2/chapter_7/section_8/a11f0d39-117f-4850-b2f5-1d71bbe67350.png",
                            "./screenshots-images-2/chapter_7/section_8/56110910-8f19-4363-bbd5-6a99a1e7e722.png",
                            "./screenshots-images-2/chapter_7/section_8/9f645e01-e5e4-4a4e-a7c2-0b1185e26824.png",
                            "./screenshots-images-2/chapter_7/section_8/b0a27931-d630-4394-bce6-750b9a5467ca.png",
                            "./screenshots-images-2/chapter_7/section_8/b18d37f3-4fd9-493f-8eae-0f8eb67677d3.png",
                            "./screenshots-images-2/chapter_7/section_8/1112a911-b867-4b64-afc6-0097022ff6c2.png",
                            "./screenshots-images-2/chapter_7/section_8/373baf16-64ee-4986-9542-f313cbcf9981.png",
                            "./screenshots-images-2/chapter_7/section_8/618ad368-6558-40c7-8253-72151ec414ab.png",
                            "./screenshots-images-2/chapter_7/section_8/096bb754-f43e-4758-98d8-45ca85971695.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Update the position of each particle\n\nThe update step of the algorithm is the most intricate, because it is where the magic hap-\npens. The update step encompasses the properties of swarm intelligence in nature into a\nmathematical model that allows the search space to be explored while honing in on good\nsolutions (figure 7.19).\n\n1 2 3 4\n\nSet up Calculate Update Calculate\nparticles. fitness of positions fitness of\n. particles. of particles. particles.\n\n3. Update the velocity and position\nof all particles while balancing\nexploration and exploitation oF\nsolutions.\n\nReturn\nbest\nsolution.\n\nFigure 7.19 Update the positions of the particles.\n\nParticles in the swarm update their position given a cognitive ability and factors in the\nenvironment around them, such as inertia and what the swarm is doing. These factors\ninfluence the velocity and position of each particle. The first step is understanding how\nvelocity is updated. The velocity determines the direction and speed of movement of the\nparticle.\n\nThe particles in the swarm move to different points in the search space to find better\nsolutions. Each particle relies on its memory of a good solution and the knowledge of the\nswarm\u2019s best solution. Figure 7.20 illustrates the movement of the particles in the swarm\nas their positions are updated.\n\nThe components of updating velocity\n\nThree components are used to calculate the new velocity of each particle: inertia, cogni-\ntive, and social. Each component influences the movement of the particle. We will look\nat each of the components in isolation before diving into how they are combined to\nupdate the velocity and, ultimately, the position of a particle:\n\nInertia\u2014The inertia component represents the resistance to movement or change\nin direction for a specific particle that influences its velocity. The inertia\ncomponent consists of two values: the inertia magnitude and the current velocity\nof the particle. The inertia value is a number between 0 and 1.\n\nInertia component:\n\ninertia * current velocity\n\n\u00b0 A value closer to 0 translates to exploration, potentially taking more\niterations.\n\n\u00b0 A value closer to 1 translates to more exploration for particles in fewer\niterations.\n\nCognitive\u2014The cognitive component represents the internal cognitive ability of a\nspecific particle. The cognitive ability is a sense of a particle knowing its best\nposition and using that position to influence its movement. The cognitive\nconstant is a number greater than 0 and less than 2. A greater cognitive constant\nmeans more exploitation by the particles.\n\nCognitive component:\n\ncognitive acceleration * (particle best position - current position)\n\ncognitive acceleration = cognitive constant * random cognitive number\n\nSocial\u2014The social component represents the ability of a particle to interact with\nthe swarm. A particle knows the best position in the swarm and uses this\ninformation to influence its movement. Social acceleration is determined by\nusing a constant and scaling it with a random number. The social constant\nremains the same for the lifetime of the algorithm, and the random factor\nencourages diversity in favoring the social factor.\n\nSocial component:\n\nsocial acceleration * (swarm best position - current position)\n\nsocial acceleration ~ social constant * random social number\n\nThe greater the social constant, the more exploration there will be, because the particle\nfavors its social component more. The social constant is a number between 0 and 2. A\ngreater social constant means more exploration.\n\nUpdating velocity\n\nNow that we understand the inertia component, cognitive component, and social com-\nponent, let\u2019s look at how they can be combined to update a new velocity for the particles\n(figure 7.21).\n\nNew velocity:\n\ninertia component + social component + cognitive component\n\n(inertia \u00abcurrent velocity) \\\n\n(social acceleration \u00a9 (swarm best position - current poaition))\n{cognitive acceleration * {particle beet position - current position)}\n\nFigure 7.21 Formula to calculate velocity\n\nBy looking at the math, we may find it difficult to understand how the different compo-\n\nnents in the function affect the velocity of the particles. Figure 7.22 depicts how the dif-\nferent factors influence a particle.\n\nPosition after\n\nupdate e\n\nIndividual\nSwarm's best\nbest position\nposition a\n\nSecial influence\n\n\\\nN\nN\n\\\n\u2018 Cognitive influence\nCesitlen Current motion - Inertia\n\nFigure 7.22 The intuition of the factors influencing velocity updates\n\nTable 7.4 shows the attributes of each particle after the fitness of each is calculated.\n\nTable 7.4 Data attributes for each particle\n\nCurrent\nfitness\n\nParticle\n\nNext, we will dive into the velocity update calculations for a particle, given the formulas\nthat we have worked through.\nHere are the constant configurations that have been set for this scenario:\n\n+ Inertia is set to 0.2. This setting favors slower exploration.\n\n* Cognitive constant is set to 0.35. Because this constant is less than the social\nconstant, the social component is favored over an individual particle\u2019s cognitive\ncomponent.\n\n+ Social constant is set to 0.45. Because this constant is more than the cognitive\nconstant, the social component is favored. Particles put more weight on the best\nvalues found by the swarm.\n\nFigure 7.23 describes the calculations of the inertia component, cognitive component,\nand social component for the velocity update formula.\n\nInertia component:\n\ninertia * current velocity\n\n=0.2*0\n\n=o\n\nCognitive component:\n\ncognitive acceleration = cognitive constant * random cognitive number\n\n= 0.35 * 0.2\n= 0.07\n\ncognitive acceleration * (particle best position - current position)\n= 0.07 * ((7,1) - [7,1))\n\n=0.07*0\n\n=0\n\nSocial component:\n\nsocial acceleration = social constant * random social number\n\n= 0.45 * 0.3\n= 0.135\n\nsocial acceleration * (swarm best position - current position)\n\n= 0.135 * ((-10,1] - (7,1])\n\n= 0.135 * sqrt ((-10 - 7)'+ (1-1)*) Distance fermula: eqrt((xt - x2)\" (y1 - y2)\")\n= 0.135 *17\n\n= 2.295\n\nNew velocity:\n\ninertia component + cognitive component + social component\n=04+0+2.295\n= 2.295\n\nFigure 7.23 Particle velocity calculation walkthrough\n\nAfter these calculations have been completed for all particles, the velocity of each parti-\ncle is updated, as represented in table 7.5.\n\nTable 7.5 Data attributes for each particle\n\nrie | acy | stim | pt | na | smu) tte | ss\n\n| 1 [225 [ 7 | 1 | aw | 7 | 1 | 26 |\n\n| 2 | 12x | + | 9 {| we | + | 9 | 104 |\n\n| 3 | 204 [ zo [| 1 | 8 {| +0 | 1 | 2 |\n4 5 5\n\n1.35 2 365 2 365\n\nPosition update\nNow that we understand how velocity is updated, we can update the current position of\neach particle, using the new velocity (figure 7.24).\n\nPosition:\n\ncurrent position + new velocity\n\nNew position:\n\ncurrent position + new velocity\n= ((7,1]) + 2.295\n= (9.295, 3.295]\n\nFigure 7.24 Calculating the new position of a particle\n\nBy adding the current position and new velocity, we can determine the new position of\neach particle and update the table of particle attributes with the new velocities. Then the\nfitness of each particle is calculated again, given its new position, and its best position is\nremembered (table 7.6).\n\nTable 7.6 Data attributes for each particle\n\nCurrent | Current | Current Best Best Best\nParticle | Velocity | aluminum | plastic | fitness | aluminum mas fitness\n\n1.626 0.626 73.538 0.626 73.538\n2.043 7.043 1,043 302.214 710 1 80\n1.35 -0.65 -3.65 179.105 -0.65 -3.65 | 179.105\n\nCalculating the initial velocity for each particle in the first iteration is fairly simple\nbecause there was no previous best position for each particle\u2014only a swarm best posi-\ntion that affected only the social component.\n\nLet\u2019s examine what the velocity update calculation will look like with the new infor-\nmation for each particle\u2019s best position and the swarm\u2019s new best position. Figure 7.25\ndescribes the calculation for particle 1 in the list.\n\nInertia component:\n\ninertia * current velocity\n= 0.2 \u00a9 2.295\n= 0.59\n\nCognitive component:\n\ncognitive acceleration = cognitive constant * random cognitive number\n= 0.35 * 0.2 Note: We're net ad justing the randem numbers for ease ef understanding only.\n= 0.07\n\ncognitive acceleration * (particle best position - current position)\n= 0.07 * ([7,1] - [9.925,3.325])\n\n= 0.07 * sqrt ((7 - 9.925)'+ (1 - 3.325)\")\n\n= 0.07 * 3.736\n\n= 0.266\n\nSocial compenent:\n\nsocial acceleration = social constant * random social number\n= 0.45 *0.3\n= 0.135\n\nsocial acceleration * (swarm best position - current position)\n= 0.135 * ((0.626,10] - [9.925,3.325])\n\n= 0.135 * sqrt ((0.626 - 9.925)'+ (10 - 3.325)')\n\n= 0.135 * 11.447\n\n= 1.545\n\nNew velocity:\n\ninertia component + cognitive component + social component\n= 0.59 + 0.266 + 1.545\n= 2.401\n\nFigure 7.25 Particle velocity calculation walkthrough\n\nIn this scenario, the cognitive component and the social component both play a role in\nupdating the velocity, whereas the scenario described in figure 7.23 is influenced by the\nsocial component, due to it being the first iteration.\n\nParticles move to different positions over several iterations. Figure 7.26 depicts the\nparticles\u2019 movement and their convergence on a solution.\n\nFigure 7.26 A visualization of the movement of particles in the search space\n\nIn the last frame of figure 7.26, all the particles have converged in a specific region in the\nsearch space. The best solution from the swarm will be used as the final solution. In real-\nworld optimization problems, it is not possible to visualize the entire search space (which\nwould make optimization algorithms unnecessary). But the function that we used for\nthe drone example is a known function called the Booth function. By mapping it to the\n3D Cartesian plane, we can see that the particles indeed converge on the minimum point\nin the search space (figure 7.27).\n\nFigure 7.27 Visualization of convergence of particles and a known surface\n\nAfter using the particle swarm optimization algorithm for the drone example, we find\nthat the optimal ratio of aluminum and plastic to minimize drag and wobble is 1:3\u2014\nthat is, 1 part aluminum and 3 parts plastic. When we feed these values into the fitness\nfunction, the result is 0, which is the minimum value for the function.\n\nThe update step can seem to be daunting, but if the components are broken into simple\nfocused functions, the code becomes simpler and easier to write, use, and understand.\nThe first functions are the inertia calculation function, the cognitive acceleration func-\ntion, and the social acceleration function. We also need a function to measure the dis-\ntance between two points, which is represented by squaring the sum of the square of the\ndifference in x values summed with the square of the difference in the y values:\n\ncalculate_inertia(inertia_constant, velocity):\n\nreturn inertia_constant * current_velocity\n\ncalculate_cognitive_acceleration(cognitive_constant):\n\nreturn cognitive_constant * random number between @ and 1\n\nealculate_social_acceleration(social_constant):\n\nreturn social_constant * random number between 0 and 1\n\ncaloulate_distance(best_x, best_y, current_x,current_y):\nreturn square_root(\npower (best_x - current_x), 2) + power(best_y - current_y), 2)\n)\n\nThe cognitive component is calculated by finding the cognitive acceleration, using the\nfunction that we defined in an earlier section, and the distance between the particle\u2019s\nbest position and its current position:\n\necaloulate_cognitive(cognitive_constant,\nparticle_best_x, particle_best_y\nparticle_current_x, particle_current_y):\nlet acceleration equal cognative_acceleration(cognitive_constant)\nlet distance equal calculate_distance(particle_best_x,\nparticle_best_y\nparticle_current_x,\nparticle_current_y)\n\nreturn acceleration * distance\n\nThe social component is calculated by finding the social acceleration, using the function\nthat we defined earlier, and the distance between the swarm\u2019s best position and the par-\nticle\u2019s current position:\n\necalculate_social(social_constant,\nswarm_best_x, swarm_best_y\nparticle_current_x, particle_current_y):\nlet acceleration equal social_acceleration(social_constant)\nlet distance equal calculate_distance(swarm_best_x,\nswarm_best_y\nparticle_current_x,\nparticle_current_y)\n\nreturn acceleration * distance\n\nThe update function wraps everything that we have defined to carry out the actual\nupdate of a particle\u2019s velocity and position. The velocity is calculated by using the inertia\ncomponent, cognitive component, and social component. The position is calculated by\nadding the new velocity to the particle\u2019s current position:\n\nupdate_particle(cognitive_constant, social_constant,particle_velocity,\nparticle_best_x,particle_best_y,\nswarm_best_x, swarm_best_y,\nparticle_current_x,particle_current_y)\nlet inertia equal calculate_inertia(inertia_constant,\nparticle_constant)\nlet cognitive equal calculate_cognitive(cognitive_constant,\nparticle _best_x,particle_best_y\nparticle_current_x, particle_current_y)\nlet social equal calculate_social(social_constant,\nswarm_best_x, swarm_best_y\nparticle_current_x, particle_current_y)\nlet particle .velocity equal inertia + cognitive + social\nlet particle.x equal particle.x + velocity\n\nlet particle.y equal particle.y + velocity\n\nEXERCISE: CALCULATE THE NEW VELOCITY AND POSITION FOR PARTICLE 1 GIVEN THE\nFOLLOWING INFORMATION ABOUT THE PARTICLES\n\n+ Inertia is set to 0.1.\n\n+ The cognitive constant is set to 0.5, and the cognitive random number is 0.2.\n\n+ The social constant is set to 0.5, and the cognitive random number is 0.5.\n\n\nSOLUTION: CALCULATE THE NEW VELOCITY AND POSITION FOR PARTICLE 1 GIVEN THE\nFOLLOWING INFORMATION ABOUT THE PARTICLES\n\nInertia component:\n\ninertia * current velocity\n=0.1*3\n\n= 0.3\n\nCognitive compenent:\n\ncognitive acceleration = cognitive constant * random cognitive number\n=0.5*0.2\n=0.1\n\ncognitive acceleration * (particle best position - current position)\n= 0.1 * ((7,1] - [4,8])\n\n= 0.1 * sqrt ((7 - 4)\"+ (1-8)\")\n\n=0.1* 7.616\n\n= 0.7616\n\nSecial component:\n\nsocial acceleration = social constant * random social number\n\n=0.5*0.5\n= 0.25\n\nsocial acceleration * (swarm best position - current position)\n= 0.25 * ((0.626,10] - [4,8])\n\n= 0.25 * sqrt ((0.626 - 4)\u2019+ (10 - 8)\u2019)\n\n= 0.25 * 3.922\n\n= 0.981\n\nNew velocity:\ninertia component + cognitive component + social component\n\n= 0.3 + 0.7616 + 0.981\n= 2.0426\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.9,
                        "section_name": "Determine the stopping criteria",
                        "section_path": "./screenshots-images-2/chapter_7/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_9/71314cb3-a289-47c0-b5ed-a086235eabb1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Determine the stopping criteria\n\nThe particles in the swarm cannot keep updating and searching indefinitely. A stopping\ncriterion needs to be determined to allow the algorithm to run for a reasonable number\nof iterations to find a suitable solution (figure 7.28).\n\n1 2\n\n3 \u2018\n\nSet up Calculate Update Calcula\nparticles. fitness of positions fitness\n. particles. of particles. particles.\n\n'S. Determine how many\niterations the algorithm runs\nfor\u2014this is the number of times\nall particles are updated,\n\nReturn\nbest\nsolution.\n\nFigure 7.28 Has the algorithm reached a stopping condition?\n\nThe number of iterations influences several aspects of finding solutions, including:\n\n+ Exploration\u2014Particles require time to explore the search space to find areas with\nbetter solutions. Exploration is also influenced by the constants defined in the\nupdate velocity function.\n\n+ Exploitation\u2014Particles should converge on a good solution after reasonable\nexploration occurs.\n\nA strategy to stop the algorithm is to examine the best solution in the swarm and deter-\nmine whether it is stagnating. Stagnation occurs when the value of the best solution\ndoesn\u2019t change or doesn\u2019t change by a significant amount. Running more iterations in\nthis scenario will not help find better solutions. When the best solution stagnates, the\nparameters in the update function can be adjusted to favor more exploration. If more\nexploration is desired, this adjustment usually means more iterations. Stagnation could\nmean that a good solution was found or that that the swarm is stuck on a local best solu-\ntion. If enough exploration occurred at the start, and the swarm gradually stagnates, the\nswarm has converged on a good solution (figure 7.29).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.1,
                        "section_name": "Use cases for particle swarm\noptimization algorithms",
                        "section_path": "./screenshots-images-2/chapter_7/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_10/b0b38192-2ca2-48b3-97b2-deddb06072ba.png",
                            "./screenshots-images-2/chapter_7/section_10/23ebf37c-42d3-4c3b-af6a-8a496ff9a169.png",
                            "./screenshots-images-2/chapter_7/section_10/3ecb0854-152e-46b9-9308-5c463795377f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Use cases for particle swarm\noptimization algorithms\n\nParticle swarm optimization algorithms are interesting because they simulate a natural\nphenomenon, which makes them easier to understand, but they can be applied to a range\nof problems at different levels of abstraction. This chapter looked at an optimization\nproblem for drone manufacturing, but particle swarm optimization algorithms can be\nused in conjunction with other algorithms, such as artificial neural networks, playing a\nsmall but critical role in finding good solutions.\n\nOne interesting application of a particle swarm optimization algorithm is deep brain\nstimulation. The concept involves installing probes with electrodes into the human\nbrain to stimulate it to treat conditions such as Parkinson\u2019s disease. Each probe contains\nelectrodes that can be configured in different directions to treat the condition correctly\nper patient. Researchers at the University of Minnesota have developed a particle swarm\noptimization algorithm to optimize the direction of each electrode to maximize the\nregion of interest, minimize the region of avoidance, and minimize energy use. Because\nparticles are effective in searching these multidimensional problem spaces, the particle\nswarm optimization algorithm is effective for finding optimal configurations for elec-\ntrodes on the probes (figure 7.30).\n\nExample of two\ndifferent types\nof electrode\nconfigurations\n\ne\ne\ne\ne\ne\ne\ne\ne\n\n880 KB0u3\n\n~~ gh\nean\u00ae\npeor\n\nPulse generator\n\nFigure 7.30 Example of factors involved for probes in deep brain stimulation\n\nHere are some other real-world applications of particle swarm optimization algorithms:\n\nOptimizing weights in an artificial neural network\u2014Artificial neural networks are\nmodeled on an idea of how the human brain works. Neurons pass signals to\nother neurons, and each neuron adjusts the signal before passing it on. An\nartificial neural network uses weights to adjust each signal. The power of the\nnetwork is finding the right balance of weights to form patterns in relationships\nof the data. Adjusting weights is computationally expensive, as the search space is\nmassive. Imagine having to brute-force every possible decimal number\ncombination for 10 weights. That process would take years.\n\nDon\u2019t panic if this concept sounds confusing. We explore how artificial neural\nnetworks operate in chapter 9. Particle swarm optimization can be used to adjust\nthe weights of neural networks faster, because it seeks optimal values in the\nsearch space without exhaustively attempting each one.\n\nMotion tracking in videos\u2014Motion tracking of people is a challenging task in\ncomputer vision. The goal is to identify the poses of people and imply a motion\nby using the information from the images in the video alone. People move\n\ndifferently, even though their joints move similarly. Because the images contain\nmany aspects, the search space becomes large, with many dimensions to predict\nthe motion for a person. Particle swarm optimization works well in high-\ndimension search spaces and can be used to improve the performance of motion\ntracking and prediction.\n\nSpeech enhancement in audio\u2014Audio recordings are nuanced. There is always\nbackground noise that may interfere with what someone is saying in the\nrecording. A solution is to remove the noise from recorded speech audio clips. A\ntechnique used for this purpose is filtering the audio clip with noise and\ncomparing similar sounds to remove the noise in the audio clip. This solution is\nstill complex, as reduction of certain frequencies may be good for parts of the\naudio clip but may deteriorate other parts of it. Fine searching and matching\nmust be done for good noise removal. Traditional methods are slow, as the search\nspace is large. Particle swarm optimization works well in large search spaces and\ncan be used to speed the process of removing noise from audio clips.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 8,
                "chapter_name": "Machine\nlearning",
                "chapter_path": "./screenshots-images-2/chapter_8",
                "sections": [
                    {
                        "section_id": 8.1,
                        "section_name": "What is machine learning?",
                        "section_path": "./screenshots-images-2/chapter_8/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_1/7b81c91a-42db-40a4-859a-fe8c54f526a7.png",
                            "./screenshots-images-2/chapter_8/section_1/9f46b591-77a0-4e10-a468-693927afe5ca.png",
                            "./screenshots-images-2/chapter_8/section_1/e4a2617f-5816-42f5-bd26-1ad52d8dcd7c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is machine learning?\n\nMachine learning can seem like a daunting concept to learn and apply, but\nwith the right framing and understanding of the process and algorithms, it\ncan be interesting and fun.\n\nSuppose that you're looking for a new apartment. You speak to friends\nand family, and do some online searches for apartments in the city. You\n\nnotice that apartments in different areas are priced differently. Here are some of your\nobservations from all your research:\n\n+ Aone-bedroom apartment in the city center (close to work) costs $5,000\nper month.\n\n+ Atwo-bedroom apartment in the city center costs $7,000 per month.\n\n+ Aone-bedroom apartment in the city center with a garage costs $6,000\nper month.\n\n+ Aone-bedroom apartment outside the city center, where you will need to travel\nto work, costs $3,000 per month.\n\n+ Atwo-bedroom apartment outside the city center costs $4,500 per month.\n\n+ Aone-bedroom apartment outside the city center with a garage costs $3,800 per\nmonth.\n\nYou notice some patterns. Apartments in the city center are most expensive and are usually\nbetween $5,000 and $7,000 per month. Apartments outside the city are cheaper.\nIncreasing the number of rooms adds between $1,500 and $2,000 per month, and access\nto a garage adds between $800 and $1,000 per month (figure 8.1).\n\nFigure 8.1 An illustration of property prices and features in different regions\n\nThis example shows how we use data to find patterns and make decisions. If you encoun-\nter a two-bedroom apartment in the city center with a garage, it\u2019s reasonable to assume\nthat the price would be approximately $8,000 per month.\n\nMachine learning aims to find patterns in data for useful applications in the real world.\nWe could spot the pattern in this small dataset, but machine learning spots them for us\nin large, complex datasets. Figure 8.2 depicts the relationships among different attributes\nof the data. Each dot represents an individual property.\n\nNotice that there are more dots closer to the city center and that there is a clear pattern\nrelated to price per month: the price gradually drops as distance to the city center\nincreases. There is also a pattern in the price per month related to the number of rooms;\nthe gap between the bottom cluster of dots and the top cluster shows that the price jumps\nsignificantly. We could naively assume that this effect may be related to the distance\nfrom the city center. Machine learning algorithms can help us validate or invalidate this\nassumption. We dive into how this process works throughout this chapter.\n\nDistance from city center vs. Number of rooms vs.\nprice per menth price per menth\n\nPrice per menth\nPrice per menth\n\n\u00a9 Fem tam tem \u00b0 1 2\nDistance frem city center Number ef reems\n\nFigure 8.2 Example visualization of relationships among data\n\nTypically, data is represented in tables. The columns are referred to as features of the\ndata, and the rows are referred to as examples. When we compare two features, the fea-\nture being measured is sometimes represented as y, and the features being changed are\ngrouped as x. We will gain a better intuition for this terminology as we work through\nsome problems.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.2,
                        "section_name": "Problems applicable to machine learning",
                        "section_path": "./screenshots-images-2/chapter_8/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_2/19fdc19b-5985-461e-9e72-73df2b4c6301.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Problems applicable to machine learning\n\nMachine learning is useful only if you have data and have questions to ask that the data\nmight answer. Machine learning algorithms find patterns in data but cannot do useful\nthings magically. Different categories of machine learning algorithms use different\napproaches for different scenarios to answer different questions. These broad categories\nare supervised learning, unsupervised learning, and reinforcement learning (figure 8.3).\n\nImage classification Mevie recemmendatiens\nFraud detectien\n\nCustomer categorization\nIdentifying cybersecurity attacks Targeted marketing\n\nSupervised\nlearning\n\nUnsupervised!\nlearning\n\nMachine\nlearning\n\nWeather forecasting\n\nStructure discovery\nLife expectancy and health risks\n\nFeature discovery\n\nEconemy predictions Data visvalization\n\nAvtenomeus navigation\nGame-playing Al\nLearning skills in different envirenments\n\nFigure 8.3 Categorization of machine learning and uses\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.3,
                        "section_name": "Supervised learning",
                        "section_path": "./screenshots-images-2/chapter_8/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_3/e4b699c7-7309-40bc-a94d-96adc156b8f8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Supervised learning\n\nOne of the most common techniques in traditional machine learning is supervised learn-\ning. We want to look at data, understand the patterns and relationships among the data,\nand predict the results if we are given new examples of different data in the same format.\nThe apartment-finding problem is an example of supervised learning to find the pattern.\nWe also see this example in action when we type a search that autocompletes or when\nmusic applications suggest new songs to listen to based on our activity and preference.\nSupervised learning has two subcategories: regression and classification.\n\nRegression involves drawing a line through a set of data points to most closely fit the\noverall shape of the data. Regression can be used for applications such as trends between\nmarketing initiatives and sales. (Is there a direct relationship between marketing through\nonline ads and actual sales of a product?) It can also be used to determine factors that\naffect something. (Is there a direct relationship between time and the value of crypto-\ncurrency, and will cryptocurrency increase exponentially in value as time passes?)\n\nClassification aims to predict categories of examples based on their features. (Can we\ndetermine whether something is a car or a truck based on its number of wheels, weight,\nand top speed?)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.4,
                        "section_name": "Unsupervised learning",
                        "section_path": "./screenshots-images-2/chapter_8/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_4/84825e1f-3e47-4ccf-b2d7-1cf52ef5d3b2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Unsupervised learning\n\nUnsupervised learning involves finding underlying patterns in data that may be difficult\nto find by inspecting the data manually. Unsupervised learning is useful for clustering\ndata that has similar features and uncovering features that are important in the data. On\nan e-commerce site, for example, products might be clustered based on customer pur-\nchase behavior. If many customers purchase soap, sponges, and towels together, it is\nlikely that more customers would want that combination of products, so soap, sponges,\nand towels would be clustered and recommended to new customers.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.5,
                        "section_name": "Reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_8/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_5/7dfd227b-6c9e-45ad-895c-1fea26462332.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Reinforcement learning\n\nReinforcement learning is inspired by behavioral psychology and operates by reward-\ning or punishing an algorithm based on its actions in an environment. It has similari-\nties to supervised learning and unsupervised learning, as well as many differences.\nReinforcement learning aims to train an agent in an environment based on rewards and\npenalties. Imagine rewarding a pet for good behavior with treats; the more it is rewarded\nfor a specific behavior, the more it will exhibit that behavior. We discuss reinforcement\nlearning in chapter 10.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.6,
                        "section_name": "A machine learning workflow",
                        "section_path": "./screenshots-images-2/chapter_8/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_6/8318236e-6c71-44cf-aa8c-afa9c2f5b735.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A machine learning workflow\n\nMachine learning isn\u2019t just about algorithms. In fact, it is often about the context of the\ndata, the preparation of the data, and the questions that are asked.\nWe can find questions in two ways:\n\n+ Aproblem can be solved with machine learning, and the right data needs to be\ncollected to help solve it. Suppose that a bank has a vast amount of transaction\ndata for legitimate and fraudulent transactions, and it wants to train a model\nwith this question: \u201cCan we detect fraudulent transactions in real time?\u201d\n\n+ We have data in a specific context and want to determine how it can be used to\nsolve several problems. An agriculture company, for example, might have data\nabout the weather in different locations, nutrition required for different plants,\nand the soil content in different locations. The question might be \u201cWhat\ncorrelations and relationships can we find among the different types of data?\u201d\nThese relationships may inform a more concrete question, such as \u201cCan we\ndetermine the best location for growing a specific plant based on the weather and\nsoil in that location?\u201d\n\nFigure 8.4 is a simplified view of the steps involved in a typical machine learning\nendeavor.\n\nCollect and\nunderstand\ndata.\n\nPrepare\ndata.\n\nFigure 8.4 A workflow for machine learning experiments and projects\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.7,
                        "section_name": "Collecting and understanding data: Know your context",
                        "section_path": "./screenshots-images-2/chapter_8/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_7/5b90bba5-c370-4087-8f92-c3fc707e5008.png",
                            "./screenshots-images-2/chapter_8/section_7/431c2fe2-e8cd-4316-b7a5-e33f6c0c8eff.png",
                            "./screenshots-images-2/chapter_8/section_7/7e71ee61-0b1f-4ce0-9214-90cb314dbbdd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Collecting and understanding data: Know your context\n\nCollecting and understanding the data you\u2019re working with is paramount to a successful\nmachine learning endeavor. If you're working in a specific area in the finance industry,\nknowledge of the terminology and workings of the processes and data in that area is\nimportant for sourcing the data that is best to help answer questions for the goal you're\ntrying to achieve. If you want to build a fraud detection system, understanding what data\nis stored about transactions and what it means is critical to identifying fraudulent trans-\nactions. Data may also need to be sourced from various systems and combined to be\neffective. Sometimes, the data we use is augmented with data from outside the organiza-\ntion to enhance accuracy. In this section, we use an example dataset about diamond\nmeasurements to understand the machine learning workflow and explore various algo-\nrithms (figure 8.5).\n\nFigure 8.5 Terminology of diamond measurements\n\nTable 8.1 describes several diamonds and their properties. X, Y, and Z describe the size of\na diamond in the three spatial dimensions. Only a subset of data is used in the examples.\n\nTable 8.1 The diamond dataset\n\nGood J\n\nIdeal\n\nVery Good\n\nFair\n\n7.06 | 7.01 | 4.20\n\nPremium\n\n7.31 | 7.22 | 4.57\n\nPremium\n\n|\nD\nH\nFair F 6.73 | 6.66 | 4.33\nJ\nH\nH\n|\n\n1.74 | Very Good \"1 63.2 55 | 4,677 | 7.62 59 80\n1.96 Fair \"1 668 55 | 6,147 | 7.62 | 7.60 | 5.08\n\n=x\n\n2.21 Premium 62.2 58 6,535 | 8.31 | 8.27 | 5.16\n\nThe diamond dataset consists of 10 columns of data, which are referred to as features.\nThe full dataset has more than 50,000 rows. Here\u2019s what each feature means:\n\nCarat\u2014The weight of the diamond. Out of interest: 1 carat equals 200 mg.\n\nCut\u2014The quality of the diamond, by increasing quality: fair, good, very good,\npremium, and ideal.\n\nColor\u2014The color of the diamond, ranging from D to J, where D is the best color\nand J is the worst color. D indicates a clear diamond, and J indicates a foggy one.\n\nClarity\u2014The imperfections of the diamond, by decreasing quality: FL, IF, VVS1,\nVVS2, VS1, VS2, SI, SI2, Il, 12, and 13. (Don\u2019t worry about understanding these\ncode names; they simply represent different levels of perfection.)\n\nDepth\u2014The percentage of depth, which is measured from the culet to the table\nof the diamond. Typically, the table-to-depth ratio is important for the \u201csparkle\u201d\naesthetic of a diamond.\n\nTable\u2014The percentage of the flat end of the diamond relative to the X\ndimension.\n\nPrice\u2014The price of the diamond when it was sold.\nX\u2014The x dimension of the diamond, in millimeters.\nY\u2014The y dimension of the diamond, in millimeters.\n\nZ\u2014The z dimension of the diamond, in millimeters.\n\nKeep this dataset in mind; we will be using it to see how data is prepared and processed\nby machine learning algorithms.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.8,
                        "section_name": "Preparing data: Clean and wrangle",
                        "section_path": "./screenshots-images-2/chapter_8/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_8/c9135a40-c618-4570-a776-74a978fb26ef.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Preparing data: Clean and wrangle\nReal-world data is never ideal to work with. Data might be sourced from different sys-\ntems and different organizations, which may have different standards and rules for data\nintegrity. There are always missing data, inconsistent data, and data in a format that is\ndifficult to work with for the algorithms that we want to use.\n\nIn the sample diamond dataset in table 8.2, again, it is important to understand that\nthe columns are referred to as the features of the data and that each row is an example.\n\nTable 8.2 The diamond dataset with missing data\nCarat Cut Color | Clarity | Depth | Table | Price | X Y Zz\n\n1 | 030 Good J sii 64.0 55 339 | 4.25 | 4.28 | 2.73\n2 | 041 Ideal ! sil 61.7 55 561 | 4.77 | 4.80 | 2.95\n3 | 0.75 | Very Good D si 63.2 56 | 2,760 | 5.80 | 5.75 | 3.65\n4 | 0.91 - H Si2 : 60 | 2,763 | 6.03 | 5.99 | 3.95\n5 | 1.20 Fair F \"1 64.6 56 | 2,809 | 6.73 | 6.66 | 4.33\n6 | 1.21 Good E \"1 57.2 62 | 3,144 | 7.01 | 6.96 | 3.99\n7 | 131 Premium J si2 59.7 59 | 3,697 | 7.06 | 7.01 | 4.20\n8 | 1.50 | Premium H \"1 62.9 60 | 4,022 | 7.31 | 7.22 | 4.57\n9 | 1.74 | Very Good H i 63.2 55 | 4,677 | 7.62 | 7.59 | 4.80\n10 | 1.83 fair J \"1 70.0 58 | 5,083 | 7.34 | 7.28 | 5.12\n11} 1.96 Fair ! \"1 66.8 55 6,147 | 7.62 | 7.60 | 5.08\n12 Premium H it 62.2 : 6,535 | 8.31 : 5.16\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.9,
                        "section_name": "Missing data",
                        "section_path": "./screenshots-images-2/chapter_8/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_9/3b0a8849-8024-4cc3-b368-c552fed76669.png",
                            "./screenshots-images-2/chapter_8/section_9/21a13383-1995-470e-a648-289fdd8ad34e.png",
                            "./screenshots-images-2/chapter_8/section_9/86446e9c-b1ae-42f7-a619-cab510e93607.png",
                            "./screenshots-images-2/chapter_8/section_9/000de7a8-920e-4251-a8a8-be7b86bf378b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Preparing data: Clean and wrangle\nReal-world data is never ideal to work with. Data might be sourced from different sys-\ntems and different organizations, which may have different standards and rules for data\nintegrity. There are always missing data, inconsistent data, and data in a format that is\ndifficult to work with for the algorithms that we want to use.\n\nIn the sample diamond dataset in table 8.2, again, it is important to understand that\nthe columns are referred to as the features of the data and that each row is an example.\n\nTable 8.2 The diamond dataset with missing data\n\nCarat Cut Color | Clarity | Depth | Table | Price | X Y Zz\n1 | 030 | Good J sn | 640 | 55 | 339 | 4.25 | 4.28 | 2.73\n2/041 | Ideal 1 st | 67 | 55 | 561 | 477 | 480 | 295\n3 | 075 | VeryGood| D | si | 632 | 56 | 2760| 580 | 5.75 | 3.65\natom | - | 4 [| sa : 60 | 2,763 | 6.03 | 5.99 | 3.95\n5 | 1.20 Fair F | 646 | 56 | 2,809 | 6.73 | 6.66 | 4.33\n6 | 121 | Good E | 572 | 62 | 3,144] 7.01 | 696 | 3.99\n7 | 131 | Premium | J si | 597 | 59 | 3697 | 7.06 | 7.01 | 420\n8 | 150 | Premium | H n | 629 | 60 | 4022 | 7.31 | 7.22 | 457\n9 | 1.74 | VeryGood | H | 632 | 55 | 4677 | 7.62 | 7.59 | 480\n10 | 1.83 fair J | 700 | 58 | 5083 | 7.34 | 7.28 | 5.12\n11] 1.96 Fair 1 | 668 | 55 | 6147 | 7.62 | 7.60 | 5.08\n12 Premium | H | 622 | - |6535|831| - | 5.16\nMissing data\n\nIn table 8.2, example 4 is missing values for the Cut and Depth features, and example 12\nis missing values for Carat, Table, and Y. To compare examples, we need complete under-\nstanding of the data, and missing values make this difficult. A goal for a machine learn-\ning project might be to estimate these values; we cover estimations in the upcoming\nmaterial. Assume that missing data will be problematic in our goal to use it for some-\nthing useful. Here are some ways to deal with missing data:\n\n+ Remove\u2014Remove the examples that have missing values for features\u2014in this\ncase, examples 4 and 12 (table 8.3). The benefit of this approach is that the data is\nmore reliable because nothing is assumed; however, the removed examples may\nhave been important to the goal we're trying to achieve.\n\nTable 8.3 The diamond dataset with missing data: removing examples\nClarity | Depth | Table\n\n+ Mean or median\u2014Another option is to replace the missing values with the mean\nor median for the respective feature.\n\nThe mean is the average calculated by adding all the values and dividing by the\nnumber of examples. The median is calculated by ordering the examples by value\nascending and choosing the value in the middle.\n\nUsing the mean is easy and efficient to do but doesn\u2019t take into account\npossible correlations between features. This approach cannot be used with\ncategorical features such as the Cut, Clarity, and Depth features in the diamond\ndataset (table 8.4).\n\nTable 8.4 The diamond dataset with missing data: using mean values\n\nCarat Cut Color | Clarity | Depth | Table | Price | X Y Zz\n\n1 | 030 | Good J | sn | 640 | 55 | 339 | 4.25 | 428 | 273\n2 | 041 | Ideal I sii | 617 | 55 | 561 | 477 | 480 | 2.95\n3 | 075 | VeryGood| D | sn | 632 | 56 | 2,760| 5.80 | 5.75 | 3.65\n4 | 091 - H\n\n5 | 1.20 Fair F\n\n6 | 121 | Good E\n\n7 | 131 Premium J \u00e9\n\n8 | 150 | Premium | H | 11 | 629 | 60 | 4,022 | 7.31 | 7.22 | 457\n9 | 1.74 | VeryGood | _H iv | 632 | 55 | 4677 | 7.62 | 759 | 480\n10 | 1.83 J n | 700 | 58 | 5,083 | 7.34 | 7.28 | 5.12\n11 | 1.96 Fair I n | 668 | 55 | 6,147 | 7.62 | 7.60 | 5.08\n12| 1.19 | Premium | H it | 622 | 57 | 6535|831| - | 5.16\n\nTo calculate the mean of the Table feature, we add every available value and\ndivide the total by the number of values used:\n\nTable mean = (55 + 55 + 56 + 60 + 56+ 62+ 59+ 60+55+58+55) /11\nTable mean = 631/11\nTable mean = 57.364\n\nUsing the Table mean for the missing values seems to make sense, because the\ntable size doesn\u2019t seem to differ radically among different examples of data. But\nthere could be correlations that we do not see, such as the relationship between\nthe table size and the width of the diamond (X dimension).\n\nOn the other hand, using the Carat mean does not make sense, because we can\nsee a correlation between the Carat feature and the Price feature if we plot the\ndata on a graph. The price seems to increase as the Carat value increases.\n\nMost frequent\u2014Replace the missing values with the value that occurs most often\nfor that feature, which is known as the mode of the data. This approach works\nwell with categorical features but doesn\u2019t take into account possible correlations\namong features, and it can introduce bias by using the most frequent values.\n\n(Advanced) Statistical approaches\u2014Use k-nearest neighbor, or neural networks.\nK-nearest neighbor uses many features of the data to find an estimated value.\nSimilar to k-nearest neighbor, a neural network can predict the missing values\naccurately, given enough data. Both algorithms are computationally expensive\nfor the purpose of handling missing data.\n\n+ (Advanced) Do nothing\u2014Some algorithms handle missing data without any\npreparation, such as XGBoost, but the algorithms that we will be exploring\nwill fail.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.1,
                        "section_name": "Ambiguous values",
                        "section_path": "./screenshots-images-2/chapter_8/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_10/2d291543-6faa-4860-92c9-e0b62cd7fb41.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Ambiguous values\n\nAnother problem is values that mean the same thing but are represented differently.\nExamples in the diamond dataset are rows 2, 9, 10, and 12. The values for the Cut and\nClarity features are lowercase instead of uppercase. Note that we know this only because\nwe understand these features and the possible values for them. Without this knowledge,\nwe might see Fair and fair as different categories. To fix this problem, we can standardize\nthese values to uppercase or lowercase to maintain consistency (table 8.5).\n\nTable 8.5 The diamond dataset with ambiguous data: standardizing values\n\nGood\nPremium\n\nPremium\n\nFair\n\nfl\nD\nH\nF\nE\nJ\n\nH\nH\nJ\n\n|\n\nH\n\nPremium\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.11,
                        "section_name": "Encoding categorical data",
                        "section_path": "./screenshots-images-2/chapter_8/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_11/e189b827-4281-4409-a078-c2a2b4ac4c40.png",
                            "./screenshots-images-2/chapter_8/section_11/2214c50a-62b6-4876-8ef1-7326fc7b6b62.png",
                            "./screenshots-images-2/chapter_8/section_11/dd1cd753-31db-4d07-a695-76a1bf3dffdf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Encoding categorical data\n\nBecause computers and statistical models work with numeric values, there will be a\nproblem with modeling string values and categorical values such as Fair, Good, SI1, and\nIl. We need to represent these categorical values as numerical values. Here are ways to\naccomplish this task:\n\n+ One-hot encoding\u2014Think about one-hot encoding as switches, all of which are\noff except one. The one that is on represents the presence of the feature at that\nposition. If we were to represent Cut with one-hot encoding, the Cut feature\nbecomes five different features, and each value is 0 except for the one that\nrepresents the Cut value for each respective example. Note that the other features\nhave been removed in the interest of space in table 8.6.\n\nTable 8.6 The diamond dataset with encoded values\n\nCarat | Cut:Fair | Cut: Good | Cut: Very Good | Cut: Premium | Cut: Ideal\n0.30 0\n0.41\n\n-\n\u00b0\n\u00b0\no\n\n0.75\n\neo}|s|olo|clo\nSlolelololol/olo|alo\n\nejs|-/cololojo|\u2014|ololo\n\nit\n\n12 1.19\n\nclolol|ol|olololc|olo|\u2014\n\ni} 1\n\n+ Label encoding\u2014Represent each category as a number between 0 and the number\nof categories. This approach should be used only for ratings or rating-related\nlabels; otherwise, the model that we will be training will assume that the number\ncarries weight for the example and can introduce unintended bias.\n\nEXERCISE: IDENTIFY AND FIX THE PROBLEM DATA IN THIS EXAMPLE\n\nDecide which data preparation techniques can be used to fix the following\ndataset. Decide which rows to delete, what values to use the mean for, and\nhow categorical values will be encoded. Note that the dataset is slightly differ-\nent from what we\u2019ve been working with thus far.\n\nCanada\nCanada\nBotswana\n\nBotswana\n\nSouth Africa\nBotswana\n\nBotswana\nSouth Africa\n\n\nSOLUTION: IDENTIFY AND FIX THE PROBLEM DATA IN THIS EXAMPLE\nOne approach for fixing this dataset involves the following three tasks:\n\n+ Remove row 8 due to missing Origin. We don\u2019t know what the dataset will be\nused for. If the Origin feature is important, this row will be missing and it\nmay cause issues. Alternatively, the value for this feature could be estimated if\nit has a relationship with other features.\n\n+ Use one-hot encoding to encode the Origin column value. In the example\nexplored thus far in the chapter, we used label encoding to convert string\nvalues to numeric values. This approach worked because the values indicated\nmore superior cut, clarity, or color. In the case of Origin, the value identifies\nwhere the diamond was sourced. By using label encoding, we introduce bias\nto the dataset, because no Origin location is better than another in this\ndataset.\n\n+ Find the mean for missing values. Row 1, 2, 4, and 5 are missing values for Y,\nX, Table, and Z, respectively. Using a mean value should be a good technique\nbecause, as we know about diamonds, the dimensions and table features are\nrelated.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.12,
                        "section_name": "Testing and training data",
                        "section_path": "./screenshots-images-2/chapter_8/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_12/c851a4d7-253d-4cd5-a808-5e5e343a1d17.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Testing and training data\n\nBefore we jump into training a linear regression model, we need to ensure that we have\ndata to teach (or train) the model, as well as some data to test how well it does in predict-\ning new examples. Think back to the property-price example. After gaining a feel for the\nattributes that affect price, we could make a price prediction by looking at the distance\nand number of rooms. For this example, we will use table 8.7 as the training data because\nwe have more real-world data to use for training later.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.13,
                        "section_name": "Training a model: Predict with linear regression",
                        "section_path": "./screenshots-images-2/chapter_8/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_13/7502a6ab-55f8-4435-b48f-2aba5bb9f45e.png",
                            "./screenshots-images-2/chapter_8/section_13/775c1e92-7dc8-46d6-bf44-ae3ca23f4c9b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training a model: Predict with linear regression\n\nChoosing an algorithm to use is based largely on two factors: the question that is being\nasked and the nature of the data that is available. If the question is to make a prediction\nabout the price of a diamond with a specific carat weight, regression algorithms can be\nuseful. The algorithm choice also depends on the number of features in the dataset\nand the relationships among those features. If the data has many dimensions (there are\nmany features to consider to make a prediction), we can consider several algorithms and\napproaches.\n\nRegression means predicting a continuous value, such as the price or carat of the dia-\nmond. Continuous means that the values can be any number in a range. The price of\n$2,271, for example, is a continuous value between 0 and the maximum price of any\ndiamond that regression can help predict.\n\nLinear regression is one of the simplest machine learning algorithms; it finds relation-\nships between two variables and allows us to predict one variable given the other. An\n\nexample is predicting the price of a diamond based on its carat value. By looking at many\nexamples of known diamonds, including their price and carat values, we can teach a\nmodel the relationship and ask it to estimate predictions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.14,
                        "section_name": "Fitting a line to the data",
                        "section_path": "./screenshots-images-2/chapter_8/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_14/e18cae78-2a8f-41a1-a1fd-a71f70f940b1.png",
                            "./screenshots-images-2/chapter_8/section_14/b6e23614-5726-428c-bb54-39a244bd4f03.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Fitting a line to the data\n\nLet\u2019s start trying to find a trend in the data and attempt to make some predictions.\nFor exploring linear regression, the question we're asking is \u201cIs there a correlation\nbetween the carats of a diamond and its price, and if there is, can we make accurate\n\npredictions?\u201d\n\nWe start by isolating the carat and price features and plotting the data on a graph.\nBecause we want to find the price based on carat value, we will treat carats as x and price\n\nas y. Why did we choose this approach?\n\n* Carat as the independent variable (x)\u2014An independent variable is one that is\nchanged in an experiment to determine the effect on a dependent variable. In\nthis example, the value for carats will be adjusted to determine the price of a\n\ndiamond with that value.\n\n+ Price as the dependent variable (y)\u2014A dependent variable is one that is being\ntested. It is affected by the independent variable and changes based on the\nindependent variable value changes. In our example, we are interested in the\n\nprice given a specific carat value.\n\nFigure 8.6 shows the carat and price data plotted on a graph, and table 8.7 describes the\n\nactual data.\n;. Table 8.7 Carat and price data\n6,008 \u00b0 Carat (x) | Price (y)\n1 0.30 339\n5000s\n. 2 0.41 561\nsane \u00b0 3 | 075 | 2760\n3 \u00b0 4 091 | 2,763\nE\naaeel 6 131 3,697\n7 1.50 | 4,022\n1.828 8 1.74 4,677\n-\u00b0 9 1.96 | 6,147\nth oh vn im ah ib th oh as 10 | 221 | 6535\n\nCarat\n\nFigure 8.6 A scatterplot of carat and price data\n\n\nNotice that compared with Price, the Carat values are tiny. The price goes into the thou-\nsands, and carats are in the range of decimals. To make the calculations easier to under-\nstand for the purposes of learning in this chapter, we can scale the Carat values to be\ncomparable to the Price values. By multiplying every Carat value by 1,000, we get num-\nbers that are easier to compute by hand in the upcoming walkthroughs. Note that by\nscaling all the rows, we are not affecting the relationships in the data, because every\nexample has the same operation applied to it. The resulting data (figure 8.7) is repre-\nsented in table 8.8.\n\nTable 8.8 Data with adjusted\ncarat values\n\n60004\n\n44,0004 \u00b0\n\nPrice\n\n3,000\n\n2,0004\n\n2,000\n\n+ a\n250 58@ 758 1,002 1,258 1,500 1,758 2,008 2,500\nCarat\n\nFigure 8.7 A scatterplot of carat and price data\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.15,
                        "section_name": "Finding the mean of the features",
                        "section_path": "./screenshots-images-2/chapter_8/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_15/5e853a74-ec3a-430d-be3a-a84d6a5998d4.png",
                            "./screenshots-images-2/chapter_8/section_15/a8881ff7-fe22-433b-821d-dee54bb4422a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Finding the mean of the features\n\nThe first thing we need to do to find a regression line is find the mean for each feature.\nThe mean is the sum of all values divided by the number of values. The mean is 1,229 for\ncarats, represented by the vertical line on the x axis. The mean is $3,431 for price, repre-\n\nsented by the horizontal line on the y axis (figure 8.8).\n\n6,000)\n\n5,000)\n\n4,000) e\n\n|\n\n30004\n\n2,000)\n\n2,000\n\nFigure 8.8 The means of x and y represented by vertical and horizontal lines\n\nThe mean is important because mathematically, any regression line we find will pass\nthrough the intersection of the mean of x and the mean of y. Many lines may pass\nthrough this point. Some regression lines might be better than others at fitting the data.\nThe method of least squares aims to create a line that minimizes the distances between\nthe line and among all the points in the dataset. The method of least squares is a popular\nmethod for finding regression lines. Figure 8.9 illustrates examples of regression lines.\n\nPrice\n\nFigure 8.9 Possible regression lines\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.16,
                        "section_name": "Finding regression lines with the least-squares method",
                        "section_path": "./screenshots-images-2/chapter_8/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_16/a47bb80a-c104-4209-9a26-00c183803849.png",
                            "./screenshots-images-2/chapter_8/section_16/0331fdf8-5ca4-4967-b0f9-832af1dbf5ca.png",
                            "./screenshots-images-2/chapter_8/section_16/29732d99-2557-41ab-bcd1-b5eb46fbc419.png",
                            "./screenshots-images-2/chapter_8/section_16/e14c7205-ef04-4b7c-9206-b2b706d7b310.png",
                            "./screenshots-images-2/chapter_8/section_16/80476e50-e577-47da-9b75-ad26934a0b83.png",
                            "./screenshots-images-2/chapter_8/section_16/9edfbc3b-a273-4a1e-b810-c1be50d62495.png",
                            "./screenshots-images-2/chapter_8/section_16/dd70f81f-7b78-4606-9bfd-ed0aab26e095.png",
                            "./screenshots-images-2/chapter_8/section_16/c2f4cf68-8708-496f-b18a-eebeaa92bca6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Finding regression lines with the least-squares method\n\nBut what is the regression line\u2019s purpose? Suppose that we're building a subway that tries\nto be as close as possible to all major office buildings. It will not be feasible to have a\nsubway line that visits every building; there will be too many stations and it will cost a\nlot. So, we will try to create a straight-line route that minimizes the distance to each\nbuilding. Some commuters may have to walk farther than others, but the straight line is\noptimized for everyone's office. This goal is exactly what a regression line aims to achieve;\nthe buildings are data points, and the line is the straight subway path (figure 8.10).\n\nNot feasible Feasible\n\nFigure 8.10 Intuition of regression lines\n\nLinear regression will always find a straight line that fits the data to minimize distance\namong points overall. Understanding the equation for a line is important because we\nwill be learning how to find the values for the variables that describe a line.\n\nA straight line is represented by the equation y = c + mx (figure 8.11):\n\n+ y: The dependent variable\n+ x: The independent variable\n+ m:The slope of the line\n\n+ oc: The y-value where the line intercepts the y axis\n\n3\n\nyrintercept\n1\n\n\u201ce\n\ny=o+tme\n\ny=2xe1\n\nslope *m?\n4i2=2\n\nFigure 8.11 Intuition of the equation that represents a line\n\nThe method of least squares is used to find the regression line. At a high level, the process\ninvolves the steps depicted in figure 8.12. To find the line that\u2019s closest to the data, we\nfind the difference between the actual data values and the predicted data values. The\ndifferences for data points will vary. Some differences will be large, and some will be\nsmall. Some differences will be negative values, and some will be positive values. By\nsquaring the differences and summing them, we take into consideration all differences\nfor all data points. Minimizing the total difference is getting the least square difference\nto achieve a good regression line. Don\u2019t worry if figure 8.12 looks a bit daunting; we will\n\nwork through each step.\n\nRepresenting\n\na line.\n\nSubstitute\nthe known\n\nmean values.\n\nCalculate\nthe slepe.\n\nCaloulate\nthe\ny-intercept.\n\nyeotmx\n\n3431 =o+m* 1229\n\nsum of (x - mean of x) * (y - mean of y)\nm=\n\nsum of (x - mean of x)\u201d\n\nem ~m #1229 + 3431\n\nFigure 8.12 The basic workflow for calculating a regression line\n\nThus far, our line has some known variables. We know that an x value is 1,229 anda y\nvalue is 3,431, as shown in step 2.\n\nNext, we calculate the difference between every Carat value and the Carat mean, as\nwell as the difference between every Price value and the Price mean, to find (x\u2014 mean of\nx) and (y\u2014 mean of y), which is used in step 3 (table 8.9).\n\nTable 8.9 The diamond dataset and calculations\n\nx- mean of x\n|_300-1,229 | -929 |\n410 -1,229 561 - 3,431\n750 - 1,229 2,760 \u2014 3,431\n910 - 1,229 2,763 - 3,431\n\n|_2100-1,229 | -29 | 2,809-3,431\n\n1,740 - 1,229 4,677 - 3,431\n1,960 - 1,229 6,147 \u2014 3,431\n2,210 - 1,229 6,535 \u2014 3,431\n\nFor step 3, we also need to calculate the square of the difference between every carat and\nthe carat mean to find (x\u2014 mean of x)A2. We also need to sum these values to minimize,\nwhich equals 3,703,690 (table 8.10).\n\nTable 8.10 The diamond dataset and calculations, part 2\n\nCarat (x) | Price (y) | x- mean of x\n\n410 - 1,229\n\n561 - 3,431\n\n(x - mean of x)A2\n\n-2,870 670,761\n\n750 - 1,229 2,760 - 3,431 | -671 229,441\n910 - 1,229 2,763 - 3,431 | -668 101,761\n2,100 - 1,229 2,809 - 3,431 | -622 841\n3,697 - 3,431 | 266 6,561\n\n1,740 - 1,229 | 511\n\n4,677 - 3,431\n\n261,121\n\n1,960 - 1,229 | 731\n\n6,147 - 3,431\n\n2,716 534,361\n\n2,210 -1,229 | 981\n\n6,535 \u2014 3,431\n\n962,361\n3,703,690\n\nSums\n\nThe last missing value for the equation in step 3 is the value for (x\u2014 mean of x) * (y-mean\nof y). Again, the sum of the values is required. The sum equals 11,624,370 (table 8.11).\n\nTable 8.11 The diamond dataset and calculations, part 3\n\nCarat (x)\n\n339 \u2014 3,431 | -3,092\n\n(x- mean\n(x-mean | of x)*(y-\nof x)A2_ | mean of y)\n\n410 - 1,229 561 - 3,431 | -2,870| 670,761 2,350,530\n750 - 1,229 2,760 - 3,431 | -671 229,441 321,409\n910 - 1,229 2,763 - 3,431 | -668 | 101,761 213,092\n\n2,809 \u2014 3,431\n\n18,038\n\n1,310 - 1,229\n\n3,697 - 3,431\n\n266 6,561 21,546\n\n1,500 - 1,229\n1,740 - 1,229\n1,960 - 1,229\n\n4,022 - 3,431\n4,677 \u2014 3,431\n6,147 \u2014 3,431\n\n261,121\n534,361\n\n1,985,396\n\n2,210 - 1,229\n\n6,535 \u2014 3,431\n\n3,104 | 962,361 3,045,024\n\n3,703,690\nSums\n\n11,624,370\n\n\nNow we can plug in the calculated values to the least-squares equation to calculate m:\n\nm = 11624370 / 3703690\nm= 3.139\n\nNow that we have a value for m, we can calculate c by substituting the mean values for x\nand y. Remember that all regression lines will pass this point, so it is a known point\nwithin the regression line:\n\nyoo+mx\n\n3431 = c+ 0.3186x\n3431 = c+ 391.5594\n3431 - 391.5594=\u00a2\nc= 3,039 .4406\n\nComplete regression line:\n\n\u00a5 = 3039.4406 + 0.3186x\n\nFinally, we can plot the line by generating some values for carats between the minimum\nvalue and maximum value, plugging them into the equation that represents the regres-\nsion line, and then plotting it (figure 8.13):\n\nx (Carat) minimum = 300\nx (Carat) maximum = 2210\n\nSample between the minimum and maximum at intervals of 500:\nx = (300, 2210)\n\nPlug the values fer x inte the regression line:\ny = (-426 + 3.139(300) = 515.7,\n~426 + 3.139(2210) = 6511.19)\n\nComplete x and y samples:\nX = [300, 2210)\ny = (3981, 9975]\n\n\n6,000)\n\n5,0004\n\n44,0004\n\nPrice\n\n3,000.\n\n2,000)\n\n1,000}\n\n+\n250 58 758 1,000 1,258 1,500 1,758 2,008 2,500\nCarat\n\nFigure 8.13 A regression line plotted with the data points\n\nWe've trained a linear regression line based on our dataset that accurately fits the date\nso we\u2019ve done some machine learning by hand.\n\nEXERCISE: CALCULATE A REGRESSION LINE USING THE LEAST-SQUARES METHOD\n\nFollowing the steps described and using the following dataset, calculate the\nregression line with the least-squares method.\n\n\nSOLUTION: CALCULATE A REGRESSION LINE USING THE LEAST-SQUARES METHOD\n\nThe means for each dimension need to be calculated. The means are 1,253 for\nx and 3,422 for y. The next step is calculating the difference between each\nvalue and its mean. Next, the square of the difference between xand the mean\nof xis calculated and summed, which results in 3,251,610. Finally, the differ-\nence between x and the mean of x is multiplied by the difference between y\nand the mean of y and summed, resulting in 10,566,940.\n\n(x - mean of x) *\nCarat (x) (y- mean of y)\n\n2,269,566\n\n205,209\n\n117,649\n9,409\n18,769\n\n558,009 2,299,266\n3,251,610 10,566,940\n\nThe values can be used to calculate the slope, m:\n\nm = 10566940 / 3251610\nm = 3.25\n\nRemember the equation for a line:\nyret+ mx\nSubstitute the mean values for x and y and the newly calculated m:\n\n3422 = c + 3.35 * 1253\nc = -775.55\n\nSubstitute the minimum and maximum values for x to calculate points to plot a line:\n\nPoint 1, we use the minimum value for Carat: x = 320\ny = 775.55 + 3.25 * 320\ny = 1 815.55\n\n2000\n\nPoint 2, we use the maximum value for Carat: x\ny = 775.55 + 3.25 * 2000\ny = 7 275.55\n\nNow that we have an intuition about how to use linear regression and how regression\nlines are calculated, let\u2019s take a look at the pseudocode.\n\nThe code is similar to the steps that we walked through. The only interesting aspects are\nthe two for loops used to calculate summed values by iterating over every element in\nthe dataset:\n\nfit_regression_line(carats, prices):\nlet mean_X equal mean(carats)\nlet mean_Y equal mean(price)\nlet sum_x_squared equal 0\nfor i in range(n):\nlet ans equal (carats[i] - mean_X) **2\nsum_x_squared equal sum_x_squared + ans\nlet sum_multiple equal 0\nfor i in range(n):\nlet ans equal (carats[{i]-mean_X) * (price[i] -mean_\u00a5)\nsum_multiple equal sum_multiple + ans\nlet bl equal sum_multiple / sum_x_squared\nlet b0 equal mean_Y - (bl *mean_X)\n\nlet min_x equal min(carats)\n\nexpress the First point of the\nlet max_x equal maxccarass) regression line by y= etm\n\nlet yl equal b0+b1*min_x\n\nlet y2 equal b0+b1l *max_x * \u2122 express the second point of the\nregression line by y = \u00a2 + mx\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.17,
                        "section_name": "Testing the model: Determine the accuracy of the model",
                        "section_path": "./screenshots-images-2/chapter_8/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_17/41b1ec06-87bc-421c-889b-ae2858c9188b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Testing the model: Determine the accuracy of the model\n\nNow that we have determined a regression line, we can use it to make price predictions\nfor other Carat values. We can measure the performance of the regression line with new\nexamples in which we know the actual price and determine how accurate the linear\nregression model is.\n\nWe can\u2019t test the model with the same data that we used to train it. This approach\nwould result in high accuracy and be meaningless. The trained model must be tested\nwith real data that it hasn\u2019t been trained with.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.18,
                        "section_name": "Separating training and testing data",
                        "section_path": "./screenshots-images-2/chapter_8/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_18/2cb8d2fb-6d0d-4b29-b40f-a216cba5dc30.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Separating training and testing data\n\nTraining and testing data are usually split 80/20, with 80% of the available data used as\ntraining data and 20% used to test the model. Percentages are used because the number\nof examples needed to train a model accurately is difficult to know; different contexts\nand questions being asked may need more or less data.\n\nFigure 8.14 and table 8.12 represent a set of testing data for the diamond example.\nRemember that we scaled the Carat values to be similar-size numbers to the Price values\n(all Carat values have been multiplied by 1,000) to make them easier to read and work\nwith. The dots represent the testing data points, and the line represents the trained\nregression line.\n\nTable 8.12 The carat and\nprice data\n\nCarat (x) | Price (y)\n\n6,200,\n\nsame 2 330 403\nsee 3) 710 2,772\n3 4 810 2,789\n3,000. ) 1,080 [2869 |\n\na\n\n1,390\n1,500\n1,640\n1,850\n1,910 6,632\n\nwv\n2\nB\n\n2,200,\n\n:\n\n7\n3\n\n1,000)\n\niw\n\ns\n\n<1 \u2014\u2014r .\n250 500 758 1,008 1,250 1,588 1,75\u00a2 2,000 2,500\nCarat\n\nFigure 8.14 A regression line plotted with the data points\n\nTesting a model involves making predictions with unseen training data and then com-\nparing the accuracy of the model\u2019s prediction with the actual values. In the diamond\nexample, we have the actual Price values, so we will determine what the model predicts\nand compare the difference.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.19,
                        "section_name": "Measuring the performance of the line",
                        "section_path": "./screenshots-images-2/chapter_8/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_19/2a66479a-0668-49c8-b4f0-0f34c70302f8.png",
                            "./screenshots-images-2/chapter_8/section_19/cfa03d05-2cf6-4940-a777-16c7960960f0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Measuring the performance of the line\n\nIn linear regression, a common method of measuring the accuracy of the model is cal-\nculating R? (R squared). R? is used to determine the variance between the actual value\nand a predicted value. The following equation is used to calculate the R* score:\n\n3 sum of (predicted y - mean of actual y)\u2019\nRF =\nsum of (actual y - mean of actual y)*\n\nThe first things we need to do, similar to the training step, are calculate the mean of the\nactual Price values, calculate the distances between the actual Price values and the mean\nof the prices, and then calculate the square of those values. We are using the values plot-\nted as dots in figure 8.14 (table 8.13).\n\nTable 8.13 The diamond dataset and calculations\n\n(y- mean of y)A2\n9,523,396\n9,150,625\n430,336\n408,321\n312,481\n\n486 236,196\n\n352,836\n\n2,019,241\n5,107,600\n10,265,616\n\nThe next step is calculating the predicted Price value for every Carat value, squaring the\nvalues, and calculating the sum of all those values (table 8.14).\n\nTable 8.14 The diamond dataset and calculations, part 2\n\n408,321\n\n236,196\n\nPredicted | (Predicted\ny-mean | (y-mean y-mean | y-mean\nCarat (x) | Price (y) ofy of y)A2 Predicted y ofy of y)A2\n220 342 3,086 9,523,396 264 3,164 10,009,876\n330 403 -3,025 9,150,625 609 -2,819 7,944,471\n\n1,721,527\n\n258,382\n\n1,500 4,022 594 352,836 4,282 854 728,562\n\n1,640 4,849 1,421 2,019,241 4,721 1,293 1,671,748\n\n1,850 5,688 2,260 5,107,600 5,380 1,952 3,810,559\n10,265,616 4,581,230\n\nUsing the sum of the square of the difference between the predicted price and mean, and\nthe sum of the square of the difference between the actual price and mean, we can cal-\nculate the R? score:\n\nsum of (predicted y - mean of actual y)\u2019\n\nR=\nsum of (actual y - mean of actual y)\u2019\n\nR? = 33585901 / 37806648\nR? = 088\n\nThe result\u20140.88\u2014means that the model is 88% accurate to the new unseen data. This\nresult is a fairly good one, showing that the linear regression model is fairly accurate. For\nthe diamond example, this result is satisfactory. Determining whether the accuracy is\nsatisfactory for the problem we're trying to solve depends on the domain of the problem.\nWe will be exploring performance of machine learning models in the next section.\n\nAdditional information: For a gentle introduction to fitting lines to data, reference\nhttp://mng.bz/Ed5q\u2014a chapter from Math for Programmers by Manning Publications.\nLinear regression can be applied to more dimensions. We can determine the relationship\namong Carat values, prices, and cut of diamonds, for example, through a process called\nmultiple regression. This process adds some complexity to the calculations, but the fun-\ndamental principles remain the same.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.2,
                        "section_name": "Improving accuracy",
                        "section_path": "./screenshots-images-2/chapter_8/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_20/48f5fd44-6e91-4084-9ce3-8eeb5cf9ff83.png",
                            "./screenshots-images-2/chapter_8/section_20/fd2b34d2-f641-43ca-bf69-65139ac70163.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Improving accuracy\n\nAfter training a model on data and measuring how well it performs on new testing data, we\nhave an idea of how well the model performs. Often, models don\u2019t perform as well as desired,\nand additional work needs to be done to improve the model, if possible. This improvement\ninvolves iterating on the various steps in the machine learning life cycle (figure 8.15).\n\n1 2 3 4 5s\nCollect and\nPrepare Test the Improve\nN\n\n~ -%\n\n\\ N ~~ 7\nNe ~~ ~ a7\n\n\u2122. i er A\n_~ ae\n\\ ~ ae Z\n_ ~~ __ \u2014 \u2014\n\nFigure 8.15 A refresher on the machine learning life cycle\n\nThe results may require us to pay attention to one or more of the following areas. Machine\nlearning is experimental work in which different tactics at different stages are tested\nbefore settling on the best-performing approach. In the diamond example, if the model\nthat used Carat values to predict Price performed poorly, we might use the dimensions\nof the diamond that indicate size, coupled with the Carat value, to try to predict the price\nmore accurately. Here are some ways to improve the accuracy of the model:\n\n+ Collect more data. One solution may be to collect more data related to the dataset\nthat is being explored, perhaps augmenting the data with relevant external data\nor including data that previously was not considered.\n\n+ Prepare the data differently. The data used for training may need to be prepared\nin a different way. Referring to the techniques used to fix data earlier in this\nchapter, there may be errors in the approach. We may need to use different\ntechniques to find values for missing data, replace ambiguous data, and encode\ncategorical data.\n\n+ Choose different features in the data. Other features in the dataset may be better\nsuited to predicting the dependent variable. The X dimension value might be a\ngood choice to predict the Table value, for example, because it has a physical\nrelationship with it, as shown in the diamond terminology figure (figure 8.5),\nwhereas predicting Clarity with the X dimension is meaningless.\n\n+ Use a different algorithm to train the model. Sometimes, the selected algorithm is\nnot suited to the problem being solved or the nature of the data. We can use a\ndifferent algorithm to accomplish different goals, as discussed in the next\nsection.\n\n+ Dealing with false-positive tests. Tests can be deceiving. A good test score may\nshow that the model performs well, but when the model is presented with unseen\ndata, it might perform poorly. This problem can be due to overfitting the data.\nOverfitting is when the model is too closely aligned with the training data and is\nnot flexible for dealing with new data with more variance. This approach is\nusually applicable to classification problems, which we also dive into in the next\nsection.\n\nIf linear regression didn\u2019t provide useful results, or if we have a different question to ask,\nwe can try a range of other algorithms. The next two sections will explore algorithms to\nuse when the question is different in its nature.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.21,
                        "section_name": "Classification with decision trees",
                        "section_path": "./screenshots-images-2/chapter_8/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_21/7d067a68-2631-4c07-b6cf-a82631b44605.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Classification with decision trees\n\nSimply put, classification problems involve assigning a label to an example based on its\nattributes. These problems are different from regression, in which a value is estimated.\nLet\u2019s dive into classification problems and see how to solve them.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.22,
                        "section_name": "Classification problems: Either this or that",
                        "section_path": "./screenshots-images-2/chapter_8/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_22/5345c6ed-936a-45dd-80e3-efc46a570094.png",
                            "./screenshots-images-2/chapter_8/section_22/cada8156-695f-4e5b-b25e-9b0fc71ca1ea.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Classification problems: Either this or that\n\nWe have learned that regression involves predicting a value based on one or more other\nvariables, such as predicting the price of a diamond given its Carat value. Classification\nis similar in that it aims to predict a value but predicts discrete classes instead of contin-\nuous values. Discrete values are categorical features of a dataset such as Cut, Color, or\nClarity in the diamond dataset, as opposed to continuous values such as Price or Depth.\n\nHere\u2019s another example. Suppose that we have several vehicles that are cars and trucks.\nWe will measure the weight of each vehicle and the number of wheels of each vehicle. We\nalso forget for now that cars and trucks look different. Almost all cars have four wheels,\nand many large trucks have more than four wheels. Trucks are usually heavier than cars,\nbut a large sport-utility vehicle may be as heavy as a small truck. We could find relation-\nships between the weight and number of wheels of vehicles to predict whether a vehicle\nis a car or a truck (figure 8.16).\n\nCar Car Car\n\nG&S\n\n4 wheels 6 wheels 4 wheels 4 wheels 4 wheels\n8 tons 10 tens 4 tens 2 tons 1 ton\n\nFigure 8.16 Example vehicles for potential classification based\non the number of wheels and weight\n\nEXERCISE: REGRESSION VS. CLASSIFICATION\n\nConsider the following scenarios, and determine whether each one is a regres-\nsion or classification problem:\n\n1. Based on data about rats, we have a life-expectancy feature and an obesity\nfeature. We're trying to find a correlation between the two features.\n\n2. Based on data about animals, we have the weight of each animal and whether\nor not it has wings. We're trying to determine which animals are birds.\n\n3. Based on data about computing devices, we have the screen size, weight, and\noperating system of several devices. We want to determine which devices are\ntablets, laptops, or phones.\n\n4. Based on data about weather, we have the amount of rainfall and a humidity\nvalue. We want to determine the humidity in different rainfall seasons.\nSOLUTION: REGRESSION VS. CLASSIFICATION\n\n1. Regression\u2014The relationship between two variables is being explored. Life\nexpectancy is the dependent variable, and obesity is the independent variable.\n\n2. Classification\u2014We are classifying an example as a bird or not a bird, using the\nweight and the wing characteristic of the examples.\n\n3. Classification\u2014An example is being classified as a tablet, laptop, or phone by\nusing its other characteristics.\n\n4. Regression\u2014The relationship between rainfall and humidity is being\nexplored. Humidity is the dependent variable, and rainfall is the independent\nvariable.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.23,
                        "section_name": "The basics of decision trees",
                        "section_path": "./screenshots-images-2/chapter_8/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_23/42871171-c872-4d12-90b3-aaaeb8230565.png",
                            "./screenshots-images-2/chapter_8/section_23/2bfbccf8-26ea-47c0-8ad1-b8d8a01793a0.png",
                            "./screenshots-images-2/chapter_8/section_23/07d784f3-efe9-4f2b-8144-c3092808d72b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The basics of decision trees\n\nDifferent algorithms are used for regression and classification problems. Some popular\nalgorithms include support vector machines, decision trees, and random forests. In this\nsection, we will be looking at a decision-tree algorithm to learn classification.\n\nDecision trees are structures that describe a series of decisions that are made to find a\nsolution to a problem (figure 8.17). If we're deciding whether to wear shorts for the day,\nwe might make a series of decisions to inform the outcome. Will it be cold during the\nday? If not, will we be out late in the evening, when it does get cold? We might decide to\nwear shorts on a warm day, but not if we will be out when it gets cold.\n\nShould I wear sherts today?\n\nIs sunny?\n\nOut late?\n\n~~ te\n\nne rl\n\nFigure 8.17 Example of a basic decision tree\n\nFor the diamond example, we will try to predict the cut of a diamond based on the Carat\nand Price values by using a decision tree. To simplify this example, assume that we're a\ndiamond dealer who doesn\u2019t care about each specific cut. We will group the differ-\nent cuts into two broader categories. Fair and Good cuts will be grouped into a cate-\ngory called Okay, and Very Good, Premium, and Ideal cuts will be grouped into a\ncategory called Perfect.\n\n\nOur sample dataset now looks like table 8.15.\n\nTable 8.15 The dataset used for the classification example\nCarat Price Cut\n\n0.21 327. | Okay\n0.39 397 | Perfect\n\n050 | 1,122 | Perfect\n\nwin|\u2014\n\n4 0.76 | 907 | Okay\n5 087 | 2,757 | Okay\n6 098 | 2,865 | Okay\n7 1.13 [3,045 | Perfect\n8 134 | 3,914 | Perfect\n9 1.67 | 4,849 | Perfect\n10 1.81 | 5,688 | Perfect\n\nBy looking at the values in this small example and intuitively looking for patterns, we\nmight notice something. The price seems to spike significantly after 0.98 carats, and the\nincreased price seems to correlate with the diamonds that are Perfect, whereas diamonds\nwith smaller Carat values tend to be Average. But example 3, which is Perfect, has a small\nCarat value. Figure 8.18 shows what would happen if we were to create questions to filter\nthe data and categorize it by hand. Notice that decision nodes contain our questions, and\nleaf nodes contain examples that have been categorized.\n\nPerfect diamends 7 Okay diamends\n\nFigure 8.18 Example of a decision tree designed through human intuition\n\nWith the small dataset, we could easily categorize the diamonds by hand. In real-world\ndatasets, however, there are thousands of examples to work through, with possibly thou-\nsands of features, making it close to impossible for a person to create a decision tree by\nhand. This is where decision tree algorithms come in. Decision trees can create the ques-\ntions that filter the examples. A decision tree finds the patterns that we might miss and\nis more accurate in its filtering.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.24,
                        "section_name": "Training decision trees",
                        "section_path": "./screenshots-images-2/chapter_8/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_24/48666cb6-5888-4e1e-a4e3-eb8ef1acfbca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training decision trees\n\nTo create a tree that is intelligent in making the right decisions to classify diamonds, we\nneed a training algorithm to learn from the data. There is a family of algorithms for\ndecision tree learning, and we will use a specific one named CART (Classification and\nRegression Tree). The foundation of CART and the other tree learning algorithms is this:\ndecide what questions to ask and when to ask those questions to best filter the examples\ninto their respective categories. In the diamond example, the algorithm must learn the\nbest questions to ask about the Carat and Price values, and when to ask them, to best\nsegment Average and Perfect diamonds.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.25,
                        "section_name": "Data structures for decision trees",
                        "section_path": "./screenshots-images-2/chapter_8/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_25/6902318b-fb5d-4212-945b-2c03171f421d.png",
                            "./screenshots-images-2/chapter_8/section_25/b2e0e5cb-7b5d-49a0-afdf-b75812f3956d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Data structures for decision trees\n\nTo help us understand how the decisions of the tree will be structured, we can review the\nfollowing data structures, which organize logic and data in a way that\u2019s suitable for the\ndecision tree learning algorithm:\n\nMap of classes/label groupings\u2014A map is a key-value pair of elements that cannot\nhave two keys that are the same. This structure is useful for storing the number\nof examples that match a specific label and will be useful to store the values\nrequired for calculating entropy, also known as uncertainty. We'll learn about\nentropy soon.\n\nTree of nodes\u2014As depicted in the previous tree figure (figure 8.18), several nodes\nare linked to compose a tree. This example may be familiar from some of the\nearlier chapters. The nodes in the tree are important for filtering/partitioning the\nexamples into categories:\n\n\u00b0 Decision node\u2014A node in which the dataset is being split or filtered.\n\n* Question: What question is being asked? (See the Question point\ncoming up).\n\n+ True examples: The examples that satisfy the question.\n+ False examples: The examples that don\u2019t satisfy the question.\n\n\u00b0 Examples node/leaf node\u2014A node containing a list of examples only. All\nexamples in this list would have been categorized correctly.\n\nQuestion\u2014A question can be represented differently depending on how flexible\nit can be. We could ask, \u201cIs the Carat value > 0.5 and < 1.13?\u201d To keep this\n\nexample simple to understand, the question is a variable feature, a variable value,\nand the >= operator: \u201cIs Carat >= 0.5?\u201d or \u201cIs Price >=3,045?\u201d\n\n\u00b0 Feature\u2014The feature that is being interrogated\n\n\u00b0 Value\u2014The constant value that the comparing value must be greater than\nor equal to\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.26,
                        "section_name": "Decision-tree learning life cycle",
                        "section_path": "./screenshots-images-2/chapter_8/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_26/7b3234e3-3507-48e7-b126-927bce9da9a3.png",
                            "./screenshots-images-2/chapter_8/section_26/5dafdefd-5ae7-42e9-a4c1-37870c924dee.png",
                            "./screenshots-images-2/chapter_8/section_26/7d49f2c5-2354-4499-b881-1ece950307e6.png",
                            "./screenshots-images-2/chapter_8/section_26/0866fa0f-6cbe-4dc8-afb5-a64ae8a24c74.png",
                            "./screenshots-images-2/chapter_8/section_26/a37c9d4a-56a9-483a-a1a0-4c715e258fa8.png",
                            "./screenshots-images-2/chapter_8/section_26/4dc204b1-214f-4926-b58d-120f0633de41.png",
                            "./screenshots-images-2/chapter_8/section_26/d6978b6a-e030-47be-891e-cc396bfd218b.png",
                            "./screenshots-images-2/chapter_8/section_26/e8454153-8f1c-4205-9b19-279d05333f53.png",
                            "./screenshots-images-2/chapter_8/section_26/5cda6eb1-3f60-4e17-91ae-9dc66bf6d357.png",
                            "./screenshots-images-2/chapter_8/section_26/53ed7dc2-ff74-4fb3-b2cd-2695c76802ad.png",
                            "./screenshots-images-2/chapter_8/section_26/e32bb947-2d05-499c-ab4e-a6d0799e7f60.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Decision-tree learning life cycle\n\nThis section discusses how a decision-tree algorithm filters data with decisions to clas-\nsify a dataset correctly. Figure 8.19 shows the steps involved in training a decision tree.\nThe flow described in figure 8.19 is covered throughout the rest of this section.\n\nSplit\nCalculate Generate examples and Calculate\nuncer tainty questions calculate info gain\nfor for uncer tainty based en each\nexamples. examples. based on each question.\n\nquestion.\n\nProvide loft\n\nor right Is any\nexample split example split\nwith uncer tainty\nuncer tainty e\n\n>0.\n\nFigure 8.19 A basic flow for building a decision tree\n\nIn building a decision tree, we test all possible questions to determine which one is the\nbest question to ask at a specific point in the decision tree. To test a question, we use the\nconcept of entropy\u2014the measurement of uncertainty of a dataset. If we had 5 Perfect\ndiamonds and 5 Okay diamonds, and tried to pick a Perfect diamond by randomly\nselecting a diamond from the 10, what are the chances that the diamond would be Perfect\n(figure 8.20)?\n\n5 oKay diamonds\n5 perfect diamonds GO @.\n5+5 = 10 diamonds total RP LP\n5 out of 10 chance of\n\nOGM picking a perfect diamend\n\n5/10 = 50% uncertainty\n\nFigure 8.20 Example of uncertainty\n\nGiven an initial dataset of diamonds with the Carat, Price, and Cut features, we can\ndetermine the uncertainty of the dataset by using the Gini index. A Gini index of 0\nmeans that the dataset has no uncertainty and is pure; it might have 10 Perfect dia-\nmonds, for example. Figure 8.21 describes how the Gini index is calculated.\n\n[| _Jeazat price] cut |\n[2 [ oa | sar] ow\n}2 | 6.39 | 397 | mmo | \\ \u00b0 \u00b0\n[3 | 0-50 [1,222] rem | Gini = 1 - (Okay ceunt/ tetal)* + (Perfect count/ total)\nee ; Gini = 1-(5/ 10)? +(5/ 10)?\nGin = 1- 7. 5)* + (0.5)\nGini =\n[@ | 2.34 [3,914 | even |\n| 2 | 3.67 [4,049 | ron\n[ef s.9x [5,608 | ree]\n\nFigure 8.21 The Gini index calculation\n\nThe Gini index is 0.5, so there\u2019s a 50% chance of choosing an incorrectly labeled example\nif one is randomly selected, as shown in figure 8.20 earlier.\n\nThe next step is creating a decision node to split the data. The decision node includes\na question that can be used to split the data in a sensible way and decrease the uncer-\ntainty. Remember that 0 means no uncertainty. We aim to partition the dataset into\nsubsets with zero uncertainty.\n\nMany questions are generated based on every feature of each example to split the data\nand determine the best split outcome. Because we have 2 features and 10 examples, the\ntotal number of questions generated would be 20. Figure 8.22 depicts some of the ques-\ntions asked\u2014simple questions about whether the value of a feature is greater than or\nequal to a specific value.\n\nFigure 8.22 An example of questions asked to split the data with a decision node\n\nUncertainty in a dataset is determined by the Gini index, and questions aim to reduce\nuncertainty. Entropy is another concept that measures disorder using the Gini index for\na specific split of data based on a question asked. We must have a way to determine how\nwell a question reduced uncertainty, and we accomplish this task by measuring informa-\ntion gain. Information gain describes the amount of information gained by asking a spe-\ncific question. If a lot of information is gained, the uncertainty is smaller.\n\nInformation gain is calculated by the subtracting entropy before the question is asked\nby the entropy after the question is asked, following these steps:\n\n1. Split the dataset by asking a question.\n\n2. Measure the Gini index for the left split.\n\n3. Measure the entropy for the left split compared with the dataset before the split.\n4. Measure the Gini index for the right split.\n\n5. Measure the entropy for the right split compared with the dataset before the split.\n\n6. Calculate the total entropy after by adding the left entropy and right entropy.\n\n7. Calculate the information gain by subtracting the total entropy after from the\ntotal entropy before.\n\nFigure 8.23 illustrates the data split and information gain for the question \u201cIs Price >= 3914?\u201d\n\nF\ni\na\n\nLod Log Les\n\nLoft gini = 1 - (ekay/ total)? - (perfect/tetal)\u00ae\n\nLeft gin = 2 - (3/3)? - (way?\nleft gin\n\nLt\n\nLeft entropy = Instances/tetal * ginl left \u2018right antrepy = ing tances/total* gini right\n\nloft entrepy = 3/28* right entropy = 7/10\" 0.41\nloft entrepy = @ right entropy = 6.287\n\nInformation gain = entropy befere - entrepy after\n\n\u2018entropy + right entropy\n28\n\ninformation gain = @.5 - 0.287\nInformation gain = 233\n\nFigure 8.23 Illustration of data split and information gain based on a question\n\nIn the example in figure 8.23, the information gain for all questions is calculated, and\nthe question with the highest information gain is selected as the best question to ask at\nthat point in the tree. Then the original dataset is split based on the decision node with\nthe question \u201cIs Price >= 3,914?\u201d A decision node containing this question is added to the\ndecision tree, and the left and right splits stem from that node.\n\nIn figure 8.24, after the dataset is split, the left side contains a pure dataset of Perfect\ndiamonds only, and the right side contains a dataset with mixed diamond classifications,\nincluding two Perfect diamonds and five Okay diamonds. Another question must be\nasked on the right side of the dataset to split the dataset further. Again, several questions\nare generated by using the features of each example in the dataset.\n\nPerfect\n\nFigure 8.24 The resulting decision tree after the first decision node and possible questions\n\nEXERCISE: CALCULATING UNCERTAINTY AND INFORMATION GAIN FOR A QUESTION\n\nUsing the knowledge gained and figure 8.23 as a guide, calculate the informa-\ntion gain for the question \u201cIs Carat >= 0.76?\u201d\n\nSOLUTION: CALCULATING UNCERTAINTY AND INFORMATION GAIN FOR A QUESTION\n\nThe solution depicted in figure 8.25 highlights the reuse of the pattern of\ncalculations that determine the entropy and information gain, given a ques-\ntion. Feel free to practice more questions and compare the results with the\ninformation-gain values in the figure.\n\nLnfermation gain = 0.41 - 0.19\ninformation gain = 0.22\n\n|\n\n| Ebshesl\n\n| left ginl = 1 - (ekay/total)\u2019 -(perfect/tetal)? \u2014_right ginl \u00bb 1 - (etay/tetal)\u00ae - (perfect/tetal)\u201d\n| att git t= (474? carey? ght gint = 2 = (273)? = (4/3)?\n\n| left inl = right gink = 0.444\n\n| ttanresatns/l gl tft riers! rt\n| left entropy = 4/78 Fight entropy = 317 0444\n\n| lefkentrapy = @ ight entropy \u00ab @\n\n| lnfermation gain = entrepy before - entracy after\n\n| \u2018entropy after aloft antrapy + right entropy\n\n| \u2018entrepy after\n\n|\n\nFigure 8.25 Illustration of data split and information gain based on a question at the second level\n\nThe process of splitting, generating questions, and determining information gained hap-\npens recursively until the dataset is completely categorized by questions. Figure 8.26\nshows the complete decision tree, including all the questions asked and the resulting\nsplits.\n\nPerfect\n\nPerfect\n\nFigure 8.26 The complete trained decision tree\n\nIt is important to note that decision trees are usually trained with a much larger sample\nof data. The questions asked need to be more general to accommodate a wider variety of\ndata and, thus, would need a variety of examples to learn from.\n\nWhen programming a decision tree from scratch, the first step is counting the number\n\nof examples of each class\u2014in this case, the number of Okay diamonds and the number\nof Perfect diamonds:\n\nfind_unique_label_counts(examples):\nlet class_count equal empty map\nfor example in examples:\nlet label equal example[\u2019quality\u2019]\nif label not in class_count:\nlet class_count[(label] equal \u00ae\nclass_count [label] equal class_count[label}+1\n\nreturn class_count\n\nNext, examples are split based on a question. Examples that satisfy the question are\nstored in examples true, and the rest are stored in examples_false:\n\nsplit_examples(examples, question):\nlet examples_true equal empty array\nlet examples_false equal empty array\nfor example in examples:\nif question .filter (example):\nappend example to examples_true\nelse:\nappend example to examples_false\n\nreturn examples_true, examples_false\n\nWe need a function that calculates the Gini index for a set of examples. The next func-\ntion calculates the Gini index by using the method described in figure 8.23:\ncalculate_gini(examples):\nlet label_counts equal find_unique_label_counts(examples)\nlet uncertainty equal1\nfor label in label_counts:\nlet probability_of_label equal label_counts(label) / length(examples))\nuncertainty equals uncertainty - probability_of_label \u201c2\n\nreturn uncertainty\n\ninformation_gain uses the left and right splits and the current uncertainty to\ndetermine the information gain:\n\ncalculate_information_gain(left, right, current_uncertainty):\nlet total equal length (left) + length(right)\nlet left_gini equal calculate_gini(left)\nlet left_entropy equal length(left) / total * left_gini\nlet right_gini equal calculate_gini(right)\nlet right_entropy equal length(right) / total * right_gini\nlet uncertainty_after equal left_entropy + right_entropy\nlet information_gain equal current_uncertainty - uncertainty_after\n\nreturn information_gain\n\nThe next function may look daunting, but it\u2019s iterating over all the features and their\nvalues in the dataset, and finding the best information gain to determine the best ques-\ntion to ask:\nfind_best_split (examples, number_of_features):\nlet best_gain equal \u00ae\nlet best_question equal None\nlet current_uncertainty equal calculate_gini(examples)\nfor feature_index in range(number_of_features):\nlet values equal (example[feature_index] for example in examples]\nfor value in values:\nlet question equal Question(feature index, value)\nlet true_examples, false_examples equal\nsplit_examples(examples, question)\nif length (true_examples) != 0 or length(false_examples) I= 0:\nlet gain equal calculate_information_gain\n(true_examples, false_examples, current_uncertainty)\nif gain >= best_gain:\nbest_gain, best_question equal gain, question\n\nreturn best_gain, best_question\n\nThe next function ties everything together, using the functions defined previously to\nbuild a decision tree:\n\nbuild_tree(examples, number_of_features):\nlet gain, question equal find_best_split (examples, number_of_ features)\nif gain == 0:\nreturn ExamplesNode (examples)\nlet true_examples, false_examples equal split_examples(examples, question)\nlet true_branch equal build_tree(true_examples)\nlet false_branch equal build_tree(false_examples)\n\nreturn DecisionNode (question, true_branch, false_branch)\n\nNote that this function is recursive. It splits the data and recursively splits the resulting\ndataset until there is no information gain, indicating that the examples cannot be split\nany further. As a reminder, decision nodes are used to split the examples, and example\nnodes are used to store split sets of examples.\n\nWe've now learned how to build a decision-tree classifier. Remember that the trained\ndecision-tree model will be tested with unseen data, similar to the linear regression\napproach explored earlier.\n\nOne problem with decision trees is overfitting, which occurs when the model is\ntrained too well on several examples but performs poorly for new examples. Overfitting\nhappens when the model learns the patterns of the training data but new real-world data\nis slightly different and doesn\u2019t meet the splitting criteria of the trained model. A model\nwith 100% accuracy is usually overfitted to the data. Some examples are classified incor-\nrectly in an ideal model as a consequence of the model being more general to support\ndifferent cases. Overfitting can happen with any machine learning model, not just deci-\nsion trees.\n\nFigure 8.27 illustrates the concept of overfitting. Underfitting includes too many\nincorrect classifications, and overfitting includes too few or no incorrect classifications;\nthe ideal is somewhere in between.\n\ne)\n\n0\u00b020 x(-0 PO\nCx Oo O x DOO\nOO @ K\nx xX\nx* KX x K* XX\nUnder fitted Ideal Overfitted\n\nFigure 8.27 Underfitting, ideal, and overfitting\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.27,
                        "section_name": "Classifying examples with decision trees",
                        "section_path": "./screenshots-images-2/chapter_8/section_27",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_27/5f39f6f0-dbf1-4ffd-8e81-c1bb697a56bc.png",
                            "./screenshots-images-2/chapter_8/section_27/e8f5714c-7347-4314-9682-cfaf845af8dc.png",
                            "./screenshots-images-2/chapter_8/section_27/1c0d2b12-91ef-435d-91ed-532ff0f63164.png",
                            "./screenshots-images-2/chapter_8/section_27/457f947b-d938-4731-814d-c82bccf2f421.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Classifying examples with decision trees\n\nNow that a decision tree has been trained and the right questions have been determined,\nwe can test it by providing it new data to classify. The model that we're referring to is the\ndecision tree of questions that was created by the training step.\n\nTo test the model, we provide several new examples of data and measure whether they\nhave been classified correctly, so we need to know the labeling of the testing data. In the\ndiamond example, we need more diamond data, including the Cut feature, to test the\ndecision tree (table 8.16).\n\nTable 8.16 The diamond dataset for classification\n\nCarat Price Cut\n2 0.41 967 Perfect\n3 0.52 1,012 Perfect\n4 0.76 907 Okay\n5 0.81 2,650 Okay\n6 0.90 2,634 | Okay\n8 1.42 3850 Perfect\n9 1.61 4,345 Perfect\n10 1.78 3,100 Okay\n\nFigure 8.28 illustrates the decision-tree model that we trained, which will be used to\nprocess the new examples. Each example is fed through the tree and classified.\n\nIs Price >= 39147\n\nYes No\n\nIs Carat >= 0.767\n\nPerfect diamond\n\nYes No\n\nIs Price >= 8977\n\nPerfect diamend Okay diamond\n\nFigure 8.28 The decision tree model that will process new examples\n\nThe resulting predicted classifications are detailed in table 8.17. Assume that we're try-\ning to predict Okay diamonds. Notice that three examples are incorrect. That result is\n3 of 10, which means that the model predicted 7 of 10, or 70% of the testing data correctly.\nThis performance isn\u2019t terrible, but it illustrates how examples can be misclassified.\n\nTable 8.17 The diamond dataset for classification and predictions\n\n[ [coat [pice [cat [Prediction |__|\nPf aas [09 | otay | Otay\nPz [om [00 | erect | erect\n[a7 oor [oy\n\u2014e\n|_ Perfect _|\n\ni\n\nSIS\n\nOkay\n\ne\n\nN\n\nOkay\n[7e\u2014[ 3100 | \u2014oay |\n\nA confusion matrix is often used to measure the performance of a model with test-\ning data. A confusion matrix describes the performance using the following metrics\n(figure 8.29):\n\n+ True positive (TP)\u2014Correctly classified examples as Okay\n+ True negative (TN)\u2014Correctly classified examples as Perfect\n+ False positive (FP)\u2014Perfect examples classified as Okay\n\n+ False negative (FN) \u2014Okay examples classified as Perfect\n\nPredicted pesitive Predicted negative\n\u201c ] renee |\n\nTrue negative\n\nFigure 8.29 A confusion matrix\n\nThe outcomes of testing the model with unseen examples can be used to deduce several\nmeasurements:\n\nPrecision\u2014How often Okay examples are classified correctly\nNegative precision\u2014How often Perfect examples are classified correctly\n\nSensitivity or recall\u2014Also known as the true-positive rate; the ratio of correctly\nclassified Okay diamonds to all the actual Okay diamonds in the training set\n\nSpecificity\u2014Also known as the true-negative rate; the ratio of correctly classified\nPerfect diamonds to all actual Perfect diamonds in the training set\n\nAccuracy\u2014How often the classifier is correct overall between classes\n\nFigure 8.30 shows the resulting confusion matrix, with the results of the diamond exam-\nple listed as input. Accuracy is important, but the other measurements can unveil addi-\ntional useful information about the model\u2019s performance.\n\nActual pesitive\n\nSpecificity\n3/3+\n\nAcouracy\n\nFigure 8.30 Confusion matrix for the diamond test example\n\nBy using these measurements, we can make more-informed decisions in a machine\nlearning life cycle to improve the performance of the model. As mentioned throughout\nthis chapter, machine learning is an experimental exercise involving some trial and\nerror. These metrics are guides in this process.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.28,
                        "section_name": "Other popular machine learning algorithms",
                        "section_path": "./screenshots-images-2/chapter_8/section_28",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_28/46d00f86-e992-4aef-894f-8e4b21f2e3be.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Other popular machine learning algorithms\n\nThis chapter explores two popular and fundamental machine learning algorithms. The\nlinear-regression algorithm is used for regression problems in which the relationships\nbetween features are discovered. The decision-tree algorithm is used for classification\nproblems in which the relationships between features and categories of examples are\ndiscovered. But many other machine learning algorithms are suitable in different con-\ntexts and for solving different problems. Figure 8.31 illustrates some popular algorithms\nand shows how they fit into the machine learning landscape.\n\nK- Means\nDecision Trees Mean Shift\nLogistic Regression Density-Based Spatial Clustering\nK- Nearest Neighber of Applications with Neise (DBSCAN)\n\nSuppert Vector Machines\nNaive Bayss\n\nLinear Regression\nPelynemiat Regression\nLasse Regression\n\non\n\nreduction\n\nt-Distributed Stochastic\nMeighber Embedding (t-6NE)\nPrinciple Component Analysis (PCA)\n\nArtificial Neural Networks (ANN)\nConvolutional Neural Netwerks (CNN)\nGenerative Adversarial Neural Networks (GAN)\nRecurrent Neural Networks (RNN)\n\nGenetic Algorithms\nState-Action-Reward-State-Actien (SARSA)\n\nFigure 8.31 A map of popular machine learning algorithms\n\nThe classification and regression algorithms satisfy problems similar to the ones explored\nin this chapter. Unsupervised learning contains algorithms that can help with some of\nthe data preparation steps, find hidden underlying relationships in data, and inform\nwhat questions can be asked in a machine learning experiment.\n\nNotice the introduction of deep learning in figure 8.31. Chapter 9 covers artificial\nneural networks\u2014a key concept in deep learning. This chapter will give us a better\nunderstanding of the types of problems that can be solved with these approaches and\nhow the algorithms are implemented.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.29,
                        "section_name": "Use cases for machine learning algorithms",
                        "section_path": "./screenshots-images-2/chapter_8/section_29",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_29/fd167e44-995e-4b8f-9b89-dfea3cd3c280.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Use cases for machine learning algorithms\n\nMachine learning can be applied in almost every industry to solve a plethora of problems\nin different domains. Given the right data and the right questions, the possibilities are\npotentially endless. We have all interacted with a product or service that uses some aspect\nof machine learning and data modeling in our everyday lives. This section highlights some\nof the popular ways machine learning can be used to solve real-world problems at scale:\n\nFraud and threat detection\u2014Machine learning has been used to detect and prevent\nfraudulent transactions in the finance industry. Financial institutions have gained a\nwealth of transactional information over the years, including fraudulent transaction\nreports from their customers. These fraud reports are an input to labeling and\ncharacterizing fraudulent transactions. The models might consider the location of\nthe transaction, the amount, the merchant, and so on to classify transactions, saving\nconsumers from potential losses and the financial institution from insurance losses.\nThe same model can be applied to network threat detection to detect and prevent\nattacks based on known network use and reported unusual behavior.\n\nProduct and content recommendations\u2014Many of us use e-commerce sites to\npurchase goods or media streaming services for audio and video consumption.\nProducts may be recommended to us based on what we're purchasing, or content\nmay be recommended based on our interests. This functionality is usually\nenabled by machine learning, in which patterns in purchase or viewing behavior\nis derived from people\u2019s interactions. Recommender systems are being used in\nmore and more industries and applications to enable more sales or provide a\nbetter user experience.\n\nDynamic product and service pricing\u2014Products and services are often priced\nbased on what someone is willing to pay for them or based on risk. For a ride-\nsharing system, it might make sense to hike the price if there are fewer available\ncars than the demand for a ride, sometimes referred to as surge pricing. In the\ninsurance industry, a price might be hiked if a person is categorized as high-risk.\nMachine learning is used to find the attributes and relationships between the\nattributes that influence pricing based on dynamic conditions and details about\na unique individual.\n\nHealth-condition risk prediction\u2014The medical industry requires health\nprofessionals to acquire an abundance of knowledge so that they can diagnose and\ntreat patients. Over the years, they have gained a vast amount of data about\npatients: blood types, DNA, family-illness history, geographic location, lifestyle,\nand more. This data can be used to find potential patterns that can guide the\ndiagnosis of illness. The power of using data to find diagnoses is that we can treat\nconditions before they mature. Additionally, by feeding the outcomes back into the\nmachine learning system, we can strengthen its reliability in making predictions.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 9,
                "chapter_name": "Artificial\nneural networks",
                "chapter_path": "./screenshots-images-2/chapter_9",
                "sections": [
                    {
                        "section_id": 9.1,
                        "section_name": "What are artificial neural networks?",
                        "section_path": "./screenshots-images-2/chapter_9/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_1/16cb8da7-3946-48e5-bdab-2e006b324296.png",
                            "./screenshots-images-2/chapter_9/section_1/2e64023e-7a6e-464e-9c17-b55ef190780c.png",
                            "./screenshots-images-2/chapter_9/section_1/ea831ccc-5e22-4cac-b02c-ec62f0a94033.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What are artificial neural networks?\n\nArtificial neural networks (ANNs) are powerful tools in the machine learning toolkit,\nused in a variety of ways to accomplish objectives such as image recognition, natural\nlanguage processing, and game playing. ANNs learn in a similar way to other machine\nlearning algorithms: by using training data. They are best suited to unstructured data\nwhere it\u2019s difficult to understand how features relate to one another. This chapter covers\nthe inspiration of ANNs; it also shows how the algorithm works and how ANNs are\ndesigned to solve different problems.\n\nTo gain a clear understanding of how ANNs fit into the bigger machine learning land-\nscape, we should review the composition and categorization of machine learning\nalgorithms. Deep learning is the name given to algorithms that use ANNs in varying\narchitectures to accomplish an objective. Deep learning, including ANNs, can be used\nto solve supervised learning, unsupervised learning, and reinforcement learning prob-\nlems. Figure 9.1 shows how deep learning relates to ANNs and other machine learn-\ning concepts.\n\nSupervised\nlearning\n\nUnsupervised\nlearning\n\nMachine\nlearning\n\nCY o Artificial Neural Networks (ANN)\n* ~ oF Cenvelutional Heural Networks (CH)\n=e Generative Adversarial Neural Networks (GAN)\n\nRecurrent Neural Netwerks (RNN)\n\nFigure 9.1 A map describing the flexibility of deep learning and ANNs\n\nANNs can be seen as just another model in the machine learning life cycle (chapter 8).\nFigure 9.2 recaps that life cycle. A problem needs to be identified; that data needs to\nbe collected, understood, and prepared; and the ANN model will be tested and improved\nif necessary.\n\n1\n\nCollect and\nunderstand\ndata.\n\nPrepare Traina Test the Impreve\ndata. model. acouracy.\n\nFigure 9.2 A workflow for machine learning experiments and projects\n\nNow that we have an idea of how ANNs fit into the abstract machine learning landscape\nand know that an ANN is another model that is trained in the life cycle, let\u2019s explore the\nintuition and workings of ANNs. Like genetic algorithms and swarm-intelligence algo-\nrithms, ANNs are inspired by natural phenomena\u2014in this case, the brain and nervous\nsystem. The nervous system is a biological structure that allows us to feel sensations and\nis the basis of how our brains operate. We have nerves across our entire bodies and neu-\nrons that behave similarly in our brains.\n\nNeural networks consist of interconnected neurons that pass information by using\nelectrical and chemical signals. Neurons pass information to other neurons and adjust\ninformation to accomplish a specific function. When you grab a cup and take a sip of\nwater, millions of neurons process the intention of what you want to do, the physical\naction to accomplish it, and the feedback to determine whether you were successful.\nThink about little children learning to drink from a cup. They usually start out poorly,\ndropping the cup a lot. Then they learn to grab it with two hands. Gradually, they learn\nto grab the cup with a single hand and take a sip without any problems. This process\ntakes months. What\u2019s happening is that their brains and nervous systems are learning\nthrough practice or training. Figure 9.3 depicts a simplified model of receiving inputs\n(stimuli), processing them in a neural network, and providing outputs (response).\n\nResponse\n\n[7] Effecters\n\nStimuli Recepters\n\nFigure 9.3 A simplified model of a biological neural system\n\nSimplified, a neuron (figure 9.4) consists of dendrites that receive signals from other\nneurons; a cell body and a nucleus that activates and adjusts the signal; an axon that\npasses the signal to other neurons; and synapses that carry, and in the process adjust,\nthe signal before it is passed to the next neuron\u2019s dendrites. Through approximately\n90 billion neurons working together, our brains can function at the high level of intelli-\ngence that we know.\n\nDendrites\n\nCell bedy\n\nFigure 9.4 The general composition of neurons\n\nAlthough ANNs are inspired by biological neural networks and use many of the con-\ncepts that are observed in these systems, ANNs are not identical representations of bio-\nlogical neural systems. We still have a lot to learn about the brain and nervous system.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.2,
                        "section_name": "The Perceptron: A representation of a neuron",
                        "section_path": "./screenshots-images-2/chapter_9/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_2/f6e48d60-82c1-4fab-a763-2bee0b21ad1d.png",
                            "./screenshots-images-2/chapter_9/section_2/32082420-2263-4fa3-89a4-eb3860012c16.png",
                            "./screenshots-images-2/chapter_9/section_2/c083ed81-cfed-47bb-97db-13a91cf13225.png",
                            "./screenshots-images-2/chapter_9/section_2/8452513a-058c-4461-911f-53b7ebb1c0ca.png",
                            "./screenshots-images-2/chapter_9/section_2/bb9064a1-3aec-407e-885a-ac340bc4ef62.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Perceptron: A representation of a neuron\n\nThe neuron is the fundamental concept that makes up the brain and nervous system.\nAs mentioned earlier, it accepts many inputs from other neurons, processes those\ninputs, and transfers the result to other connected neurons. ANNs are based on the\nfundamental concept of the Perceptron\u2014a logical representation of a single biological\nneuron.\n\nLike neurons, the Perceptron receives inputs (like dendrites), alters these inputs by\nusing weights (like synapses), processes the weighted inputs (like the cell body and\nnucleus), and outputs a result (like axons). The Perceptron is loosely based on a neuron.\nYou may notice that the synapses are depicted after the dendrites, representing the influ-\nence of synapses on incoming inputs. Figure 9.5 depicts the logical architecture of the\nPerceptron.\n\nInputs Weights Hidden nede Output\nDendrites\n\nSynapses\nAxon\n\nsum function Activation function\nCell bedy Nucleus\n\nFigure 9.5 Logical architecture of the Perceptron\n\nThe components of the Perceptron are described by variables that are useful in calculat-\ning the output. Weights modify the inputs; that value is processed by a hidden node; and\nfinally, the result is provided as the output.\n\nHere is a brief description of the components of the Perceptron:\n\n+ Inputs\u2014Describe the input values. In a neuron, these values would be an\ninput signal.\n\n+ Weights\u2014Describe the weights on each connection between an input and the\nhidden node. Weights influence the intensity of an input and result in a weighted\ninput. In a neuron, these connections would be the synapses.\n\n+ Hidden node (sum and activation) \u2014Sums the weighted input values and then\napplies an activation function to the summed result. An activation function\ndetermines the activation/output of the hidden node/neuron.\n\n* Output\u2014Describes the final output of the Perceptron.\n\nTo understand the workings of the Perceptron, we will examine the use of one by revis-\niting the apartment-hunting example from chapter 8. Suppose that we are real estate\nagents trying to determine whether a specific apartment will be rented within a month,\nbased on the size of the apartment and the price of the apartment. Assume that a\nPerceptron has already been trained, meaning that the weights for the Perceptron have\nalready been adjusted. We explore the way Perceptions and ANNs are trained later in\nthis chapter; for now, understand that the weights encode relationships among the inputs\nby adjusting the strength of inputs.\n\nFigure 9.6 shows how we can use a pretrained Perceptron to classify whether an apart-\nment will be rented. The inputs represent the price of a specific apartment and the size\nof that apartment. We're also using the maximum price and size to scale the inputs\n($8,000 for maximum price and 80 square meters for maximum size). For more about\nscaling data, see the next section.\n\nInputs Weights Hidden node Output\n\nMultiply inputs\n\nia and weights\n\nApply activation\nfunction\n\nPrice\n\nMax price\n\n29.3%\n\nsize chance ef being rented\n\nMax size\n\n= 0.312 \u2018Sum weighted\nInputs\n\nFigure 9.6 An example of using a trained Perceptron\n\nNotice that the price and size are the inputs and that the predicted chance of the apart-\nment being rented is the output. The weights are key to achieving the prediction. Weights\nare the variables in the network that learn relationships among inputs. The summation\nand activation functions are used to process the inputs multiplied by the weights to\nmake a prediction.\n\nNotice that we're using an activation function called the sigmoid function. Activation\nfunctions play a critical role in the Perceptron and ANNs. In this case, the activation\nfunction is helping us solve a linear problem. But when we look at ANNs in the next\nsection, we will see how activation functions are useful for receiving inputs to solve non-\nlinear problems. Figure 9.7 describes the basics of linear problems.\n\nThe sigmoid function results in an S curve between 0 and 1, given inputs between 0\nand 1. Because the sigmoid function allows changes in x to result in small changes in y,\nit allows for gradual learning. When we get to the deeper workings of ANNs later in this\nchapter, we will see how this function helps solve nonlinear problems as well.\n\n1 x = 0.25\n\n} y = 0.562\n\nLi} _|\n\n-1 e 1\n\nFigure 9.7 The sigmoid function\n\nLet\u2019s take a step back and look at the data that we\u2019re using for the Perceptron.\nUnderstanding the data related to whether an apartment was sold is important for\nunderstanding what the Perceptron is doing. Figure 9.8 illustrates the examples in the\ndataset, including the price and size of each apartment. Each apartment is labeled as one\nof two classes: rented or not rented. The line separating the two classes is the function\ndescribed by the Perceptron.\n\nva\n\nX Wot rented\nDiscriminate Linear problem:\non the x-axis classified bya\n= ge and the y-axis. ra straight line.\n\u00a3 =\n> >\nt t\ns 6 8\n@ eo\n$ $\n& &\n. + *\n6 cS\nE q\ne 7 Fo\n2,000 4,000 6,000 8,000\nPrice per menth ($) Price per month ($)\n\nFigure 9.8 Example of a linear classification problem\n\nAlthough the Perceptron is useful for solving linear problems, it cannot solve nonlinear\nproblems. If a dataset cannot be classified by a straight line, the Perceptron will fail.\n\nANNs use the concept of the Perceptron at scale. Many neurons similar to the\nPerceptron work together to solve nonlinear problems in many dimensions. Note that\nthe activation function used influences the learning capabilities of the ANN.\n\nEXERCISE: CALCULATE THE OUTPUT OF THE FOLLOWING INPUT FOR THE PERCEPTRON\n\nUsing your knowledge of how the Perceptron works, calculate the output for\nthe following:\n\n\nSOLUTION: CALCULATE THE OUTPUT OF THE FOLLOWING INPUT FOR THE PERCEPTRON\n\nMultiply inputs\n\nia and weights.\n\n0.313 * -3\n= -0.939\n\nApply activation\nfunction.\n\nsigmoid\n(\u00ae.936)\n= 0.718\n\nF1.8%\nchance of being rented\n\n0.375 *5\n= 1.875\n= 0.375 Sum weighted\ninputs.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.3,
                        "section_name": "Defining artificial neural networks",
                        "section_path": "./screenshots-images-2/chapter_9/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_3/9af9db6a-b6c1-4cda-a8b8-45a7b02f1cc8.png",
                            "./screenshots-images-2/chapter_9/section_3/71b0e552-3ab4-46c4-a096-36a4e304690d.png",
                            "./screenshots-images-2/chapter_9/section_3/3b2a5a91-c4e7-4b9d-ba3e-b039c12ba8a0.png",
                            "./screenshots-images-2/chapter_9/section_3/25e80789-9d0a-425b-9407-8fdbbd561c0e.png",
                            "./screenshots-images-2/chapter_9/section_3/b7b10771-d0ed-4bd9-aa0b-84f8a0a303a4.png",
                            "./screenshots-images-2/chapter_9/section_3/1c6df820-1250-4dad-932a-aef11858046f.png",
                            "./screenshots-images-2/chapter_9/section_3/30a64626-2947-48b0-a64f-533c4318f04f.png",
                            "./screenshots-images-2/chapter_9/section_3/94d1543f-4b00-4635-b874-45b04d61b2be.png",
                            "./screenshots-images-2/chapter_9/section_3/f4dbfcdb-8c09-4cc5-92ff-e17018cc6703.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Defining artificial neural networks\n\nThe Perceptron is useful for solving simple problems, but as the dimensions of the data\nincreases, it becomes less feasible. ANNs use the principles of the Perceptron and apply\nthem to many hidden nodes as opposed to a single one.\n\nTo explore the workings of multi-node ANNs, consider an example dataset related to\ncar collisions. Suppose that we have data from several cars at the moment that an unfore-\nseen object enters the path of their movement. The dataset contains features related to\nthe conditions and whether a collision occurred, including the following:\n\nSpeed\u2014The speed at which the car was traveling before encountering the object\n\nTerrain quality\u2014The quality of the road on which the car was traveling before\nencountering the object\n\nDegree of vision\u2014The driver\u2019s degree of vision before the car encountered\nthe object\n\nTotal experience\u2014The total driving experience of the driver of the car\n\nCollision occurred?\u2014Whether a collision occurred or not\n\nGiven this data, we want to train a machine learning model\u2014namely, an ANN\u2014to\nlearn the relationship between the features that contribute to a collision, as shown in\ntable 9.1.\n\nTable 9.1 Car collision dataset\n\nTotal Collision\nexperience | occurred?\n80,000 km\n\n120 km/h 110,000 km\n8 km/h 50,000 km\n50 km/h 1,600 km\n\nAn example ANN architecture can be used to classify whether a collision will occur\nbased on the features we have. The features in the dataset must be mapped as inputs to\nthe ANN, and the class that we are trying to predict is mapped as the output of the ANN.\nIn this example, the input nodes are speed, terrain quality, degree of vision, and total\nexperience; the output node is whether a collision happened (figure 9.9).\n\nInputs Weights Hidden nedes Output\n\nTerrain\nquality\nDegree\nof vision Oy) C) ,\n\nTotal\nexperience\n\nC) & Collision?\n\nFigure 9.9 Example ANN architecture for the car-collision example\n\nAs with the other machine learning algorithms that we've worked through, preparing\ndata is important for making an ANN classify data successfully. The primary concern is\nrepresenting data in comparable ways. As humans, we understand the concept of speed\nand degree of vision, but the ANN doesn\u2019t have this context. Directly comparing\n\n65 km/h and 36-degree vision doesn\u2019t make sense for the ANN, but comparing the ratio\nof speed with the degree of vision is useful. To accomplish this task, we need to scale\nour data.\n\nA common way to scale data so that it can be compared is to use the min-max scaling\napproach, which aims to scale data to values between 0 and 1. By scaling all the data in a\ndataset to be consistent in format, we make the different features comparable. Because\nANNs do not have any context about the raw features, we also remove bias with large\ninput values. As an example, 1,000 seems to be much larger than 65, but 1,000 in the\ncontext of total driving experience is poor, and 65 in the context of driving speed is sig-\nnificant. Min-max scaling represents these pieces of data with the correct context by\ntaking into account the minimum and maximum possible values for each feature.\n\nHere are the minimum and maximum values selected for the features in the car-\ncollision data:\n\n+ Speed\u2014The minimum speed is 0, which means that the car is not moving. We\nwill use the maximum speed of 120, because 120 km/h is the maximum legal\nspeed limit in most places around the world. We will assume that the driver\nfollows the rules.\n\n+ Terrain quality\u2014Because the data is already in a rating system, the minimum\nvalue is 0, and the maximum value is 10.\n\n+ Degree of vision\u2014We know that the total field of view in degrees is 360. So the\nminimum value is 0, and the maximum value is 360.\n\n+ Total experience\u2014The minimum value is 0 if the driver has no experience. We\nwill subjectively make the maximum value 400,000 for driving experience. The\nrationale is that if a driver has 400,000 km of driving experience, we consider\nthat driver to be highly competent, and any further experience doesn\u2019t matter.\n\nMin-max scaling uses the minimum and maximum values for a feature and finds\nthe percentage of the actual value for the feature. The formula is simple: subtract the\nminimum from the value, and divide the result by the minimum subtracted from the\nmaximum. Figure 9.10 illustrates the min-max scaling calculation for the first row of\ndata in the car-collision example:\n\nTerrain Degree of Total Collision\nSpeed quality vision experience | occurred?\n\n\nTerrain Degree Total\nSpeed quality of vision experience\na) nN \u00ae Q\u00b0\n65 km/h 5/10 180\u00b0 80,000\nMin: @ Min: @ Min:@ Min: \u00ae\nMax: 120 Max: 10 Max: 360 Max: 400,000\nvalue - min 65-0 5-0 180-0 80 -0\nmax ~-min 120-@ 1\u00a2-0 360-0 400000-0\nScaled value 0.542 0.5 @.5 0.2\n\nFigure 9.10 Min-max scaling example with car collision data\n\nNotice that all the values are between 0 and 1 and can be compared equally. The same\nformula is applied to all the rows in the dataset to ensure that every value is scaled. Note\nthat for the value for the \u201cCollision occurred?\u201d feature, Yes is replaced with 1, and No is\nreplaced with 0. Table 9.2 depicts the scaled car-collision data.\n\nTable 9.2 Car collision dataset scaled\n\nTerrain Degree of Total Collision\n= quality vision \u2014 occurred?\n\nPseudocode\n\nThe code for scaling the data follows the logic and calculations for min-max scaling\nidentically. We need the minimums and maximums for each feature, as well as the total\nnumber of features in our dataset. The scale_dataset function uses these parame-\nters to iterate over every example in the dataset and scale the value by using the scale _\ndata_feature function:\n\nFEATURE_MIN = (0, 0, 0, 0]\nFEATURE_MAX = (120, 10, 360, 400000)\n\nFEATURE_COUNT = 4\n\nscale_dataset (dataset, feature_count, feature_min, feature_max):\nlet scaled_data equal empty array\nfor data in dataset:\nlet example equal empty array\nfor i in range(0, feature count):\nappend scale_data_feature(data[i], feature_min[i], feature_max(i])\nto example\nappend example to scaled data\n\nreturn scaled data\n\nscale _data_feature(data, feature_min, feature_max):\n\nreturn (data ~ feature_min) / (feature_max - feature_min)\n\nNow that we have prepared the data in a way that is suitable for an ANN to process, let\u2019s\nexplore the architecture of a simple ANN. Remember that the features used to predict a\nclass are the input nodes, and the class that is being predicted is the output node.\n\nFigure 9.11 shows an ANN with one hidden layer, which is the single vertical layer in\nthe figure, with five hidden nodes. These layers are called hidden layers because they are\nnot directly observed from outside the network. Only the inputs and outputs are inter-\nacted with, which leads to the perception of ANNs as being black boxes. Each hidden\nnode is similar to the Perceptron. A hidden node takes inputs and weights and then\ncomputes the sum and an activation function. Then the results of each hidden node are\nprocessed by a single output node.\n\nInputs Weights Hidden nedes Output\n\nwt (2) |\nTerrain |\nquality 1\nDegree !\nof vision DH) |\nTotal 9 |\nexperience i?) wy)\n\n@ & Collision?\n\nFigure 9.11 Example ANN architecture for the car-collision problem\n\nBefore we consider the calculations and computation of an ANN, let\u2019s try to dig intui-\ntively into what the network weights are doing at a high level. Because a single hidden\nnode is connected to every input node but every connection has a different weight, inde-\npendent hidden nodes might be concerned with specific relationships among two or\nmore input nodes.\n\nFigure 9.12 depicts a scenario in which the first hidden node has strong weightings\non the connections to terrain quality and degree of vision but weak weightings on the\nconnections to speed and total experience. This specific hidden node is concerned with\nthe relationship between terrain quality and degree of vision. It might gain an under-\nstanding of the relationship between these two features and how it influences whether\ncollisions happen; poor terrain quality and poor degree of vision, for example, might\ninfluence the likelihood of collisions more than good terrain quality and an average\ndegree of vision. These relationships are usually more intricate than this simple\nexample.\n\nInputs Weights Hidden nedes Output\n\nSpeed\n\nO\n\nTerrain\nquality\n\nDegree\nof vision Oy)\nTotal 9\nexperience gd 7\n\nrave)\nrN\n& Collision?\n\nFigure 9.12 Example of a hidden node comparing terrain quality and degree of vision\n\nIn figure 9.13, the second hidden node might have strong weightings on the connections\nto terrain quality and total experience. Perhaps there is a relationship among different\nterrain qualities and variance in total driving experience that contributes to collisions.\n\nInputs Weights Hidden nedes Output\n\nae)\nTerrain\nquality\nDegree\nof vision Oy @)\n\nTotal\nexperience\n\n& Collision?\n\nFigure 9.13 Example of a hidden node comparing terrain quality and total experience\n\nThe nodes in a hidden layer can be conceptually compared with the analogy of ants dis-\ncussed in chapter 6. Individual ants fulfill small tasks that are seemingly insignificant,\nbut when the ants act as a colony, intelligent behavior emerges. Similarly, individual\nhidden nodes contribute to a greater goal in the ANN.\n\nBy analyzing the figure of the car-collision ANN and the operations within it, we can\ndescribe the data structures required for the algorithm:\n\nInput nodes\u2014The input nodes can be represented by a single array that stores the\nvalues for a specific example. The array size is the number of features in the\ndataset that are being used to predict a class. In the car-collision example, we\nhave four inputs, so the array size is 4.\n\nWeights\u2014The weights can be represented by a matrix (a 2D array), because each\ninput node has a connection to each hidden node and each input node has five\nconnections. Because there are 4 input nodes with 5 connections each, the ANN\nhas 20 weights toward the hidden layer and 5 toward the output layer, because\nthere are 5 hidden nodes and 1 output node.\n\nHidden nodes\u2014The hidden nodes can be represented by a single array that stores\nthe results of activation of each respective node.\n\nOutput node\u2014The output node is a single value representing the predicted class\nof a specific example or the chance that the example will be in a specific class.\nThe output might be 1 or 0, indicating whether a collision occurred; or it could\nbe something like 0.65, indicating a 65% chance that the example resulted in a\ncollision.\n\nThe next piece of pseudocode describes a class that represents a neural network. Notice\nthat the layers are represented as properties of the class and that all the properties are\narrays, with the exception of the weights, which are matrices. An output property rep-\nresents the predictions for the given examples, and an expected_output property is\nused during the training process:\n\nNeuralNetwork (features, labe\n\nhidden_node_count):\n\nlet input equal features\n\nlet weights_input equal a random matrix, size: features * hidden_node_count\nlet hidden equal zero array, size: hidden_node_count\n\nlet weights_hidden equal a random matrix, size: hidden_node_count\n\nlet expected_output equal labels\n\nlet output equal zero array,size: length of labels\n\nlet nn equal NeuralNetwork (scaled_feature_data,\nscaled_label_data,\n\nhidden_node_count)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.4,
                        "section_name": "Forward propagation: Using a trained ANN",
                        "section_path": "./screenshots-images-2/chapter_9/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_4/55fd3410-797a-4f85-9fd7-539d43b8c35c.png",
                            "./screenshots-images-2/chapter_9/section_4/ba3f61d3-b8d3-47a5-b55e-a08a8d7d8adf.png",
                            "./screenshots-images-2/chapter_9/section_4/7a9b4f49-d134-4677-ae8a-528b75a7b7cb.png",
                            "./screenshots-images-2/chapter_9/section_4/9b30aa85-ec54-4aef-a6c8-97e0c83f8c58.png",
                            "./screenshots-images-2/chapter_9/section_4/6ccec3a9-ddb2-4d59-8837-b17f8de2a88c.png",
                            "./screenshots-images-2/chapter_9/section_4/67e00ef2-fc6e-475b-834e-a24199754772.png",
                            "./screenshots-images-2/chapter_9/section_4/b9610e12-ef81-4423-885b-f62390a30e4f.png",
                            "./screenshots-images-2/chapter_9/section_4/efeb0b0a-af38-479c-9989-3945c53f4cc2.png",
                            "./screenshots-images-2/chapter_9/section_4/5a2bfd5a-4096-4652-8d59-e10ae86db8f5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Forward propagation: Using a trained ANN\n\nA trained ANN is a network that has learned from examples and adjusted its weights to\nbest predict the class of new examples. Don\u2019t panic about how the training happens and\nhow the weights are adjusted; we will tackle this topic in the next section. Understanding\nforward propagation will assist us in grasping backpropagation (how weights are trained).\nNow that we have a grounding in the general architecture of ANNs and the intuition\nof what nodes in the network might be doing, let\u2019s walk through the algorithm for using\na trained ANN (figure 9.14).\n1 2 3 4\n\nSum results\nof weighted\ninputs for\neach hidden\n\nnode.\n\nActivation\nfunction for\neach hidden\nnede\n\nMultiply\ninputs and\nweights.\n\nInput an\nexample.\n\nActivatien\nfunction for\noutput node\n\neach hidden\nnode.\n\nFigure 9.14 Life cycle of forward propagation in an ANN\n\nAs mentioned previously, the steps involved in calculating the results for the nodes in an\nANN are similar to the Perceptron. Similar operations are performed on many nodes\n\nthat\nthat\n\nwork together; this addresses the Perceptron\u2019s flaws and is used to solve problems\nhave more dimensions. The general flow of forward propagation includes the fol-\n\nlowing steps:\n\n1.\n\nInput an example\u2014Provide a single example from the dataset for which we want\nto predict the class.\n\n. Multiply inputs and weights\u2014Multiply every input by each weight of its\n\nconnection to hidden nodes.\n\n. Sum results of weighted inputs for each hidden node\u2014Sum the results of the\n\nweighted inputs.\n\n. Activation function for each hidden node\u2014Apply an activation function to the\n\nsummed weighted inputs.\n\n. Sum results of weighted outputs of hidden nodes to the output node\u2014Sum the\n\nweighted results of the activation function from all hidden nodes.\n\n. Activation function for output node\u2014Apply an activation function to the summed\n\nweighted hidden nodes.\n\nFor the purpose of exploring forward propagation, we will assume that the ANN has\nbeen trained and the optimal weights in the network have been found. Figure 9.15 depicts\nthe weights on each connection. The first box next to the first hidden node, for example,\nhas the weight 3.35, which is related to the Speed input node; the weight -5.82 is related\nto the Terrain Quality input node; and so on.\n\nTotal J\nexperience c\u00b0] 7\n\n&)\nTerrain\nquality\n\nDegree\nof vision\n\nInputs Weights Hidden nedes Output\n\nSpeed\n\n645)\n\n-6.96\n6. ap) & oats\u201d\n\na.\n6.10\n\na)\n\nFigure 9.15 Example of weights in a pretrained ANN\n\nBecause the neural network has been trained, we can use it to predict the chance of col-\nlisions by providing it with a single example. Table 9.3 serves as a reminder of the scaled\ndataset that we are using.\n\nTable 9.3 Car collision dataset scaled\n\nTerrain Degree of Total Collision\noccurred?\n\nIf you\u2019ve ever looked into ANNs, you may have noticed some potentially frightening\nmathematical notations. Let\u2019s break down some of the concepts that can be represented\nmathematically.\n\nThe inputs of the ANN are denoted by X. Every input variable will be X subscripted\nby a number. Speed is X,, Terrain Quality is X,, and so on. The output of the network is\ndenoted by y, and the weights of the network are denoted by W. Because we have two\nlayers in the ANN\u2014a hidden layer and an output layer\u2014there are two groups of weights.\nThe first group is superscripted by W,, and the second group is W,. Then each weight is\ndenoted by the nodes to which it is connected. The weight between the Speed node and\nthe first hidden node is W,,,,, and the weight between the Terrain Quality node and the\nfirst hidden node is W,, ,. These denotations aren\u2019t necessarily important for this exam-\nple, but understanding them now will support future learning.\n\nFigure 9.16 shows how the following data is represented in an ANN:\n\nTerrain Degree of Total Collision\nSpeed quality vision experience | occurred?\n\n1 0.542 0.5 0.5 0.200 0\n\n\nInputs Weights Hidden nedes Output\n\nSpeed\n\nTerrain\nquality\n\ny & Collision?\n\nDegree\nof vision\n\nTotal\nexperience\n\nFigure 9.16 Mathematical denotation of an ANN\n\nAs with the Perceptron, the first step is calculating the weighted sum of the inputs and\nthe weight of each hidden node. In figure 9.17, each input is multiplied by each weight\nand summed for every hidden node.\n\nInputs Weights Hidden nedes Output\n\nTerrain\nquality \u00a30542 * 2.43)\n\n(5 *-4.35) a\nZ| 7.\" Collision?\nane\n\nDegree\nof vision\n\nTotal\nexperience\n\nFigure 9.17 Weighted sum calculation for each hidden node\n\nThe next step is calculating the activation of each hidden node. We are using the sigmoid\nfunction, and the input for the function is the weighted sum of the inputs calculated for\neach hidden node (figure 9.18).\n\nInputs Weights Hidden nedes Output\n\n715393)\n\nSpeed\n\nTerrain\nquality\n\ny & Collision?\n\nDegree\nof vision\n\nTotal\nexperience\n\nFigure 9.18 Activation function calculation for each hidden node\n\nNow we have the activation results for each hidden node. When we mirror this result\nback to neurons, the activation results represent the activation intensity of each neuron.\nBecause different hidden nodes may be concerned with different relationships in the\ndata through the weights, the activations can be used in conjunction to determine an\noverall activation that represents the chance of a collision, given the inputs.\n\nFigure 9.19 depicts the activations for each hidden node and the weights from each\nhidden node to the output node. To calculate the final output, we repeat the process of\ncalculating the weighted sum of the results from each hidden node and applying the\nsigmoid activation function to that result.\n\nNOTE The sigma symbol (Z) in the hidden nodes depicts the sum\noperation.\n\nInputs Weights Hidden nedes Output\n\nE7664 + 8.45)\n+ 6a )\n\nSpeed rape) \u00b0\nTerrain x\nquality 1\nDegree\nof vision Ove (+=) DS\n\nTotal 9\nexperience i?) 7 %3\n\n0.214% chance\nof collision\n\nFigure 9.19 Final activation calculation for the output node\n\nWe have calculated the output prediction for our example. The result is 0.00214, but\nwhat does this number mean? The output is a value between 0 and 1 that represents the\nprobability that a collision will occur. In this case, the output is 0.214 percent (0.00214 x\n100), indicating that the chance of a collision is almost 0.\n\nThe following exercise uses another example from the dataset.\n\nEXERCISE: CALCULATE THE PREDICTION FOR THE EXAMPLE BY USING FORWARD\nPROPAGATION WITH THE FOLLOWING ANN\n\nTerrain Degree of Total Collision\nSpeed quality vision experience | occurred?\n\n\nInputs Weights Hidden nedes Output\n\nSpeed\n\nTerrain\nquality\n\ny & Collision?\n\nDegree\nof vision\n\nTotal\nexperience\n\nSOLUTION: CALCULATE THE PREDICTION FOR THE EXAMPLE BY USING FORWARD\nPROPAGATION WITH THE FOLLOWING ANN\n\nInputs Weights Hidden nedes Output\n\nfae \u00a9 3.35)\n+ (04\n\nTerrain\nquality\n\n=riv)y & Collision?\n\nDegree\nof vision\n\nTotal\nexperience\n\n\nInputs Weights Hidden nedes Output\n\nspeed x, (:+)\nTerrain *\nquality rN a S LJ ZS gp eo\nDegree SH\nSS 5\n\u00e9|) F-13005)\n\n770178)\n\n= 0.06536\n\nTotal\nexperience\n\n\u00a2\n= 0.22129\n\nWhen we run this example through our pretrained ANN, the output is 0.99996, or\n99.996 percent, so there is an extremely high chance that a collision will occur. By apply-\ning some human intuition to this single example, we can see why a collision is likely. The\ndriver was traveling at the maximum legal speed, on the poorest-quality terrain, with a\npoor field of vision.\n\nPseudocode\n\nOne of the important functions for activation in our example is the sigmoid function.\nThis method describes the mathematical function that represents the S curve:\n\nsigmoid (x): Exp is a mathematical constant called\n\nreturn 1 / (1+ exp(-x)) <_\u2014 tuler\u2019s number, approximately a.718a8.\n\nNotice that the same neural network class defined earlier in the chapter is described in\nthe following code. This time, a forward_propagation function is included. This\nfunction sums the input and weights between input and hidden nodes, applies the sig-\nmoid function to each result, and stores the output as the result for the nodes in the\nhidden layer. This is done for the hidden node output and weights to the output node\nas well:\n\nNeuralNetwork (features, labels, hidden_node_count):\nlet input equal features\nlet weights_input equal a random matrix, size: features * hidden_node_count\nlet hidden equal zero array, size: hidden_node_count\nlet weights_hidden equal a random matrix, size: hidden_node_count\nlet expected_output equal labels\n\nlet output equal zero array ,size: length of labels\n\nThe symbol \u00ab implies\n\nforward_propagation(): matric multiplication.\nlet hidden_weighted_sum equal input - weights_input )\nlet hidden equal sigmoid (hidden_weighted_sum)\nlet output_weighted_sum equal hidden +weights_hidden\n\nlet output equal sigmoid (output_weighted_sum)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.5,
                        "section_name": "Backpropagation: Training an ANN",
                        "section_path": "./screenshots-images-2/chapter_9/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_5/5684dbd7-cc65-42ba-a4d9-6eb7067e9ee0.png",
                            "./screenshots-images-2/chapter_9/section_5/9f369c85-f551-4014-978a-d3a0235623c2.png",
                            "./screenshots-images-2/chapter_9/section_5/325deea2-e6b3-452c-9105-25c851cb2c18.png",
                            "./screenshots-images-2/chapter_9/section_5/dea15b5c-6f97-488e-9dbc-ec873e52437e.png",
                            "./screenshots-images-2/chapter_9/section_5/e0deefac-ff12-445c-af8c-899dc2576065.png",
                            "./screenshots-images-2/chapter_9/section_5/afbdb9f4-06e5-404f-9986-e8baaa04f8a1.png",
                            "./screenshots-images-2/chapter_9/section_5/f4b43f68-66e6-4116-a6f6-de52f3885bcf.png",
                            "./screenshots-images-2/chapter_9/section_5/7131ed34-8f30-463d-8b26-06a7dea4201b.png",
                            "./screenshots-images-2/chapter_9/section_5/e41da665-09b0-4993-83cc-90c3249d1774.png",
                            "./screenshots-images-2/chapter_9/section_5/905fee1d-457c-4521-93a5-1ef113c8f77d.png",
                            "./screenshots-images-2/chapter_9/section_5/144a2609-21fb-434a-b12a-7b112248d8c4.png",
                            "./screenshots-images-2/chapter_9/section_5/1acba68a-6c8e-4ace-8b17-9912c5c13c06.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Backpropagation: Training an ANN\n\nUnderstanding how forward propagation works is useful for understanding how ANNs\nare trained, because forward propagation is used in the training process. The machine\nlearning life cycle and principles covered in chapter 8 are important for tackling back-\npropagation in ANNs. An ANN can be seen as another machine learning model. We still\nneed to have a question to ask. We're still collecting and understanding data in the con-\ntext of the problem, and we need to prepare the data in a way that is suitable for the\nmodel to process.\n\nWe need a subset of data for training and a subset of data for testing how well the\nmodel performs. Also, we will be iterating and improving through collecting more data,\npreparing it differently, or changing the architecture and configuration of the ANN.\n\nTraining an ANN consists of three main phases. Phase A involves setting up the ANN\narchitecture, including configuring the inputs, outputs, and hidden layers. Phase B is\nforward propagation. And phase C is backpropagation, which is where the training hap-\npens (figure 9.20).\n\na\n2 Define Initialize\n8 ANN ANN\n: larchitecture weights.\n<\nSum results Activation\nVv\nInput an Multiply of weighted function fer\ne example. inputs and Inputs for h hid.\ns weights. each hidden eaol on\n> node. node\ns\n>\ns\na\n\u00b0\ni\na\n\u00b0\nc\ns\n=\nie\ns\na\noO Sum results Activation\nof hidden functien for\nnodes. output node\n2\n&\ns Calculate Update Reached\nfed cost for weights stepping 3\ne example. in the ANN. condition\n()\n\nFigure 9.20 Life cycle of training an ANN\n\nPhase A, Phase B, and Phase C describe the phases and operations involved in the back-\npropagation algorithm.\n\nPhase A: Setup\n\n1. Define ANN architecture. This step involves defining the input nodes, the output\nnodes, the number of hidden layers, the number of neurons in each hidden layer,\nthe activation functions used, and more.\n\n2. Initialize ANN weights. The weights in the ANN must be initialized to some value.\nWe can take various approaches. The key principle is that the weights will be\nadjusted constantly as the ANN learns from training examples.\n\nPhase B: Forward propagation\n\nThis process is the same one that we covered in Phase A. The same calculations are car-\nried out. The predicted output, however, will be compared with the actual class for each\nexample in the training set to train the network.\n\nPhase C: Training\n\n1. Calculate cost. Following from forward propagation, the cost is the difference\nbetween the predicted output and the actual class for the examples in the training\nset. The cost effectively determines how bad the ANN is at predicting the class of\nexamples.\n\n2. Update weights in the ANN. The weights of the ANN are the only things that can\nbe adjusted by the network itself. The architecture and configurations that we\ndefined in phase A don't change during training the network. The weights\nessentially encode the intelligence of the network. Weights are adjusted to be\nlarger or smaller, affecting the strength of the inputs.\n\n3. Define a stopping condition. Training cannot happen indefinitely. As with many of\nthe algorithms explored in this book, a sensible stopping condition needs to be\ndetermined. If we have a large dataset, we might decide that we will use 500\nexamples in our training dataset over 1,000 iterations to train the ANN. In this\nexample, the 500 examples will be passed through the network 1,000 times, and\nthe weights will be adjusted in every iteration.\n\nWhen we worked through forward propagation, the weights were already defined\nbecause the network was pretrained. Before we start training the network, we need to\ninitialize the weights to some value, and the weights need to be adjusted based on train-\ning examples. One approach to initializing weights is to choose random weights from a\nnormal distribution.\n\nFigure 9.21 illustrates the randomly generated weights for our ANN. It also shows the\ncalculations for forward propagation for the hidden nodes, given a single training exam-\nple. The first example input used in the forward propagation section is used here to\nhighlight the differences in output, given different weights in the network.\n\nInputs Weights Hidden nedes Output\n\nEwsaa* 8.86)\n\nSpeed\n\nTerrain\nquality\n\n& Collision?\n\nDegree\nof vision\n\nean)\n0:36)\ne77)\n0:63)\nTotal\n\nexperience\n\nFigure 9.21 Example initial weights for an ANN\n\nThe next step is forward propagation (figure 9.22). The key change is checking the differ-\nence between the obtained prediction and the actual class.\n\nInputs Weights Hidden nedes Output\n\u00e9) 7 asta\n= 0.74059\nspeed (> xo oS /EeX\nTerrain mm % (+*) Lf\nquality S\n<S7\n\nDegree E\nefvcion <OD* (+*) ZW Predicted)\n\n= 0.84276\n\nActual(y)\n-08\n\nTotal\nexperience\n\n7 (1.32998)\n= 8.79006\n\nFigure 9.22 Example of forward propagation with randomly initialized weights\n\nBy comparing the predicted result with the actual class, we can calculate a cost. The cost\nfunction that we will use is simple: subtract the predicted output from the actual output.\nIn this example, 0.84274 is subtracted from 0.0, and the cost is -0.84274. This result indi-\ncates how incorrect the prediction was and can be used to adjust the weights in the ANN.\nWeights in the ANN are adjusted slightly every time a cost is calculated. This happens\nthousands of times using training data to determine the optimal weights for the ANN to\nmake accurate predictions. Note that training too long on the same set of data can lead\nto overfitting, described in chapter 8.\n\nHere is where some potentially unfamiliar math comes into play: the Chain Rule.\nBefore we use the Chain Rule, let\u2019s gain some intuition about what the weights mean and\nhow adjusting them improves the ANN\u2019s performance.\n\nIf we plot possible weights against their respective cost on a graph, we find some\nfunction that represents the possible weights. Some points on the function yield a lower\ncost, and other points yield a higher cost. We are seeking points that minimize cost\n(figure 9.23).\n\nCost\nCost\n\nWeight Weight\n\nFigure 9.23 Weight versus cost plotted\n\nA useful tool from the field of calculus, called gradient descent, can help us move the\nweight closer to the minimum value by finding the derivative. The derivative is import-\nant because it measures the sensitivity to change for that function. For example, velocity\nmight be the derivative of an object\u2019s position with respect to time; and acceleration is\nthe derivative of the object\u2019s velocity with respect to time. Derivatives can find the slope\nat a specific point in the function. Gradient descent uses the knowledge of the slope to\ndetermine which way to move and by how much. Figures 9.24 and 9.25 describe how the\nderivatives and slope indicate the direction of the minimums.\n\nNegative slope = move right Positive slope = move left\n\nCost\nCost\n\nWeight Weight\n\nFigure 9.24 Derivatives\u2019 slopes and direction of minimums\n\nAdjustment 1 Adjustment 2 Adjustment 3\n& & &\n& & &\nWeight Weight Weight\n\nFigure 9.25 Example of adjusting a weight by using gradient descent\n\nWhen we look at one weight in isolation, it may seem trivial to find a value that mini-\nmizes the cost, but many weights being balanced affect the cost of the overall network.\nSome weights may be close to their optimal points in reducing cost, and others may not,\neven though the ANN performs well.\n\nBecause many functions comprise the ANN, we can use the Chain Rule. The Chain\nRule is a theorem from the field of calculus that calculates the derivative of a composite\nfunction. A composite function uses a function g as the parameter for a function \u00a3 to\nproduce a function h, essentially using a function as a parameter of another function.\n\nFigure 9.26 illustrates the use of the Chain Rule in calculating the update value for\nweights in the different layers of the ANN.\n\nCalculate update f\ninput * (2 \u00a2 cos\n\nInput nedes and hidden nedes:\n/@ (output) * hidden weight) * sigmoid derivative (hidden)\n\nCalculate update for weights between hidden nedes and output nede\nhidden \u00ab (2 \u00a2 cost \u00ab sigmoid_derivative(output))\n\n7 (2.09132)\n= 0.74859\n\n:\n\n| 7 (.6707508)\n\nTerrain enaa7e\nquality\na) y & Collision?\nDegree\nof vision :\n4278\nTotal '\nexperience\n\nFigure 9.26 Formula for calculating weight updates with the Chain Rule\n\nWe can calculate the weight update by plugging the respective values into the formula\ndescribed. The calculations look scary, but pay attention to the variables being used and\ntheir role in the ANN. Although the formula looks complex, it uses the values that we\nhave already calculated (figure 9.27).\n\nCaloulate update for weights be tween input nedes and hidden nedes:\ninput * (2 * cost * sigmoid_derivative output) * hidden weight) * signoid_derivative (hidden)\n4.542 \u00a9 (2 * -0.86274 \u00a2 sigmoid_derivative(e.84274) - 0.86) * signoid_derivative(\u00ae.74859)\n\n8\n\nCalculate update fer weights between hidden nedes and eutput nede:\nhidden * (2 * cost * sigmoid_derivative(output))\nigmoid_derivative(0.84274))\n\n7 (0.65814)\n\n= 0.65\nPlies 7 .6707508)\n\nTerrain mm\n\nquality\n\nof vision a TAY [Pa _|\n. = e.aeare\n\nTotal 9\nexperience i?)\n\n= 0.79086\n\nFigure 9.27 Weight-update calculation with the Chain Rule\n\nHere\u2019s a closer look at the calculations used in figure 9.27:\n\nCalculate update for weights between hidden nodes and output node:\nhidden * (2 * cost \u00ab sigmoid_derivative(output))\n\n0.74859 * (2 * -0.84274 * sigmoid_derivative(@.84274))\n0.84274 \u00a9 @.210)\n\nCalculate update for weights between input nodes and hidden nedes:\ninput * (2 * cost * gigmoid_derivative (output) * hidden weight) * sigmoid_derivative (hidden)\n\n0.542 \u00a9 (2 * -0.84274 * sigmoid_derivative (0.84274) * 0.86) * sigmoid_derivative(@.74859)\n542 * (2 * -0.84274 10 * 0.86) \u00a9 0.218\n0360\n\nNow that the update values are calculated, we can apply the results to the weights in the\nANN by adding the update value to the respective weights. Figure 9.28 depicts the appli-\ncation of the weight-update results to the weights in the different layers.\n\nCalculate ad justed weight:\n\nweight + weight update Caloulate adjusted weight:\n\nweight + weight update\n0.62 + (-8,265)\n\nweight 0.86 Js = 0.355\n\nbecomes @.624\n\nTerrain\nquality\n\nDegree\n\n94934)\n= 0.72098\n\nTotal\nexperience\n\nFigure 9.28 Example of the final weight-update for the ANN\n\nEXERCISE: CALCULATE THE NEW WEIGHTS FOR THE HIGHLIGHTED WEIGHTS\n\nCaloulate update fer weights between input nedes and hidden nedes.\n\nCalculate update fer weights between hidden nedes and eutput nede.\n\n(2.08132)\n= 0.74859\n\n2.6707508)\n= 0.04276\n\nTerrain x.\nquality 1\ny Collision?\nD : &\nDegree .\nof visien Oy) 2 (+2) S Coat\n\n94994) = -8eeane\n\nTotal\nexperience\n\n32998)\n084\n\n\nSOLUTION: CALCULATE THE NEW WEIGHTS FOR THE HIGHLIGHTED WEIGHTS\n\njor weights between input nedes and hidden nedes\neight) * sigmoid.\n\nivative (hidden)\n\nweight + veight update Calculate update for weights between hidden nodes and output node:\n\nwen hidden * (2 * co: tive (output)\n. @_derivative(\u00ae.84274))\n65704 * (2 * 9.84274 \u00ab 8.210)\n33\nght + weight update\n7 eon)\n= 0.74859\nSpeed\n\n7 (8.65014)\n= 0.65706\n\nTerrain =\nquality 1\nDegree\n\nof visi Oy *2\n\nTotal 9\nexperience ~y *3\n\n7 (e35128)\n= 0.58693\n\n34)\n= 0.72098\n\n7 (1.32998)\n= 0.79084\n\nThe problem that the Chain Rule is solving may remind you of the drone problem exam-\nple in chapter 7. Particle-swarm optimization is effective for finding optimal values in\nhigh-dimensional spaces such as this one, which has 25 weights to optimize. Finding the\nweights in an ANN is an optimization problem. Gradient descent is not the only way to\noptimize weights; we can use many approaches, depending on the context and problem\nbeing solved.\n\nBackpropagation: Training an ANN 313\n\nThe derivative is important in the backpropagation algorithm. The following piece of\npseudocode revisits the sigmoid function and describes the formula for its derivative,\nwhich we need to adjust weights:\n\nsigmoid (x): xp is a mathematical constant called\n\nreturn 1/ (1+ exp(-x)) + euler\u2019s number, approximately a.718a8.\n\nsigmoid_derivative (x):\nreturn sigmoid (x) * (1 - sigmoid (x))\n\nWe revisit the neural network class, this time with a backpropagation function that\ncomputes the cost, the amount by which weights should be updated by using the Chain\nRule, and adds the weight-update results to the existing weights. This process will com-\npute the change for each weight given the cost. Remember that cost is calculated by using\nthe example features, predicted output, and expected output. The difference between the\npredicted output and expected output is the cost:\n\nNeuralNetwork (features, labels, hidden_node_count):\nlet input equal features\nlet weights_input equal a random matrix, size: features * hidden_node_count\nlet hidden equal zero array, size: hidden_node_count\nlet weights_hidden equal a random matrix, size: hidden_node_count\nlet expected_output equal labels\n\nlet output equal zero array size: length of labels\n\nback_propagation(): The symbol \u00ab implies\nlet cost equal expected_output - output moari< multiplication.\nlet weights_hidden_update equal\nhidden (2 * cost * sigmoid_derivative(output))\nlet weights_input_update equal\ninput + (2 * cost * sigmoid_derivative (output) * weights_hidden)\n* sigmoid_derivative (hidden)\nlet weights_hidden equal weights_hidden + weights_hidden_update\n\nlet weights_input equal weights_input + weights_input_update\n\nBecause we have a class that represents a neural network, functions to scale data, and\nfunctions for forward propagation and backpropagation, we can piece this code together\nto train a neural network.\n\nIn this piece of pseudocode, we have a run_neural_network function that accepts\nepochs as an input. This function scales the data and creates a new neural network\nwith the scaled data, labels, and number of hidden nodes. Then the function runs\nforward_propagation and back_propagation for the specified number of\nepochs:\n\nrun_neural_network (epochs):\nlet scaled_feature_data equal\nscale_dataset (feature_data, feature_count, features_min, features_max)\nlet nn equal NeuralNetwork (scaled_feature_data,\nscaled_label_data,\nhidden_node_count)\nfor epoch in range(epochs):\nnn.forward_propagation()\nnn.back_propagation()\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.6,
                        "section_name": "Options for activation functions",
                        "section_path": "./screenshots-images-2/chapter_9/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_6/55250af9-b528-4415-97aa-5390e85eb6ea.png",
                            "./screenshots-images-2/chapter_9/section_6/279b09eb-e1bc-403b-96e6-39102578fe5d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Options for activation functions\n\nThis section aims to provide some intuition about activation functions and their proper-\nties. In the examples of the Perceptron and ANN, we used a sigmoid function as the acti-\nvation function, which was satisfactory for the examples that we were working with.\nActivation functions introduce nonlinear properties to the ANN. If we do not use an activa-\ntion function, the neural network will behave similarly to linear regression as described in\nchapter 8. Figure 9.29 describes some commonly used activation functions.\n\nStep unit Sigmold\n\nHyperbolic Rectified\ntangent linear unit (ReLU)\n\n1\n\nFigure 9.29 Commonly used activation functions\n\nDifferent activation functions are useful in different scenarios and have different\nbenefits:\n\nStep unit\u2014The step unit function is used as a binary classifier. Given an input\nbetween -1 and 1, it outputs a result of exactly 0 or 1. A binary classifier is not\nuseful for learning from data in a hidden layer, but it can be used in the output\nlayer for binary classification. If we want to know whether something is a cat or a\ndog, for example, 0 could indicate cat, and 1 could indicate dog.\n\nSigmoid\u2014The sigmoid function results in an S curve between 0 and 1, given an\ninput between -1 and 1. Because the sigmoid function allows changes in x to\nresult in small changes in y, it allows for learning and solving nonlinear\nproblems. The problem sometimes experienced with the sigmoid function is that\nas values approach the extremes, derivative changes become tiny, resulting in\npoor learning. This problem is known as the vanishing gradient problem.\n\nHyperbolic tangent\u2014The hyperbolic tangent function is similar to the sigmoid\nfunction, but it results in values between -1 and 1. The benefit is that the\nhyperbolic tangent has steeper derivatives, which allows for faster learning. The\nvanishing gradient problem is also a problem at the extremes for this function, as\nwith the sigmoid function.\n\nRectified linear unit (ReELU)\u2014The ReLU function results in 0 for input values\nbetween -1 and 0, and results in linearly increasing values between 0 and 1. Ina\nlarge ANN with many neurons using the sigmoid or hyperbolic tangent function,\nall neurons activate all the time (except when they result in 0), resulting in lots of\ncomputation and many values being adjusted finely to find solutions. The RLU\nfunction allows some neurons to not activate, which reduces computation and\nmay find solutions faster.\n\nThe next section touches on some considerations for designing an ANN.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.7,
                        "section_name": "Designing artificial neural networks",
                        "section_path": "./screenshots-images-2/chapter_9/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_7/7547b0c2-9483-4429-9003-aec545cf26a2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Designing artificial neural networks\n\nDesigning ANNs is experimental and dependent on the problem that is being solved.\nThe architecture and configuration of an ANN usually change through trial and error as\nwe attempt to improve the performance of the predictions. This section briefly lists the\nparameters of the architecture that we can change to improve performance or address\ndifferent problems. Figure 9.30 illustrates an artificial neural network with a different\nconfiguration to the one seen throughout this chapter. The most notable difference is the\nintroduction of a new hidden layer and the network now has two outputs.\n\nNOTE As in most scientific or engineering problems, the answer to \u201cWhat is\nthe ideal ANN design?\u201d is often \u201cIt depends.\u201d Configuring ANNs requires a\ndeep understanding of the data and the problem being solved. A clear-cut gen-\neralized blueprint for architectures and configurations doesn\u2019t exist . . . yet.\n\nHidden layer Hidden layer\n6 nedes 5 nodes\n\n30 weights 10 weights\n\nIS\n\nLL Sf\nA AY\n7S\nBW SKA\nCKO\n\nTX \\\n\n24 weights\n\n4 inputs\n\nFigure 9.30 An example of a multilayer ANN with more than one output\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.8,
                        "section_name": "Inputs and outputs",
                        "section_path": "./screenshots-images-2/chapter_9/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_8/530a9946-e773-46ee-9819-8fd4e3b05af9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Inputs and outputs\n\nThe inputs and outputs of an ANN are the fundamental parameters for use of the net-\nwork. After an ANN model has been trained, the trained ANN model will potentially be\nused in different contexts and systems, and by different people. The inputs and outputs\ndefine the interface of the network. Throughout this chapter, we saw an example of an\nANN with four inputs describing the features of a driving scenario and one output\ndescribing the likelihood of a collision. We may have a problem when the inputs and\noutputs mean different things, however. If we have a 16- by 16-pixel image that rep-\nresents a handwritten digit, for example, we could use the pixels as inputs and the digit\nthey represent as the output. The input would consist of 256 nodes representing the pixel\nvalues, and the output would consist of 10 nodes representing 0 to 9, with each result\nindicating the probability that the image is the respective digit.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.9,
                        "section_name": "Hidden layers and nodes",
                        "section_path": "./screenshots-images-2/chapter_9/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_9/0a3c857d-9667-49b8-b505-6edc69d3e2ba.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Hidden layers and nodes\n\nAn ANN can consist of multiple hidden layers with varying numbers of nodes in each\nlayer. Adding more hidden layers allows us to solve problems with higher dimensions\nand more complexity in the classification discrimination line. In the example in figure\n9.8, a simple straight line classified data accurately. Sometimes, the line is nonlinear but\nfairly simple. But what happens when the line is a more-complex function with many\ncurves potentially across many dimensions (which we can\u2019t even visualize)? Adding\nmore layers allows these complex classification functions to be found. The selection of\nthe number of layers and nodes in an ANN usually comes down to experimentation and\niterative improvement. Over time, we may gain intuition about suitable configurations,\nbased on experiencing similar problems and solving them with similar configurations.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.1,
                        "section_name": "Weights",
                        "section_path": "./screenshots-images-2/chapter_9/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_10/74f6ad6b-22f9-4b0b-b593-f2f5f20da3c0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Weights\n\nWeight initialization is important because it establishes a starting point from which the\nweight will be adjusted slightly over many iterations. Weights that are initialized to be\ntoo small lead to the vanishing gradient problem described earlier, and weights that are\ninitialized to be too large lead to another problem, the exploding gradient problem\u2014in\nwhich weights move erratically around the desired result.\n\nVarious weight-initialization schemes exist, each with its own pros and cons. A rule of\nthumb is to ensure that the mean of the activation results in a layer is O\u2014the mean of all\nresults of the hidden nodes in a layer. Also, the variance of the activation results should\nbe the same: the variability of the results from each hidden node should be consistent\nover several iterations.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.11,
                        "section_name": "adBias",
                        "section_path": "./screenshots-images-2/chapter_9/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_11/66dba78e-e4b6-40b2-9368-b459cf51702f.png",
                            "./screenshots-images-2/chapter_9/section_11/c4248a7e-1e08-487e-9b28-1f79e9764637.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "We can use bias in an ANN by adding a value to the weighted sum of the input nodes or\nother layers in the network. A bias can shift the activation value of the activation func-\ntion. A bias provides flexibility in an ANN and shifts the activation function left or right.\n\n\nA simple way to understand bias is to imagine a line that always passes through 0,0 on\na plane; we can influence this line to pass through a different intercept by adding +1 toa\nvariable. This value will be based on the problem to be solved.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.12,
                        "section_name": "Activation functions",
                        "section_path": "./screenshots-images-2/chapter_9/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_12/07fea3e6-d318-4ad4-9080-5b326a2af9f8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Activation functions\n\nEarlier we covered the common activation functions used in ANNs. A key rule of thumb\nis to ensure that all nodes on the same layer use the same activation function. In multi-\nlayer ANNs, different layers may use different activation functions based on the problem\nto be solved. A network that determines whether loans should be granted, for example,\nmight use the sigmoid function in the hidden layers to determine probabilities and a step\nfunction in the output to get a clear 0 or 1 decision.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.13,
                        "section_name": "Cost function and learning rate",
                        "section_path": "./screenshots-images-2/chapter_9/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_13/60fb94b8-e309-4c7e-a594-35cfb72d5259.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Cost function and learning rate\n\nWe used a simple cost function in the example described earlier where the predicted\noutput is subtracted from the actual expected output, but many cost functions exist.\nCost functions influence the ANN greatly, and using the correct function for the prob-\nlem and dataset at hand is important because it describes the goal for the ANN. One of\nthe most common cost functions is mean square error, which is similar to the function\nused in the machine learning chapter (chapter 8). But cost functions must be selected\nbased on understanding of the training data, size of the training data, and desired preci-\nsion and recall measurements. As we experiment more, we should look into the cost\nfunction options.\n\nFinally, the learning rate of the ANN describes how dramatically weights are adjusted\nduring backpropagation. A slow learning rate may result in a long training process\nbecause weights are updated by tiny amounts each time, and a high learning rate might\nresult in dramatic changes in the weights, making for a chaotic training process. One\nsolution is to start with a fixed learning rate and to adjust that rate if the training stag-\nnates and doesn\u2019t improve the cost. This process, which would be repeated through the\ntraining cycle, requires some experimentation. Stochastic gradient descent is a useful\ntweak to the optimizer that combats these problems. It works similarly to gradient\ndescent but allows weights to jump out of local minimums to explore better solutions.\n\nStandard ANNs such as the one described in this chapter are useful for solving non-\nlinear classification problems. If we are trying to categorize examples based on many\nfeatures, this ANN style is likely to be a good option.\n\nThat said, an ANN is not a silver bullet and shouldn't be the go-to algorithm for any-\nthing. Simpler, traditional machine learning algorithms described in chapter 8 often\nperform better in many common use cases. Remember the machine learning life cycle.\nYou may want to try several machine learning models during your iterations while seek-\ning improvement.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.14,
                        "section_name": "Artificial neural network types and use cases",
                        "section_path": "./screenshots-images-2/chapter_9/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_14/2813321b-a46b-4471-842d-1cebf2d5fd42.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Artificial neural network types and use cases\n\nANNs are versatile and can be designed to address different problems. Specific architec-\ntural styles of ANNs are useful for solving certain problems. Think of an ANN architec-\ntural style as being the fundamental configuration of the network. The examples in this\nsection highlight different configurations.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.15,
                        "section_name": "Convolutional neural network",
                        "section_path": "./screenshots-images-2/chapter_9/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_15/d78b79af-7f1a-4b0a-adce-032833aef95a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Convolutional neural network\n\nConvolutional neural networks (CNNs) are designed for image recognition. These net-\nworks can be used to find the relationships among different objects and unique areas\nwithin images. In image recognition, convolution operates on a single pixel and its neigh-\nbors within a certain radius. This technique is traditionally used for edge detection,\nimage sharpening, and image blurring. CNNs use convolution and pooling to find rela-\ntionships among pixels in an image. Convolution finds features in images, and pooling\ndownsamples the \u201cpatterns\u201d by summarizing features, allowing unique signatures in\nimages to be encoded concisely through learning from multiple images (figure 9.31).\n\nFully connected ANN Output\n\nConvolution Convolution\n\nPooling Pooling\n\nFigure 9.31 Simple example of a CNN\n\nCNNs are used for image classification. If you\u2019ve ever searched for an image online, you\nhave likely interacted indirectly with a CNN. These networks are also useful for optical\ncharacter recognition for extracting text data from an image. CNNs have been used in\nthe medical industry for applications that detect anomalies and medical conditions via\nX-rays and other body scans.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.16,
                        "section_name": "Recurrent neural network",
                        "section_path": "./screenshots-images-2/chapter_9/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_16/4a3d1579-57b8-4d3c-a4c8-0c1d847a8b45.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Recurrent neural network\n\nWhereas standard ANNs accept a fixed number of inputs, recurrent neural networks\n(RNNs) accept a sequence of inputs with no predetermined length. These inputs are like\nspoken sentences. RNNs have a concept of memory consisting of hidden layers that rep-\nresent time; this concept allows the network to retain information about the relation-\nships among the sequences of inputs. When we are training a RNN, the weights in the\nhidden layers throughout time are also influenced by backpropagation; multiple weights\nrepresent the same weight at different points in time (figure 9.32).\n\nInputs Hidden layer Output\n\nFigure 9.32 Simple example of a RNN\n\nRNNs are useful in applications pertaining to speech and text recognition and predic-\ntion. Related use cases include autocompletion of sentences in messaging applications,\ntranslation of spoken language to text, and translation between spoken languages.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.17,
                        "section_name": "Generative adversarial network",
                        "section_path": "./screenshots-images-2/chapter_9/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_17/7375ee41-f62d-4032-921e-3de99bdb30b3.png",
                            "./screenshots-images-2/chapter_9/section_17/db49ef49-68f7-44c1-a9f9-3376dc5ee6fe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generative adversarial network\n\nA generative adversarial network (GAN) consists of a generator network and a discrimi-\nnator network. For example, the generator creates a potential solution such as an image\nor a landscape, and a discriminator uses real images of landscapes to determine the real-\nism or correctness of the generated landscape. The error or cost is fed back into the net-\nwork to further improve its ability to generate convincing landscapes and determine\ntheir correctness. The term adversarial is key, as we saw with game trees in chapter 3.\nThese two components are competing to be better at what they do and, through that\ncompetition, generate incrementally better solutions (figure 9.33).\n\nDataset of Real\nreal landscapes landscape\n\nDiscriminator\nConvolutional Network\n\nGenerator Generated\nDeconvolutional Network landscape\n\nFigure 9.33 Simple example of aGAN\n\nGANS are used to generate convincing fake videos (also known as deepfakes) of famous\npeople, which raises concern about the authenticity of information in the media. GANs\nalso have useful applications such as overlaying hairstyles on people\u2019s faces. GANs have\nbeen used to generate 3D objects from 2D images, such as generating a 3D chair from a\n2D picture. This use case may seem to be unimportant, but the network is accurately\nestimating and creating information from a source that is incomplete. It is a huge step in\nthe advancement of AI and technology in general.\n\nThis chapter aimed to tie together the concepts of machine learning with the\nsomewhat-mysterious world of ANNs. For further learning about ANNs and deep learn-\ning, try Grokking Deep Learning (Manning Publications); and for a practical guide to a\nframework for building ANNs, see Deep Learning with Python (Manning Publications).\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 10,
                "chapter_name": "Reinforcement learning\nwith Q-learning",
                "chapter_path": "./screenshots-images-2/chapter_10",
                "sections": [
                    {
                        "section_id": 10.1,
                        "section_name": "What is reinforcement learning?",
                        "section_path": "./screenshots-images-2/chapter_10/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_1/a505e039-80cf-4825-9b35-20a80216abfb.png",
                            "./screenshots-images-2/chapter_10/section_1/37a36de1-a130-4ddd-9ce0-4bc01a21e021.png",
                            "./screenshots-images-2/chapter_10/section_1/c21ca604-12f5-48aa-9869-0460078dee3a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What is reinforcement learning?\n\nReinforcement learning (RL) is an area of machine learning inspired by\nbehavioral psychology. The concept of reinforcement learning is based on\ncumulative rewards or penalties for the actions that are taken by an agent in\na dynamic environment. Think about a young dog growing up. The dog is\nthe agent in an environment that is our home. When we want the dog to\nsit, we usually say, \u201cSit.\u201d The dog doesn\u2019t understand English, so we might\nnudge it by lightly pushing down on its hindquarters. After it sits, we usu-\nally pet the dog or give it a treat. This process will need to be repeated sev-\neral times, but after some time, we have positively reinforced the idea of\n\nsitting. The trigger in the environment is saying \u201cSit\u201d; the behavior learned is sitting; and\nthe reward is pets or treats.\n\nReinforcement learning is another approach to machine learning alongside supervised\nlearning and unsupervised learning. Whereas supervised learning uses labeled data to\nmake predictions and classifications, and unsupervised learning uses unlabeled data to\nfind clusters and trends, reinforcement learning uses feedback from actions performed\nto learn what actions or sequence of actions are more beneficial in different scenarios\ntoward an ultimate goal. Reinforcement learning is useful when you know what the goal\nis but don\u2019t know what actions are reasonable to achieve it. Figure 10.1 shows the map of\nmachine learning concepts and how reinforcement learning fits in.\n\nClassification\n\nSupervised Unsupervised\n\nlearning\n\nlearning Dimensionallty\n\nRegression\n\nreduction\n\nMachine\nlearning\nalgorithms\n\n\u2018Relnforcement} Deep\n\nlearning\n\nlearning\n\nQ- Learning Deep reinforcement learning\n\nFigure 10.1 How reinforcement learning fits into machine learning\n\nReinforcement learning can be achieved through classical techniques or deep learning\ninvolving artificial neural networks. Depending on the problem being solved, either\napproach may be better.\n\nFigure 10.2 illustrates when different machine learning approaches may be used. We\nwill be exploring reinforcement learning through classical methods in this chapter.\n\nDeep Deep\nANNs learning\n\nClassic\nmachine\nlearning\n\nClassic\nmethods\n\nExamples to\nlearn from\n\nDeep\nreinforcement\nlearning\n\nReinforcement\nlearning\n\nGoals but\nno examples\n\nFigure 10.2 Categorization of machine learning, deep learning, and reinforcement learning\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.2,
                        "section_name": "The inspiration for reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_10/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_2/b80e082c-2e5a-4946-b816-0188ae10f7c6.png",
                            "./screenshots-images-2/chapter_10/section_2/86717a54-41b4-4afe-951e-a0a0e727991b.png",
                            "./screenshots-images-2/chapter_10/section_2/71ce63dc-b982-47bd-9e35-e3978e3fc251.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The inspiration for reinforcement learning\n\nReinforcement learning in machines is derived from behavioral psychology, a field that\nis interested in the behavior of humans and other animals. Behavioral psychology usu-\nally explains behavior by a reflex action, or something learned in the individual\u2019s his-\ntory. The latter includes exploring reinforcement through rewards or punishments,\nmotivators for behaviors, and aspects of the individual\u2019s environment that contribute to\nthe behavior.\n\nTrial and error is one of the most common ways that most evolved animals learn\nwhat is beneficial to them and what is not. Trial and error involves trying something,\npotentially failing at it, and trying something different until you succeed. This process\nmay happen many times before a desired outcome is obtained, and it\u2019s largely driven by\nsome reward.\n\nThis behavior can be observed throughout nature. Newborn chicks, for example, try\nto peck any small piece of material that they come across on the ground. Through trial\nand error, the chicks learn to peck only food.\n\nAnother example is chimpanzees learning through trial and error that using a stick to\ndig the soil is more favorable than using their hands. Goals, rewards, and penalties are\nimportant in reinforcement learning. A goal for a chimpanzee is to find food; a reward\n\nor penalty may be the number of times it has dug a hole or the time taken to dig a hole.\nThe faster it can dig a hole, the faster it will find some food.\n\nFigure 10.3 looks at the terminology used in reinforcement learning with reference to\nthe simple dog-training example.\n\n- Environment rr |\n\nFigure 10.3 Example of reinforcement learning: teaching a dog to sit by using food as a reward\n\nReinforcement learning has negative and positive reinforcement. Positive reinforcement is\nreceiving a reward after performing an action, such as a dog getting a treat after it sits.\nNegative reinforcement is receiving a penalty after performing an action, such as a dog\ngetting scolded after it tears up a carpet. Positive reinforcement is meant to motivate\ndesired behavior, and negative reinforcement is meant to discourage undesired behavior.\n\nAnother concept in reinforcement learning is balancing instant gratification with\nlong-term consequences. Eating a chocolate bar is great for getting a boost of sugar and\nenergy; this is instant gratification. But eating a chocolate bar every 30 minutes will likely\ncause health problems later in life; this is a long-term consequence. Reinforcement learn-\ning aims to maximize the long-term benefit over short-term benefit, although short-\nterm benefit may contribute to long-term benefit.\n\nReinforcement learning is concerned with the long-term consequence of actions in an\nenvironment, so time and the sequence of actions are important. Suppose that we're\nstranded in the wilderness, and our goal is to survive as long as possible while traveling\nas far as possible in hopes of finding safety. We're positioned next to a river and have two\noptions: jump into the river to travel downstream faster or walk along the side of the\nriver. Notice the boat on the side of the river in figure 10.4. By swimming, we will travel\nfaster but might miss the boat by being dragged down the wrong fork in the river. By\nwalking, we will be guaranteed to find the boat, which will make the rest of the journey\n\nmuch easier, but we don\u2019t know this at the start. This example shows how important the\nsequence of actions is in reinforcement learning. It also shows how instant gratification\nmay lead to long-term detriment. Furthermore, in a landscape that didn\u2019t contain a boat,\nthe consequence of swimming is that we will travel faster but have soaked clothing,\nwhich may be problematic when it gets cold. The consequence of walking is that we will\ntravel slower but not wet our clothing, which highlights the fact that a specific action\nmay work in one scenario but not in others. Learning from many simulation attempts is\nimportant to finding more-generalist approaches.\n\nSwim in the river?\n\nWalk along the river?\n\nFigure 10.4 An example of possible actions that have long-term consequences\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.3,
                        "section_name": "Problems applicable to\nreinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_10/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_3/5808b3d7-286d-4a53-b5a4-7f16432633c8.png",
                            "./screenshots-images-2/chapter_10/section_3/8a1fe6d5-e260-4135-ac13-51077acb173c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Problems applicable to\nreinforcement learning\n\nTo sum it up, reinforcement learning aims to solve problems in which a goal is known\nbut the actions required to achieve it are not. These problems involve controlling an\nagent\u2019s actions in an environment. Individual actions may be rewarded more than oth-\ners, but the main concern is the cumulative reward of all actions.\n\nReinforcement learning is most useful for problems in which individual actions build\nup toward a greater goal. Areas such as strategic planning, industrial-process automa-\ntion, and robotics are good cases for the use of reinforcement learning. In these areas,\nindividual actions may be suboptimal to gain a favorable outcome. Imagine a strategic\ngame such as chess. Some moves may be poor choices based on the current state of the\nboard, but they help set the board up for a greater strategic win later in the game.\n\nReinforcement learning works well in domains in which chains of events are important\nfor a good solution.\n\nTo work through the steps in a reinforcement learning algorithm, we will use the\nexample car-collision problem from chapter 9 as inspiration. This time, however, we will\nbe working with visual data about a self-driving car in a parking lot trying to navigate to\nits owner. Suppose that we have a map ofa parking lot, including a self-driving car, other\ncars, and pedestrians. Our self-driving car can move north, south, east, and west. The\nother cars and pedestrians remain stationary in this example.\n\nThe goal is for our car to navigate the road to its owner while colliding with as few cars\nand pedestrians as possible\u2014ideally, not colliding with anything. Colliding with a car is\nnot good because it damages the vehicles, but colliding with a pedestrian is more severe.\nIn this problem, we want to minimize collisions, but if we have a choice between collid-\ning with a car and a pedestrian, we should choose the car. Figure 10.5 depicts this\nscenario.\n\nCar\nPedestrian\nGoal\n\nAgent\n\nFigure 10.5 The self-driving car in a parking lot problem\n\nWe will be using this example problem to explore the use of reinforcement learning for\nlearning good actions to take in dynamic environments.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.4,
                        "section_name": "The life cycle of reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_10/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_4/3885c81d-46c8-4bac-907c-6fbe2e2ea67a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The life cycle of reinforcement learning\n\nLike other machine learning algorithms, a reinforcement learning model needs to be\ntrained before it can be used. The training phase centers on exploring the environment\nand receiving feedback, given specific actions performed in specific circumstances or\nstates. The life cycle of training a reinforcement learning model is based on the Markov\nDecision Process, which provides a mathematical framework for modeling decisions (fig-\nure 10.6). By quantifying decisions made and their outcomes, we can train a model to\nlearn what actions toward a goal are most favorable.\n\nCN\na OD\nRte S\n\ni\n\nChanged state\n\nAgent\n\nEnvirenment\n\nFigure 10.6 The Markov Decision Process for reinforcement learning\n\nBefore we can start tackling the challenge of training a model by using reinforcement\nlearning, we need an environment that simulates the problem space we are working in.\nOur example problem entails a self-driving car trying to navigate a parking lot filled\nwith obstacles to find its owner while avoiding collisions. This problem needs to be mod-\neled as a simulation so that actions in the environment can be measured toward the\ngoal. This simulated environment is different from the model that will learn what actions\nto take.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.5,
                        "section_name": "Simulation and data: Make the environment come alive",
                        "section_path": "./screenshots-images-2/chapter_10/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_5/fb044ec0-94a6-4a00-99f5-78916d67cd3e.png",
                            "./screenshots-images-2/chapter_10/section_5/63572d5a-b092-443d-b46b-8e3b921ce756.png",
                            "./screenshots-images-2/chapter_10/section_5/fb2f34ae-32cf-497a-99b2-e37e9dc290a8.png",
                            "./screenshots-images-2/chapter_10/section_5/3c4fe1a9-8bdf-4bfe-8cc4-004119416292.png",
                            "./screenshots-images-2/chapter_10/section_5/47a49d32-4264-4e66-bf59-9440ec62ddea.png",
                            "./screenshots-images-2/chapter_10/section_5/8cf66b8f-84ff-4394-9423-d7260e5cbadf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Simulation and data: Make the environment come alive\n\nFigure 10.7 depicts a parking-lot scenario containing several other cars and pedestrians.\nThe starting position of the self-driving car and the location of its owner are represented\nas black figures. In this example, the self-driving car that applies actions to the environ-\nment is known as the agent.\n\nThe self-driving car, or agent, can take several actions in the environment. In this sim-\nple example, the actions are moving north, south, east, and west. Choosing an action\nresults in the agent moving one block in that direction. The agent can\u2019t move diagonally.\n\nF vorth\nt South\n=> East\n& West\n\nFigure 10.7 Agent actions in the parking-lot environment\n\nWhen actions are taken in the environment, rewards or penalties occur. Figure 10.8\nshows the reward points awarded to the agent based on the outcome in the environment.\nA collision with another car is bad; a collision with a pedestrian is terrible. A move to an\nempty space is good; finding the owner of the self-driving car is better. The specified\nrewards aim to discourage collisions with other cars and pedestrians, and to encourage\nmoving into empty spaces and reaching the owner. Note that there could be a reward for\nout-of-bounds movements, but we will simply disallow this possibility for the sake of\nsimplicity.\n\nMove into car. ap g -100\nMove intopedestrian. QD rt -1,000\nMove inte empty space. QD +100\n\nMove into geal. ap Y +500\n\nFigure 10.8 Rewards due to specific events in the environment due to actions performed\n\nNOTE An interesting outcome of the rewards and penalties described is that\nthe car may drive forward and backward on empty spaces indefinitely to\naccumulate rewards. We will dismiss this as a possibility for this example, but\nit highlights the importance of crafting good rewards.\n\nThe simulator needs to model the environment, the actions of the agent, and the rewards\nreceived after each action. A reinforcement learning algorithm will use the simulator to\nlearn through practice by taking actions in the simulated environment and measuring\nthe outcome. The simulator should provide the following functionality and information\nat minimum:\n\nInitialize the environment. This function involves resetting the environment,\nincluding the agent, to the starting state.\n\nGet the current state of the environment. This function should provide the current\nstate of the environment, which will change after each action is performed.\n\nApply an action to the environment. This function involves having the agent apply\nan action to the environment. The environment is affected by the action, which\nmay result in a reward.\n\nCalculate the reward of the action. This function is related to applying the action\nto the environment. The reward for the action and effect on the environment\nneed to be calculated.\n\nDetermine whether the goal is achieved. This function determines whether the\nagent has achieved the goal. The goal can also sometimes be represented as is\ncomplete. In an environment in which the goal cannot be achieved, the\nsimulator needs to signal completion when it deems necessary.\n\nFigures 10.9 and 10.10 depict possible paths in the self-driving-car example. In figure\n10.9, the agent travels south until it reaches the boundary; then it travels east until it\nreaches the goal. Although the goal is achieved, the scenario resulted in five collisions\nwith other cars and one collision with a pedestrian\u2014not an ideal result. Figure 10.10\ndepicts the agent traveling along a more specific path toward the goal, resulting in no\ncollisions, which is great. It\u2019s important to note that given the rewards that we have spec-\nified, the agent is not guaranteed to achieve the shortest path; because we heavily encour-\nage avoiding obstacles, the agent may find any path that is obstacle-free.\n\nDamage\nExample\nSolution - 1 pedestrian\n\n- Sears\n\nFigure 10.9 A bad solution to the parking-lot problem\n\nExample Damage\n\nselgtion - @ pedestrians\n\n- Scars\n\nFigure 10.10 A good solution to the parking-lot problem\n\nAt this moment, there is no automation in sending actions to the simulator. It\u2019s like a\ngame in which we provide input as a person instead of an AI providing the input. The\nnext section explores how to train an autonomous agent.\n\nThe pseudocode for the simulator encompasses the functions discussed in this section.\nThe simulator class would be initialized with the information relevant to the starting\nstate of the environment.\n\nThe move_agent function is responsible for moving the agent north, south, east, or\nwest, based on the action. It determines whether the movement is within bounds, adjusts\nthe agent\u2019s coordinates, determines whether a collision occurred, and returns a reward\nscore based on the outcome:\n\nSimulator (road, road_size_x,road_size_y,\n\nagent_start_x,agent_start_y, goal_x,goal_y):\n\n\u00bb_agent (action):\nif action equals COMMAND_NORTH:\nlet next_x equal agent_x-1\nlet next_y equal agent_y\nelse if action equals COMMAND_SOUTH:\nlet next_x equal agent_x+1\nlet next_y equal agent_y\nelse if action equals COMMAND_EAST:\nlet next_x equal agent_x\nlet next_y equal agent_y+1\nelse if action equals COMMAND_WEST:\nlet next_x equal agent_x\nlet next_y equal agent_y-1\nif is_within_bounds(next_x,next_y) equals True:\nlet reward_update equal cost_movement (next_x, next_y)\nlet agent_x equal next_x\nlet agent_y equal next_y\nelse:\nlet reward_update equal ROAD_OUT_OF_BOUNDS_REWARD\n\nreturn reward_update\n\nHere are descriptions of the next functions in the pseudocode:\n\n+ The cost_movement function determines the object in the target coordinate\nthat the agent will move to and returns the relevant reward score.\n\n+ The is_within_bounds function isa utility function that makes sure the\ntarget coordinate is within the boundary of the road.\n\n+ The is_goal_achieved function determines whether the goal has been\nfound, in which case the simulation can end.\n\n+ The get_state function uses the agent\u2019s position to determine a number that\nenumerates the current state. Each state must be unique. In other problem\nspaces, the state may be represented by the actual native state itself.\n\ncost_movement (next_x,next_y):\n\nif road(next_x](next_y] equals ROAD_OBSTACLE_PERSON:\nreturn ROAD_OBSTACLE_PERSON_REWARD\n\nelse if road{next_x]{next_y] equals ROAD_OBSTACLE_CAR:\nreturn ROAD_OBSTACLE_CAR_REWARD\n\nelse if road[next_x][next_y] equals ROAD_GOAL:\nreturn ROAD_GOAL_REWARD\n\nelse:\n\nyeturn ROAD_EMPTY_REWARD\n\nis_within_bounds(next_x, next_y):\nif road_size_x > next_x >= @ and road_size_y > next_y >= 0:\nreturn True\n\nreturn False\n\nis_goal_achieved():\nif agent_x equals goal_x and agent_y equals goal_y:\nreturn True\n\nreturn False\n\nget_state():\n\nreturn (road_size_x * agent_x)+ agent_y\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.6,
                        "section_name": "Training with the simulation using Q-learning",
                        "section_path": "./screenshots-images-2/chapter_10/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_6/6a3d8971-5366-41c9-8ddc-d02e78868017.png",
                            "./screenshots-images-2/chapter_10/section_6/97720444-5cdd-44b4-bdd0-9bd537f5af7a.png",
                            "./screenshots-images-2/chapter_10/section_6/40885f30-d87b-4bab-a3e5-8ac1c20510b8.png",
                            "./screenshots-images-2/chapter_10/section_6/2209f873-d414-4825-a70e-0a60045ff76a.png",
                            "./screenshots-images-2/chapter_10/section_6/a857755f-ec3a-4786-98e9-1820d0accc97.png",
                            "./screenshots-images-2/chapter_10/section_6/12172efd-e117-4e9b-b939-9eb7a399c50f.png",
                            "./screenshots-images-2/chapter_10/section_6/2beec463-8d4c-4650-8c43-c632ebb32b4b.png",
                            "./screenshots-images-2/chapter_10/section_6/2360a254-0115-4268-9d89-3bfaebe2b1ee.png",
                            "./screenshots-images-2/chapter_10/section_6/7a2d110b-2f4c-4e88-80b2-3f93b964d3d3.png",
                            "./screenshots-images-2/chapter_10/section_6/14e5d1f0-03e5-422d-8062-3a8da94db809.png",
                            "./screenshots-images-2/chapter_10/section_6/32e057c8-dfdc-4211-bb51-28f18a2954b7.png",
                            "./screenshots-images-2/chapter_10/section_6/41eea118-957b-4937-9b4f-982774528eb5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training with the simulation using Q-learning\n\nQ-learning is an approach in reinforcement learning that uses the states and actions in\nan environment to model a table that contains information describing favorable actions\nbased on specific states. Think of Q-learning as a dictionary in which the key is the state\nof the environment and the value is the best action to take for that state.\n\nReinforcement learning with Q-learning employs a reward table called a Q-table. A\nQ-table consists of columns that represent the possible actions and rows that represent\nthe possible states in the environment. The point of a Q-table is to describe which actions\nare most favorable for the agent as it seeks a goal. The values that represent favorable\nactions are learned through simulating the possible actions in the environment and\nlearning from the outcome and change in state. It\u2019s worth noting that the agent has a\nchance of choosing a random action or an action from the Q-table, as shown later in\n\nfigure 10.13. The Q represents the function that provides the reward, or quality, of an\naction in an environment.\n\nFigure 10.11 depicts a trained Q-table and two possible states that may be represented\nby the action values for each state. These states are relevant to the problem we're solving;\nanother problem might allow the agent to move diagonally as well. Note that the number\nof states differs based on the environment and that new states can be added as they are\ndiscovered. In state 1, the agent is in the top-left corner, and in state 2, the agent is in the\nposition below its previous state. The Q-table encodes the best actions to take, given each\nrespective state. The action with the largest number is the most beneficial action. In this\nfigure, the values in the Q-table have already been found through training. Soon, we will\nsee how they\u2019re calculated.\n\nState 1\n\nState2\n\nFigure 10.11 An example Q-table and states that it represents\n\nThe big problem with representing the state using the entire map is that the configura-\ntion of other cars and people is specific to this problem. The Q-table learns the best\nchoices only for this map.\n\nA better way to represent state in this example problem is to look at the objects\nadjacent to the agent. This approach allows the Q-table to adapt to other parking-lot\n\nconfigurations, because the state is less specific to the example parking lot from which it\nis learning. This approach may seem to be trivial, but a block could contain another car\nor a pedestrian, or it could be an empty block or an out-of-bounds block, which works\nout to four possibilities per block, resulting in 65,536 possible states. With this much\nvariety, we would need to train the agent in many parking-lot configurations many times\nfor it to learn good short-term action choices (figure 10.12).\n\nState 1\n\nState2\n\n0 \u00a2 q\neb Gay 8 \u00a2\nFigure 10.12 A better example of a Q-table and states that it represents\n\nKeep the idea of a reward table in mind as we explore the life cycle of training a model\nusing reinforcement learning with Q-learning. It will represent the model for actions\nthat the agent will take in the environment.\n\nLet\u2019s take a look at the life cycle of a Q-learning algorithm, including the steps involved\nin training. We will look at two phases: initialization, and what happens over several\niterations as the algorithm learns (figure 10.13):\n\nA. Initialize\n\nB. Repeat for niterations\n\nInitialize\nQ- table.\n\nReference\nactien in\nQ-table.\n\nPick a\nrandem actien\n\nIs goat\nachieved\n\nInitialize\nsimulator.\n\nApply\naction to\nlenvirenment.\n\nFigure 10.13 Life cycle of a Q-learning reinforcement learning algorithm\n\nInitialize. The initialize step involves setting up the relevant parameters and\ninitial values for the Q-table:\n\n1. Initialize Q-table. Initialize a Q-table in which each column is an action\nand each row represents a possible state. Note that states can be added to\nthe table as they are encountered, because it can be difficult to know the\nnumber of states in the environment at the beginning. The initial action\nvalues for each state are initialized with Os.\n\n2. Set parameters. This step involves setting the parameters for different\nhyperparameters of the Q-learning algorithm, including:\n\n+ Chance of choosing a random action\u2014This is the value threshold for\nchoosing a random action over choosing an action from the Q-table.\n\n+ Learning rate\u2014The learning rate is similar to the learning rate in\nsupervised learning. It describes how quickly the algorithm learns\nfrom rewards in different states. With a high learning rate, values in\nthe Q-table change erratically, and with a low learning rate, the values\nchange gradually but it will potentially take more iterations to find\ngood values.\n\n+ Discount factor\u2014The discount factor describes how much potential\nfuture rewards are valued, which translates to favoring immediate\ngratification or long-term reward. A small value favors immediate\nrewards; a large value favors long-term rewards.\n\nRepeat for n iterations. The following steps are repeated to find the best actions in\nthe same states by evaluating these states multiple times. The same Q-table will\nbe updated over all iterations. The key concept is that because the sequence of\nactions for an agent is important, the reward for an action in any state may\nchange based on previous actions. For this reason, multiple iterations are\nimportant. See an iteration as a single attempt to achieving a goal:\n\n1.\n\nInitialize simulator. This step involves resetting the environment to the\nstarting state, with the agent in a neutral state.\n\nGet environment state. This function should provide the current state of the\nenvironment. The state of the environment will change after each action is\nperformed.\n\nIs goal achieved? Determine whether the goal is achieved (or the simulator\ndeems the exploration to be complete). In our example, this goal is picking up\nthe owner of the self-driving car. If the goal is achieved, the algorithm ends.\n\nPick a random action. Determine whether a random action should be\nselected. If so, a random action will be selected (north, south, east, or\nwest). Random actions are useful for exploring the possibilities in the\nenvironment instead of learning a narrow subset.\n\nReference action in Q-table. If the decision to select a random action is not\nselected, the current environment state is transposed to the Q-table, and\nthe respective action is selected based on the values in the table. More\nabout the Q-table is coming up.\n\nApply action to environment .This step involves applying the selected\naction to the environment, whether that action is random or one selected\nfrom the Q-table. An action will have a consequence in the environment\nand yield a reward.\n\n7. Update Q-table. The following material describes the concepts involved in\nupdating the Q-table and the steps that are carried out.\n\nThe key aspect of Q-learning is the equation used to update the values of the Q-table.\nThis equation is based on the Bellman equation, which determines the value of a decision\nmade at a certain point in time, given the reward or penalty for making that decision.\nThe Q-learning equation is an adaptation of the Bellman equation. In the Q-learning\nequation, the most important properties for updating Q-table values are the current\nstate, the action, the next state given the action, and the reward outcome. The learning\nrate is similar to the learning rate in supervised learning, which determines the extent to\nwhich a Q-table value is updated. The discount is used to indicate the importance of\npossible future rewards, which is used to balance favoring immediate rewards versus\nlong-term rewards:\n\nQ(state, action) = \u2018The maximum valve of all actions on next state\n\n(1 - alpha) * Q(state, action) + alpha * (reward + gamma * Q(next state, all actions))\n\nLearningrate Currentvalue Learningrate Discount\n\nBecause the Q-table is initialized with 0s, it looks similar to figure 10.14 in the initial\nstate of the environment.\n\nFigure 10.14 An example initialized Q-table\n\nNext, we explore how to update the Q-table by using the Q-learning equation based on\ndifferent actions with different reward values. These values will be used for the learning\nrate (alpha) and discount (gamma):\n\n+ Learning rate (alpha): 0.1\n+ Discount (gamma): 0.6\n\nFigure 10.15 illustrates how the Q-learning equation is used to update the Q-table, if the\nagent selects the East action from the initial state in the first iteration. Remember that\nthe initial Q-table consists of 0s. The learning rate (alpha), discount (gamma), current\naction value, reward, and next best state are plugged into the equation to determine the\nnew value for the action that was taken. The action is East, which results in a collision\nwith another car, which yields -100 as a reward. After the new value is calculated, the\nvalue of East on state 1 is -10.\n\nAction => Reward oo fj -100\n\nQ(1, east) =\n(1 - alpha) * (1, east) + alpha * (reward + gamma * max of Q(2, all actions))\n\nQ(1, east) = (1- 0.1) * 0+ 0.1 * (-100+ 0.6 * 0)\n\nQ(1, east) =-10\n\nState 1\n\nFigure 10.15 Example Q-table update calculation for state 1\n\nThe next calculation is for the next state in the environment following the action that\nwas taken. The action is South and results in a collision with a pedestrian, which yields\n-1,000 as the reward. After the new value is calculated, the value for the South action on\nstate 2 is -100 (figure 10.16).\n\nAction & Reward \u00ab\u2122D pp -1,000\n\nQ(2, south) =\n(1 - alpha) * Q(2, south) + alpha * (reward + gamma * max of Q(3, all actions))\n\nQ(2, south) = (1- 0.1) * 0+ 0.1 * (-1000+ 0.6 * 0)\n\nQ(2, south) = -100\n\nState2\n\nFigure 10.16 Example Q-table update calculation for state 2\n\nFigure 10.17 illustrates how the calculated values differ in a Q-table with populated val-\nues because we worked on a Q-table initialized with 0s. The figure is an example of the\nQ-learning equation updated from the initial state after several iterations. The simula-\ntion can be run multiple times to learn from multiple attempts. So, this iteration is suc-\nceeding many before it, where the values of the table have been updated. The action for\nEast results in a collision with another car and yields -100 as a reward. After the new\nvalue is calculated, the value for East on state 1 changes to -34.\n\nAction => Reward oo fj -100\n\nQ(1,east)=\n(1 - alpha) * 9(1, east) + alpha * (reward + gamma * max of Q(2, all actions))\n\nQ(1, east) = (1 - 0.1) * -35 + 0.1 * (-100 + 0.6 * 125)\n\nQ(1, east) = -34\n\nState\n\nFigure 10.17 Example Q-table update calculation for state 1 after several iterations\n\nEXERCISE: CALCULATE THE CHANGE IN VALUES FOR THE Q-TABLE\n\nUsing the Q-learning update equation and the following scenario, calculate\nthe new value for the action performed. Assume that the last move was East\nwith a value of -67:\n\nQ(state, action) = The maximum valve ef all actions on next state\n(1 - alpha) * Q(state, action) + alpha * (reward + gamma * Q(next state, all actions))\n\nLearningrate Currentvalue Learningrate Discount\n\nAction => Reward an (} -1000\n\nState 45\n\nSOLUTION: CALCULATE THE CHANGE IN VALUES FOR THE Q-TABLE\n\nThe hyperparameter and state values are plugged into the Q-learning equa-\ntion, resulting in the new value for Q(1, east):\n\n+ Learning rate (alpha): 0.1\n\n+ Discount (gamma): 0.6\n\n* QQ(L, east): -67\n\n* Max of Q(2, all actions): 112\nQ(1, east) =\n(1 - alpha) * Q(1, east) + alpha * (reward + gamma * max of Q(2, all actions))\nQ(1, east) = (1- 0.1) * -67+ 0.1 * (-100 + 0.6 * 112)\n\nQ(1, east) = -64\n\nThis pseudocode describes a function that trains a Q-table by using Q-learning. It could\nbe broken into simpler functions but is represented this way for readability. The function\nfollows the steps described in this chapter.\n\nThe Q-table is initialized with 0s; then the learning logic is run for several iterations.\nRemember that an iteration is an attempt to achieve the goal.\n\nThe next piece of logic runs while the goal has not been achieved:\n\n1. Decide whether a random action should be taken to explore possibilities in the\nenvironment. If not, the highest value action for the current state is selected from\nthe Q-table.\n\n2. Proceed with the selected action, and apply it to the simulator.\n\n3. Gather information from the simulator, including the reward, the next state\ngiven the action, and whether the goal is reached.\n\n4. Update the Q-table based on the information gathered and hyperparameters.\nNote that in this code, the hyperparameters are passed through as arguments of\nthis function.\n\n5. Set the current state to the state outcome of the action just performed.\n\nThese steps will continue until a goal is found. After the goal is found and the desired\nnumber of iterations is reached, the result is a trained Q-table that can be used to test in\nother environments. We look at testing the Q-table in the next section:\n\ntrain_with_q_learning(observation_space, action_space,\nnumber_of_iterations, learning rate,\ndiscount, chance_of_random_move):\nlet q_table equal a matrix of zeros [observation_space, action_space]\nfor i in range(number_of_iterations):\nlet simulator equal Simulator (DEFAULT_ROAD, DEFAULT_ROAD_SIZE_X,\nDEFAULT_ROAD_SIZE_Y, DEFAULT_START_X,\nDEFAULT_START_Y, DEFAULT_GOAL_X,\nDEFAULT_GOAL_Y)\nlet state equal simulator.get_state()\nlet done equal False\nwhile not done:\nif random.uniform (0, 1) > chance_of_random_move:\nlet action equal get_random_move()\nelse:\n\nlet action max(q_table[state])\nlet reward equal simulator move_agent (action)\nlet next_state equal simulator.get_state()\n\nlet done equal simulator.is_goal_achieved()\n\nlet current_value equal q_table(state, action]\n\nlet next_state_max_value equal max(q_table(next_state])\n\nlet new_value equal (1 - learning_rate) * current_value + learning_rate *\n\n(reward + discount * next_state_max_value)\n\nlet q_table[state, action] equal new_value\n\nlet state equal next_state\n\nreturn q_table\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.7,
                        "section_name": "Testing with the simulation and Q-table",
                        "section_path": "./screenshots-images-2/chapter_10/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_7/08f2c216-2931-4087-8dc9-3665e5bed0ae.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Testing with the simulation and Q-table\n\nWe know that in the case of using Q-learning, the Q-table is the model that encompasses\nthe learnings. When presented with a new environment with different states, the algo-\nrithm references the respective state in the Q-table and chooses the highest-valued\naction. Because the Q-table has already been trained, this process consists of getting the\ncurrent state of the environment and referencing the respective state in the Q-table to\nfind an action until a goal is achieved (figure 10.18).\n\nStatet\n\nMest\nfavorable =>\naction\n\nFigure 10.18 Referencing a Q-table to determine what action to take\n\nBecause the state learned in the Q-table considers the objects directly next to the agent\u2019s\ncurrent position, the Q-table has learned good and bad moves for short-term rewards,\nso the Q-table could be used in a different parking-lot configuration, such as the one\nshown in figure 10.18. The disadvantage is that the agent favors short-term rewards over\nlong-term rewards because it doesn\u2019t have the context of the rest of the map when taking\neach action.\n\nOne term that will likely come up in the process of learning more about reinforce-\nment learning is episodes. An episode includes all the states between the initial state and\nthe state when the goal is achieved. If it takes 14 actions to achieve a goal, we have 14\nepisodes. If the goal is never achieved, the episode is called infinite.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.8,
                        "section_name": "Measuring the performance of training",
                        "section_path": "./screenshots-images-2/chapter_10/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_8/56f4e759-0852-4713-8c59-8663586be732.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Measuring the performance of training\n\nReinforcement learning algorithms can be difficult to measure generically. Given a spe-\ncific environment and goal, we may have different penalties and rewards, some of which\nhave a greater effect on the problem context than others. In the parking-lot example, we\nheavily penalize collisions with pedestrians. In another example, we may have an agent\nthat resembles a human and tries to learn what muscles to use to walk naturally as far as\npossible. In this scenario, penalties may be falling or something more specific, such as\ntoo-large stride lengths. To measure performance accurately, we need the context of the\nproblem.\n\nOne generic way to measure performance is to count the number of penalties in a\ngiven number of attempts. Penalties could be events that we want to avoid that happen\nin the environment due to an action.\n\nAnother measurement of reinforcement learning performance is average reward per\naction. By maximizing the reward per action, we aim to avoid poor actions, whether the\ngoal was reached or not. This measurement can be calculated by dividing the cumulative\nreward by the total number of actions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.9,
                        "section_name": "Model-free and model-based learning",
                        "section_path": "./screenshots-images-2/chapter_10/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_9/1eea89eb-b7b0-42a2-9157-ad1e227db0c0.png",
                            "./screenshots-images-2/chapter_10/section_9/2afa0206-3905-44ac-9449-723a45c2c687.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Model-free and model-based learning\n\nTo support your future learning in reinforcement learning, be aware of two approaches\nfor reinforcement learning: model-based and model-free, which are different from the\nmachine learning models discussed in this book. Think of a model as being an agent\u2019s\nabstract representation of the environment in which it is operating.\n\nWe may have a model in our heads about locations of landmarks, intuition of direc-\ntion, and the general layout of the roads within a neighborhood. This model has been\nformed from exploring some roads, but we\u2019re able to simulate scenarios in our heads to\nmake decisions without trying every option. To decide how we will get to work, for\nexample, we can use this model to make a decision; this approach is model-based. Model-\nfree learning is similar to the Q-learning approach described in this chapter; trial and\nerror is used to explore many interactions with the environment to determine favorable\nactions in different scenarios.\n\nFigure 10.19 depicts the two approaches in road navigation. Different algorithms can\nbe employed to build model-based reinforcement learning implementations.\n\nModel-free\n\nModel-based\n\nAn immediate decision based on\nhow favorable it is ~\n\nY  Anintuition of the world\n~ where decisions can be\nsimulated without trying\n\na each and every one XK\n\u201cNY 7\n\nFigure 10.19 Examples of model-based and model-free reinforcement learning\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.1,
                        "section_name": "Deep learning approaches to\nreinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_10/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_10/ecc4efe2-a5d0-4a9b-b927-f2fabbec4e11.png",
                            "./screenshots-images-2/chapter_10/section_10/859ba3f4-dd78-4407-8639-c9c9e6160188.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep learning approaches to\nreinforcement learning\n\nQ-learning is one approach to reinforcement learning. Having a good understanding of\nhow it functions allows you to apply the same reasoning and general approach to other\nreinforcement learning algorithms. Several alternative approaches depend on the prob-\nlem being solved. One popular alternative is deep reinforcement learning, which is\nuseful for applications in robotics, video-game play, and problems that involve images\nand video.\n\nDeep reinforcement learning can use artificial neural networks (ANNs) to process the\nstates of an environment and produce an action. The actions are learned by adjusting\nweights in the ANN, using the reward feedback and changes in the environment.\nReinforcement learning can also use the capabilities of convolutional neural networks\n(CNNs) and other purpose-built ANN architectures to solve specific problems in differ-\nent domains and use cases.\n\nFigure 10.20 depicts, at a high level, how an ANN can be used to solve the parking-lot\nproblem in this chapter. The inputs to the neural network are the states; the outputs are\nprobabilities for best action selection for the agent; and the reward and effect on the envi-\nronment can be fed back using backpropagation to adjust the weights in the network.\n\nAction\n\nState\n\nReward\n\nFigure 10.20 Example of using an ANN for the parking-lot problem\n\nThe next section looks at some popular use cases for reinforcement learning in the real\nworld.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.11,
                        "section_name": "Use cases for reinforcement learning",
                        "section_path": "./screenshots-images-2/chapter_10/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_11/f3bdfd5b-e3f1-4f01-a899-2d0ade38b969.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Use cases for reinforcement learning\n\nReinforcement learning has many applications where there is no or little historic data to\nlearn from. Learning happens through interacting with an environment that has heuris-\ntics for good performance. Use cases for this approach are potentially endless. This sec-\ntion describes some popular use cases for reinforcement learning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.12,
                        "section_name": "Robotics",
                        "section_path": "./screenshots-images-2/chapter_10/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_12/1d32b7fb-3edb-49ff-bb39-69d955776f65.png",
                            "./screenshots-images-2/chapter_10/section_12/cc961534-bb6f-412b-a972-1a80f5b08f90.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Robotics\n\nRobotics involves creating machines that interact with real-world environments to\naccomplish goals. Some robots are used to navigate difficult terrain with a variety of\nsurfaces, obstacles, and inclines. Other robots are used as assistants in a laboratory, tak-\ning instructions from a scientist, passing the right tools, or operating equipment. When\nit isn\u2019t possible to model every outcome of every action in a large, dynamic environment,\nreinforcement learning can be useful. By defining a greater goal in an environment and\nintroducing rewards and penalties as heuristics, we can use reinforcement learning to\n\ntrain robots in dynamic environments. A terrain-navigating robot, for example, may\nlearn which wheels to drive power to and how to adjust its suspension to traverse diffi-\ncult terrain successfully. This goal is achieved after many attempts.\n\nThese scenarios can be simulated virtually if the key aspects of the environment can\nbe modeled in a computer program. Computer games have been used in some projects\nas a baseline for training self-driving cars before they\u2019re trained on the road in the real\nworld. The aim in training robots with reinforcement learning is to create more-general\nmodels that can adapt to new and different environments while learning more-general\ninteractions, much the way that humans do.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.13,
                        "section_name": "Recommendation engines",
                        "section_path": "./screenshots-images-2/chapter_10/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_13/2c0f9d9f-025b-44f5-b673-8bd4d577355f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Recommendation engines\n\nRecommendation engines are used in many of the digital products we use. Video stream-\ning platforms use recommendation engines to learn an individual\u2019s likes and dislikes in\nvideo content and try to recommend something most suitable for the viewer. This\napproach has also been employed in music streaming platforms and e-commerce stores.\nReinforcement learning models are trained by using the behavior of the viewer when\nfaced with decisions to watch recommended videos. The premise is that if a recom-\nmended video was selected and watched in its entirety, there\u2019s a strong reward for the\nreinforcement learning model, because it has assumed that the video was a good recom-\nmendation. Conversely, if a video never gets selected or little of the content is watched,\nit\u2019s reasonable to assume that the video did not appeal to the viewer. This result would\nresult in a weak reward or a penalty.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.14,
                        "section_name": "Financial trading",
                        "section_path": "./screenshots-images-2/chapter_10/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_14/afeda779-8a75-493f-ab39-903ab0c6ad6c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Financial trading\n\nFinancial instruments for trading include stock in companies, cryptocurrency, and\nother packaged investment products. Trading is a difficult problem. Analysts monitor\npatterns in price changes and news about the world, and use their judgment to make a\ndecision to hold their investment, sell part of it, or buy more. Reinforcement learning\ncan train models that make these decisions through rewards and penalties based on\nincome made or loss incurred. Developing a reinforcement learning model to trade well\ntakes a lot of trial and error, which means that large sums of money could be lost in\ntraining the agent. Luckily, most historic public financial data is freely available, and\nsome investment platforms provide sandboxes to experiment with.\n\nAlthough a reinforcement learning model could help generate a good return on\ninvestment, here\u2019s an interesting question: if all investors were automated and completely\nrational, and the human element was removed from trading, what would the market\nlook like?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.15,
                        "section_name": "Game playing",
                        "section_path": "./screenshots-images-2/chapter_10/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_15/6bab880b-7b23-4517-bc39-e61eedc4437e.png",
                            "./screenshots-images-2/chapter_10/section_15/b17ec2d9-5f37-44de-8a00-365d0bb340e0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Game playing\n\nPopular strategy computer games have been pushing players\u2019 intellectual capabilities for\nyears. These games typically involve managing many types of resources while planning\nshort-term and long-term tactics to overcome an opponent. These games have filled\n\narenas, and the smallest mistakes have cost top-notch players many matches.\nReinforcement learning has been used to play these games at the professional level and\nbeyond. These reinforcement learning implementations usually involve an agent watch-\ning the screen the way a human player would, learning patterns, and taking actions. The\nrewards and penalties are directly associated with the game. After many iterations of\nplaying the game in different scenarios with different opponents, a reinforcement learn-\ning agent learns what tactics work best toward the long-term goal of winning the game.\nThe goal of research in this space is related to the search for more-general models that\ncan gain context from abstract states and environments and understand things that can-\nnot be mapped out logically. As children, for example, we never got burned by multiple\nobjects before learning that hot objects are potentially dangerous. We developed an intu-\nition and tested it as we grew older. These tests reinforced our understanding of hot\nobjects and their potential harm or benefit.\n\nIn the end, AI research and development strives to make computers learn to solve\nproblems in ways that humans are already good at: in a general way, stringing abstract\nideas and concepts together with a goal in mind and finding good solutions to\nproblems.\n",
                        "extracted-code": ""
                    }
                ]
            }
        ]
    }
}