{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "ABSTRACT",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "ABSTRACT",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/fc3b5440-b7b5-4cf7-9abe-c12d56f31b76.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "ABSTRACT\n\nMotivated by recent advances in large language models for Natural Language Processing (NLP), we\ndesign a time-series foundation model for forecasting whose out-of-the-box zero-shot performance\non a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting\nmodels for each individual dataset. Our model is based on pretraining a decoder style attention\nmodel with input patching, using a large time-series corpus comprising both real-world and synthetic\ndatasets. Experiments on a diverse set of previously unseen forecasting datasets suggests that the\nmodel can yield accurate zero-shot forecasts across different domains, forecasting horizons and\ntemporal granularities.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_1/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_2/ae4d1e94-0368-4737-912a-49816cffe752.png",
                            "./screenshots-images-2/chapter_1/section_2/8d1a2eba-bced-4110-8c0e-f8184aac8dd8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "1 Introduction\n\nTime-series data is ubiquitous in various domains such as retail, finance, manufacturing, healthcare and natural\nsciences. In many of these domains, one of the most important use-cases of time-series data is forecasting. Time-series\nforecasting is critical to several scientific and industrial applications, like retail supply chain optimization, energy and\ntraffic prediction, and weather forecasting. In recent times, Deep learning models [SFGJ20, OCCB 19] have emerged as a\npopular approach for forecasting rich, multivariate, time-series data, often outperforming classical statistical approaches\nsuch as ARIMA or GARCH [BJ68]. In several forecasting competitions such as the M5 competition [MSA22] and\nIARAI Traffic4cast contest [KKN*21] deep network based solutions performed very well.\n\nAt the same time, we are witnessing a rapid progress in the Natural Language Processing (NLP) domain on large\nfoundation models for downstream NLP tasks. Large language models (LLMs) are growing in popularity because they\ncan be used to generate text, translate languages, write different kinds of creative content, and answer your questions in\nan informative way [RWC* 19]. They are trained on massive amounts of data, which allows them to learn the patterns\nof human language. This makes them very powerful tools that can be used for a variety of downstream tasks, often in a\nzero-shot learning mode.\n\nThis motivates the question: \u201cCan large pretrained models trained on massive amounts of time-series data learn temporal\npatterns that can be useful for time-series forecasting on previously unseen datasets?\u201d In particular, can we design a\ntime-series foundation model that obtains good zero-shot out-of-the-box forecasting performance ? Such a pretrained\ntime-series foundation model, if possible, would bring significant benefits for downstream forecasting users in terms\nof no additional training burden and significantly reduced compute requirements. It is not immediately obvious that\nsuch a foundation model for time-series forecasting is possible. Unlike in NLP, there is no well defined vocabulary or\ngrammar for time-series. Additionally, such a model would need to support forecasting with varying history lengths\n(context) , prediction lengths (horizon) and time granularities. Furthermore, unlike the huge volume of public text data\nfor pretraining language models, vast amounts of time-series data is not readily available. In spite of these issues, we\nprovide evidence to answer the above question in the affirmative.\n\n\nIn particular, we design TimesFM, a single foundation model for time-series forecasting that, when applied to a variety\nof previously-unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy\n(compared to the best supervised models trained individually for these datasets). Our model can work well across\ndifferent forecasting history lengths, prediction lengths and time granularities at inference time. The key elements of\nour foundation model are twofold: 1) a large-scale time-series corpus built using both real-world (mostly time-series\ndata from web search queries! and Wikipedia page visits\u201d) and synthetic data, which meets the volume and diversity of\ndata needed for training our foundation model, and 2) a decoder style attention architecture with input patching, that\ncan be efficiently pre-trained on this time-series corpus.\n\nCompared to the latest large language models, our time-series foundation model is much smaller in both parameter\nsize (200M parameters) and pretraining data size (O(100B) timepoints); yet we show that even at such scales, it is\npossible to pretrain a practical foundation model for forecasting whose zero-shot performance comes close to the\naccuracy of fully-supervised approaches on a diverse set of time-series data. Our work also suggests that unlike recent\nwork [GFQW23] that recommends Large Language Models such as GPT-3 and LLama-2 as out-of-the-box zero-shot\nforecasters, foundation models trained from scratch exclusively on time-series data can obtain much better zero-shot\nperformance at a tiny fraction of its costs.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.3,
                        "section_name": "Related Work",
                        "section_path": "./screenshots-images-2/chapter_1/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_3/3eb0e53d-a1fe-4048-ae3a-24f7cdf5e72e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In the last decade, deep learning models [SFGJ20, OCCB19] have emerged as powerful contenders in forecasting\ntime-series in the presence of large training datasets and have been shown to outperform traditional statistical methods\nsuch as ARIMA and Exponential smoothing [McK84]. Forecasting models can be categorized broadly into: (i)\nLocal univariate models that include traditional methods like ARIMA, exponential smoothing [McK84] and non-\nautoregressive models like Prophet [TL18]. These models are trained individually for each time-series in a dataset in\norder to predict the corresponding time-series\u2019s future. (ii) Global univariate models like DeepAR [SFGJ20], Temporal\nConvolutions [BBO17], N-BEATS [OCCB19] and long-term forecasting models such as [NNSK22, DKL* 23] that are\ntrained globally on many time-series but during inference they predict the future of a time-series as a function of its own\npast and other related covariates. (iii) Global multivariate models that take in the past of all time-series in the dataset to\npredict the future of all the time-series. Such models include the classical VAR model [ZW06] as well as deep learning\nmodels like [SYD19, ZMW+ 22, CLY*23] to name a few.\n\nAll the works cited above have primarily been applied in the supervised setting with the notable exception of\nPatchTST [NNSK22] and N-BEATS [OCCB19]. PatchTST has a section on dataset-to-dataset transfer learning\nin the semi-supervised setting. [OCCB21] also show that the N-BEATS architecture lends itself to transfer learn\nbetween various source-target dataset pairs. However, none of these works aim to train a single foundation model that\ncan work on a plethora of datasets. For an in-depth discussion about transfer learning in time-series we refer the reader\nto the survey in [MLZ*23].\n\nThere has been some very recent work on re-using or fine-tuning large language models for time-series forecasting. In\nparticular, [GFQW23] benchmarks pretrained LLMs like GPT-3 and LLaMA-2 for zero-shot forecasting performance.\nAs we show later, our model obtains much superior zero-shot performance at a tiny fraction of these model sizes.\n[ZNW+23] and [CPC23] show how to fine-tune a GPT-2 [RWC* 19] backbone model for time-series forecasting tasks.\nWith the exception of a transfer-learning study (forecasting on a target dataset after having trained on a source dataset),\nthese papers mostly focus on fine-tuning a pretrained model on target datasets, and not on pretraining a single foundation\nmodel with good out-of-the box zero-shot performance on a variety of datasets. To the best of our knowledge, the very\nrecent work in TimeGPT-1 [GMC23] is the only other parallel work on a zero-shot foundation model for time-series\nforecasting. However the model is not public access, and several model details and the benchmark dataset have not\nbeen revealed.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.4,
                        "section_name": "Problem Definition",
                        "section_path": "./screenshots-images-2/chapter_1/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_4/1bb5b460-2beb-40e7-9483-9f40d5623d85.png",
                            "./screenshots-images-2/chapter_1/section_4/eb20cf9b-0b2e-4546-9d5f-c53eb112ee89.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The task at hand is to build a general purpose zero-shot forecaster that takes in the past C' time-points of a time-series as\ncontext and predicts the future H time-points. Let the context be denoted by yi. := {y1,-+: , yx} where we follow a\nnumpy-like notation for indices. Similarly the actual values in the horizon are denoted by y,,;.1,, 4. Note that since\nwe are building a single pre-trained model, we cannot have dataset specific dynamic or static covariates during training\ntime. The task is then to learn a foundation model that can map any time-series context to horizon,\n\nfs (Yut) \u2014 \u00a5esit+H- ()\n\nThe accuracy of the prediction can be measured by a metric that quantifies their closeness to the actual values, for\ninstance, Mean Absolute Error (MAE) defined in Equation 6.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.5,
                        "section_name": "Model Architecture",
                        "section_path": "./screenshots-images-2/chapter_1/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_5/ba6fd0b7-b6d8-4a40-a2c7-0eae350f2881.png",
                            "./screenshots-images-2/chapter_1/section_5/cda2207a-1a0a-4c99-bd6d-a85d8ab71fc3.png",
                            "./screenshots-images-2/chapter_1/section_5/17b9b091-3bb1-45ca-876d-1231ae308b20.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "4 Model Architecture\n\nA foundation model for time-series forecasting should be able to adapt to variable context and horizon lengths, while\nhaving enough capacity to encode all patterns from a large pretraining datasets. Transformers have been shown to be\nable to adapt to different context lengths in NLP [RWC* 19]. However, there are several time-series specific design\nchoices. The main guiding principles for our architecture are the following:\n\nPatching. Inspired by the success of patch based modeling in the recent long horizon forecasting work [NNSK22] we\nalso choose to break down the time-series into patches during training. A patch of a time-series is a natural analogue for\na token in language models and patching has been shown to improve performance. Moreover this improves inference\nspeed as the number of tokens being fed into the transformer is reduced by a factor of the patch length. On the other\nhand, increasing the patch length all the way to the context length moves us away from decoder-only training and the\nefficiencies that come with it. We delve into this further in Section 6.2.\n\nDecoder-only model. A key difference between our architecture and PatchTST [NNSK22] is that our model is trained\nin decoder-only mode [LSP* 18]. In other words, given a sequence of input patches, the model is optimized to predict\nthe next patch as a function of all past patches. Similar to LLMs this can be done in parallel over the entire context\nwindow, and automatically enables the model to predict the future after having seen varying number of input patches.\n\nLonger output patches. In LLMs the output is always generated in an auto-regressive fashion one token at a time.\nHowever, in long-horizon forecasting it has been observed that directly predicting the full horizon yields better accuracy\nthan multi-step auto-regressive decoding [ZCZX23]. But this is not possible when the horizon length is not known\napriori, as in the case of zero-shot forecasting which is our primary goal.\n\nWe propose a middle ground by allowing our output patches for prediction to be longer than the input patches. As\nan example, suppose the input patch length is 32 and output patch length is 128. During training, the model is\nsimultaneously trained to use the first 32 time-points to forecast the next 128 time-steps, the first 64 time-points to\nforecast time-steps 65 to 192, the first 96 time-points to forecast time-steps 97 to 224 and so on. During inference,\nsuppose the model is given a new time-series of length 256 and tasked with forecasting the next 256 time-steps into\nthe future. The model will first generate the future predictions for time-steps 257 to 384, then condition on the initial\n256 length input plus the generated output to generate time-steps 385 to 512. On the other hand, if in a model the\noutput patch length was fixed to the input patch length of 32, then for the same task we would have to go through 8\nauto-regressive generation steps instead of just the 2 above. However, there is a trade-off. If the output patch length\nis too long, then it is difficult to handle time-series whose lengths are less than the output patch length for instance\nmonthly, yearly time-series in our pretraining data.\n\nPatch Masking. If we use patches naively, the model might only learn to predict well for context lengths that are\nmultiples of the input patch length. Therefore we make a careful use of masking during training. Parts of patches as\nwell as entire patches from the beginning of the context window can be masked in a data batch. We employ a specific\nrandom masking strategy (described later) during training that helps the model see all possible context lengths starting\nfrom | to a maximum context length.\n\nNow that we have mentioned the guiding principles, we next formally describe each component of our model architecture\n(illustrated in Figure 1), which we name as TimesFM (Time-series Foundation Model).\n\nInput Layers. The job of the input layers is to preprocess the time-series into input tokens to the transformer layers.\nWe first break the input into contiguous non-overlapping patches. Then each patch is processed by a Residual Block\ninto a vector of size model_dim. Along with the input, we also supply a binary padding mask m).;, where 1 denotes\nthat the corresponding input in y,.,, should be ignored and vice-versa. The Residual Block is essentially a Multi-layer\nPerceptron (MLP) block with one hidden layer with a skip connection, similar to that defined in [DKL~ 23].\n\nIn other words, the inputs y;.;, are broken down into patches of size input_patch_len (p). The j-th patch can be\ndenoted as \u00a5; = Yp(j\u20141)+1:p;- Similarly the mask can also be patched as mj = Mp(j\u20141)+1-p;- Then the j-th input\ntoken to the subsequent transformer layers can be denoted as,\n\nt; = InputResidualBlock(y, \u00a9 (1 \u2014 mj)) + PE; (2)\n\nwhere PE; denotes the j-th positional encoding as defined in the original transformer paper [VSP~ 17]. There will be\nN = |L/p| such input tokens.\n\nStacked Transformer. The bulk of the parameters in our model are in num_layers (n;) transformer layers stacked on\ntop of each other. Each of these layers have the standard multi-head self-attention (SA) followed by a feed-forward\n\n(same\nResidual Block network)\n\nx num Layers\n\ninput_patch_len=32 output_patch_len=128\n\nFigure 1: We provide an illustration of the TimesFM model architecture during training, where we show a input\ntime-series of a specific length that can be broken down into input patches. Each patch along is processed into a vector\nby a residual block (as defined in the model definition) to the model dimension of the transformer layers. The vector is\nthen added to positional encodings and fed into n, stacked transformer layers. SA refers to self-attention (note that we\nuse multi-head causal attention) and FFN is the fully connected layer in the transformer. The output tokens are then\nmapped through a residual block to an output of size output _patch_len, which is the forecast for the time window\nfollowing the last input patch seen by the model so far.\n\nnetwork (FFN). The main hyperparameters are model_dim which is equal to the dimension of the input tokens t ;\u2019s and\nnumber of heads (num_heads). We set the hidden size of the FFNs to be equal to mode1_dim as well. We use causal\nattention that is each output token can only attend to input tokens that come before it in the sequence (including the\ncorresponding input token). This can be described by the equation\n\n0; = StackedTransformer((t),171),--+ ,(t;,\u2122my;)), (3)\n\nfor all j \u20ac [N]. rn; is the masking indicator for the j-th token defined as min{m,(j\u20141)+1:p;} ie if a patch has\nany non-masked time-point the corresponding token marked as not being masked. All patches that are masked out\ncompletely are not attended to by the causal self attention.\n\nOutput Layers. The remaining task is to map the output tokens into predictions. We train in decoder only mode\ni.e each output token should be able to be predictive of the part of the time-series that follows the last input patch\ncorresponding to it. This is common for popular large language models like [RWC* 19]. However, one key difference\nin our time-series foundation model is that input patch length need not be equal to output patch length i.e we should be\nable to predict a larger chunk of the time-series based on the encoded information from the input patches seen so far.\nLet the output patch length be output _patch_len (h). We use another Residual Block to map the output tokens to the\npredictions. This can be described as,\n\nYpj+ipj+h = OutputResidualBlock(o,). (4)\nThus we encode all the data in y).\u00bb; into 0; and use that to predict the subsequent /h time-points ypj+1:\u00bbj+n- This is\ndone for all patches in one training mini-batch.\n\nLoss Function. In this work, we focus on point forecasting. Therefore we can use a point forecasting loss during\ntraining like Mean Squared Error (MSE). The loss that is minimized during training can be expressed as,\n\nN\n; 1 re\nTrainLoss = NV } . MSE(\u00a5pj+.:pj+hs\u00a5pj+1:pj+h)- (5)\nj=\n\nNote that if one is interested in probabilistic forecasting, then it is easy to have multiple output heads for each output\npatch, each head minimizing a separate quantile loss as in [WTNM17]. Another approach can be to output the logits of\n\na probability distribution family and minimize the maximum likelihood loss for probabilistic forecasting [ADSS21,\nSFGJ20].\n\nTraining. We train the model with standard mini-batch gradient descent in decoder-only fashion, that goes through all\nwindows for a time-series and across time-series. The only non-standard part is the way we sample the mask during\ntraining. For each time-series in the batch, we sample a random number r between 0 and p \u2014 1. Then we set the\nm)., = 1 and the rest as zero i.e we mask out a fraction of the first input patch. However, this is sufficient to cover all\ninput context lengths from | to the maximum training context length. We explain this using an example below:\n\nSuppose the maximum context length is 512 and p = 32. Then if r = 4, the output prediction after seeing the first\npatch (from 0 ) is optimized to predict after seeing 28 = 32 \u2014 4 time-points, the output of the next patch (from 02) is\noptimized to predict after seeing 28 + 32 time-points, and so on. When this argument is repeated for all such r\u2019s, the\nmodel has seen all possible context lengths till 512.\n\nInference. The trained network can be used to produce forecasts for any horizon using auto-regressive decoding similar\nto large language models. Given an input y).,, (assume L is a multiple of p for simplicity) it can first predict \u00a5p41.1+h-\nThen, we can use the concatenated vector \u00a51.14 = [Y1:13\u00a51+1:1+A] as an input to the network to generate the next\noutput patch prediction \u00a5z441:1421 and so on. If L is not a multiple of p, we simply append zeros to make it a\nmultiple of p and mark the corresponding entries in the mask as 1.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.6,
                        "section_name": "Pretraining Details",
                        "section_path": "./screenshots-images-2/chapter_1/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_6/b5ab008e-a845-402c-b1ec-6c7ae0b77b22.png",
                            "./screenshots-images-2/chapter_1/section_6/9e3d4356-473d-4462-aa83-0cec28565973.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "5 Pretraining Details\n\nWe would like our pretraining corpus to include large volumes of temporal data representing a variety of domains, trend\nand seasonality patterns and time granularities that ideally capture the forecasting use-cases which we are interested in\nserving by the deployed model. It is challenging to find a large time-series dataset that meets the volume and diversity\nof data needed for training our foundation model. We address this problem by sourcing the bulk of data used to train\nour models from three major sources: Google trends, Wiki Pageview statistics and synthetic time-series. In summary\nthe main data sources are:\n\nGoogle Trends. Google Trends > captures search interest over time for millions of queries. We choose around 22k head\nqueries based on their search interest over 15 years from 2007 to 2022. Beyond these head queries the time-series\nbecome more than 50% sparse. We download the search interest over time for these queries in hourly, daily, weekly and\nmonthly granularities to form our dataset. The date ranges are Jan. 2018 to Dec. 2019 for hourly and Jan. 2007 to Dec.\n2021 for the other granularities. The trends datasets amounts to roughly 0.5B time-points.\n\nWiki Pageviews. Wiki Pageviews * captures the hourly views of all Wikimedia pages. We download all pageview data\nfrom Jan. 2012 to Nov. 2023, clean and aggregate the views by page into hourly, daily, weekly and monthly granularities,\nand filter out pageview time-series with excessive zeros. The final corpus contains roughly 300B time-points.\n\nSynthetic Data. Another major component of our pretraining data is of synthetic origin. We create generators for\nARMA [McK84] processes, seasonal patterns (mixture of sines and cosines of different frequencies), trends (linear,\nexponential with a few change-points) and step functions. A synthetic time-series can be an additive combination of\none or more of these processes. We create 3M synthetic time-series each of length 2048 time-points. More details about\nour synthetic data generation are presented in Appendix A.8.\n\nOther real-world data sources. Along with the wiki and trends data, we also add time-series from several other publicly\navailable datasets to our pretraining corpus. We add all the granularities of the M4 dataset [MSA22], the hourly and 15\nminute Electricity and the hourly Traffic datasets (see [ZZP*21]). We also add the 10-minute granularity Weather\ndataset used for evaluations in [ZZP*21]. M4 has a good mix of granularities with around 100k time-series in total.\nTraffic and Electricity are large long-term forecasting datasets with > 800 and > 300 time-series each having tens of\nthousands of time-points. In addition, we add all the 15 min granularity traffic time-series from [WJJ* 23].\n\nDataset Mixing and Training. We train on a mixture distribution over these datasets that aims to give sufficient weight\nto all granularities and datasets. The training loader samples 80% real data and 20% synthetic, with the real data\nmixture providing equal weights to the groups: hourly + sub-hourly, daily, weekly, and monthly datasets. We train with\na maximum context length of 512 whenever the length of the time-series allows that. For weekly granularity we do not\nhave sufficiently long time-series; therefore a maximum context length of 256 is used. For the same reason, a maximum\ncontext length of 64 is used while training on > monthly granularity data. We also use only the standard normalization\npart of reversible instance normalization [KKT+21] \u2014 i.e, the context of each time-series is scaled by the context mean\nand standard deviation of the first input patch in the context.\n\n\u2018https: //trends. google.com\n\u201cnttps://en. wikipedia. org/wiki/Wikipedia:Pageview_statistics\n\nTable 1: Composition of TimesFM pretraining dataset.\n\nDataset Granularity #Time series # Time points\nSynthetic 3,000,000 6,144,000,000\nElectricity Hourly 321 8,443,584\nTraffic Hourly 862 15,122,928\nWeather [ZZP*21] 10 Min 42 2,213,232\nFavorita Sales Daily 111,840 139,179,538\nLibCity [WJJ+23] 15 Min 6,159 34,253,622\nM4 hourly Hourly 414 353,500\nM4 daily Daily 4,227 9,964,658\nM4 monthly Monthly 48,000 10,382,411\nM4 quarterly Quarterly 24,000 2,214,108\nM4 yearly Yearly 22,739 840,644\nWiki hourly Hourly 5,608,693  239,110,787,496\nWiki daily Daily 68,448,204 \u2014115,143,501,240\nWiki weekly Weekly 66,579,850 \u2014 16,414,251,948\nWiki monthly Monthly 63,151,306 3,789,760,907\nTrends hourly Hourly 22,435 393,043,680\nTrends daily Daily 22,435 122,921,365\nTrends weekly Weekly 22,435 16,585,438\nTrends monthly Monthly 22,435 3,821,760\nYou \u00a5 aes\nPL PELLE SO \u00a9, ge e Po \u00a3 ge oe pS Sf ra\nLEE LOE a 0 Pe ae 4 of oo\n(a) Monash Archive [GBW* 21) (b) Darts [HLP* 22) (c) ETT (Horizons 96 and 192) [ZZP*21)}\n\nFigure 2: We report average performance in three groups of datasets. In all figures, the lower the metric the better and\nthe error bars represent one standard error. Note that among the baselines only TimesFM and IImtime are zero-shot. In\n(a) we report results on the Monash datasets. Since the datasets have different scales, we take the Geometric Mean\n(GM) of the MAE\u2019s scaled by the MAE of a naive baseline. We can see that TimesFM is the top model. In (b), we\nreport the similarly scaled MAE on the Darts benchmarks. TimesFM is within significance of the best performing\nmethods which are ARIMA and IImtime in this case. Note that these datasets have one time-series each and therefore\nstatistical methods are competitive with deep learning ones. Finally, in (c) we report the average MAE for 96 and 192\nhorizon prediction tasks on 4 ETT datasets i.e 8 tasks in total. TimesFM and PatchTST are the best performing models\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.7,
                        "section_name": "Empirical Results",
                        "section_path": "./screenshots-images-2/chapter_1/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_7/bdfaf373-2182-467e-bb82-87014508b91f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "6 Empirical Results\n\nWe evaluate our model in zero-shot settings on three groups of well known public datasets against the best performing\nbaselines for each group. These datasets have been intentionally held out from our pretraining data. We show that a\nsingle pretrained model can come close or surpass the performance of baselines models on the benchmarks even when\nthe baselines are specially trained or tuned for each specific task. Subsequently, we perform ablation studies that justify\ndifferent choices made in our architecture.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.8,
                        "section_name": "Zero-shot Evaluation",
                        "section_path": "./screenshots-images-2/chapter_1/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_8/b5f28942-9033-4d93-b0c4-d1b80b93cb37.png",
                            "./screenshots-images-2/chapter_1/section_8/0af38be4-32af-458d-9006-f1260e74149a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "6.1 Zero-shot Evaluation\n\nTo benchmark our model\u2019s performance, we choose three groups of commonly used forecasting datasets that cover\nvarious domains, sizes, granularities, and horizon lengths: Darts [HLP*22], Monash [GBW*21] and Informer\ndatasets [ZZP~ 21], to test the generalization power of our foundation model against other baselines.\n\nIn all cases, we report performance on the official metrics and scalings of the datasets, using either their standard test\nsplits or common test splits in other literature. We present a summary of the results below - more details can be found\nin Appendix A.5. We provide the hyper-parameters and other details about our model in Appendix A.6.\n\nMonash [GBW*21]. Monash archive is a collection of 30 datasets of different training and prediction lengths that\ncovers granularities ranging from minutes to years and domains including finance, demand forecasting, weather and\ntraffic. The archive reports four official metrics for several statistical baselines such as Exponential Smoothing(ETS) and\nARIMA, as well as supervised ML baselines like CatBoost [PGV* 18], DeepAR [SFGJ20] and WaveNet [ODZ* 16].\nFollowing Ilmtime [GFQW23] we start from the Monash Huggingface repository * and filter out the datasets that\ncontain missing values. This leaves us with 18 datasets which we specify in Appendix A.5.2.\n\nOut of the four official metrics, following prior work [GFQW23], we report our performance in terms of mean MAE\n(see Appendix A.2). As the datasets have massively different scales, for each dataset we normalize the metric by the\nmetric achieved by a naive baseline that just constantly predicts the last value in the context for each time-series. Then\nthe scaled MAE\u2019s are averaged across all datasets. The scaled aggregation was also used in [GFQW23]. In Figure 2a,\nwe use the Geometric Mean (GM) for averaging since it is more robust for normalized metrics [FW86]. We also report\nthe Arithmetic Mean based aggregated metrics in Figure 4 in the appendix.\n\nThe mean scaled MAE across all datasets is plotted in Figure 2a along with standard error bars. We compare the\nperformance of TimesFM with the baseline models implemented in Monash, and the zero-shot IImtime [GFQW23]\nmodel that uses GPT-3 [RWC* 19] with a specific prompting technique. Note that the zero-shot models are marked as\n(Zero-Shot). TimesFM is the top model even though we never trained on these datasets. It is slightly better but within\nsignificance of N-BEATS but outperforms deep supervised models like DeepAR [SFGJ20], and improves on Ilmtime\u2019s\nperformance by more than 25%.\n\nDarts [HLP*+22]. This is a collection of 8 univariate datasets which include interesting seasonalities and addi-\ntive+multiplicative trends. We report performance of several baselines implemented in the Darts package like\nTCN [LVRH16], N-HiTS [COO*23] and N-BEATS [OCCB19]. All these baselines are supervised. As before,\nwe also report zero-shot forecasting results from IImtime [GFQW23] using GPT-3 [RWC*19]. Other supervised\nbaselines in [GFQW23] like SM-GP [WA13] and ARIMA [McK84] are also added.\n\nWe report the official metric for this dataset group that is MAE for each individual dataset in Appendix A.5. In Figure 2b,\nwe present the average scaled MAE across all 8 datasets, as we did for the Monash datasets. TimesFM is within\nstatistical significance of the best models that is IImtime and seasonal ARIMA in this case. Note that since there are\nonly 8 individual time-series in this dataset group, the standard errors are not sharp and therefore does not provide a\nclear ordering among the models. Also, note that for ARIMA, the seasonality needs to be encoded correctly in the\nparameters for the best results, which needed manual tuning. Further, since these datasets are used in numerous time\nseries blog posts for illustrative purposes, data contamination for IImtime cannot be ruled out.\n\nInformer [ZZP~ 21]. The Informer datasets have been widely used for benchmarking various supervised long-horizon\nforecasting methods. A few of these datasets are used in pretraining, so we focus on the other datasets in this collection\n(ETTm1, ETTm2. ETTh1 and ETTh2) related to electricity transformer temperatures over a two year period in 1 hour\nand 15 minutes granularities. Note that the long horizon baselines usually report rolling validation results on the test set\nwhich would amount to millions of tokens for evaluating IImtime [GFQW23] and would be too expensive. Therefore,\nfollowing Ilmtime, we compare all methods on the last test window. Also, it is reasonable to directly average the\nMAE for these datasets since the results are reported on standard normalized dataset (using the statistics of the training\nportion).\n\nWe consider the task of predicting horizon length 96 and 192, given a context length of 512 for all methods. The MAE\naveraged over all 8 tasks (4 datasets with two horizons each) is presented in Figure 2b. TimesFM performs the best\nand the supervised PatchTST [NNSK22] baseline (which is a state-of-the-art long horizon deep forecasting method) is\nwithin significance of it. The other long horizon methods are quite a bit worse even though they have been trained these\ndatasets. IImtime is better than FEDFormer but worse than PatchTST with statistical significance.\n\nWe present visual examples of our forecasts along with baselines in Appendix A.9.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.9,
                        "section_name": "Ablation",
                        "section_path": "./screenshots-images-2/chapter_1/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_9/0a0bd9a9-5925-43ad-8050-dfa48fe3ce25.png",
                            "./screenshots-images-2/chapter_1/section_9/37c4ef83-794c-429e-b9af-695120c92ea8.png",
                            "./screenshots-images-2/chapter_1/section_9/db9540dd-f6b9-4397-a28f-3258b68018bd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "6.2 Ablation\nNext, we perform several ablation studies that inform the design decisions we made for our model architecture.\n\nScaling. Performance curves with respect to number of parameters in a model have been a keenly studied area in\nthe context of LLMs. [KMH*~ 20] established a power law like relationship between the number of parameters in a\nlanguage model and its downstream performance i.e the more the number of paramaters the better the performance.\n\nSnttps: //huggingface.co/datasets/monash_tsf\n\nScaled MAE 16M)\nss = 8\n\n0.70\n088 \u2014.\n10) Pa\n\nTWraFLOPS top. scale)\n\n(a) Scaled MAE (GM) on Monash datasets as a func-\ntion of FLOPS across three model sizes 17M, 70M\nand 200M. The first 4 points are from 17M and 70M\ncheckpoints while the last 3 are from 200M.\n\n2 ry e cy abo we\noutput patch len\n(b) Ablation with respect to output patch length for\nthe task of predicting 512 steps into the future on\nETT datasets on the original test set in [ZZP~ 21].\nWe report the average across all 4 ETT datasets.\n\n0.50\nos mm 20% synthotic\nfmm No synthetic\noF 0.45\ng\n083 06\n\u00a7 ack\noaz! 2 40\ng05 5\noan} = fe\n_ = g\nZoa0 Boa oasg\n2 Pr Fa\n7} E03 0.208\n3078! z 2\nH B02\nSor} & 0.25\n0.76| on\n0.75{ 5\n00 Monash ET etme\n\nte a\n\ninput patch Jen\n\n- (d) Average scaled MAE for Monash on the left and\n(c) Scaled MAE (GM) for our 70M models on average MAE on ETT datasets on the right. We com-\nMonash datasets for different input patch lengths. pare the performance of 200M model with and with-\n\nWe also plot error bars denoting one standard error. out the synthetic data.\n\nFigure 3: Ablation studies with respect to various design choices.\n\nHowever, [HBM*22] established a more nuanced scaling law that lays down methods to train compute optimal models\nbased on the number of tokens available in a training dataset.\n\nWe perform a preliminary scaling study where we train three TimesFM models of sizes 17M, 70M and 200M parameters,\nusing the same pre-training dataset till 1.5M iterations with a global batch-size of 4096. Then we collect checkpoints\nthat represent varying number of FLOPS (Floating Point OPerationS) across the different model runs. Then we plot\nthe performance on Scaled MAE (GM) on Monash as a function of FLOPS, in Figure 3a. This is now a standard\nway to perform scaling studies in LLMs (see recent work like [GD23]). It can be clearly seen that the errors decrease\nmonotonically with the number of FLOPS (in log scale). All experiments were performed on a TPUvSe\u00ae setup with 16\ntensor-cores. For the 200M model it takes 2 days to complete 1.5M iterations on our setup.\n\nAutoregressive Decoding. In recent long-term forecasting works [ZCZX23, NNSK22, DKL* 23] it has been observed\nthat directly predicting the entire forecasting horizon in one shot from a decoder can yield better results than auto-\nregressive decoding on long horizon benchmarks. For a foundation model, the horizon length of the task is not known\nbefore inference time, therefore one-shot decoding might not be possible for very long horizons. However, as mentioned\nearlier, by keeping the output __patch_len longer than input_patch_len one can ensure fewer autoregressive steps.\nThis was one of the key decisions in the design of TimesFM, that is quite different from LLMs. In order to showcase\nthis we choose the task of predicting 512 time-steps into the future for the ETT datasets on the original rolling validation\ntask of the ETT test sets [ZZP*21]. In Figure 3b, we present results from models with output_patch_len varying from 8\nto 128. We see a monotonic decrease in average MAE with output_patch_len.\n\nInput Patch Length. The size of input_patch_len represents an important trade-off. We have typically seen that\nincreasing its value from 8 to 32 increases performance but having too high a input_patch_len is impractical since\nthat makes the model shift from decoder only training more towards encoder-decoder style training. Note that in the\n\n\u201chttps: //cloud.google.com/tpu/docs/v5e-training\n\n\"Training\" paragraph of Section 4, we describe the mask sampling strategy to support any context length. If in the\nextreme case p is set the maximum context length we have to individually sample all possible context windows from 1\nto maximum context length, which would be required for encoder-decoder style of training.\n\nIn Figure 3c, we show the mean scaled MAE (GM) TimesFM(ZS) - 70M model on Monash with input_patch_len\nvarying from 8 to 128. Note that both models have been trained to about 1.5M steps even though the p=8 model is three\ntimes slower to train. We can see that p = 16, 32 marks the best performance, with the error increasing towards either\nend. Note that p = 32 model is almost twice as fast to train compared to p = 16 and thus constitutes a prudent choice.\n\nDataset Ablation. Next we showcase the need for synthetic data. Intuitively, the majority of our real datasets have\ncommonly found granularities like hourly, daily etc which have specific periodic patterns like 24 time-point period for\nhourly data. This can make the model not generalize well to underrepresented frequencies. We train a 200M model\nwith no synthetic data added in the mix and showcase the performance on Monash and ETT datasets in Figure 3. It can\nbe seen that there is a performance drop on Monash because many of the datasets in Monash have under-represented\ngranularities like quarterly, yearly or 10 minutes etc. Perhaps even more compelling is the comparison on ETT datasets.\nWe can see that there is almost no difference between the two models on the hourly ETTh datasets which has a well\nrepresented granularity. However, for the 15min ETTm datasets the model with synthetic data performs quite a bit\nbetter.\n\nWe provide a finetuning study in the same setting as [ZNW* 23] in Appendix A.3, where our model performs better\nthan all baselines on all the reported datasets. This shows the utility of our model on downstream tasks.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.1,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_1/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_10/18cf8ddb-6b89-4f32-9efb-e2ed00c08dcc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "7 Conclusion\n\nIn this paper, we presented TimesFM, a practical foundation model for forecasting whose zero-shot performance comes\nclose to the accuracy of fully-supervised forecasting models on a diverse set of time-series data. This model is pretrained\non real-world and synthetic datasets comprising O(100B) timepoints. We discuss limitations and future work in more\ndetail in Appendix A.1.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.11,
                        "section_name": "Impact Statement",
                        "section_path": "./screenshots-images-2/chapter_1/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_11/a91622ca-8f7b-403f-8eb7-7bc19131b2f7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "8 Impact Statement\n\nThis paper shows that it is possible to train a single pretrained model that has phenomenal zero-shot performance on a\nvariety of forecasting tasks, thus opening up exciting possibilities for downstream applications. Therefore it is crucial to\ndiscuss ethical and societal considerations of using such a model and how some of the related concerns can be mitigated.\n\nData Privacy. Note that most of our data sources are publicly available and are aggregated i.e no individual user activity\nconstitutes a time-point. Further the Google Trends data is differentially private.\n\nBias. Biases can creep into a foundation model through a variety of sources especially through data. The model might\nperpetuate these biases in forecasts, leading to unfair outcomes. Biased forecasts can have real-world consequences. For\nexample, a biased forecast of crime rates in a neighborhood could lead to increased police presence, disproportionately\nimpacting certain communities. Note that our model is not trained with any covariates so some of these sensitivities are\nreduced but cannot be ruled out.\n\nIn this regard, we believe that it is best to release the exact details of the datasets used for training and therefore we\nhave summarized our data sources in Table 1. Moreover we plan to do a open weights release of our model so that the\ncommunity can analyze the model for downstream tasks. We will ensure that we release a good model card [MWZ* 19].\nThis will also aid in finetuning the model with more diverse data-sources.\n\nTraining cost. Our largest model has 200M parameters which is much smaller compared to even smaller scale LLMs.\nOur data volume is quite large but still smaller compared to SOTA LLMs. We have revealed the exact computation\nrequirements for the models i.e 16 core TPUvSe for 2 days. Note that experimentation and trial runs of course cost\nmore than the final model run. Therefore in the interest of equitability we would like to release the weights of our\nmodel in a responsible manner.\n\nLastly we would like to note that similar to LLMs there could be inputs on which the model does not perform well or\nhallucinates. Therefore in many critical use cases it might be recommended to use the model in a human-in-the-loop\nfashion or alternatively do a wide range of testing or finetuning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.12,
                        "section_name": "Limitations and Future Work",
                        "section_path": "./screenshots-images-2/chapter_1/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_12/bb8e1d27-b49d-41c1-9035-d959c2e35732.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A.1 Limitations and Future Work\n\nOur work shows that we can train a 200M parameter pretrained forecasting model that has impressive zero-shot\nperformance on a variety of real world forecasting benchmarks with different context and horizon lengths. In this\nsection we would like to discuss limitations and future work.\n\nPrompt Tuning. In LLMs it is well known that prompt tuning techniques like chain-of-thought [WWS* 22] can\ndrastically improve performance in cases where the model is inaccurate with simple prompts. Such techniques are\nless clear for time-series foundation model. We can tune simple hyper-parameters like context length as the moment.\nHowever, with probabilistic forecasting we might be able to output different statistics as well as come up with techniques\nthat align more with user\u2019s expectations while not decreasing likelihood.\n\nProbabilistic Forecasting. It should be straightforward to train with probabilistic loss functions in our framework as\ndetailed in the \"Loss Function\" part of Section 4. However, being one of the first works of building a single foundation\nmodel for forecasting, this was not our main focus and is left to future explorations. Note that as mentioned before we\nplan to release our model weights and after that such loss functions [SFGJ20, ADSS21] can be added during finetuning.\n\nCovariate handling. Currently the model is not pretrained with covariates as one of the key challenges is finding large\nvolumes of pretrained data with meaningful covariates (apart from date features). We also need methods to have a joint\nuniversal representation of covariates. Currently there are two simple techniques we can think of for handling covariates\n(i) In a zero-shot setting at inference time we can predict in-context and linearly regress the residual on covaraites. Then\nour model + the residual model can be used for forecasting in the horizon. (ii) during finetuning it is straightforward to\nhandle covariates by adding them as inputs to the input and output residual blocks. Categorical variables can be added\nas embeddings.\n\nMore finetuning studies. We perform a fintuning study in Appendix A.3 following a prior work. However, a more in\ndepth study that involves finetuning in the presence of covariates would be beneficial. This being one of the first works\nof building a single foundation model for forecasting, this was not our main focus and is left to future explorations.\nIdeas in recent work such as [CLY*+23] could be useful in this regard.\n\nOther architectures. Given the cost of training foundation models we did not perform much hyper-parameter tuning\nin our pretraining, while following some well established best practices for training transformers. In a similar vein,\nit would also be interesting to try out alternatives like the exciting directions of all MLP structures like [CLY*23] or\nefficient linear state space models like Mamba [GD23] (and references there in).\n\nInterpretability. Deep foundation models trained on a huge corpuses of data could be inherently less interpretable\ncompared to statistical methods like ARIMA, ETS [BJ68]. In this regard methods like LOCO, SHAP (see [VW23]\nand references there in) could be used to some extent to attribute feature importances to different lags in the context\nsupplied to the model. However, this does not solve the problem to a full extent and one of the best things to do would\nbe to open source a version of the model with a proper model card. [MWZ* 19].\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.13,
                        "section_name": "Metrics",
                        "section_path": "./screenshots-images-2/chapter_1/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_13/4c4eb13d-375a-42cc-ba2b-31830699450c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A. Metrics\nThe metrics that are used for reporting results in this paper are:\n* MAE [GBW~ 21]\n\nMAE(\u00a5p41:14H)\u00a5i41L4+H) = ql\u00a5enatH _ Vrsicsully- (6)\n+ msMAPE [GBW* 21]\nl\u00a5 2lynsi ~ desi\nmsMAPE(y141:2+4,YL+uL+H) = H > max(ond + Wand te05 49\" (7)\n\ni=l\n\nIn Monash benchmarks [GBW~ 21] \u20ac = 0.1 was used. This metric is used in order to avoid undefined values\nin other normalized metrics like MAPE. In multivariate datasets the metrics are calculated for each time-series\nand then we take the mean or the median. In this paper we only use the mean versions.\n\nAggregating across datasets. Since the datasets have wildly different scales averaging unnormalized metrics like MAE\nis not kosher. Therefore following [GFQW23] we scale the metric of each baseline for a dataset by the same metric\nachieved by a naive baseline on that dataset. The naive baseline just makes the constant prediction y,, repeated across\nthe prediction length. We did not need to do that for the Informer datasets since on these datasets metrics are usually\nreported on standard normalized data [NNSK22].\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.14,
                        "section_name": "Finetuning study on ETT",
                        "section_path": "./screenshots-images-2/chapter_1/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_14/3ff07798-b98e-49a3-812e-9527c9133d79.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A.3 Finetuning study on ETT\n\nIn this section, we test whether TimesFM can be fintuned on a small fraction of a dataset to provide even better\nperformance. We follow the same protocol as in GPT4TS [ZNW* 23] (see Table 13 in their paper). [ZNW* 23] finetune\nGPT2 input and output blocks on long-term forecasting benchmarks on 10% of the of the original datasets and compare\nit against models trained from scratch on the same data. Then the models are evaluated on the original test set task\nof [ZZP*21]. We also tune the input and output residual blocks on 10% of the training set and present the results in\nTable 2. We can see that our model performs the best by a large margin. In ETTh1, ETTh2, ETTm!1 our finetuned\nmodel is better than 18%, 3% and 12% better than GPT4TS, respectively. In fact we can see our 10% finetuned model\u2019s\nperformances are comparable or better than that of most baselines trained on the whole training dataset as reported in\nTable 14 of [ZNW~*23]. This shows that the inductive biases encoded in our model weights by finetuning on a large\ntime-series corpus are better for downstream forecasting task than an off the shelf language model like GPT2, even\nthough our model is orders of magnitude smaller.\n\nTable 2: MAE of different methods on ETT datasets. All methods use 10% of the original training set for training or\nfinetuning. The baseline numbers are from Table 13 in [ZZP*21].\n\nDataset | TimesFM(FT) | GPT4TS(FT) | DLinear | PatchTST | TimeNet | FEDFormer | Autoformer\n96 0.398 0.456 0.495 0.485 0.628 0.499 0.552\n192 0.424 0.516 0.538 0.524 0.593 0.555 0.598\nETThI | 336 0.436 0.535 0.622 0.550 0.648 0.574 0.619\n720 0.445 0.591 0.743 0.610 0.641 0.614 0.616\nAvg 0.426 0.525 0.600 0.542 0.628 0.561 0.596\n96 0.356 0.374 0411 0.389 0.409 0.416 0.451\n192 0.400 0.411 0.519 0.414 0.467 0.474 0.477\nETTh2 | 336 0.428 0.433 0.572 0.441 0.494 0.501 0.543\n720 0.457 0.464 0.648 0.480 0.491 0.509 0.523\nAvg 0.410 0.421 0.538 0.431 0.465 0.475 0.499\n96 0.345 0.404 0.392 0.419 0.501 0.518 0.614\n192 0.374 0.423 0.412 0.434 0.528 0.546 0.592\nETTm! | 336 0.397 0.439 0.434 0.454 0.568 0.775 0.677\n720 0.436 0.498 0.477 0.556 0.549 0.579 0.630\nAvg 0.388 0.441 0.429 0.466 0.537 0.605 0.628\n96 0.263 0.269 0.303 0.274 0.285 0.399 0.454\n192 0.309 0.309 0.345 0.317 0.323 0.379 0.691\nETTm2 | 336 0.349 0.346 0.385 0.353 0.353 0.559 1.407\n720 0.415 0.417 0.440 0.427 0.449 0.614 1.166\nAvg 0.334 0.335 0.368 0.343 0.353 0.488 0.930\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.15,
                        "section_name": "Pretraining PatchTST",
                        "section_path": "./screenshots-images-2/chapter_1/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_15/ee461bd8-0c77-4424-8069-786adf8291b5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A4_ Pretraining PatchTST\n\nSince TimesFM applies a similar patching strategy as PatchTST [NNSK22], for an ablation study we use the same\ndata loader and pretrain a PatchTST model of 200M parameters to the same number of FLOPS as the final 200M\nTimesFM model. We denote it as PatchTST(ZS). The two models share the same hyperparameters of the transformer\nstack. For PatchTST(ZS) we use the same input patch length = 32, and a stride of length half of input patch size (i.e.\nstride = 16) as done in the original PatchTST paper.\n\nWe report the detailed results on Monash and ETT in Appendix A.5.2 and A.5.3. It can be seen that the results are not\nthat good for PatchTST(ZS) on Monash. This is expected since our pretrain data loader will predominantly have context\nlengths of 512 instead of shorter context lengths as in Monash. Moreover the PatchTST model does fewer iterations at\nthe same number of FLOPS. On ETT datasets, the PatchTST(ZS) model is performs similarly to TimesFM(ZS) and\nPatchTST. This is also expected since the context length for this study is indeed 512.\n\nAs PatchTST(ZS) is an encoder-decoder model, to pretrain it for zero shot forecasting one should theoretically prepare\nall possible context lengths and horizon lengths in the pretrain datasets. Pretraining it to its maximum performance\nrequires much more compute and likely more careful tuning compared to pretraining TimesFM.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.16,
                        "section_name": "Additional Empirical Results",
                        "section_path": "./screenshots-images-2/chapter_1/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_16/cab8af3e-0ca8-46c5-be01-09f413e80aaf.png",
                            "./screenshots-images-2/chapter_1/section_16/eab7379f-4498-4b3a-b08c-61f5e1309034.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "AS Additional Empirical Results\n\nIn this section, we provide more detailed tables for our zero-shot datasets and experiments described in Section 6.1.\nThe AM based aggregated metrics are presented in Figure 4.\n\na \" oa\ngros gos is\noo ee . , ow nee 8 e oo\nGEDOPIGIE APP P EOE PPE PSS A\n(a) Monash Archive [(GBW* 21} (b) Darts [HLP* 22) (c) ETT (Horizons 96 and 192) [ZZP*21}\n\nFigure 4: We report average performance in three groups of datasets. In all figures, the lower the metric the better and\nthe error bars represent one standard error. Note that among the baselines only TimesFM and IImtime are zero-shot. In\n(a) we report results on the Monash datasets. Since the datasets have different scales, we take the Arithmetric Mean\n(AM) the MAEB\u2019s scaled by the MAE of a naive baseline. We can see that TimesFM is within significance of the top\nmodel N-BEATS. In (b), we report the similarly scaled MAE on the Darts benchmarks. TimesFM is within significance\nof the top of method which is ARIMA in this case. Note that these datasets have one time-series each and therefore\nstatistical methods are competitive with deep learning ones. Finally, in (c) we report the average MAE for 96 and 192\nhorizon prediction tasks on 4 ETT datasets i.e 8 tasks in total. TimesFM and PatchTST are the best performing models\nin this case.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.17,
                        "section_name": "A5.1_ Darts",
                        "section_path": "./screenshots-images-2/chapter_1/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_17/737b2f2f-50a2-4e64-9c2f-d53d1c817443.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A5.1_ Darts\n\nWe present the MAE results individually from all 8 datasets in Table 3. It can be seen that TimesFM performs well for\nall datasets with clear seasonal patterns. On an average we are within significant level of the best model. Note that there\nare only 8 time-series as a whole in Darts and theerfore these evaluations have very wide confidence intervals.\n\nIn Figure 8 we present visual comparisons of our forecasts vs some of the baselines.\n\nTable 3: MAE for Darts datasets. We also include the naive baseline that predicts the last values in the context\nrepeatedly.\nGP ARIMA = TCN N-BEATS N-HiTS  Ilmtime(ZS) TimesFM(@ZS) PatchTST NAIVE\n\nAirPassengersDataset 34.67 24.03 54.96 97.89 59.16 34.37 62.51 44.65 8145\nAusBeerDataset 102.05 17.13 30.90 10.39 34.23 16.13 11.94 21.97 96.35\nGasRateCO2Dataset 2.27 2.37 2.64 2.63 3.85 3.50 2.50 2.67 2.29\nMonthlyMilkDataset 30.33 37.19 70.86 33.64 32.73 9.68 28.09 42.60 85.71\nSunspotsDataset 53.74 43.56 51.82 TBS 49.93 47.34 41.40 62.33 48.24\nWineDataset 4552.06 2306.70 3287.14 4562.02 3909.51 1569.32 2871.33 2498.69 4075.28\nWoolyDataset 649.98 $88.78 1158.79 903.01 382.09 808.73 728.92 342.28 1210.33\nHeartRateDataset 5.65 5.56 5.49 6.57 6.10 5.85 6.74 5.92\nScaled MAE (Arithmetic Mean) 0.8193 0.6045. (0.8427 0.9176 0.8109 0.6641 0.6829 0.7462 1.0000\n\nScaled MAE (Geometric Mean) 0.7509 0.5219 0.7946 0.7316 0.6936 0.4882 0.5767 0.6458 1.0000\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.18,
                        "section_name": "Monash",
                        "section_path": "./screenshots-images-2/chapter_1/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_18/35624e37-3cbe-4523-a63e-9c89179eed58.png",
                            "./screenshots-images-2/chapter_1/section_18/b04f9150-f8f3-4ee8-a855-1485c3a0bd20.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A5.2> Monash\n\nTable 4: We present the mean MAE results for our methods along size Monash baselines. We also include the naive\nbaseline that predicts the last values in the context repeatedly.\n\nTeams) SES Tas TRATS ETS DEE TARINA FR Galloos FENN Deepak BEATS \u2014WiewNet Trnslorner PuISTZS) Tinea) NAIVE\nDasa\n\n\u2018retaken cesnciy dwar \u201csm aoe asin SM Tone ene er\nrey VTsetk \u2014 SaNetk \u2014SSNetk WML? et Stet Fonetk LASIK EUSGI Lite adel tet Tek\npeseanae ovate ma FMT 19098 14s esi at gaat a tana ae \u2018on\nfener Beau ash 38 as 3st 08 at 1 aa Lat\nwaa Crt mer rt aa 433 bred fr in Err isu\n= Ro aSke ee ee oe dea res ae\nene seaty anes TK 9889038 S830 94121 RIND SKA 36 wwssr23 Tw9322 T1473 Tews Gams) \u2014 TAMIA SS ura\nteen gets Tsk Wso1ei9TuskaeWrnat \u201cwwass2 Laer aT wost97 owns USL) Moan sISTI2\u2014 wsnLAT 12000.06\n\u2018Seren seth \u2018ans \"90210 Serve SD SUM asi 17 sso 302321 Dia SNM Sty Han su\nims riStensS SWINISe? THSKIESK ASST) Geta atbesnan 5) eus$i90 HOUDIAE sOKLKOD AULD SHMEZIEMD OST \u2014NNTARLSL Te\nSova debe soca MSATE Comer) Sr MIM ais te a ISA Nowak nk. rn pored\nIredaat wine) DyK2 wo) 30H Qes7dl waht aaySsk | aeST\u2014a64M8 SSID ID aS ST ona\n\u2018eke howe rT om oe aa \u2018oor om \u201801 \u2018ont \u2018oa\n\u2018eae wee Lip tae my te 2 ow is fr tur ey iad 13s ut\n\u2018eupcrtey xa 2180 2% am Sue ae as nn 7 he 3 Prt\ntere aos wen sin ass ses Ses te am any\nSerra Ere) Das wo \"wae \u201ctear ie taas sete 20) teat\ntor weeky son 120030 ous om tou IStbe ot iwis SHAS rey\nSaled MAE antmese Mis) Ta Tar Tm oie Tania oa\nScaled MAE (Geomatics) Q9TIS EMSS are cord are jms asses ae oases\n\nIn Table 4 we present the actual MAE numbers that are behind the main Figure 2a. In Figure 9, we present some\nexamples of our zero-shot forecasts. For most datasets, we set the context window to be the maximum length of the\n\nseries in the dataset capped at 512 (similar to statistcal models used in the official Monash baselines). For some datasets,\nwe did some inference time-tuning of the context length, i.e we predict the last horizon length number of points in the\ntraining set with context lengths 32, 64 and maximum allowed and chose the best one in terms of this validation metric.\nThis is fair as most Monash DL baselines use different context lengths for different datasets during training and our\nmodel is completely zero-shot. The max context lengths used for these datasets are (cif 2016, 32), (tourism yearly, 32),\n(covid deaths, 32), (bitcoin, 32), (tourism monthly, 32) and (tourism monthly, 64).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.19,
                        "section_name": "Informer",
                        "section_path": "./screenshots-images-2/chapter_1/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_19/fe6517e3-8ea8-41f7-b9f3-ee572f40b709.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A5.3 Informer\n\nWe present the MAE on the last split of the test set for all dataset, horizon pairs considered in Table 5. Owing to\nexpensive evaluations for Ilmtime, the results are reported on the last test window of the original test split, as done\nin [GFQW23].\n\nTable 5: MAE for ETT datasets for prediction horizons 96 and 192. Owing to expensive evaluations for Ilmtime, the\nresults are reported on the last test window of the original test split.\n\nIlmtime(ZS)* PatchTST PatchTST(ZS) FEDFormer AutoFormer Informer TimesFM(ZS)\n\nDataset\n\nETThI (horizon=96) 0.42 0.41 0.39 0.58 0.55 0.76 0.45\nETThI (horizon=192) 0.50 0.49 0.50 0.64 0.64 0.78 0.53\nETTh2 (horizon=96) 0.33 0.28 0.37 0.67 0.65 1.94 0.35\nETTh2 (horizon=192) 0.70 0.68 0.59 0.82 0.82 2.02 0.62\nETTm! (horizon=96) 0.37 0.33 0.24 O41 0.54 0.71 0.19\nETTm! (horizon=192) 071 0.31 0.26 0.49 0.46 0.68 0.26\nETTm2? (horizon=96) 0.29 0.23 0.22 0.36 0.29 0.48 0.24\nETTm2 (horizon=192) 0.31 0.25 0.22 0.25 0.30 O51 0.27\n\nAvg 0.45 0.37 0.35 0.53 0.53 0.99 0.36\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.2,
                        "section_name": "More Details on Models",
                        "section_path": "./screenshots-images-2/chapter_1/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_20/53c398e3-d2be-4b9c-a545-de7337df48d9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A.6 More Details on Models\n\nWe now present implementation details about TimesFM and other baselines.\n\nTimesFM. For our main 200M model we use 16 attention heads, 20 layers, a input patch length of 32 and output patch\nlength of 128. The model dimension is set to 1280. We train with layer norm and a cosine decay learning rate schedule\nwith peak learning rate of 5e \u2014 4. The hyper-parameters of TimesFM for various sizes are provided in Table 6. Note\nthat the settings are for the base models and not ablation models. The hidden dims of both the residual block and the\nFEN in the transformer layers are set as the same as model dimensions. We keep layer norm in transformer layers but\nnot in the residual blocks.\n\nTable 6: Hyper-parameters for TimesFM\nnum_layers model_dims output_patch_len input_patch_len num_heads dropout\n\nSize\n\n200M 20 1280 128 32 16 0.2\n70M 10 1024 128 32 16 0.2\n17M 10 512 128 32 16 0.2\n\nMonash Baselines. The raw metrics for the Monash baselines are directly taken from Tables 9 and 11 of the\nsupplementary material of the original paper [GBW* 21]. For Ilmtime, we use the precomputed outputs provided by the\nauthors of [GFQW23].\n\nDarts Baselines. For all the Darts baselines we use the precomputed outputs provided by the authors of [GFQW23].\nFor more details please see Section C.1 in that paper.\n\nInformer Baselines. For FEDFormer [ZMW+22], Autoformer [WXWL21], Informer [ZZP+21] and\nPatchTST [NNSK22] we use the original hyperparameters and implementation. The results presented in the main paper\nare obtained on the last test window of length horizon length as stated in the IImtime [GFQW23] paper.\n\nWe generate the Ilmtime predictions using the code provided by the authors \u2019 but adapted to the ETT datasets. Note that\nas of January 2024, OpenAI has discontinued access to GPT-3, therefore we had to use the GPT-3.5-Turbo model.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.21,
                        "section_name": "Date Features",
                        "section_path": "./screenshots-images-2/chapter_1/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_21/f863b5ca-3994-4eaa-b64e-5022b995c456.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A.7 Date Features\n\nAs we mentioned earlier, since we are building a single pre-trained model, we cannot have dataset specific dynamic\nor static covariates during training time. However, the datetime column is ubiquitous in all time-series data, so we\ncan technically have date derived features like day of the week, month of the year etc processed into a vector at each\ntime-point \u00a2, denoted by x; \u20ac R\".\n\nIf so, the learning task can be rewritten as\nfi (Yun, Xut+H) \u2014 Yosit+H-\nThere are many options to incorporate these features into the model, one being to directly concatenate them after the\n\ntime-points in each patch. For this paper we decide to focus on the univariate time-series input, and will investigate this\nenhancement in the future.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.22,
                        "section_name": "Synthetic Data",
                        "section_path": "./screenshots-images-2/chapter_1/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_22/57f7f5d8-6e0a-4b8c-b94c-ed5639d1223f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A.8 Synthetic Data\n\nWe create the synthetic data to reflect common time-series patterns using traditional statistical models. We start with\nfour simple times series patterns:\n\n* Piece-wise linear trends (I), where the number of the piece-wise linear components is randomly chosen\nbetween 2 and 8.\n\n* ARMA(p, q) (II), where 1 < p,q < 8 and the corresponding coefficients are generated from either a\nmultivariate Gaussian or a uniform, then normalized.\n\n* Seasonal patterns. In particular we create the sine (III) and the cosine (IV) waves of different random periods\nbetween 4 and max context length / 2 time-points and time delays.\n\nWe then randomly enable / disable these four components (I) - (IV), generate their time-series of length 2048 respectively,\nand sum them up using uniformly sampled random weights to create each times series in the synthetic datasets. We also\nchoose to apply the trend multiplicatively 50% of the times the trend component is chosen.\n",
                        "extracted-code": ""
                    }
                ]
            }
        ]
    }
}