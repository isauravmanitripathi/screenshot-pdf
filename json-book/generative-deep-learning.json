{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "Chapter 1. Generative Modeling",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "Chapter 1. Generative Modeling",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/60be7316-f629-4f7a-922a-3bb989ce9bdd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 1. Generative Modeling\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\n\u00a2 Learn the key differences between generative and discriminative models.\n\n\u00a9 Understand the desirable properties of a generative model through a simple example.\n\n\u00a2 Learn about the core probabilistic concepts that underpin generative models.\n\n\u00a2 Explore the different families of generative models.\n\n\u00a9 Clone the codebase that accompanies this book, so that you can get started building generative\nmodels!\n\nThis chapter is a general introduction to the field of generative modeling.\n\nWe will start with a gentle theoretical introduction to generative modeling and see how it is the natural\ncounterpart to the more widely studied discriminative modeling. We will then establish a framework that\ndescribes the desirable properties that a good generative model should have. We will also lay out the core\nprobabilistic concepts that are important to know, in order to fully appreciate how different approaches\ntackle the challenge of generative modeling.\n\nThis will lead us naturally to the penultimate section, which lays out the six broad families of generative\nmodels that dominate the field today. The final section explains how to get started with the codebase that\naccompanies this book.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.2,
                        "section_name": "What Is Generative Modeling?",
                        "section_path": "./screenshots-images-2/chapter_1/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_2/e4879259-f13e-4826-ae57-62128158f5e6.png",
                            "./screenshots-images-2/chapter_1/section_2/d82ab405-d22b-424d-827d-d37461ec062b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What Is Generative Modeling?\n\nGenerative modeling can be broadly defined as follows:\n\nGenerative modeling is a branch of machine learning that involves training a model to produce new\ndata that is similar to a given dataset.\n\nWhat does this mean in practice? Suppose we have a dataset containing photos of horses. We can train a\ngenerative model on this dataset to capture the rules that govern the complex relationships between pix-\nels in images of horses. Then we can sample from this model to create novel, realistic images of horses\nthat did not exist in the original dataset. This process is illustrated in Figure 1-1.\n\nTraining data Generated samples\n\nTraining\n\nFigure 1-1. A generative model trained to generate realistic photos of horses\n\nIn order to build a generative model, we require a dataset consisting of many examples of the entity we\nare trying to generate. This is known as the training data, and one such data point is called an observa-\ntion.\n\nEach observation consists of many features. For an image generation problem, the features are usually\nthe individual pixel values; for a text generation problem, the features could be individual words or\ngroups of letters. It is our goal to build a model that can generate new sets of features that look as if they\nhave been created using the same rules as the original data. Conceptually, for image generation this is an\nincredibly difficult task, considering the vast number of ways that individual pixel values can be assigned\nand the relatively tiny number of such arrangements that constitute an image of the entity we are trying\nto generate.\n\nA generative model must also be probabilistic rather than deterministic, because we want to be able to\nsample many different variations of the output, rather than get the same output every time. If our model\nis merely a fixed calculation, such as taking the average value of each pixel in the training dataset, it is\nnot generative. A generative model must include a random component that influences the individual\nsamples generated by the model.\n\nIn other words, we can imagine that there is some unknown probabilistic distribution that explains why\nsome images are likely to be found in the training dataset and other images are not. It is our job to build a\nmodel that mimics this distribution as closely as possible and then sample from it to generate new, dis-\ntinct observations that look as if they could have been included in the original training set.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.3,
                        "section_name": "Generative Versus Discriminative Modeling",
                        "section_path": "./screenshots-images-2/chapter_1/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_3/d5182475-ed8a-4856-bd77-27fc42356777.png",
                            "./screenshots-images-2/chapter_1/section_3/4ecc3e47-53ac-4683-a614-5d313feeae86.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generative Versus Discriminative Modeling\n\nIn order to truly understand what generative modeling aims to achieve and why this is important, it is\nuseful to compare it to its counterpart, discriminative modeling. If you have studied machine learning,\nmost problems you will have faced will have most likely been discriminative in nature. To understand\n\nthe difference, let\u2019s look at an example.\n\nSuppose we have a dataset of paintings, some painted by Van Gogh and some by other artists. With\nenough data, we could train a discriminative model to predict if a given painting was painted by Van\nGogh. Our model would learn that certain colors, shapes, and textures are more likely to indicate that a\npainting is by the Dutch master, and for paintings with these features, the model would upweight its pre-\ndiction accordingly. Figure 1-2 shows the discriminative modeling process\u2014note how it differs from the\ngenerative modeling process shown in Figure 1-1.\n\nTraining data\n\nPrediction Prediction\n\n0.83\nlikely to be a Van Gogh\n\nFigure 1-2. A discriminative model trained to predict if a given image is painted by Van Gogh\n\nWhen performing discriminative modeling, each observation in the training data has a label. For a binary\nclassification problem such as our artist discriminator, Van Gogh paintings would be labeled 1 and non\u2014\nVan Gogh paintings labeled 0. Our model then learns how to discriminate between these two groups and\noutputs the probability that a new observation has label 1\u2014i.e., that it was painted by Van Gogh.\n\nIn contrast, generative modeling doesn\u2019t require the dataset to be labeled because it concerns itself with\ngenerating entirely new images, rather than trying to predict a label of a given image.\n\nLet\u2019s define these types of modeling formally, using mathematical notation:\n\nDiscriminative modeling estimates p(y|x).\n\nThat is, discriminative modeling aims to model the probability of a label y given some observation x.\n\nGenerative modeling estimates p(x).\n\nThat is, generative modeling aims to model the probability of observing an observation x. Sampling from\nthis distribution allows us to generate new observations.\n\nCONDITIONAL GENERATIVE MODELS.\n\nNote that we can also build a generative model to model the conditional probability p(x|y)\u2014the probability of seeing\nan observation x with a specific label y.\n\nFor example, if our dataset contains different types of fruit, we could tell our generative model to specifically gener-\nate an image of an apple.\n\nAn important point to note is that even if we were able to build a perfect discriminative model to identify\nVan Gogh paintings, it would still have no idea how to create a painting that looks like a Van Gogh. It can\nonly output probabilities against existing images, as this is what it has been trained to do. We would in-\nstead need to train a generative model and sample from this model to generate images that have a high\nchance of belonging to the original training dataset.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.4,
                        "section_name": "The Rise of Generative Modeling",
                        "section_path": "./screenshots-images-2/chapter_1/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_4/9aa671fa-0042-4243-b0f8-12d99f6aba9f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Rise of Generative Modeling\n\nUntil recently, discriminative modeling has been the driving force behind most progress in machine\nlearning. This is because for any discriminative problem, the corresponding generative modeling prob-\nlem is typically much more difficult to tackle. For example, it is much easier to train a model to predict if\na painting is by Van Gogh than it is to train a model to generate a Van Gogh-style painting from scratch.\nSimilarly, it is much easier to train a model to predict if a page of text was written by Charles Dickens\nthan it is to build a model to generate a set of paragraphs in the style of Dickens. Until recently, most gen-\nerative challenges were simply out of reach and many doubted that they could ever be solved. Creativity\nwas considered a purely human capability that couldn\u2019t be rivaled by AI.\n\nHowever, as machine learning technologies have matured, this assumption has gradually weakened. In\nthe last 10 years many of the most interesting advancements in the field have come through novel ap-\nplications of machine learning to generative modeling tasks. For example, Figure 1-3 shows the striking\nprogress that has already been made in facial image generation since 2014.\n\niba\n\n2014 2015 2016\nhas\n\n2019 2020 2021 2022\n\nFigure 1-3. Face generation using generative modeling has improved significantly over the last decade (adapted from Brundage et al, 2018)\"\n\n2023\n\nAs well as being easier to tackle, discriminative modeling has historically been more readily applicable to\npractical problems across industry than generative modeling. For example, a doctor may benefit from a\nmodel that predicts if a given retinal image shows signs of glaucoma, but wouldn\u2019t necessarily benefit\nfrom a model that can generate novel pictures of the back of an eye.\n\nHowever, this is also starting to change, with the proliferation of companies offering generative services\nthat target specific business problems. For example, it is now possible to access APIs that generate origi-\nnal blog posts given a particular subject matter, produce a variety of images of your product in any set-\nting you desire, or write social media content and ad copy to match your brand and target message.\nThere are also clear positive applications of generative AI for industries such as game design and cine-\nmatography, where models trained to output video and music are beginning to add value.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.5,
                        "section_name": "Generative Modeling and AI",
                        "section_path": "./screenshots-images-2/chapter_1/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_5/2863a74a-c84a-4f75-99f5-0767f78ec8e8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generative Modeling and AI\n\nAs well as the practical uses of generative modeling (many of which are yet to be discovered), there are\nthree deeper reasons why generative modeling can be considered the key to unlocking a far more sophis-\nticated form of artificial intelligence that goes beyond what discriminative modeling alone can achieve.\n\nFirstly, purely from a theoretical point of view, we shouldn\u2019t limit our machine training to simply catego-\nrizing data. For completeness, we should also be concerned with training models that capture a more\ncomplete understanding of the data distribution, beyond any particular label. This is undoubtedly a more\ndifficult problem to solve, due to the high dimensionality of the space of feasible outputs and the relative-\nly small number of creations that we would class as belonging to the dataset. However, as we shall see,\nmany of the same techniques that have driven development in discriminative modeling, such as deep\nlearning, can be utilized by generative models too.\n\nSecondly, as we shall see in Chapter 12, generative modeling is now being used to drive progress in other\nfields of AI, such as reinforcement learning (the study of teaching agents to optimize a goal in an environ-\nment through trial and error). Suppose we want to train a robot to walk across a given terrain. A tradi-\ntional approach would be to run many experiments where the agent tries out different strategies in the\nterrain, or a computer simulation of the terrain. Over time the agent would learn which strategies are\nmore successful than others and therefore gradually improve. A challenge with this approach is that it is\nfairly inflexible because it is trained to optimize the policy for one particular task. An alternative ap-\nproach that has recently gained traction is to instead train the agent to learn a world model of the envi-\nronment using a generative model, independent of any particular task. The agent can quickly adapt to\nnew tasks by testing strategies in its own world model, rather than in the real environment, which is of-\nten computationally more efficient and does not require retraining from scratch for each new task.\n\nFinally, if we are to truly say that we have built a machine that has acquired a form of intelligence that is\ncomparable to a human\u2019s, generative modeling must surely be part of the solution. One of the finest ex-\namples of a generative model in the natural world is the person reading this book. Take a moment to con-\nsider what an incredible generative model you are. You can close your eyes and imagine what an ele-\nphant would look like from any possible angle. You can imagine a number of plausible different endings\nto your favorite TV show, and you can plan your week ahead by working through various futures in your\nmind\u2019s eye and taking action accordingly. Current neuroscientific theory suggests that our perception of\nreality is not a highly complex discriminative model operating on our sensory input to produce predic-\ntions of what we are experiencing, but is instead a generative model that is trained from birth to produce\nsimulations of our surroundings that accurately match the future. Some theories even suggest that the\noutput from this generative model is what we directly perceive as reality. Clearly, a deep understanding\nof how we can build machines to acquire this ability will be central to our continued understanding of\nthe workings of the brain and general artificial intelligence.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.6,
                        "section_name": "Our First Generative Model",
                        "section_path": "./screenshots-images-2/chapter_1/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_6/b09e06e1-6770-4f3e-b007-4754bc19d771.png",
                            "./screenshots-images-2/chapter_1/section_6/29f6972d-a8d2-4134-8194-39f77af57664.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Our First Generative Model\n\nWith this in mind, let\u2019s begin our journey into the exciting world of generative modeling. To begin with,\nwe'll look at a toy example of a generative model and introduce some of the ideas that will help us to\nwork through the more complex architectures that we will encounter later in the book.\n\nHello World!\n\nLet\u2019s start by playing a generative modeling game in just two dimensions. I have chosen a rule that has\nbeen used to generate the set of points X in Figure 1-4. Let\u2019s call this rule paata- Your challenge is to\nchoose a different point x = (x1, \u00a32) in the space that looks like it has been generated by the same rule.\n\nFigure 1-4. A set of points in two dimensions, generated by an unknown rule paata\n\nWhere did you choose? You probably used your knowledge of the existing data points to construct a men-\ntal model, Pmodel, Of whereabouts in the space the point is more likely to be found. In this respect, Pmodet\nis an estimate of pata. Perhaps you decided that Pmodet Should look like Figure 1-5\u2014a rectangular box\nwhere points may be found, and an area outside of the box where there is no chance of finding any\npoints.\n\n%\n\nA %\npoint-generating\nmodel\n\nP model\n\nFigure 1-5. The orange box, pynoder is an estimate of the true data-generating distribution, pata\n\nTo generate a new observation, you can simply choose a point at random within the box, or more formal-\nly, sample from the distribution pode. Congratulations, you have just built your first generative model!\nYou have used the training data (the black points) to construct a model (the orange region) that you can\neasily sample from to generate other points that appear to belong to the training set.\n\nLet\u2019s now formalize this thinking into a framework that can help us understand what generative model-\ning is trying to achieve.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.7,
                        "section_name": "The Generative Modeling Framework",
                        "section_path": "./screenshots-images-2/chapter_1/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_7/c0769799-2387-4608-b657-e5f63d380a2f.png",
                            "./screenshots-images-2/chapter_1/section_7/d4875ebf-3128-4612-a598-3c37ac5c3d91.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Generative Modeling Framework\n\nWe can capture our motivations and goals for building a generative model in the following framework.\n\nTHE GENERATIVE MODELING FRAMEWORK\n\n\u00a9 We have a dataset of observations X.\n\u00a9 We assume that the observations have been generated according to some unknown distribution,\n\nPadata-\n\u00a9 We want to build a generative model pPmodet that mimics Pgata- If we achieve this goal, we can sample\n\nfrom Pmodel to generate observations that appear to have been drawn from Pgata-\n\u00a2 Therefore, the desirable properties of Pmode are:\n\nAccuracy\nIf Pmodel is high for a generated observation, it should look like it has been drawn from Pgata-\nIf Pmodet is low for a generated observation, it should not look like it has been drawn from\nPadata-\n\nGeneration\nIt should be possible to easily sample a new observation from Pmodel-\n\nRepresentation\n\nIt should be possible to understand how different high-level features in the data are represent-\ned by Pmodel-\n\nLet\u2019s now reveal the true data-generating distribution, pgara, and see how the framework applies to this\nexample. As we can see from Figure 1-6, the data-generating rule is simply a uniform distribution over\nthe land mass of the world, with no chance of finding a point in the sea.\n\nFigure 1-6. The orange box, Pode is an estimate of the true data-generating distribution, Paata (the gray area)\n\nClearly, our model, Pmodet, is an oversimplification of pgata. We can inspect points A, B, and C to under-\nstand the successes and failures of our model in terms of how accurately it mimics Pdata:\n\n\u00a2 Point A is an observation that is generated by our model but does not appear to have been generated\nby Pdata as it\u2019s in the middle of the sea.\n\n\u00a2 Point B could never have been generated by Pmodel as it sits outside the orange box. Therefore, our\nmodel has some gaps in its ability to produce observations across the entire range of potential\npossibilities.\n\n\u00a9 Point C is an observation that could be generated by Pmodei and also by Pgata-\n\nDespite its shortcomings, the model is easy to sample from, because it is simply a uniform distribution\nover the orange box. We can easily choose a point at random from inside this box, in order to sample\nfrom it.\n\nAlso, we can certainly say that our model is a simple representation of the underlying complex distribu-\ntion that captures some of the underlying high-level features. The true distribution is separated into areas\nwith lots of land mass (continents) and those with no land mass (the sea). This is a high-level feature that\nis also true of our model, except we have one large continent, rather than many.\n\nThis example has demonstrated the fundamental concepts behind generative modeling. The problems we\nwill be tackling in this book will be far more complex and high-dimensional, but the underlying frame-\nwork through which we approach the problem will be the same.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.8,
                        "section_name": "Representation Learning",
                        "section_path": "./screenshots-images-2/chapter_1/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_8/25e177f6-1124-4114-94d0-fdee89a950c6.png",
                            "./screenshots-images-2/chapter_1/section_8/7344a2d5-d3d9-431a-bbc2-3488269fcaa7.png",
                            "./screenshots-images-2/chapter_1/section_8/6f2e9093-9852-4b93-b6f4-9937955daeb7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Representation Learning\n\nIt is worth delving a little deeper into what we mean by learning a representation of the high-dimensional\ndata, as it is a topic that will recur throughout this book.\n\nSuppose you wanted to describe your appearance to someone who was looking for you in a crowd of peo-\nple and didn\u2019t know what you looked like. You wouldn\u2019t start by stating the color of pixel 1 of a photo of\nyou, then pixel 2, then pixel 3, etc. Instead, you would make the reasonable assumption that the other\nperson has a general idea of what an average human looks like, then amend this baseline with features\nthat describe groups of pixels, such as I have very blond hair or I wear glasses. With no more than 10 or so\nof these statements, the person would be able to map the description back into pixels to generate an im-\nage of you in their head. The image wouldn\u2019t be perfect, but it would be a close enough likeness to your\nactual appearance for them to find you among possibly hundreds of other people, even if they've never\nseen you before.\n\nThis is the core idea behind representation learning. Instead of trying to model the high-dimensional sam-\nple space directly, we describe each observation in the training set using some lower-dimensional latent\nspace and then learn a mapping function that can take a point in the latent space and map it to a point in\nthe original domain. In other words, each point in the latent space is a representation of some high-di-\nmensional observation.\n\nWhat does this mean in practice? Let\u2019s suppose we have a training set consisting of grayscale images of\nbiscuit tins (Figure 1-7).\n\nFigure 1-7. The biscuit tin dataset\n\nTo us, it is obvious that there are two features that can uniquely represent each of these tins: the height\nand width of the tin. That is, we can convert each image of a tin to a point in a latent space of just two di-\nmensions, even though the training set of images is provided in high-dimensional pixel space. Notably,\nthis means that we can also produce images of tins that do not exist in the training set, by applying a suit-\nable mapping function f to a new point in the latent space, as shown in Figure 1-8.\n\nRealizing that the original dataset can be described by the simpler latent space is not so easy for a ma-\nchine\u2014it would first need to establish that height and width are the two latent space dimensions that\nbest describe this dataset, then learn the mapping function f that can take a point in this space and map\nit to a grayscale biscuit tin image. Machine learning (and specifically, deep learning) gives us the ability to\ntrain machines that can find these complex relationships without human guidance.\n\nLatent space\n\nFigure 1-8. The 2D latent space of biscuit tins and the function f that maps a point in the latent space back to the original image domain\n\nOne of the benefits of training models that utilize a latent space is that we can perform operations that af-\nfect high-level properties of the image by manipulating its representation vector within the more man-\nageable latent space. For example, it is not obvious how to adjust the shading of every single pixel to\nmake an image of a biscuit tin taller. However, in the latent space, it\u2019s simply a case of increasing the\nheight latent dimension, then applying the mapping function to return to the image domain. We shall see\n\nan explicit example of this in the next chapter, applied not to biscuit tins but to faces.\n\nThe concept of encoding the training dataset into a latent space so that we can sample from it and decode\nthe point back to the original domain is common to many generative modeling techniques, as we shall\nsee in later chapters of this book. Mathematically speaking, encoder-decoder techniques try to transform\nthe highly nonlinear manifold on which the data lies (e.g., in pixel space) into a simpler latent space that\ncan be sampled from, so that it is likely that any point in the latent space is the representation of a well-\n\nformed image, as shown in Figure 1-9.\n\n& Manifold within a\nhigh-dimensional space\n\n\u2014_\u2014\u2014\n\nSimpler latent space that\ncan be sampled from\n\nFigure 1-9. The dog manifold in high-dimensional pixel space is mapped to a simpler latent space that can be sampled from\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.9,
                        "section_name": "Core Probability Theory",
                        "section_path": "./screenshots-images-2/chapter_1/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_9/34d77c93-7573-499a-bee6-ade5431f37f3.png",
                            "./screenshots-images-2/chapter_1/section_9/6b789ed3-a750-4b02-a167-e537a6ce8081.png",
                            "./screenshots-images-2/chapter_1/section_9/f579b924-74b5-4d80-ba92-66d19a0e06f9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Core Probability Theory\n\nWe have already seen that generative modeling is closely connected to statistical modeling of probability\ndistributions. Therefore, it now makes sense to introduce some core probabilistic and statistical concepts\nthat will be used throughout this book to explain the theoretical background of each model.\n\nIf you have never studied probability or statistics, don\u2019t worry. To build many of the deep learning mod-\nels that we shall see later in this book, it is not essential to have a deep understanding of statistical theory.\nHowever, to gain a full appreciation of the task that we are trying to tackle, it\u2019s worth trying to build up a\nsolid understanding of basic probabilistic theory. This way, you will have the foundations in place to un-\nderstand the different families of generative models that will be introduced later in this chapter.\n\nAs a first step, we shall define five key terms, linking each one back to our earlier example of a generative\nmodel that models the world map in two dimensions:\n\nSample space\n\nThe sample space is the complete set of all values an observation x can take.\n\nNOTE\n\nIn our previous example, the sample space consists of all points of latitude and longitude x = (1, @2) on the\nworld map. For example, x = (40.7306, -73.9352) is a point in the sample space (New York City) that belongs to\nthe true data-generating distribution. x = (11.3493, 142.1996) is a point in the sample space that does not be-\nlong to the true data-generating distribution (it\u2019s in the sea).\n\nProbability density function\n\nA probability density function (or simply density function) is a function p(x) that represents the rel-\native likelihood of a continuous random variable falling within different intervals, with the inte-\ngral of p(x) over the entire range of possible values being equal to 1.\n\nNOTE\n\nIn the world map example, the density function of our generative model is 0 outside of the orange box and\nconstant inside of the box, so that the integral of the density function over the entire sample space equals 1.\n\n\nWhile there is only one true density function pgata (x) that is assumed to have generated the ob-\nservable dataset, there are infinitely many density functions Pmodet (x) that we can use to estimate\n\nPaata (X)-\n\nParametric modeling\n\nParametric modeling is a technique that we can use to structure our approach to finding a suitable\nPmodet (X). A parametric model is a family of density functions pg (x) that can be described using a\nfinite number of parameters, 0.\n\nNOTE\n\nIf we assume a uniform distribution as our model family, then the set all possible boxes we could draw on\nFigure 1-5 is an example of a parametric model. In this case, there are four parameters: the coordinates of the\nbottom-left (91, 4) and top-right (43, 84) corners of the box.\n\nThus, each density function pg (x) in this parametric model (i.e., each box) can be uniquely represented by\nfour numbers, 0 = (61,92, 03, 94) .\n\nLikelihood\nThe likelihood \u00a3(6|x) of a parameter set @ is a function that measures the plausibility of 0, given\nsome observed point x. It is defined as follows:\n& (8|x) = po (x)\n\nThat is, the likelihood of @ given some observed point x is defined to be the value of the density\nfunction parameterized by 9, at the point x. If we have a whole dataset X of independent observa-\ntions, then we can write:\n\n\u00a3 OX) = TI vo (x)\n\nxeX\n\nNOTE\n\nIn the world map example, an orange box that only covered the left half of the map would have a likelihood of\n0\u2014it couldn't possibly have generated the dataset, as we have observed points in the right half of the map.\nThe orange box in Figure 1-5 has a positive likelihood, as the density function is positive for all data points un-\nder this model.\n\nSince the product of a large number of terms between 0 and 1 can be quite computationally diffi-\ncult to work with, we often use the log-likelihood \u00a2 instead:\n\n\u00a3(0|X) = > log po (x)\n\nxeX,\n\nThere are statistical reasons why the likelihood is defined in this way, but we can also see that this\ndefinition intuitively makes sense. The likelihood of a set of parameters 0 is defined to be the prob-\nability of seeing the data if the true data-generating distribution was the model parameterized by 0.\n\nWARNING\n\nNote that the likelihood is a function of the parameters, not the data. It should not be interpreted as the proba-\nbility that a given parameter set is correct\u2014in other words, it is not a probability distribution over the para-\nmeter space (ie., it doesn\u2019t sum/integrate to 1, with respect to the parameters).\n\nIt makes intuitive sense that the focus of parametric modeling should be to find the optimal value 6\nof the parameter set that maximizes the likelihood of observing the dataset X.\n\nMaximum likelihood estimation\n\nMaximum likelihood estimation is the technique that allows us to estimate 6_the set of parameters\n6 of a density function pg (x) that is most likely to explain some observed data X. More formally:\n\n6 = argmax\u00e9 (6|X)\n6\n\nGis also called the maximum likelihood estimate (MLE).\n\nNOTE\nIn the world map example, the MLE is the smallest rectangle that still contains all of the points in the training\nset.\n\nNeural networks typically minimize a loss function, so we can equivalently talk about finding the\nset of parameters that minimize the negative log-likelihood:\n\n6 = argmin \u2014 \u00a3(6|X)) = argmin \u2014 log pp (X))\n6 6\n\nGenerative modeling can be thought of as a form of maximum likelihood estimation, where the parame-\nters @ are the weights of the neural networks contained in the model. We are trying to find the values of\nthese parameters that maximize the likelihood of observing the given data (or equivalently, minimize the\nnegative log-likelihood).\n\nHowever, for high-dimensional problems, it is generally not possible to directly calculate pg (x)\u2014it is in-\ntractable. As we shall see in the next section, different families of generative models take different ap-\nproaches to tackling this problem.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.1,
                        "section_name": "Generative Model Taxonomy",
                        "section_path": "./screenshots-images-2/chapter_1/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_10/373c689a-be71-49ac-92a9-23b0423238b1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generative Model Taxonomy\n\nWhile all types of generative models ultimately aim to solve the same task, they all take slightly different\napproaches to modeling the density function pg (x). Broadly speaking, there are three possible\napproaches:\n\n1. Explicitly model the density function, but constrain the model in some way, so that the density func-\ntion is tractable (i.e., it can be calculated).\n\n2. Explicitly model a tractable approximation of the density function.\n\n3. Implicitly model the density function, through a stochastic process that directly generates data.\n\nThese are shown in Figure 1-10 as a taxonomy, alongside the six families of generative models that we\nwill explore in Part II of this book. Note that these families are not mutually exclusive\u2014there are many\nexamples of models that are hybrids between two different kinds of approaches. You should think of the\nfamilies as different general approaches to generative modeling, rather than explicit model architectures.\n\nGenerative Explicit Variational autoencoders\nmodels density Chapter 3\n\nEnergy-based models\nChapter 7\n\nAutoregressive models\nNormalizing flow models\nChapter 6\n\nGenerative adversarial\nnetworks\nChapter 4\n\nFigure 1-10. A taxonomy of generative modeling approaches\n\nImplicit\ndensity\n\nThe first split that we can make is between models where the probability density function p(x) is mod-\neled explicitly and those where it is modeled implicitly.\n\nImplicit density models do not aim to estimate the probability density at all, but instead focus solely on\nproducing a stochastic process that directly generates data. The best-known example of an implicit gener-\native model is a generative adversarial network. We can further split explicit density models into those that\ndirectly optimize the density function (tractable models) and those that only optimize an approximation\nof it.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.11,
                        "section_name": "The Generative Deep Learning Codebase",
                        "section_path": "./screenshots-images-2/chapter_1/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_11/91367769-de73-4846-8d80-8849a4b9c346.png",
                            "./screenshots-images-2/chapter_1/section_11/06ee70c8-a7c9-4ff2-a6ec-bc615b3457cf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Generative Deep Learning Codebase\n\nThe final section of this chapter will get you set up to start building generative deep learning models by\nintroducing the codebase that accompanies this book.\n\nTIP\n\nMany of the examples in this book are adapted from the excellent open source implementations that are available\nthrough the Keras website. I highly recommend you check out this resource, as new models and examples are con-\nstantly being added.\n\nCloning the Repository\n\nTo get started, you'll first need to clone the Git repository. Git is an open source version control system\nand will allow you to copy the code locally so that you can run the notebooks on your own machine, or in\na cloud-based environment. You may already have this installed, but if not, follow the instructions rele-\nvant to your operating system.\n\nTo clone the repository for this book, navigate to the folder where you would like to store the files and\ntype the following into your terminal:\n\ngit clone https://github.com/davidADSP/Generative Deep Learning _2nd_ Edition.git\n\nYou should now be able to see the files in a folder on your machine.\n\nUsing Docker\n\nThe codebase for this book is intended to be used with Docker, a free containerization technology that\nmakes getting started with a new codebase extremely easy, regardless of your architecture or operating\nsystem. If you have never used Docker, don\u2019t worry\u2014there is a description of how to get started in the\nREADME file in the book repository.\n\nRunning on a GPU\n\nIf you don\u2019t have access to your own GPU, that\u2019s also no problem! All of the examples in this book will\ntrain on a CPU, though this will take longer than if you use a GPU-enabled machine. There is also a section\nin the README about setting up a Google Cloud environment that gives you access to a GPU on a pay-as-\nyou-go basis.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.12,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_1/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_12/ca94712e-c3cf-4e53-8b4a-24848e7810e0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nThis chapter introduced the field of generative modeling, an important branch of machine learning that\ncomplements the more widely studied discriminative modeling. We discussed how generative modeling\nis currently one of the most active and exciting areas of AI research, with many recent advances in both\ntheory and applications.\n\nWe started with a simple toy example and saw how generative modeling ultimately focuses on modeling\nthe underlying distribution of the data. This presents many complex and interesting challenges, which\nwe summarized into a framework for understanding the desirable properties of any generative model.\n\nWe then walked through the key probabilistic concepts that will help to fully understand the theoretical\nfoundations of each approach to generative modeling and laid out the six different families of generative\nmodels that we will explore in Part II of this book. We also saw how to get started with the Generative\nDeep Learning codebase, by cloning the repository.\n\nIn Chapter 2, we will begin our exploration of deep learning and see how to use Keras to build models\nthat can perform discriminative modeling tasks. This will give us the necessary foundation to tackle gen-\nerative deep learning problems in later chapters.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 2,
                "chapter_name": "Chapter 2. Deep Learning",
                "chapter_path": "./screenshots-images-2/chapter_2",
                "sections": [
                    {
                        "section_id": 2.1,
                        "section_name": "Chapter 2. Deep Learning",
                        "section_path": "./screenshots-images-2/chapter_2/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_1/b5db4bee-36a8-402e-96f7-fc5ffe984ced.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 2. Deep Learning\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\n\u00a2 Learn about the different types of unstructured data that can be modeled using deep learning.\n\n\u00a2 Define a deep neural network and understand how it can be used to model complex datasets.\n\n\u00a2 Build a multilayer perceptron to predict the content of an image.\n\n\u00a2 Improve the performance of the model by using convolutional layers, dropout, and batch normaliza-\ntion layers.\n\nLet\u2019s start with a basic definition of deep learning:\n\nDeep learning is a class of machine learning algorithms that uses multiple stacked layers of process-\ning units to learn high-level representations from unstructured data.\n\nTo understand deep learning fully, we need to delve into this definition a bit further. First, we\u2019ll take a\nlook at the different types of unstructured data that deep learning can be used to model, then we'll dive\ninto the mechanics of building multiple stacked layers of processing units to solve classification tasks.\nThis will provide the foundation for future chapters where we focus on deep learning for generative\ntasks.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.2,
                        "section_name": "Data for Deep Learning",
                        "section_path": "./screenshots-images-2/chapter_2/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_2/ac64d8ae-a07f-44c0-9b97-05801e5b3750.png",
                            "./screenshots-images-2/chapter_2/section_2/c3a4654a-ef02-4270-b26d-be5065eb247a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Data for Deep Learning\n\nMany types of machine learning algorithms require structured, tabular data as input, arranged into col-\numns of features that describe each observation. For example, a person\u2019s age, income, and number of\nwebsite visits in the last month are all features that could help to predict if the person will subscribe to a\nparticular online service in the coming month. We could use a structured table of these features to train a\nlogistic regression, random forest, or XGBoost model to predict the binary response variable\u2014did the per-\nson subscribe (1) or not (0)? Here, each individual feature contains a nugget of information about the ob-\nservation, and the model would learn how these features interact to influence the response.\n\nUnstructured data refers to any data that is not naturally arranged into columns of features, such as im-\nages, audio, and text. There is of course spatial structure to an image, temporal structure to a recording\nor passage of text, and both spatial and temporal structure to video data, but since the data does not ar-\nrive in columns of features, it is considered unstructured, as shown in Figure 2-1.\n\nStructured data Unstructured data\nID Age Gender Height(cm) Location ms\n[ooo] 54{ Mm | 186 | London _| Q DIMIIDHP Tsrvice teribe\n}ooo2| 35 | F | 166 _| NewYork |\n\n}ooo4] 23 | M | 164 | London _|\nfooosy| 25 | _-w [ wo | caro ||| \u2018masse audio Te\nfooos] 23 |r| 181] being\n0\n\nFigure 2-1. The difference between structured and unstructured data\n\nWhen our data is unstructured, individual pixels, frequencies, or characters are almost entirely uninfor-\nmative. For example, knowing that pixel 234 of an image is a muddy shade of brown doesn\u2019t really help\nidentify if the image is of a house or a dog, and knowing that character 24 of a sentence is an e doesn\u2019t\nhelp predict if the text is about football or politics.\n\nPixels or characters are really just the dimples of the canvas into which higher-level informative features,\nsuch as an image of a chimney or the word striker, are embedded. If the chimney in the image were\nplaced on the other side of the house, the image would still contain a chimney, but this information\nwould now be carried by completely different pixels. If the word striker appeared slightly earlier or later\nin the text, the text would still be about football, but different character positions would provide this in-\nformation. The granularity of the data combined with the high degree of spatial dependence destroys the\nconcept of the pixel or character as an informative feature in its own right.\n\nFor this reason, if we train logistic regression, random forest, or XGBoost models on raw pixel values, the\ntrained model will often perform poorly for all but the simplest of classification tasks. These models rely\non the input features to be informative and not spatially dependent. A deep learning model, on the other\nhand, can learn how to build high-level informative features by itself, directly from the unstructured\ndata.\n\nDeep learning can be applied to structured data, but its real power, especially with regard to generative\nmodeling, comes from its ability to work with unstructured data. Most often, we want to generate un-\nstructured data such as new images or original strings of text, which is why deep learning has had such a\nprofound impact on the field of generative modeling.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.3,
                        "section_name": "Deep Neural Networks",
                        "section_path": "./screenshots-images-2/chapter_2/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_3/3d8b5d03-2f5b-49d0-8c00-5e0d8df93150.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep Neural Networks\n\nThe majority of deep learning systems are artificial neural networks (ANNs, or just neural networks for\nshort) with multiple stacked hidden layers. For this reason, deep learning has now almost become synony-\nmous with deep neural networks. However, any system that employs many layers to learn high-level rep-\nresentations of the input data is also a form of deep learning (e.g., deep belief networks).\n\nLet\u2019s start by breaking down exactly what we mean by a neural network and then see how they can be\n\nused to learn high-level features from unstructured data.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.4,
                        "section_name": "What Is a Neural Network?",
                        "section_path": "./screenshots-images-2/chapter_2/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_4/9f599317-fdab-4b86-a3ec-0f00bdae7746.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "What Is a Neural Network?\n\nA neural network consists of a series of stacked layers. Each layer contains units that are connected to the\nprevious layer\u2019s units through a set of weights. As we shall see, there are many different types of layers,\nbut one of the most common is the fully connected (or dense) layer that connects all units in the layer di-\nrectly to every unit in the previous layer.\n\nNeural networks where all adjacent layers are fully connected are called multilayer perceptrons (MLPs).\nThis is the first type of neural network that we will study. An example of an MLP is shown in Figure 2-2.\n\nInput __ A fully connected layer\n\n\\\\/\nX/\n\nWeights\n\nFigure 2-2. An example of a multilayer perceptron that predicts if a face is smiling\n\nThe input (e.g., an image) is transformed by each layer in turn, in what is known as a forward pass\nthrough the network, until it reaches the output layer. Specifically, each unit applies a nonlinear transfor-\nmation to a weighted sum of its inputs and passes the output through to the subsequent layer. The final\noutput layer is the culmination of this process, where the single unit outputs a probability that the origi-\nnal input belongs to a particular category (e.g., smiling).\n\nThe magic of deep neural networks lies in finding the set of weights for each layer that results in the most\naccurate predictions. The process of finding these weights is what we mean by training the network.\n\nDuring the training process, batches of images are passed through the network and the predicted outputs\nare compared to the ground truth. For example, the network might output a probability of 80% for an im-\nage of someone who really is smiling and a probability of 23% for an image of someone who really isn\u2019t\nsmiling. A perfect prediction would output 100% and 0% for these examples, so there is a small amount\nof error. The error in the prediction is then propagated backward through the network, adjusting each\nset of weights a small amount in the direction that improves the prediction most significantly. This\nprocess is appropriately called backpropagation. Gradually, each unit becomes skilled at identifying a\nparticular feature that ultimately helps the network to make better predictions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.5,
                        "section_name": "Learning High-Level Features",
                        "section_path": "./screenshots-images-2/chapter_2/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_5/bfc3cfc2-a5d7-40bf-8718-ef4a06893671.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Learning High-Level Features\n\nThe critical property that makes neural networks so powerful is their ability to learn features from the\ninput data, without human guidance. In other words, we do not need to do any feature engineering,\nwhich is why neural networks are so useful! We can let the model decide how it wants to arrange its\nweights, guided only by its desire to minimize the error in its predictions.\n\nFor example, let\u2019s walk through the network shown in Figure 2-2, assuming it has already been trained to\n\naccurately predict if a given input face is smiling:\n\n1. Unit A receives the value for an individual channel of an input pixel.\n\n2. Unit B combines its input values so that it fires strongest when a particular low-level feature such as\nan edge is present.\n\n3. Unit C combines the low-level features so that it fires strongest when a higher-level feature such as\nteeth are seen in the image.\n\n4. Unit D combines the high-level features so that it fires strongest when the person in the original im-\n\nage is smiling.\n\nUnits in each subsequent layer are able to represent increasingly sophisticated aspects of the original in-\nput, by combining lower-level features from the previous layer. Amazingly, this arises naturally out of the\ntraining process\u2014we do not need to tell each unit what to look for, or whether it should look for high-lev-\n\nel features or low-level features.\n\nThe layers between the input and output layers are called hidden layers. While our example only has two\nhidden layers, deep neural networks can have many more. Stacking large numbers of layers allows the\nneural network to learn progressively higher-level features by gradually building up information from\nthe lower-level features in previous layers. For example, ResNet,\" designed for image recognition, con-\n\ntains 152 layers.\n\nNext, we'll dive straight into the practical side of deep learning and get set up with TensorFlow and Keras\n\nso that you can start building your own deep neural networks.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.6,
                        "section_name": "TensorFlow and Keras",
                        "section_path": "./screenshots-images-2/chapter_2/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_6/9fb12892-840a-4f18-96aa-685829a81726.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "TensorFlow and Keras\n\nTensorFlow is an open source Python library for machine learning, developed by Google. TensorFlow is\none of the most utilized frameworks for building machine learning solutions, with particular emphasis\non the manipulation of tensors (hence the name). It provides the low-level functionality required to train\nneural networks, such as computing the gradient of arbitrary differentiable expressions and efficiently\nexecuting tensor operations.\n\nKeras is a high-level API for building neural networks, built on top of TensorFlow (Figure 2-3). It is ex-\ntremely flexible and very user-friendly, making it an ideal choice for getting started with deep learning.\nMoreover, Keras provides numerous useful building blocks that can be plugged together to create highly\ncomplex deep learning architectures through its functional API.\n\nTensorFlow Keras\n\nFigure 2-3. TensorFlow and Keras are excellent tools for building deep learning solutions\n\nIf you are just getting started with deep learning, I can highly recommend using TensorFlow and Keras.\nThis setup will allow you to build any network that you can think of in a production environment, while\nalso giving you an easy-to-learn API that enables rapid development of new ideas and concepts. Let\u2019s start\nby seeing how easy it is to build a multilayer perceptron using Keras.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.7,
                        "section_name": "Multilayer Perceptron (MLP)",
                        "section_path": "./screenshots-images-2/chapter_2/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_7/1b7c7f3b-60b3-4c4b-b9b5-63ed7cda1873.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Multilayer Perceptron (MLP)\n\nIn this section, we will train an MLP to classify a given image using supervised learning. Supervised learn-\ning is a type of machine learning algorithm in which the computer is trained on a labeled dataset. In oth-\ner words, the dataset used for training includes input data with corresponding output labels. The goal of\nthe algorithm is to learn a mapping between the input data and the output labels, so that it can make pre-\ndictions on new, unseen data.\n\nThe MLP is a discriminative (rather than generative) model, but supervised learning will still play a role\nin many types of generative models that we will explore in later chapters of this book, so it is a good place\nto start our journey.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at\nnotebooks/02_deeplearning/01_mlp/mlp.ipynb in the book repository.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.8,
                        "section_name": "Preparing the Data",
                        "section_path": "./screenshots-images-2/chapter_2/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_8/44ddd65a-12f6-48ed-89e2-5dc459fa25ea.png",
                            "./screenshots-images-2/chapter_2/section_8/bc56c4a3-bee4-4773-a9de-0ca4bdb304e8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Preparing the Data\nFor this example we will be using the CIFAR-10 dataset, a collection of 60,000 32 x 32-pixel color images\n\nthat comes bundled with Keras out of the box. Each image is classified into exactly one of 10 classes, as\nshown in Figure 2-4.\n\nAirplan\n\nAutomobile\n\nAl a3]\nefi\nAE\n\n2\nEy\n\nSapo aT\n\nan BER eae\n\no i=]\n& 8\n\nx\na\n\n2\ns\n\nTruck\n\nmg |) EE ads fen\nGEES RRhhi<\nit 22828\n\n}\nms\nne a\n&\nol\n\nFigure 2-4. Example images from the CIFAR-10 dataset (source: Krizhevsky, 2009\"\n\nBy default, the image data consists of integers between 0 and 255 for each pixel channel. We first need to\npreprocess the images by scaling these values to lie between 0 and 1, as neural networks work best when\nthe absolute value of each input is less than 1.\n\nWe also need to change the integer labeling of the images to one-hot encoded vectors, because the neural\nnetwork output will be a probability that the image belongs to each class. If the class integer label of an\nimage is 7, then its one-hot encoding is a vector of length 10 (the number of classes) that has Os in all but\nthe ith element, which is 1. These steps are shown in Example 2-1.\n\nExample 2-1. Preprocessing the CIFAR-10 dataset\n\nimport numpy as np\nfrom tensorflow.keras import datasets, utils\n\n(x_train, y_train), (x_test, y_ test) = datasets.cifarl0.load_data() O\nNUM_CLASSES = 10\n\nx_train = x_train.astype('float32') / 255.0 @\nx_test = x_test.astype('float32') / 255.0\n\ny_train = utils.to_categorical(y train, NUM_CLASSES) \u00ae\ny_test = utils.to_categorical(y test, NUM_CLASSES)\n\n@ Load the CIFAR-10 dataset. x_train and x_test are numpy arrays of shape [50000, 32, 32,\n3] and [10000, 32, 32, 3],respectively. y train and y_ test are numpy arrays of shape\n[50000, 1] and [10000, 1], respectively, containing the integer labels in the range 0 to 9 for\nthe class of each image.\n\n\u00ae@ Scale each image so that the pixel channel values lie between 0 and 1.\n\n\u00ae One-hot encode the labels\u2014the new shapes of y_ train and y test are [50000, 10] and\n[10000, 10], respectively.\n\nWe can see that the training image data (x_train) is stored ina tensor of shape [50000, 32, 32, 3].\nThere are no columns or rows in this dataset; instead, this is a tensor with four dimensions. A tensor is\njust a multidimensional array\u2014it is the natural extension of a matrix to more than two dimensions. The\nfirst dimension of this tensor references the index of the image in the dataset, the second and third relate\nto the size of the image, and the last is the channel (i.e., red, green, or blue, since these are RGB images).\n\nFor example, Example 2-2 shows how we can find the channel value of a specific pixel in an image.\n\nExample 2-2. The green channel (1) value of the pixel in the (12,13) position of image 54\n\nx_train[54, 12, 13, 1]\n# 0.36862746\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.9,
                        "section_name": "Building the Model",
                        "section_path": "./screenshots-images-2/chapter_2/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_9/8912ad9a-8cc3-42dc-b822-6f678b7c9d09.png",
                            "./screenshots-images-2/chapter_2/section_9/6374b0a4-dbf0-4fc5-a5ce-960254bf3328.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Building the Model\n\nIn Keras you can either define the structure of a neural network as a Sequential model or using the\nfunctional API.\n\nA Sequential model is useful for quickly defining a linear stack of layers (i.e., where one layer follows\non directly from the previous layer without any branching). We can define our MLP model using the\nSequential class as shown in Example 2-3.\n\nExample 2-3. Building our MLP using a Sequential model\n\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential ([\nlayers.Flatten(input_shape=(32, 32, 3)),\n\nlayers.Dense(200, activation = \u2018relu'),\nlayers.Dense(150, activation = \u2018relu'),\nlayers.Dense(10, activation = 'softmax'),\n\nMany of the models in this book require that the output from a layer is passed to multiple subsequent lay-\ners, or conversely, that a layer receives input from multiple preceding layers. For these models, the\nSequential class is not suitable and we would need to use the functional API instead, which is a lot\nmore flexible.\n\nTIP\n\nIrecommend that even if you are just starting out building linear models with Keras, you still use the functional API\nrather than Sequential models, since it will serve you better in the long run as your neural networks become more\narchitecturally complex. The functional API will give you complete freedom over the design of your deep neural\nnetwork.\n\n\nExample 2-4 shows the same MLP coded using the functional API. When using the functional API, we use\nthe Model class to define the overall input and output layers of the model.\n\nExample 2-4. Building our MLP using the functional API\n\nfrom tensorflow.keras import layers, models\n\ninput_layer = layers.Input(shape=(32, 32, 3))\nx = layers.Flatten()(input_layer)\n\nx = layers.Dense(units=200, activation = '\u2018relu')(x)\nx = layers.Dense(units=150, activation = '\u2018relu')(x)\noutput_layer = layers.Dense(units=10, activation = '\u2018softmax' ) (x)\n\nmodel = models.Model(input_layer, output_layer)\n\nBoth methods give identical models\u2014a diagram of the architecture is shown in Figure 2-5.\n\nActivation Activation Activation\n(relu) (relu) (softmax)\n\n->015 airplane\n\n-\u00bb 0.07 automobile\n\n. e Na iid\n=f , |\nLo). \u2018on LX) +036 ship\n\nInput layer\n(32 x 32 x3)\nGEXS23) \u201c0.05 truck\nDense layer Dense layer\n(200 units) (150 units) (10 units)\nFlatten layer\n\nFigure 2-5. A diagram of the MLP architecture\n\nLet\u2019s now look in more detail at the different layers and activation functions used within the MLP.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.1,
                        "section_name": "Layers",
                        "section_path": "./screenshots-images-2/chapter_2/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_10/74154b03-68e7-47a6-a63f-f746821bf50a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Layers\nTo build our MLP, we used three different types of layers: Input , Flatten, and Dense.\n\nThe Input layer is an entry point into the network. We tell the network the shape of each data element\nto expect as a tuple. Notice that we do not specify the batch size; this isn\u2019t necessary as we can pass any\nnumber of images into the Input layer simultaneously. We do not need to explicitly state the batch size\ninthe Input layer definition.\n\nNext we flatten this input into a vector, using a Flatten layer. This results in a vector of length 3,072 (=\n32 x 32 x 3). The reason we do this is because the subsequent Dense layer requires that its input is flat,\nrather than a multidimensional array. As we shall see later, other layer types require multidimensional\narrays as input, so you need to be aware of the required input and output shape of each layer type to un-\nderstand when it is necessary to use Flatten.\n\nThe Dense layer is one of the most fundamental building blocks of a neural network. It contains a given\nnumber of units that are densely connected to the previous layer\u2014that is, every unit in the layer is con-\nnected to every unit in the previous layer, through a single connection that carries a weight (which can\nbe positive or negative). The output from a given unit is the weighted sum of the inputs it receives from\nthe previous layer, which is then passed through a nonlinear activation function before being sent to the\nfollowing layer. The activation function is critical to ensure the neural network is able to learn complex\nfunctions and doesn\u2019t just output a linear combination of its inputs.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.11,
                        "section_name": "Activation functions",
                        "section_path": "./screenshots-images-2/chapter_2/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_11/bb79ec2a-cdc2-47e3-ac2b-ce39c69cbbca.png",
                            "./screenshots-images-2/chapter_2/section_11/a2c3039a-9e4b-4b4a-9d59-d6145bce3edc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Activation functions\n\nThere are many kinds of activation function, but three of the most important are ReLU, sigmoid, and\nsoftmax.\n\nThe ReLU (rectified linear unit) activation function is defined to be 0 if the input is negative and is other-\nwise equal to the input. The LeakyReLU activation function is very similar to ReLU, with one key differ-\nence: whereas the ReLU activation function returns 0 for input values less than 0, the LeakyReLU func-\ntion returns a small negative number proportional to the input. ReLU units can sometimes die if they al-\nways output 0, because of a large bias toward negative values pre-activation. In this case, the gradient is 0\nand therefore no error is propagated back through this unit. LeakyReLU activations fix this issue by al-\nways ensuring the gradient is nonzero. ReLU-based functions are among the most reliable activations to\nuse between the layers of a deep network to encourage stable training.\n\nThe sigmoid activation is useful if you wish the output from the layer to be scaled between 0 and 1\u2014for\nexample, for binary classification problems with one output unit or multilabel classification problems,\nwhere each observation can belong to more than one class. Figure 2-6 shows ReLU, LeakyReLU, and sig-\nmoid activation functions side by side for comparison.\n\ntl a flxd=x $0), fld=x tl aA\n\nyet\n\nhex\n\nS(x)=0 x fix=ax : x : x\nReLU LeakyReLU Sigmoid\n\nFigure 2-6. The ReLU, LeakyReLU, and sigmoid activation functions\n\nThe softmax activation function is useful if you want the total sum of the output from the layer to equal 1;\nfor example, for multiclass classification problems where each observation only belongs to exactly one\nclass. It is defined as:\n\nHere, J is the total number of units in the layer. In our neural network, we use a softmax activation in the\nfinal layer to ensure that the output is a set of 10 probabilities that sum to 1, which can be interpreted as\nthe likelihood that the image belongs to each class.\n\nIn Keras, activation functions can be defined within a layer (Example 2-5) or as a separate layer\n(Example 2-6).\n\nExample 2-5. A ReLU activation function defined as part of a Dense layer\n\nx = layers.Dense(units=200, activation = '\u2018relu')(x)\n\nExample 2-6. A ReLU activation function defined as its own layer\n\n*\n\"\n\nlayers .Dense(units=200) (x)\n\n*\n\"\n\nlayers .Activation('relu' ) (x)\n\nIn our example, we pass the input through two Dense layers, the first with 200 units and the second\nwith 150, both with ReLU activation functions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.12,
                        "section_name": "Inspecting the model",
                        "section_path": "./screenshots-images-2/chapter_2/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_12/101d28da-938b-4b11-a8ac-342462348c4d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Inspecting the model\n\nWe can use the model.summary() method to inspect the shape of the network at each layer, as shown\nin Table 2-1.\n\nTable 2-1. Output from the model. summary() method\n\nLayer (type) Outputshape Param #\n\nInputLayer (None, 32, 32,3) 0\n\nFlatten (None, 3072) 0\nDense (None, 200) 614,600\nDense (None, 150) 30,150\nDense (None, 10) 1,510\nTotal params 646,260\nTrainable params 646,260\n\nNon-trainable params 0\n\nNotice how the shape of our Input layer matches the shape of x_train and the shape of our Dense\noutput layer matches the shape of y_train.Keras uses None asa marker for the first dimension to\nshow that it doesn\u2019t yet know the number of observations that will be passed into the network. In fact, it\ndoesn\u2019t need to; we could just as easily pass 1 observation through the network at a time as 1,000. That\u2019s\nbecause tensor operations are conducted across all observations simultaneously using linear algebra\u2014\nthis is the part handled by TensorFlow. It is also the reason why you get a performance increase when\ntraining deep neural networks on GPUs instead of CPUs: GPUs are optimized for large tensor operations\nsince these calculations are also necessary for complex graphics manipulation.\n\nThe summary method also gives the number of parameters (weights) that will be trained at each layer. If\never you find that your model is training too slowly, check the summary to see if there are any layers that\ncontain a huge number of weights. If so, you should consider whether the number of units in the layer\ncould be reduced to speed up training.\n\nTIP\n\nMake sure you understand how the number of parameters is calculated in each layer! It\u2019s important to remember\nthat by default, each unit within a given layer is also connected to one additional bias unit that always outputs 1. This\nensures that the output from the unit can still be nonzero even when all inputs from the previous layer are 0.\n\nTherefore, the number of parameters in the 200-unit Dense layer is 200 * (3,072 + 1) = 614,600.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.13,
                        "section_name": "Compiling the Model",
                        "section_path": "./screenshots-images-2/chapter_2/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_13/184a82d7-3d90-4c71-af02-7a99c4f6f64e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Compiling the Model\nIn this step, we compile the model with an optimizer and a loss function, as shown in Example 2-7.\nExample 2-7. Defining the optimizer and the loss function\n\nfrom tensorflow.keras import optimizers\n\nopt = optimizers.Adam(learning rate=0.0005)\nmodel .compile(loss='categorical_crossentropy', optimizer=opt,\nmetrics=[ 'accuracy'])\n\nLet\u2019s now look in more detail at what we mean by loss functions and optimizers.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.14,
                        "section_name": "Loss functions",
                        "section_path": "./screenshots-images-2/chapter_2/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_14/e1dea986-b78b-41f6-88b6-600bdcb556c5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Loss functions\n\nThe loss function is used by the neural network to compare its predicted output to the ground truth. It re-\nturns a single number for each observation; the greater this number, the worse the network has per-\nformed for this observation.\n\nKeras provides many built-in loss functions to choose from, or you can create your own. Three of the\nmost commonly used are mean squared error, categorical cross-entropy, and binary cross-entropy. It is\nimportant to understand when it is appropriate to use each.\n\nIf your neural network is designed to solve a regression problem (i.e., the output is continuous), then you\nmight use the mean squared error loss. This is the mean of the squared difference between the ground\ntruth y; and predicted value p; of each output unit, where the mean is taken over all n output units:\n\n15 2\nMSE= \u2014 dw Pi)\n\nIf you are working on a classification problem where each observation only belongs to one class, then\ncategorical cross-entropy is the correct loss function. This is defined as follows:\n\n\u2014 Sv log (pi)\ni=l\n\nFinally, if you are working on a binary classification problem with one output unit, or a multilabel prob-\nlem where each observation can belong to multiple classes simultaneously, you should use binary cross-\nentropy:\n\nn\n\n-2 SY (i log (p:) + (1 \u2014 yi) log (1 \u2014 pi)\n\n=1\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.15,
                        "section_name": "Optimizers",
                        "section_path": "./screenshots-images-2/chapter_2/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_15/87b42161-ab0d-467d-a718-979c8ff2151f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Optimizers\n\nThe optimizer is the algorithm that will be used to update the weights in the neural network based on the\ngradient of the loss function. One of the most commonly used and stable optimizers is Adam (Adaptive\nMoment Estimation)= In most cases, you shouldn\u2019t need to tweak the default parameters of the Adam op-\ntimizer, except the learning rate. The greater the learning rate, the larger the change in weights at each\ntraining step. While training is initially faster with a large learning rate, the downside is that it may result\nin less stable training and may not find the global minimum of the loss function. This is a parameter that\n\nyou may want to tune or adjust during training.\n\nAnother common optimizer that you may come across is RMSProp (Root Mean Squared Propagation).\nAgain, you shouldn\u2019t need to adjust the parameters of this optimizer too much, but it is worth reading the\n\nKeras documentation to understand the role of each parameter.\n\nWe pass both the loss function and the optimizer into the compile method of the model, as well as a\nmetrics parameter where we can specify any additional metrics that we would like to report on during\n\ntraining, such as accuracy.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.16,
                        "section_name": "Training the Model",
                        "section_path": "./screenshots-images-2/chapter_2/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_16/a167c6e6-13b0-4566-a41f-387df0e915aa.png",
                            "./screenshots-images-2/chapter_2/section_16/fb87a49e-72cc-4d98-8c34-b7b7c798baf0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the Model\n\nThus far, we haven\u2019t shown the model any data. We have just set up the architecture and compiled the\n\nmodel with a loss function and optimizer.\nTo train the model against the data, we simply call the fit method, as shown in Example 2-8.\n\nExample 2-8. Calling the \u00a3it method to train the model\n\nmodel.fit(x_train 0\n, y_train @\n, batch_size = 32 \u00ae\n, epochs = 10 @\n, shuffle = True \u00a9\n)\n\n@ The raw image data.\n@ The one-hot encoded class labels.\n\n\u00ae The batch_size determines how many observations will be passed to the network at each train-\ning step.\n\n@ The epochs determine how many times the network will be shown the full training data.\n\n\u00a9 If shuffle = True, the batches will be drawn randomly without replacement from the training\n\ndata at each training step.\n\nThis will start training a deep neural network to predict the category of an image from the CIFAR-10\ndataset. The training process works as follows.\n\nFirst, the weights of the network are initialized to small random values. Then the network performs a se-\nries of training steps. At each training step, one batch of images is passed through the network and the er-\nrors are backpropagated to update the weights. The batch_size determines how many images are in\neach training step batch. The larger the batch size, the more stable the gradient calculation, but the slow-\ner each training step.\n\nTIP\n\nIt would be far too time-consuming and computationally intensive to use the entire dataset to calculate the gradient\n\nat each training step, so generally a batch size between 32 and 256 is used. It is also now recommended practice to in-\n4\n\ncrease the batch size as training progresses. -\n\n\nThis continues until all observations in the dataset have been seen once. This completes the first epoch.\nThe data is then passed through the network again in batches as part of the second epoch. This process\nrepeats until the specified number of epochs have elapsed.\n\nDuring training, Keras outputs the progress of the procedure, as shown in Figure 2-7. We can see that the\ntraining dataset has been split into 1,563 batches (each containing 32 images) and it has been shown to\nthe network 10 times (i.e., over 10 epochs), at a rate of approximately 2 milliseconds per batch. The cate-\ngorical cross-entropy loss has fallen from 1.8377 to 1.3696, resulting in an accuracy increase from 33.69%\nafter the first epoch to 51.67% after the tenth epoch.\n\nmodel. fit(x_train, y_train, batch_size=32, epochs=10, shuffle=-True) \u00a9 Nh V 4 PF Hf\n\nEpoch 1/10\n1563/1563 [: - 3s 2ms/step - loss: 1.8377 - accuracy: @.3369\nEpoch 2/10\n1563/1563 [=== ~ 3s 2ms/step - loss: 1.6552 - accuracy: 0.4076\nEpoch 3/10\n1563/1563 [- 3s 2ms/step - loss: 1.5743 - accuracy: 0.4396\nEpoch 4/10\n1563/1563 [- 3s 2ms/step - loss: 1.5288 - accuracy: 0.4549\nEpoch 5/10\n1563/1563 [ 3s 2ms/step - loss: 1.4888 - accuracy: 0.4706\nEpoch 6/10\n1563/1563 [=ss=sssssssscasscasscssss==s==] - 2s 2ms/step - loss: 1.4542 - accuracy: 0.4851\nEpoch 7/10\n1563/1563 [- 3s 2ms/step - loss: 1.4332 - accuracy: 0.4908\nEpoch 8/10\n1563/1563 [- 2s 2ms/step - loss: 1.4094 - accuracy: 0.4992\nEpoch 9/10\n1563/1563 [====s=s=sss=s=s=s=s=s=========] - 2s 2ms/step - loss: 1.3896 - accuracy: 0.5045\n\nEpoch 10/10\n1563/1563 [=========:\n\n2ms/step - loss: 1.3696 - accuracy: @.5167\n\nFigure 2-7. The output from the \u00a3it method\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.17,
                        "section_name": "Evaluating the Model",
                        "section_path": "./screenshots-images-2/chapter_2/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_17/c99540b3-c0ab-4ec7-8ca1-2e792f285b89.png",
                            "./screenshots-images-2/chapter_2/section_17/92bc2c48-9122-4fb4-9ea0-bfffdec88e5a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Evaluating the Model\n\nWe know the model achieves an accuracy of 51.9% on the training set, but how does it perform on data it\nhas never seen?\n\nTo answer this question we can use the evaluate method provided by Keras, as shown in Example 2-9.\n\nExample 2-9. Evaluating the model performance on the test set\n\nmodel.evaluate(x_test, y test)\n\nFigure 2-8 shows the output from this method.\n\n10000/10000 (===sassa=saseassesss=s=ss=====) - 1s 55u8/step\n\n[1.4358007415771485, 0.4896]\n\nFigure 2-8. The output from the evaluate method\n\nThe output is a list of the metrics we are monitoring: categorical cross-entropy and accuracy. We can see\nthat model accuracy is still 49.0% even on images that it has never seen before. Note that if the model\nwere guessing randomly, it would achieve approximately 10% accuracy (because there are 10 classes), so\n49.0% is a good result, given that we have used a very basic neural network.\n\nWe can view some of the predictions on the test set using the predict method, as shown in Example 2-\n10.\n\nExample 2-10. Viewing predictions on the test set using the predict method\n\nCLASSES = np.array(['airplane', \u2018automobile\u2019, 'bird', \u2018cat', \u2018deer', 'dog'\n, \u2018frog', \u2018horse', \u2018ship', \u2018truck'])\n\npreds = model.predict(x_test) \u00ae\npreds_single = CLASSES[np.argmax(preds, axis = -1)] @\nactual_single = CLASSES[np.argmax(y test, axis = -1)]\n\n@ preds isan array of shape [10000, 10] \u2014i.e., a vector of 10 class probabilities for each\nobservation.\n\n@ We convert this array of probabilities back into a single prediction using numpy\u2019s argmax func-\ntion. Here, axis = \u20141 tells the function to collapse the array over the last dimension (the classes\ndimension), so that the shape of preds_single isthen [10000, 1].\n\nWe can view some of the images alongside their labels and predictions with the code in Example 2-11. As\nexpected, around half are correct.\n\nExample 2-11. Displaying predictions of the MLP against the actual labels\n\nimport matplotlib.pyplot as plt\n\nn_to_show = 10\nindices = np.random.choice(range(len(x_test)), n_to_show)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n\nimg = x_test[idx]\n\nax = fig.add_subplot(1, n_to_show, itl)\n\nax.axis('off')\n\nax.text(0.5, -0.35, \u2018pred = ' + str(preds_single[idx]), fontsize=10\n, ha='center', transform=ax.transAxes)\n\nax.text(0.5, -0.7, \u2018act = ' + str(actual_single[idx]), fontsize=10\n, ha='center', transform=ax.transAxes)\n\nax.imshow(img)\n\nFigure 2-9 shows a randomly chosen selection of predictions made by the model, alongside the true\n\nlabels.\n\npred = horse pred = cat pred = ship pred = ship pred = deer\nact = horse act = bird act = ship act = truck act= frog\n}\n\nFame . . .\n\npred = airplaine pred = cat pred = ship pred = frog pred = dog\nact= airplane act=cat act = ship act = frog act = cat\n\nFigure 2-9. Some predictions made by the model, alongside the actual labels\n\nCongratulations! You\u2019ve just built a multilayer perceptron using Keras and used it to make predictions on\nnew data. Even though this is a supervised learning problem, when we come to building generative mod-\nels in future chapters many of the core ideas from this chapter (such as loss functions, activation func-\ntions, and understanding layer shapes) will still be extremely important. Next we\u2019ll look at ways of im-\nproving this model, by introducing a few new layer types.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.18,
                        "section_name": "Convolutional Neural Network (CNN)",
                        "section_path": "./screenshots-images-2/chapter_2/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_18/0b5de072-ab8d-4c39-8d5c-963ecfc25bfb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Convolutional Neural Network (CNN)\n\nOne of the reasons our network isn\u2019t yet performing as well as it might is because there isn\u2019t anything in\nthe network that takes into account the spatial structure of the input images. In fact, our first step is to\n\nflatten the image into a single vector, so that we can pass it to the first Dense layer!\n\nTo achieve this we need to use a convolutional layer.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.19,
                        "section_name": "Convolutional Layers",
                        "section_path": "./screenshots-images-2/chapter_2/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_19/c710304a-3e98-4027-983d-2f483e1c01f3.png",
                            "./screenshots-images-2/chapter_2/section_19/f3cea52a-ff70-4982-90e3-81ab3e1c83a5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Convolutional Layers\nFirst, we need to understand what is meant by a convolution in the context of deep learning.\n\nFigure 2-10 shows two different 3 x 3 x 1 portions of a grayscale image being convoluted with a3 x 3x1\nfilter (or kernel). The convolution is performed by multiplying the filter pixelwise with the portion of the\nimage, and summing the results. The output is more positive when the portion of the image closely\nmatches the filter and more negative when the portion of the image is the inverse of the filter. The top ex-\nample resonates strongly with the filter, so it produces a large positive value. The bottom example does\n\nnot resonate much with the filter, so it produces a value near zero.\n\n3 x 3 portion of an image Filter\n\n28\n\nafaefos\n\nFigure 2-10. A 3 x 3 convolutional filter applied to two portions of a grayscale image\n\nIf we move the filter across the entire image from left to right and top to bottom, recording the convolu-\ntional output as we go, we obtain a new array that picks out a particular feature of the input, depending\non the values in the filter. For example, Figure 2-11 shows two different filters that highlight horizontal\n\nand vertical edges.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nYou can see this convolutional process worked through manually in the Jupyter notebook located at\nnotebooks/02_deeplearning/02_cnn/convolutions.ipynb in the book repository.\n\n\nInput layer\n1*64\u00ab64x1\nbatch_size height \u00ab width x channels\n\nOutput\n1*x64* 64 x2\ncall wort S33 x] batch size \u00ab height \u00bb width x channels\n\nFigure 2-11. Two convolutional filters applied to a grayscale image\n\nA convolutional layer is simply a collection of filters, where the values stored in the filters are the weights\nthat are learned by the neural network through training. Initially these are random, but gradually the fil-\n\nters adapt their weights to start picking out interesting features such as edges or particular color combi-\nnations.\n\nIn Keras, the Conv2D layer applies convolutions to an input tensor with two spatial dimensions (such as\n\nan image). For example, the code shown in Example 2-12 builds a convolutional layer with two filters, to\nmatch the example in Figure 2-11.\n\nExample 2-12. A Conv2D layer applied to grayscale input images\n\nfrom tensorflow.keras import layers\n\ninput_layer = layers.Input(shape=(64,64,1))\nconv_layer_1 = layers.Conv2D(\n\nfilters = 2\n\n, kernel_size = (3,3)\n\n, strides = 1\n\n, padding = \"same\"\n\n) (input_layer)\n\nNext, let\u2019s look at two of the arguments to the Conv2D layer in more detail\u2014 strides and padding.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.2,
                        "section_name": "Stride",
                        "section_path": "./screenshots-images-2/chapter_2/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_20/9297c1b3-88ee-4450-aa08-c7c8e2af65ca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stride\n\nThe strides parameter is the step size used by the layer to move the filters across the input. Increasing\nthe stride therefore reduces the size of the output tensor. For example, when strides = 2, the height\nand width of the output tensor will be half the size of the input tensor. This is useful for reducing the spa-\ntial size of the tensor as it passes through the network, while increasing the number of channels.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.21,
                        "section_name": "Padding",
                        "section_path": "./screenshots-images-2/chapter_2/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_21/9c63163f-2a27-4a44-b598-5d132a145300.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Padding\n\nThe padding = \"same\" input parameter pads the input data with zeros so that the output size from the\nlayer is exactly the same as the input size when strides = 1.\n\nFigure 2-12 shows a 3 x 3 kernel being passed over a 5 x 5 input image, with padding = \"same\" and\nstrides = 1. The output size from this convolutional layer would also be 5 x 5, as the padding allows\nthe kernel to extend over the edge of the image, so that it fits five times in both directions. Without pad-\nding, the kernel could only fit three times along each direction, giving an output size of 3 x 3.\n\nFigure 2-12. A 3 x 3x 1 kernel (gray) being passed over a 5 x 5 x 1 input image (blue), with padding =\nthe 5 x 5 x 1 output (green) (source: Dumoulin and Visin, 2018)\u201d\n\nsame\" and strides = 1, to generate\n\nSetting padding = \"same\" is a good way to ensure that you are able to easily keep track of the size of\nthe tensor as it passes through many convolutional layers. The shape of the output from a convolutional\nlayer with padding = \"same\" is:\n\ninput height input width\nstride\u2019 stride\n\n; fiters)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.22,
                        "section_name": "Stacking convolutional layers",
                        "section_path": "./screenshots-images-2/chapter_2/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_22/b5b5867c-9e71-4bff-ad59-3f381b58c018.png",
                            "./screenshots-images-2/chapter_2/section_22/545c2450-7ca8-43d7-b0c6-0d8f36e8783e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stacking convolutional layers\n\nThe output of a Conv2D layer is another four-dimensional tensor, now of shape (batch_size,\nheight, width, filters) ,so we can stack Conv2D layers on top of each other to grow the depth of\nour neural network and make it more powerful. To demonstrate this, let\u2019s imagine we are applying\nConv2D layers to the CIFAR-10 dataset and wish to predict the label of a given image. Note that this time,\ninstead of one input channel (grayscale) we have three (red, green, and blue).\n\nExample 2-13 shows how to build a simple convolutional neural network that we could train to succeed\nat this task.\n\nExample 2-13. Code to build a convolutional neural network model using Keras\n\nfrom tensorflow.keras import layers, models\n\ninput_layer = layers.Input(shape=(32,32,3))\nconv_layer_1 = layers.Conv2D(\n\nfilters = 10\n\n, kernel_size = (4,4)\n\n, strides = 2\n\n, padding = \u2018same\u2019\n\n) (input_layer)\nconv_layer_2 = layers.Conv2D(\n\nfilters = 20\n\n, kernel_size = (3,3)\n\n, strides = 2\n\n, padding = \u2018same\u2019\n\n) (conv_layer_1)\nflatten_layer = layers.Flatten()(conv_layer_ 2)\noutput_layer = layers.Dense(units=10, activation = \u2018softmax')(flatten_layer)\nmodel = models.Model(input_layer, output_layer)\n\nThis code corresponds to the diagram shown in Figure 2-13.\n\n10 channels\n\n3channels 1280 nodes\n\n3 channels Z.\nre! 10 channels\n\na\n\n@ output size\n@ (2* 16 x 16 \u00ab 10)\n\n(2x32 323)\n\noutput size\n(2x8x8x 20)\n\nWH: W\n\n10 filters 20 filters\neachsized 4x 4x3 each sized 3 x3 \u00ab10\n(applied with strides = 2 (applied with strides = 2 output size output size\nand padding = \"same\") and padding = \"same\") (?* 1280) (?* 10)\n\nCONV LAYER1\n\nFigure 2-13. A diagram of a convolutional neural network\n\nNote that now that we are working with color images, each filter in the first convolutional layer has a\ndepth of 3 rather than 1 (i.e., each filter has shape 4 x 4 x 3, rather than 4 x 4 x 1). This is to match the\nthree channels (red, green, blue) of the input image. The same idea applies to the filters in the second\nconvolutional layer that have a depth of 10, to match the 10 channels output by the first convolutional\nlayer.\n\nTIP\n\nIn general, the depth of the filters in a layer is always equal to the number of channels output by the preceding layer.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.23,
                        "section_name": "Inspecting the model",
                        "section_path": "./screenshots-images-2/chapter_2/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_23/7a716a4e-05a6-4d19-8030-a0088846bceb.png",
                            "./screenshots-images-2/chapter_2/section_23/d4d85b4e-d7ab-4384-ae4c-aefc6a14f47f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Inspecting the model\n\nIt\u2019s really informative to look at how the shape of the tensor changes as data flows through from one con-\nvolutional layer to the next. We can use the model.summary() method to inspect the shape of the tensor\nas it passes through the network (Table 2-2).\n\nTable 2-2. CNN model summary\n\nLayer (type) Output shape Param #\n\nInputLayer (None, 32, 32,3) 0\n\nConv2D (None, 16, 16,10) 490\nConv2D (None, 8, 8, 20) 1,820\nFlatten (None, 1280) 0\nDense (None, 10) 12,810\nTotal params 15,120\nTrainable params 15,120\n\nNon-trainable params 0\n\nLet\u2019s walk through our network layer by layer, noting the shape of the tensor as we go:\n\n1. The input shape is (None, 32, 32, 3) \u2014Keras uses None to represent the fact that we can pass\nany number of images through the network simultaneously. Since the network is just performing\ntensor algebra, we don\u2019t need to pass images through the network individually, but instead can pass\nthem through together as a batch.\n\n2. The shape of each of the 10 filters in the first convolutional layer is 4 x 4 x 3. This is because we have\nchosen each filter to have a height and width of 4(kernel_size = (4,4) ) and there are three\nchannels in the preceding layer (red, green, and blue). Therefore, the number of parameters (or\nweights) in the layer is (4 x 4 x 3 + 1) x 10 = 490, where the + 1 is due to the inclusion of a bias term\nattached to each of the filters. The output from each filter will be the pixelwise multiplication of the\nfilter weights and the 4 x 4 x 3 section of the image it is covering. As strides = 2 and padding =\n\"same\" , the width and height of the output are both halved to 16, and since there are 10 filters the\noutput of the first layer is a batch of tensors each having shape [16, 16, 10].\n\n3. In the second convolutional layer, we choose the filters to be 3 x 3 and they now have depth 10, to\nmatch the number of channels in the previous layer. Since there are 20 filters in this layer, this gives\na total number of parameters (weights) of (3 x 3 x 10 + 1) x 20 = 1,820. Again, we use strides = 2\nand padding = \"same\", so the width and height both halve. This gives us an overall output shape\nof (None, 8, 8, 20).\n\n4. We now flatten the tensor using the Keras Flatten layer. This results in a set of 8 x 8 x 20 = 1,280\nunits. Note that there are no parameters to learn ina Flatten layer as the operation is just a re-\nstructuring of the tensor.\n\n\n5. We finally connect these units to a 10-unit Dense layer with softmax activation, which represents\nthe probability of each category in a 10-category classification task. This creates an extra 1,280 x 10 =\n\n12,810 parameters (weights) to learn.\n\nThis example demonstrates how we can chain convolutional layers together to create a convolutional\nneural network. Before we see how this compares in accuracy to our densely connected neural network,\nwe'll examine two more techniques that can also improve performance: batch normalization and\n\ndropout.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.24,
                        "section_name": "Batch Normalization",
                        "section_path": "./screenshots-images-2/chapter_2/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_24/a79dcaa8-61e4-48e2-9c1f-31f75c5fd71e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Batch Normalization\n\nOne common problem when training a deep neural network is ensuring that the weights of the network\nremain within a reasonable range of values\u2014if they start to become too large, this is a sign that your net-\nwork is suffering from what is known as the exploding gradient problem. As errors are propagated back-\nward through the network, the calculation of the gradient in the earlier layers can sometimes grow expo-\nnentially large, causing wild fluctuations in the weight values.\n\nWARNING\nIf your loss function starts to return NaN , chances are that your weights have grown large enough to cause an over-\n\nflow error.\n\nThis doesn\u2019t necessarily happen immediately as you start training the network. Sometimes it can be hap-\npily training for hours when suddenly the loss function returns NaN and your network has exploded.\nThis can be incredibly annoying. To prevent it from happening, you need to understand the root cause of\nthe exploding gradient problem.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.25,
                        "section_name": "Covariate shift",
                        "section_path": "./screenshots-images-2/chapter_2/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_25/e8ca422a-e8df-4d48-929c-6992549fbe97.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Covariate shift\n\nOne of the reasons for scaling input data to a neural network is to ensure a stable start to training over\nthe first few iterations. Since the weights of the network are initially randomized, unscaled input could\npotentially create huge activation values that immediately lead to exploding gradients. For example, in-\nstead of passing pixel values from 0-255 into the input layer, we usually scale these values to between -1\nand 1.\n\nBecause the input is scaled, it\u2019s natural to expect the activations from all future layers to be relatively well\nscaled as well. Initially this may be true, but as the network trains and the weights move further away\nfrom their random initial values, this assumption can start to break down. This phenomenon is known as\n\ncovariate shift.\n\nCOVARIATE SHIFT ANALOGY\n\nImagine you're carrying a tall pile of books, and you get hit by a gust of wind. You move the books in a direction op-\nposite to the wind to compensate, but as you do so, some of the books shift, so that the tower is slightly more unstable\nthan before. Initially, this is OK, but with every gust the pile becomes more and more unstable, until eventually the\nbooks have shifted so much that the pile collapses. This is covariate shift.\n\nRelating this to neural networks, each layer is like a book in the pile. To remain stable, when the network updates the\nweights, each layer implicitly assumes that the distribution of its input from the layer beneath is approximately con-\nsistent across iterations. However, since there is nothing to stop any of the activation distributions shifting signifi-\ncantly in a certain direction, this can sometimes lead to runaway weight values and an overall collapse of the\nnetwork.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.26,
                        "section_name": "Training using batch normalization",
                        "section_path": "./screenshots-images-2/chapter_2/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_26/d2d72103-4fa2-47b3-a482-efe65c04f494.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training using batch normalization\n\nBatch normalization is a technique that was originally proposed as a solution to this problem and has\nsince been shown to bring other benefits such as a smoother parameter space and smoother gradients.\nThe solution is surprisingly simple. During training, a batch normalization layer calculates the mean and\nstandard deviation of each of its input channels across the batch and normalizes by subtracting the mean\nand dividing by the standard deviation. There are then two learned parameters for each channel, the\nscale (gamma) and shift (beta). The output is simply the normalized input, scaled by gamma and shifted\nby beta. Figure 2-14 shows the whole process.\n\nInput: Values of x over a mini-batch: B = {21...m};\nParameters to be learned: y, 6\n\nOutput: {y; = BNy,6(xi)}\n\n// mini-batch mean\n\n// mini-batch variance\n\n// normalize\n\nYi \u2014 Y2i + B = BNy,2(x:) // scale and shift\n\nAlgorithm 1: Batch Normalizing Transform, applied to\nactivation x over a mini-batch.\n\n6\nFigure 2-14. The batch normalization process (source: loffe and Szegedy, 2015)\n\nWe can place batch normalization layers after dense or convolutional layers to normalize the output.\n\nTIP\nReferring to our previous example, it\u2019s a bit like connecting the layers of books with small sets of adjustable springs\n\nthat ensure there aren\u2019t any overall huge shifts in their positions over time.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.27,
                        "section_name": "Prediction using batch normalization",
                        "section_path": "./screenshots-images-2/chapter_2/section_27",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_27/80557132-a550-42a3-a5e6-4c81797427ce.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Prediction using batch normalization\n\nYou might be wondering how this layer works at prediction time. When it comes to prediction, we may\nonly want to predict a single observation, so there is no batch over which to calculate the mean and stan-\ndard deviation. To get around this problem, during training a batch normalization layer also calculates\nthe moving average of the mean and standard deviation of each channel and stores this value as part of\nthe layer to use at test time.\n\nHow many parameters are contained within a batch normalization layer? For every channel in the pre-\nceding layer, two weights need to be learned: the scale (gamma) and shift (beta). These are the trainable\nparameters. The moving average and standard deviation also need to be calculated for each channel, but\nsince they are derived from the data passing through the layer rather than trained through backpropaga-\ntion, they are called nontrainable parameters. In total, this gives four parameters for each channel in the\npreceding layer, where two are trainable and two are nontrainable.\n\nIn Keras, the BatchNormalization layer implements the batch normalization functionality, as shown.\nin Example 2-14.\n\nExample 2-14. A BatchNormalization layer in Keras\n\nfrom tensorflow.keras import layers\nlayers.BatchNormalization(momentum = 0.9)\n\nThe momentum parameter is the weight given to the previous value when calculating the moving average\nand moving standard deviation.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.28,
                        "section_name": "Dropout",
                        "section_path": "./screenshots-images-2/chapter_2/section_28",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_28/7499a4f7-d24d-4829-9933-cb7be96d5715.png",
                            "./screenshots-images-2/chapter_2/section_28/87a7b26c-0d5f-48f9-a4c0-6ca9caac7ae8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Dropout\n\nWhen studying for an exam, it is common practice for students to use past papers and sample questions\nto improve their knowledge of the subject material. Some students try to memorize the answers to these\nquestions, but then come unstuck in the exam because they haven\u2019t truly understood the subject matter.\nThe best students use the practice material to further their general understanding, so that they are still\nable to answer correctly when faced with new questions that they haven\u2019t seen before.\n\nThe same principle holds for machine learning. Any successful machine learning algorithm must ensure\nthat it generalizes to unseen data, rather than simply remembering the training dataset. If an algorithm\nperforms well on the training dataset, but not the test dataset, we say that it is suffering from overfitting.\nTo counteract this problem, we use regularization techniques, which ensure that the model is penalized if\nit starts to overfit.\n\nThere are many ways to regularize a machine learning algorithm, but for deep learning, one of the most\n1\ncommon is by using dropout layers. This idea was introduced by Hinton et al. in 2012 and presented ina\n8\n2014 paper by Srivastava et al.\u00bb\n\nDropout layers are very simple. During training, each dropout layer chooses a random set of units from.\nthe preceding layer and sets their output to 0, as shown in Figure 2-15.\n\nIncredibly, this simple addition drastically reduces overfitting by ensuring that the network doesn\u2019t be-\ncome overdependent on certain units or groups of units that, in effect, just remember observations from\nthe training set. If we use dropout layers, the network cannot rely too much on any one unit and there-\nfore knowledge is more evenly spread across the whole network.\n\nThis makes the model much better at generalizing to unseen data, because the network has been trained\nto produce accurate predictions even under unfamiliar conditions, such as those caused by dropping ran-\ndom units. There are no weights to learn within a dropout layer, as the units to drop are decided stochas-\ntically. At prediction time, the dropout layer doesn\u2019t drop any units, so that the full network is used to\nmake predictions.\n\nDROPOUT ANALOGY\n\nReturning to our analogy, it\u2019s a bit like a math student practicing past papers with a random selection of key formulae\nmissing from their formula book. This way, they learn how to answer questions through an understanding of the\ncore principles, rather than always looking up the formulae in the same places in the book. When it comes to test\ntime, they will find it much easier to answer questions that they have never seen before, due to their ability to gener-\nalize beyond the training material.\n\nThe Dropout layer in Keras implements this functionality, with the rate parameter specifying the pro-\nportion of units to drop from the preceding layer, as shown in Example 2-15.\n\nExample 2-15. A Dropout layer in Keras\n\nfrom tensorflow.keras import layers\nlayers.Dropout(rate = 0.25)\n\nDropout layers are used most commonly after dense layers since these are the most prone to overfitting\ndue to the higher number of weights, though you can also use them after convolutional layers.\n\nTIP\n\nBatch normalization also has been shown to reduce overfitting, and therefore many modern deep learning architec-\ntures don\u2019t use dropout at all, relying solely on batch normalization for regularization. As with most deep learning\nprinciples, there is no golden rule that applies in every situation\u2014the only way to know for sure what\u2019s best is to test\ndifferent architectures and see which performs best on a holdout set of data.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.29,
                        "section_name": "Building the CNN",
                        "section_path": "./screenshots-images-2/chapter_2/section_29",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_29/c3645767-d94d-4a89-9e63-96ce95368873.png",
                            "./screenshots-images-2/chapter_2/section_29/967375ed-9835-4785-86b4-60773a46fe47.png",
                            "./screenshots-images-2/chapter_2/section_29/1ac0294f-79c5-43ce-9811-22c97a278238.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Building the CNN\n\nYou\u2019ve now seen three new Keras layer types: Conv2D, BatchNormalization, and Dropout . Let\u2019s put\nthese pieces together into a CNN model and see how it performs on the CIFAR-10 dataset.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nYou can run the following example in the Jupyter notebook in the book repository called\nnotebooks/02_deeplearning/02_cnn/cnn.ipynb.\n\nThe model architecture we shall test is shown in Example 2-16.\n\nExample 2-16. Code to build a CNN model using Keras\n\nfrom tensorflow.keras import layers, models\n\ninput_layer = layers.Input((32,32,3))\n\n*\n\"\n\nlayers.Conv2D(filters = 32, kernel_size = 3\n\n, strides = 1, padding = 'same')(input_layer)\nx = layers.BatchNormalization() (x)\nx = layers.LeakyReLU() (x)\n\nx = layers.Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = '\u2018same')(x\nx = layers.BatchNormalization() (x)\nx = layers.LeakyReLU() (x)\n\nx = layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = '\u2018same')(x\nx = layers.BatchNormalization() (x)\nx = layers.LeakyReLU() (x)\n\nx = layers.Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x\nx = layers.BatchNormalization() (x)\n\nx = layers.LeakyReLU() (x)\n\nx = layers.Flatten() (x)\n\nx = layers.Dense(128) (x)\n\nx = layers.BatchNormalization() (x)\n\nx = layers.LeakyReLU() (x)\n\nx = layers.Dropout(rate = 0.5)(x)\n\noutput_layer = layers.Dense(10, activation = '\u2018softmax') (x)\n\nmodel = models.Model(input_layer, output_layer)\n\nWe use four stacked Conv2D layers, each followed by a BatchNormalization anda LeakyReLU layer.\n\nAfter flattening the resulting tensor, we pass the data through a Dense layer of size 128, again followed\n\nby a BatchNormalization anda LeakyReLU layer. This is immediately followed by a Dropout layer\n\nfor regularization, and the network is concluded with an output Dense layer of size 10.\n\nTIP\n\nThe order in which to use the batch normalization and activation layers is a matter of preference. Usually batch nor-\nmalization layers are placed before the activation, but some successful architectures use these layers the other way\n\naround. If you do choose to use batch normalization before activation, you can remember the order using the acro-\nnym BAD (batch normalization, activation, then dropout)!\n\nThe model summary is shown in Table 2-3.\n\nTable 2-3. Model summary of the CNN for CIFAR-10\n\nLayer (type)\nInputLayer\n\nConv2D\nBatchNormalization\nLeakyReLU\n\nConv2D\nBatchNormalization\nLeakyReLU\n\nConv2D\nBatchNormalization\nLeakyReLU\n\nConv2D\nBatchNormalization\nLeakyReLU\n\nFlatten\n\nDense\nBatchNormalization\nLeakyReLU\n\nDropout\n\nOutput shape\n(None, 32, 32, 3)\n(None, 32, 32, 32)\n(None, 32, 32, 32)\n(None, 32, 32, 32)\n(None, 16, 16, 32)\n(None, 16, 16, 32)\n(None, 16, 16, 32)\n(None, 16, 16, 64)\n(None, 16, 16, 64)\n(None, 16, 16, 64)\n(None, 8, 8, 64)\n(None, 8, 8, 64)\n(None, 8, 8, 64)\n(None, 4096)\n(None, 128)\n(None, 128)\n(None, 128)\n\n(None, 128)\n\nParam #\n\n896\n\n128\n\n9,248\n\n128\n\n18,496\n\n256\n\n36,928\n\n256\n\n0\n\n524,416\n\n512\n\nDense (None, 128) 524,416\n\nBatchNormalization (None, 128) 512\nLeakyReLU (None, 128) 0\nDropout (None, 128) 0\nDense (None, 10) 1290\nTotal params 592,554\n\nTrainable params 591,914\n\nNon-trainable params 640\n\nTIP\n\nBefore moving on, make sure you are able to calculate the output shape and number of parameters for each layer by\nhand. It\u2019s a good exercise to prove to yourself that you have fully understood how each layer is constructed and how\nit is connected to the preceding layer! Don\u2019t forget to include the bias weights that are included as part of the\n\nConv2D and Dense layers.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.3,
                        "section_name": "Training and Evaluating the CNN",
                        "section_path": "./screenshots-images-2/chapter_2/section_30",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_30/50f5faf2-5ba1-4012-917e-62e38594c58e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training and Evaluating the CNN\n\nWe compile and train the model in exactly the same way as before and call the evaluate method to de-\ntermine its accuracy on the holdout set (Figure 2-16).\n\nmodel.evaluate(x_test, y test, batch_size=1000)\n\n10000/10000 [== =] - 15s 1ms/step\n\n[0.8423407137393951, 0.7155999958515167]\n\nFigure 2-16. CNN performance\n\nAs you can see, this model is now achieving 71.5% accuracy, up from 49.0% previously. Much better!\nFigure 2-17 shows some predictions from our new convolutional model.\n\nThis improvement has been achieved simply by changing the architecture of the model to include convo-\nlutional, batch normalization, and dropout layers. Notice that the number of parameters is actually fewer\nin our new model than the previous model, even though the number of layers is far greater. This demon-\nstrates the importance of being experimental with your model design and being comfortable with how\nthe different layer types can be used to your advantage. When building generative models, it becomes\neven more important to understand the inner workings of your model since it is the middle layers of\nyour network that capture the high-level features that you are most interested in.\n\nLn\n4 =\n- \u2122\npred = dog pred = frog pred = truck pred = ship pred = ship\nact= dog act = frog act= truck act = ship act = ship\n7\n7.\n* a a\npred = dog pred = cat pred = horse pred = airplaine pred = ship\nact=dog act =cat act=deer act = airplane act = airplane\n\nFigure 2-17. CNN predictions\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 2.31,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_2/section_31",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_31/d403b71a-33bf-4dca-8960-c3007a4ef3db.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nThis chapter introduced the core deep learning concepts that you will need to start building deep genera-\ntive models. We started by building a multilayer perceptron (MLP) using Keras and trained the model to\npredict the category of a given image from the CIFAR-10 dataset. Then, we improved upon this architec-\nture by introducing convolutional, batch normalization, and dropout layers to create a convolutional\n\nneural network (CNN).\n\nA really important point to take away from this chapter is that deep neural networks are completely flexi-\nble by design, and there really are no fixed rules when it comes to model architecture. There are guide-\nlines and best practices, but you should feel free to experiment with layers and the order in which they\nappear. Don\u2019t feel constrained to only use the architectures that you have read about in this book or else-\nwhere! Like a child with a set of building blocks, the design of your neural network is only limited by\nyour own imagination.\n\nIn the next chapter, we shall see how we can use these building blocks to design a network that can gen-\nerate images.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 3,
                "chapter_name": "Chapter 3. Variational Autoencoders",
                "chapter_path": "./screenshots-images-2/chapter_3",
                "sections": [
                    {
                        "section_id": 3.1,
                        "section_name": "Chapter 3. Variational Autoencoders",
                        "section_path": "./screenshots-images-2/chapter_3/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_1/83b96a27-9888-4bb5-94a1-164e0cec5c86.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 3. Variational Autoencoders\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\n\u00a2 Learn how the architectural design of autoencoders makes them perfectly suited to generative\nmodeling.\n\n\u00a2 Build and train an autoencoder from scratch using Keras.\n\n\u00a9 Use autoencoders to generate new images, but understand the limitations of this approach.\n\n\u00a2 Learn about the architecture of the variational autoencoder and how it solves many of the problems\nassociated with standard autoencoders.\n\n\u00a2 Build a variational autoencoder from scratch using Keras.\n\n\u00a9 Use variational autoencoders to generate new images.\n\n\u00a9 Use variational autoencoders to manipulate generated images using latent space arithmetic.\n\nIn 2013, Diederik P. Kingma and Max Welling published a paper that laid the foundations for a type of\nneural network known as a variational autoencoder (VAE). This is now one of the most fundamental and\nwell-known deep learning architectures for generative modeling and an excellent place to start our jour-\nney into generative deep learning.\n\nIn this chapter, we shall start by building a standard autoencoder and then see how we can extend this\nframework to develop a variational autoencoder. Along the way, we will pick apart both types of models,\nto understand how they work at a granular level. By the end of the chapter you should have a complete\nunderstanding of how to build and manipulate autoencoder-based models and, in particular, how to\nbuild a variational autoencoder from scratch to generate images based on your own dataset.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_3/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_2/7f796e1a-a178-4f99-b260-e676f412036f.png",
                            "./screenshots-images-2/chapter_3/section_2/711e0f3d-e7fe-4785-8850-b8f1e9efed12.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nLet\u2019s start with a simple story that will help to explain the fundamental problem that an autoencoder is\n\ntrying to solve.\n\nBRIAN, THE STITCH, AND THE WARDROBE\nImagine that on the floor in front of you is a pile of all the clothing you own\u2014trousers, tops, shoes, and\ncoats, all of different styles. Your stylist, Brian, is becoming increasingly frustrated with how long it takes\n\nhim to find the items you require, so he devises a clever plan.\n\nHe tells you to organize your clothes into a wardrobe that is infinitely high and wide ( ). When\nyou want to request a particular item, you simply need to tell Brian its location and he will sew the item\nfrom scratch using his trusty sewing machine. It soon becomes obvious that you will need to place similar\nitems near to each other, so that Brian can accurately re-create each item given only its location.\n\nFigure 3-1. A man standing in front of an infinite 2D wardrobe (created with 1)\n\nAfter several weeks of practice, you and Brian have adjusted to each other\u2019s understandings of the\nwardrobe layout. It is now possible for you to tell Brian the location of any item of clothing that you de-\nsire, and he can accurately sew it from scratch!\n\nThis gives you an idea\u2014what would happen if you gave Brian a wardrobe location that was empty? To\nyour amazement, you find that Brian is able to generate entirely new items of clothing that haven\u2019t exist-\ned before! The process isn\u2019t perfect, but you now have limitless options for generating new clothing, just\nby picking an empty location in the infinite wardrobe and letting Brian work his magic with the sewing\nmachine.\n\nLet\u2019s now explore how this story relates to building autoencoders.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.3,
                        "section_name": "Autoencoders",
                        "section_path": "./screenshots-images-2/chapter_3/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_3/bcfba241-d70f-4f64-96fd-de0cf32a0c90.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Autoencoders\n\nA diagram of the process described by the story is shown in Figure 3-2. You play the part of the encoder,\nmoving each item of clothing to a location in the wardrobe. This process is called encoding. Brian plays\nthe part of the decoder, taking a location in the wardrobe and attempting to re-create the item. This\nprocess is called decoding.\n\nOriginal item\n\nReconstruction\n\n2\n\n10.0 = -75 50 -25 0.0 25 50 75\n\nFigure 3-2. Items of clothing in the infinite wardrobe\u2014each black dot represents an item of clothing\n\nEach location in the wardrobe is represented by two numbers (i.e., a 2D vector). For example, the\ntrousers in Figure 3-2 are encoded to the point [6.3, -0.9]. This vector is also known as an embedding be-\ncause the encoder attempts to embed as much information into it as possible, so that the decoder can pro-\nduce an accurate reconstruction.\n\nAn autoencoder is simply a neural network that is trained to perform the task of encoding and decoding\nan item, such that the output from this process is as close to the original item as possible. Crucially, it can\nbe used as a generative model, because we can decode any point in the 2D space that we want (in particu-\nlar, those that are not embeddings of original items) to produce a novel item of clothing.\n\nLet\u2019s now see how we can build an autoencoder using Keras and apply it to a real dataset!\n\nRUNNING THE CODE FOR THIS EXAMPLE\n\nThe code for this example can be found in the Jupyter notebook located at notebooks/03_vae/01_autoencoder/autoen-\ncoder.ipynb in the book repository.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.4,
                        "section_name": "The Fashion-MNIST Dataset",
                        "section_path": "./screenshots-images-2/chapter_3/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_4/2e261f4e-2729-406b-90bb-97e71e380536.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Fashion-MNIST Dataset\n\nFor this example, we\u2019ll be using the Fashion-MNIST dataset\u2014a collection of grayscale images of clothing\nitems, each of size 28 x 28 pixels. Some example images from the dataset are shown in Figure 3-3.\n\naT i . li\n\nFigure 3-3. Examples of images from the Fashion-MNIST dataset\n\nThe dataset comes prepackaged with TensorFlow, so it can be downloaded as shown in Example 3-1.\n\nExample 3-1. Loading the Fashion-MNIST dataset\n\nfrom tensorflow.keras import datasets\n\n(x_train,y train), (x_test,y_ test) = datasets.fashion_mnist.load_data()\n\nThese are 28 x 28 grayscale images (pixel values between 0 and 255) out of the box, which we need to pre-\nprocess to ensure that the pixel values are scaled between 0 and 1. We will also pad each image to 32 x 32\n\nfor easier manipulation of the tensor shape as it passes through the network, as shown in Example 3-2.\n\nExample 3-2. Preprocessing the data\n\ndef preprocess(imgs):\nimgs = imgs.astype(\"float32\") / 255.0\nimgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)\nimgs = np.expand_dims(imgs, -1)\nreturn imgs\n\nx_train = preprocess(x_train)\n\nx_test = preprocess(x_test)\n\nNext, we need to understand the overall structure of an autoencoder, so that we can code it up using\nTensorFlow and Keras.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.5,
                        "section_name": "The Autoencoder Architecture",
                        "section_path": "./screenshots-images-2/chapter_3/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_5/d89e9a67-ef2c-4c83-b108-d5be04e4da50.png",
                            "./screenshots-images-2/chapter_3/section_5/11225034-231d-4b8a-aeb5-d8ad0e7263f1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Autoencoder Architecture\nAn autoencoder is a neural network made up of two parts:\n\ne An encoder network that compresses high-dimensional input data such as an image into a lower-di-\nmensional embedding vector\n\ne A decoder network that decompresses a given embedding vector back to the original domain (e.g.,\nback to an image)\n\nA diagram of the network architecture is shown in Figure 3-4. An input image is encoded to a latent em-\nbedding vector z, which is then decoded back to the original pixel space.\n\nEncoder 8\n\nFigure 3-4. Autoencoder architecture diagram\n\nThe autoencoder is trained to reconstruct an image, after it has passed through the encoder and back out\nthrough the decoder. This may seem strange at first\u2014why would you want to reconstruct a set of images\nthat you already have available to you? However, as we shall see, it is the embedding space (also called\nthe latent space) that is the interesting part of the autoencoder, as sampling from this space will allow us\nto generate new images.\n\nLet\u2019s first define what we mean by an embedding. The embedding (z) is a compression of the original im-\nage into a lower-dimensional latent space. The idea is that by choosing any point in the latent space, we\ncan generate novel images by passing this point through the decoder, since the decoder has learned how\n\nto convert points in the latent space into viable images.\n\nIn our example, we will embed images into a two-dimensional latent space. This will help us to visualize\nthe latent space, since we can easily plot points in 2D. In practice, the latent space of an autoencoder will\nusually have more than two dimensions in order to have more freedom to capture greater nuance in the\nimages.\n\nAUTOENCODERS AS DENOISING MODELS\n\nAutoencoders can be used to clean noisy images, since the encoder learns that it is not useful to capture the position\nof the random noise inside the latent space in order to reconstruct the original. For tasks such as this, a 2D latent\nspace is probably too small to encode sufficient relevant information from the input. However, as we shall see, in-\ncreasing the dimensionality of the latent space quickly leads to problems if we want to use the autoencoder as a gen-\nerative model.\n\n\nLet\u2019s now see how to build the encoder and decoder.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.6,
                        "section_name": "The Encoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_6/d8b88dd9-1122-4fdf-a842-aaf2c3199db8.png",
                            "./screenshots-images-2/chapter_3/section_6/f3614e63-6930-4c54-b96d-8e6e66eefedd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Encoder\n\nIn an autoencoder, the encoder\u2019s job is to take the input image and map it to an embedding vector in the\nlatent space. The architecture of the encoder we will be building is shown in Table 3-1.\n\nTable 3-1. Model summary of the encoder\n\nLayer (type) Output shape Param #\n\nInputLayer (None, 32, 32,1) 0\n\nConv2D (None, 16, 16,32) 320\nConv2D (None, 8, 8, 64) 18,496\nConv2D (None, 4, 4,128) 73,856\nFlatten (None, 2048) 0\nDense (None, 2) 4,098\nTotal params 96,770\nTrainable params 96,770\n\nNon-trainable params 0\n\nTo achieve this, we first create an Input layer for the image and pass this through three Conv2D layers\nin sequence, each capturing increasingly high-level features. We use a stride of 2 to halve the size of the\noutput of each layer, while increasing the number of channels. The last convolutional layer is flattened\n\nand connected toa Dense layer of size 2, which represents our two-dimensional latent space.\nExample 3-3 shows how to build this in Keras.\n\nExample 3-3. The encoder\n\nencoder_input = layers.Input(\n\nshape=(32, 32, 1), name = \u201cencoder _input\"\n)o\nx = layers.Conv2D(32, (3, 3), strides = 2, activation = 'relu', padding=\"same\") (\nencoder_input\n)@\nx = layers.Conv2D(64, (3, 3), strides = 2, activation = 'relu', padding=\"same\") (x\nx = layers.Conv2D(128, (3, 3), strides = 2, activation = 'relu', padding=\"same\") (:\n\nshape_before flattening = K.int_shape(x)[1:]\n\nx = layers.Flatten()(x) \u00a9\n\nencoder output = layers.Dense(2, name=\"encoder_output\")(x) @\n\nencoder = models.Model(encoder_input, encoder_output) @\n\n@ Define the Input layer of the encoder (the image).\n\n@ Stack Conv2D layers sequentially on top of each other.\n\n\u00ae Flatten the last convolutional layer to a vector.\n\n\u00ae Connect this vector to the 2D embeddings with a Dense layer.\n\n\u00a9 The Keras Model that defines the encoder\u2014a model that takes an input image and encodes it into\na 2D embedding.\n\nTIP\nI strongly encourage you to experiment with the number of convolutional layers and filters to understand how the\narchitecture affects the overall number of model parameters, model performance, and model runtime.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.7,
                        "section_name": "The Decoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_7/e9d8ccef-0cb3-4147-806b-fb46d4b04ff0.png",
                            "./screenshots-images-2/chapter_3/section_7/16383c05-fe1a-40d5-bec4-b1cc803e7222.png",
                            "./screenshots-images-2/chapter_3/section_7/f57b0961-7c0e-4066-b4f4-ea13468a6c68.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Decoder\n\nThe decoder is a mirror image of the encoder\u2014instead of convolutional layers, we use convolutional\n\ntranspose layers, as shown in Table 3-2.\n\nTable 3-2. Model summary of the decoder\n\nLayer (type) Output shape Param #\nInputLayer (None, 2) 0\n\nDense (None, 2048) 6,144\nReshape (None, 4, 4,128) 0\n\nConv2DTranspose (None, 8, 8,128) 147,584\nConv2DTranspose (None, 16, 16,64) 73,792\n\nConv2DTranspose (None, 32, 32,32) 18,464\n\nConv2D (None, 32, 32,1) 289\nTotal params 246,273\nTrainable params 246,273\n\nNon-trainable params 0\n\nCONVOLUTIONAL TRANSPOSE LAYERS\nStandard convolutional layers allow us to halve the size of an input tensor in both dimensions (height\nand width), by setting strides = 2.\n\nThe convolutional transpose layer uses the same principle as a standard convolutional layer (passing a\nfilter across the image), but is different in that setting strides = 2 doubles the size of the input tensor\nin both dimensions.\n\nIn a convolutional transpose layer, the strides parameter determines the internal zero padding be-\ntween pixels in the image, as shown in Figure 3-5. Here, a 3 x 3 x 1 filter (gray) is being passed across a 3 x\n3x 1 image (blue) with strides = 2, to produce a 6 * 6 x 1 output tensor (green).\n\nFigure 3-5. A convolutional transpose layer example (source: Dumoulin and Visin, 2018\n\nIn Keras, the Conv2DTranspose layer allows us to perform convolutional transpose operations on ten-\nsors. By stacking these layers, we can gradually expand the size of each layer, using strides of 2, until we\nget back to the original image dimension of 32 x 32.\n\nExample 3-4 shows how we build the decoder in Keras.\n\nExample 3-4. The decoder\n\ndecoder_input = layers.Input(shape=(2,), name=\"decoder_input\") O\nx = layers.Dense(np.prod(shape_before flattening) )(decoder_input) @\nx = layers.Reshape(shape_before_flattening)(x) \u00a9\n\nx = layers.Conv2DTranspose(\n\n128, (3, 3), strides=2, activation = 'relu', padding=\"same\"\n)(x) @\nx = layers.Conv2DTranspose(\n\n64, (3, 3), strides=2, activation = 'relu', padding=\"same\"\n)(x)\n\nx = layers.Conv2DTranspose(\n\n32, (3, 3), strides=2, activation = 'relu', padding=\"same\"\n)(x)\ndecoder_output = layers.Conv2D(\n\n1,\n\n(3, 3),\n\nstrides = 1,\n\nactivation=\"sigmoid\",\n\npadding=\"same\",\n\nname=\"decoder_output\"\n\n)(x)\n\ndecoder = models.Model(decoder_input, decoder_output) @\n\n@ Define the Input layer of the decoder (the embedding).\n\n\u00ae Connect the input toa Dense layer.\n\n\u00ae Reshape this vector into a tensor that can be fed as input into the first Conv2DTranspose layer.\n\n\u00ae Stack Conv2DTranspose layers on top of each other.\n\n\u00a9 The Keras Model that defines the decoder\u2014a model that takes an embedding in the latent space\nand decodes it into the original image domain.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.8,
                        "section_name": "Joining the Encoder to the Decoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_8/a56bae83-1691-446f-8eee-0fe2134aa158.png",
                            "./screenshots-images-2/chapter_3/section_8/6e2cd047-90c4-4820-adc7-a53a0ae9d7d6.png",
                            "./screenshots-images-2/chapter_3/section_8/1248a197-4c31-4856-abb7-6711d745713b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Joining the Encoder to the Decoder\n\nTo train the encoder and decoder simultaneously, we need to define a model that will represent the flow\nof an image through the encoder and back out through the decoder. Luckily, Keras makes it extremely\neasy to do this, as you can see in Example 3-5. Notice the way in which we specify that the output from\nthe autoencoder is simply the output from the encoder after it has been passed through the decoder.\n\nExample 3-5. The full autoencoder\n\nautoencoder = Model(encoder_input, decoder(encoder_output)) @\n\n@ The Keras Model that defines the full autoencoder\u2014a model that takes an image and passes it\nthrough the encoder and back out through the decoder to generate a reconstruction of the original\nimage.\n\nNow that we\u2019ve defined our model, we just need to compile it with a loss function and optimizer, as\nshown in Example 3-6. The loss function is usually chosen to be either the root mean squared error\n(RMSE) or binary cross-entropy between the individual pixels of the original image and the\nreconstruction.\n\nExample 3-6. Compiling the autoencoder\n\n# Compile the autoencoder\n\nautoencoder.compile(optimizer=\"adam\", loss=\"binary crossentropy\")\n\nCHOOSING THE LOSS FUNCTION\nOptimizing for RMSE means that your generated output will be symmetrically distributed around the av-\nerage pixel values (because an overestimation is penalized equivalently to an underestimation).\n\nOn the other hand, binary cross-entropy loss is asymmetrical\u2014it penalizes errors toward the extremes\nmore heavily than errors toward the center. For example, if the true pixel value is high (say 0.7), then\ngenerating a pixel with value 0.8 is penalized more heavily than generating a pixel with value 0.6. If the\ntrue pixel value is low (say 0.3), then generating a pixel with value 0.2 is penalized more heavily than\ngenerating a pixel with value 0.4.\n\nThis has the effect of binary cross-entropy loss producing slightly blurrier images than RMSE loss (as it\ntends to push predictions toward 0.5), but sometimes this is desirable as RMSE can lead to obviously pix-\nelized edges.\n\nThis has the effect of binary cross-entropy loss producing slightly blurrier images than RMSE loss (as it\ntends to push predictions toward 0.5), but sometimes this is desirable as RMSE can lead to obviously pix-\nelized edges.\n\nThere is no right or wrong choice\u2014you should choose whichever works best for your use case after\nexperimentation.\n\nWe can now train the autoencoder by passing in the input images as both the input and output, as shown\nin Example 3-7.\n\nExample 3-7. Training the autoencoder\n\nautoencoder.fit(\nx_train,\nx_train,\nepochs=5,\nbatch_size=100,\nshuffle=True,\nvalidation_data=(x_test, x_test),\n\nNow that our autoencoder is trained, the first thing we need to check is that it is able to accurately recon-\nstruct the input images.\n\nReconstructing Images\n\nWe can test the ability to reconstruct images by passing images from the test set through the autoencoder\nand comparing the output to the original images. The code for this is shown in Example 3-8.\n\nExample 3-8. Reconstructing images using the autoencoder\n\nexample _ images = x_test[:5000]\npredictions = autoencoder.predict(example_images)\n\nIn Figure 3-6 you can see some examples of original images (top row), the 2D vectors after encoding, and\nthe reconstructed items after decoding (bottom row).\n\naw=Mdh iT eaana-=\na\n\n[18,04] [17,58] [6.3,-09] [37,-09] [04,30] [48,01] [09,15] [08,26] [-1.8,-20] [-34,-11]\nDecoding | J\n\nae ee |\n2an i @ihaat-~-=\u2014\n\nFigure 3-6. Examples of encoding and decoding items of clothing\n\nNotice how the reconstruction isn\u2019t perfect\u2014there are still some details of the original images that aren\u2019t\ncaptured by the decoding process, such as logos. This is because by reducing each image to just two num-\nbers, we naturally lose some information.\n\nLet\u2019s now investigate how the encoder is representing images in the latent space.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.9,
                        "section_name": "Visualizing the Latent Space",
                        "section_path": "./screenshots-images-2/chapter_3/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_9/bdf9a73b-cbe0-4de9-a9da-b78caf970a26.png",
                            "./screenshots-images-2/chapter_3/section_9/345e6120-da7a-4c9e-b5e6-96bdd0fd44dc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Visualizing the Latent Space\n\nWe can visualize how images are embedded into the latent space by passing the test set through the en-\ncoder and plotting the resulting embeddings, as shown in Example 3-9.\n\nExample 3-9. Embedding images using the encoder\n\nembeddings = encoder.predict(example_images)\n\nplt.figure(figsize=(8, 8))\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=3)\nplt.show()\n\nThe resulting plot is the scatter plot shown in Figure 3-2\u2014each black point represents an image that has\nbeen embedded into the latent space.\n\nIn order to better understand how this latent space is structured, we can make use of the labels that come\nwith the Fashion-MNIST dataset, describing the type of item in each image. There are 10 groups altogeth-\ner, shown in Table 3-3.\n\nTable 3-3. The Fashion-MNIST labels\n\nID Clothing label\n0 T-shirt/top\n1 Trouser\n\n2 Pullover\n\n3) Dress\n\n4 Coat\n\n5 Sandal\n\n6 Shirt\n\n7 Sneaker\n\n8 Bag\n\n9 Ankle boot\n\nWe can color each point based on the label of the corresponding image to produce the plot in Figure 3-7.\nNow the structure becomes very clear! Even though the clothing labels were never shown to the model\nduring training, the autoencoder has naturally grouped items that look alike into the same parts of the la-\ntent space. For example, the dark blue cloud of points in the bottom-right corner of the latent space are\nall different images of trousers and the red cloud of points toward the center are all ankle boots.\n\n75 -50 -25 O00 25 50 75\nFigure 3-7. Plot of the latent space, colored by clothing label\n\n-10.0\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.1,
                        "section_name": "Generating New Images",
                        "section_path": "./screenshots-images-2/chapter_3/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_10/78b6cf4d-a218-4942-acd1-e7f9ed13f21d.png",
                            "./screenshots-images-2/chapter_3/section_10/aa522c65-93d2-48dc-9be9-2c8fe0cdd19f.png",
                            "./screenshots-images-2/chapter_3/section_10/211a4987-4506-4a97-a08c-14ec0b881c1f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generating New Images\n\nWe can generate novel images by sampling some points in the latent space and using the decoder to con-\nvert these back into pixel space, as shown in Example 3-10.\n\nExample 3-10. Generating novel images using the decoder\n\nmins, maxs = np.min(embeddings, axis=0), np.max(embeddings, axis=0)\nsample = np.random.uniform(mins, maxs, size=(18, 2))\nreconstructions = decoder.predict(sample)\n\nSome examples of generated images are shown in Figure 3-8, alongside their embeddings in the latent\n\nspace.\n\n[56,11] [-9,-0.8] [6.7.0.4] [-94,8.2] [57,6.9][-0.8, -0.2]\n\n@i- oad\n[-54,5.5] [1.3,-2.4] [-6,-15] [15,81] [-64,0.2] [51,09]\n\n2) [-9.4, -1] [-9.3, 34] [6.9, 0.5] [-9.6, 4.8] [-0.5, 7] [-0.6, -2.3]\n\n+00 -75 50 25 00 25 50 75\n\nFigure 3-8. Generated items of clothing\n\nEach blue dot maps to one of the images shown on the right of the diagram, with the embedding vector\nshown underneath. Notice how some of the generated items are more realistic than others. Why is this?\n\nTo answer this, let\u2019s first make a few observations about the overall distribution of points in the latent\n\nspace, referring back to Figure 3-7:\n\n\u00a2 Some clothing items are represented over a very small area and others over a much larger area.\n\n\u00a2 The distribution is not symmetrical about the point (0, 0), or bounded. For example, there are far\nmore points with positive y-axis values than negative, and some points even extend to a y-axis value\n> 8.\n\n\u00a2 There are large gaps between colors containing few points.\n\nThese observations actually make sampling from the latent space quite challenging. If we overlay the la-\ntent space with images of decoded points on a grid, as shown in Figure 3-9, we can begin to understand\nwhy the decoder may not always generate images to a satisfactory standard.\n\naa\n8 af\naf\n6 ft\nTf\ntf\n4 oe i\n= |\nee |\naee |\new @\n0\n\u201ca oa @&\n~~\n-2 | ee ate ae te me\nae ee ss . '\n\n100 -75 -50 -25 O00 25 50 75\n\nFigure 3-9. A grid of decoded embeddings, overlaid with the embeddings from the original images in the dataset, colored by item type\n\nFirstly, we can see that if we pick points uniformly in a bounded space that we define, we\u2019re more likely\nto sample something that decodes to look like a bag (ID 8) than an ankle boot (ID 9) because the part of\nthe latent space carved out for bags (orange) is larger than the ankle boot area (red).\n\nSecondly, it is not obvious how we should go about choosing a random point in the latent space, since the\ndistribution of these points is undefined. Technically, we would be justified in choosing any point in the\n2D plane! It\u2019s not even guaranteed that points will be centered around (0, 0). This makes sampling from\nour latent space problematic.\n\nLastly, we can see holes in the latent space where none of the original images are encoded. For example,\nthere are large white spaces at the edges of the domain\u2014the autoencoder has no reason to ensure that\npoints here are decoded to recognizable clothing items as very few images in the training set are encoded\nhere.\n\nSecondly, it is not obvious how we should go about choosing a random point in the latent space, since the\ndistribution of these points is undefined. Technically, we would be justified in choosing any point in the\n2D plane! It\u2019s not even guaranteed that points will be centered around (0, 0). This makes sampling from\nour latent space problematic.\n\nLastly, we can see holes in the latent space where none of the original images are encoded. For example,\nthere are large white spaces at the edges of the domain\u2014the autoencoder has no reason to ensure that\npoints here are decoded to recognizable clothing items as very few images in the training set are encoded\nhere.\n\nEven points that are central may not be decoded into well-formed images. This is because the autoen-\ncoder is not forced to ensure that the space is continuous. For example, even though the point (-1, -1)\nmight be decoded to give a satisfactory image of a sandal, there is no mechanism in place to ensure that\n\nthe point (-1.1, -1.1) also produces a satisfactory image of a sandal.\n\nIn two dimensions this issue is subtle; the autoencoder only has a small number of dimensions to work\nwith, so naturally it has to squash clothing groups together, resulting in the space between clothing\ngroups being relatively small. However, as we start to use more dimensions in the latent space to gener-\nate more complex images such as faces, this problem becomes even more apparent. If we give the au-\ntoencoder free rein over how it uses the latent space to encode images, there will be huge gaps between\ngroups of similar points with no incentive for the spaces in between to generate well-formed images.\n\nIn order to solve these three problems, we need to convert our autoencoder into a variational\nautoencoder.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.11,
                        "section_name": "Variational Autoencoders",
                        "section_path": "./screenshots-images-2/chapter_3/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_11/a00862ed-be67-42a0-a8de-528cbdb83e02.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Variational Autoencoders\n\nTo explain, let\u2019s revisit the infinite wardrobe and make a few changes...\n\nREVISITING THE INFINITE WARDROBE\n\nSuppose now, instead of placing every item of clothing at a single point in the wardrobe, you decide to al-\nlocate a general area where the item is more likely to be found. You reason that this more relaxed ap-\nproach to item location will help to solve the current issue around local discontinuities in the wardrobe.\n\nAlso, in order to ensure you do not become too careless with the new placement system, you agree with\nBrian that you will try to place the center of each item\u2019s area as close to the middle of the wardrobe as\npossible and that deviation of the item from the center should be as close to one meter as possible (not\nsmaller and not larger). The further you stray from this rule, the more you have to pay Brian as your\nstylist.\n\nAfter several months of operating with these two simple changes, you step back and admire the new\nwardrobe layout, alongside some examples of new clothing items that Brian has generated. Much better!\nThere is plenty of diversity in the generated items, and this time there are no examples of poor-quality\ngarments. It seems the two changes have made all the difference!\n\nLet\u2019s now try to understand what we need to do to our autoencoder model to convert it into a variational\nautoencoder and thus make it a more sophisticated generative model.\n\nThe two parts that we need to change are the encoder and the loss function.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.12,
                        "section_name": "The Encoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_12/08799e4a-4837-4545-a31c-524f98965fd4.png",
                            "./screenshots-images-2/chapter_3/section_12/85d86cf6-e46b-46e6-ac2c-e595e4b1ed67.png",
                            "./screenshots-images-2/chapter_3/section_12/70cd0176-f530-4e72-b843-7eb6fc0b472a.png",
                            "./screenshots-images-2/chapter_3/section_12/7bd1f7c2-9b74-41ff-b380-6b2ef512e809.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Encoder\n\nIn an autoencoder, each image is mapped directly to one point in the latent space. In a variational au-\ntoencoder, each image is instead mapped to a multivariate normal distribution around a point in the la-\ntent space, as shown in Figure 3-10.\n\nEncode Jencoue\n\nAutoencoder Variational autoencoder\n\nFigure 3-10. The difference between the encoders in an autoencoder and a variational autoencoder\n\nTHE MULTIVARIATE NORMAL DISTRIBUTION\n\nAnormal distribution (or Gaussian distribution) WV (41, 7) is a probability distribution characterized by a\ndistinctive bell curve shape, defined by two variables: the mean (1) and the variance (0?). The standard\ndeviation (c) is the square root of the variance.\n\nThe probability density function of the normal distribution in one dimension is:\n\n1 _ few?\nf(@| ma?) = hoe\n\nFigure 3-11 shows several normal distributions in one dimension, for different values of the mean and\nvariance. The red curve is the standard normal (or unit normal) WV (0, 1)\u2014the normal distribution with\nmean equal to 0 and variance equal to 1.\n\nWe can sample a point z from a normal distribution with mean y and standard deviation o using the fol-\nlowing equation:\n\nz=p+oe\n\nwhere \u20ac is sampled from a standard normal distribution.\n\n08\n\n0.6\nf(x)\n\n04\n\n0.2\n\n0.0\n\nFigure 3-11. The normal distribution in one dimension (source: Wikipedia)\n\nThe concept of a normal distribution extends to more than one dimension\u2014the probability density func-\ntion for a multivariate normal distribution (or multivariate Gaussian distribution) WY (1, X) in k dimen-\n\nsions with mean vector 4 and symmetric covariance matrix \u00bb is as follows:\n\nexp \u2014}(x~p)\"5 (x -p))\n\nV 2n)* ||\n\nF (21, ++, Bk) =\n\nIn this book, we will typically be using isotropic multivariate normal distributions, where the covariance\nmatrix is diagonal. This means that the distribution is independent in each dimension (i.e., we can sam-\nple a vector where each element is normally distributed with independent mean and variance). This is\nthe case for the multivariate normal distribution that we will use in our variational autoencoder.\n\nA multivariate standard normal distribution WV (0,1) is a multivariate distribution with a zero-valued\nmean vector and identity covariance matrix.\n\nNORMAL VERSUS GAUSSIAN\n\nIn this book, the terms normal and Gaussian are used interchangeably and the isotropic and multi-\nvariate nature of the distribution is usually implied. For example, \u201cwe sample from a Gaussian dis-\ntribution\u201d can be interpreted to mean \u201cwe sample from an isotropic, multivariate Gaussian\ndistribution.\u201d\n\n\nThe encoder only needs to map each input to a mean vector and a variance vector and does not need to\nworry about covariance between dimensions. Variational autoencoders assume that there is no correla-\ntion between dimensions in the latent space.\n\nVariance values are always positive, so we actually choose to map to the logarithm of the variance, as this\ncan take any real number in the range (\u2014oo, oo). This way we can use a neural network as the encoder to\nperform the mapping from the input image to the mean and log variance vectors.\n\nTo summarize, the encoder will take each input image and encode it to two vectors that together define a\nmultivariate normal distribution in the latent space:\n\nZ_mean\n\nThe mean point of the distribution\n\nz_log_var\n\nThe logarithm of the variance of each dimension\n\nWe can sample a point z from the distribution defined by these values using the following equation:\n\nz = z_mean + z sigma * epsilon\n\nwhere:\n\nz_sigma = exp(z_log_ var * 0.5)\nepsilon ~ N(0,I)\n\nTIP\n\nThe derivation of the relationship between z_sigma (o) and z_log_var (log (0?)) is as follows:\n\no =exp (log (a)) =exp (2 log (7) /2) =exp (log (0?) /2)\n\nThe decoder of a variational autoencoder is identical to the decoder of a plain autoencoder, giving the\noverall architecture shown in Figure 3-12.\n\nFigure 3-12. VAE architecture diagram\n\nWhy does this small change to the encoder help?\n\nPreviously, we saw that there was no requirement for the latent space to be continuous\u2014even if the point\n(-2, 2) decodes to a well-formed image of a sandal, there\u2019s no requirement for (-2.1, 2.1) to look similar.\nNow, since we are sampling a random point from an area around z_mean, the decoder must ensure that\nall points in the same neighborhood produce very similar images when decoded, so that the reconstruc-\ntion loss remains small. This is a very nice property that ensures that even when we choose a point in the\nlatent space that has never been seen by the decoder, it is likely to decode to an image that is well formed.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.13,
                        "section_name": "Building the VAE encoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_13/ff04ca4c-4ad0-4dd5-abfb-4a6b03d6ec6a.png",
                            "./screenshots-images-2/chapter_3/section_13/999741ee-326b-42f2-a656-d0231e7c0b0c.png",
                            "./screenshots-images-2/chapter_3/section_13/efd91eab-a49f-4614-8715-92ce2f406eb1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Let\u2019s now see how we build this new version of the encoder in Keras.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/03_vae/02_vae_fashion/vae_fash-\nion.ipynb in the book repository.\n\nThe code has been adapted from the excellent VAE tutorial created by Francois Chollet, available on the Keras\nwebsite.\n\nFirst, we need to create a new type of Sampling layer that will allow us to sample from the distribution\ndefined by z_mean and z_log_var, as shown in Example 3-11.\n\nExample 3-11. The Sampling layer\n\nclass Sampling(layers.Layer): \u00ae\ndef call(self, inputs):\nz_mean, z_log_var = inputs\nbatch = tf.shape(z_mean) [0]\ndim = tf.shape(z_mean)[1]\nepsilon = K.random_normal(shape=(batch, dim) )\n\nreturn z_mean + tf.exp(0.5 * z_ log var) * epsilon @\n\n@ We create a new layer by subclassing the Keras base Layer class (see the \u201cSubclassing the Layer\nClass\u201d sidebar).\n\n@ We use the reparameterization trick (see \u201cThe Reparameterization Trick\u201d sidebar) to build a sam-\nple from the normal distribution parameterized by z_mean and z_log var.\n\nSUBCLASSING THE LAYER CLASS\nYou can create new layers in Keras by subclassing the abstract Layer class and defining the call\nmethod, which describes how a tensor is transformed by the layer.\n\nFor example, in the variational autoencoder, we can create a Sampling layer that can handle the sam-\npling of z froma normal distribution with parameters defined by z_mean and z_log_var.\n\nThis is useful when you want to apply a transformation to a tensor that isn\u2019t already included as one of\nthe out-of-the-box Keras layer types.\n\nTHE REPARAMETERIZATION TRICK\n\nRather than sample directly from a normal distribution with parameters z_mean and z_log_var,we\ncan sample epsilon froma standard normal and then manually adjust the sample to have the correct\nmean and variance.\n\nThis is known as the reparameterization trick, and it\u2019s important as it means gradients can backpropagate\nfreely through the layer. By keeping all of the randomness of the layer contained within the variable\nepsilon, the partial derivative of the layer output with respect to its input can be shown to be deter-\nministic (i.e., independent of the random epsilon ), which is essential for backpropagation through the\nlayer to be possible.\n\nThe complete code for the encoder, including the new Sampling layer, is shown in Example 3-12.\n\nExample 3-12. The encoder\n\nencoder_input = layers.Input(\nshape=(32, 32, 1), name=\"encoder_input\"\n\n*\n\"\n\nlayers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\") (\nencoder_input\n\n)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\" ) (x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\") (x)\n\nshape_before flattening = K.int_shape(x)[1:]\n\nx = layers.Flatten() (x)\nz_mean = layers.Dense(2, name=\"z_mean\")(x) @\nz_log_var = layers.Dense(2, name=\"z_log var\") (x)\n\nz = Sampling()([z_mean, z_log var]) @\n\nencoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\") \u00ae\n\n@ Instead of connecting the Flatten layer directly to the 2D latent space, we connect it to layers\nz_mean and z_log_ var.\n\n@ The Sampling layer samples a point z in the latent space from the normal distribution defined\nby the parameters z_mean and z_log_var.\n\n\u00ae The Keras Model that defines the encoder\u2014a model that takes an input image and outputs\nz_mean, z_log_var,andasampled point z from the normal distribution defined by these\nparameters.\n\nAsummary of the encoder is shown in Table 3-4.\n\nTable 3-4. Model summary of the VAE encoder\n\nLayer (type) Output shape Param # Connected to\nInputLayer (input) (None, 32,32,1) 0 0\n\nConv2D (conv2d_1) (None, 16, 16,32) 320 [input]\nConv2D (conv2d_2) (None, 8, 8, 64) 18,496 [conv2d_1]\nConv2D (conv2d_3) (None, 4, 4,128) 73,856 [conv2d_2]\nFlatten (flatten) (None, 2048) 0 [conv2d_3]\nDense (z_mean) (None, 2) 4,098 [flatten]\nDense (z_log_var) (None, 2) 4,098 [flatten]\nSampling (z) (None, 2) 0 [z_mean, z_log_var]\nTotal params 100,868\n\nTrainable params 100,868\n\nNon-trainable params 0\n\nThe only other part of the original autoencoder that we need to change is the loss function.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.14,
                        "section_name": "The Loss Function",
                        "section_path": "./screenshots-images-2/chapter_3/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_14/e3a0d32c-676c-4eff-9278-4b60c022d3cf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Loss Function\n\nPreviously, our loss function only consisted of the reconstruction loss between images and their attempt-\ned copies after being passed through the encoder and decoder. The reconstruction loss also appears in a\nvariational autoencoder, but we now require one extra component: the Kullback-Leibler (KL) divergence\nterm.\n\nKL divergence is a way of measuring how much one probability distribution differs from another. Ina\nVAE, we want to measure how much our normal distribution with parameters z_mean and z_log_var\ndiffers from a standard normal distribution. In this special case, it can be shown that the KL divergence\nhas the following closed form:\n\nkl_loss = -0.5 * sum(1 + z_log var - z mean * 2 - exp(z_log var))\n\nor in mathematical notation:\n\nDrsIN (uso || N (0,1)] = \u20145 D> (1+ tog (02) \u2014 4? - 0\u201d)\n\nThe sum is taken over all the dimensions in the latent space. k1_loss is minimized to0 when z_mean =\n0 and z_log var = 0 for all dimensions. As these two terms start to differ from 0, k1_loss increases.\n\nIn summary, the KL divergence term penalizes the network for encoding observations to z_mean and\nz_log_var variables that differ significantly from the parameters of a standard normal distribution,\n\nnamely z_mean = 0 and z_log_var = 0.\nWhy does this addition to the loss function help?\n\nFirstly, we now have a well-defined distribution that we can use for choosing points in the latent space\u2014\nthe standard normal distribution. Secondly, since this term tries to force all encoded distributions toward\nthe standard normal distribution, there is less chance that large gaps will form between point clusters.\nInstead, the encoder will try to use the space around the origin symmetrically and efficiently.\n\nIn the original VAE paper, the loss function for a VAE was simply the addition of the reconstruction loss\nand the KL divergence loss term. A variant on this (the 6-VAE) includes a factor that weights the KL diver-\ngence to ensure that it is well balanced with the reconstruction loss. If we weight the reconstruction loss\ntoo heavily, the KL loss will not have the desired regulatory effect and we will see the same problems that\nwe experienced with the plain autoencoder. If the KL divergence term is weighted too heavily, the KL di-\nvergence loss will dominate and the reconstructed images will be poor. This weighting term is one of the\nparameters to tune when you're training your VAE.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.15,
                        "section_name": "Training the Variational Autoencoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_15/4f6e823e-31cf-457a-9bcb-de4f5cee1bbe.png",
                            "./screenshots-images-2/chapter_3/section_15/5997a866-12d1-4b0b-bdae-4bd30dc2510a.png",
                            "./screenshots-images-2/chapter_3/section_15/a70944c6-9c36-4678-a128-1e4670c88148.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the Variational Autoencoder\n\nExample 3-13 shows how we build the overall VAE model as a subclass of the abstract Keras Model class.\nThis allows us to include the calculation of the KL divergence term of the loss function in a custom\n\ntrain_step method.\n\nExample 3-13. Training the VAE\n\nclass VAE(models.Model):\ndef _ init__(self, encoder, decoder, **kwargs):\nsuper(VAE, self)._init_ (**kwargs)\nself.encoder = encoder\nself.decoder = decoder\nself.total_loss_tracker = metrics.Mean(name=\"total_loss\")\nself.reconstruction_loss_tracker = metrics .Mean(\nname=\"reconstruction_loss\"\n)\n\nself.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n\ndef metrics(self):\nreturn [\nself.total_loss_tracker,\nself.reconstruction_loss_tracker,\nself.kl_loss_tracker,\n\ndef call(self, inputs): @\nz_mean, z_log var, z = encoder(inputs)\nreconstruction = decoder(z)\nreturn z_mean, z_log_var, reconstruction\n\ndef train_step(self, data): @\nwith tf.GradientTape() as tape:\nz_mean, z_log var, reconstruction = self(data)\nreconstruction_loss = tf.reduce_mean(\n500\n* losses.binary_crossentropy(\ndata, reconstruction, axis=(1, 2, 3)\n\n) 9\nkl_loss = tf.reduce_mean(\n\ntf.reduce_sum(\n\n-0.5\n* (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log var)),\naxis = l,\n\n)\n\ntotal_loss = reconstruction_loss + kl_loss @\n\ngrads = tape.gradient(total_loss, self.trainable_ weights)\nself.optimizer.apply gradients(zip(grads, self.trainable weights) )\n\nself.total_loss_tracker.update_state(total_loss)\nself.reconstruction_loss_tracker.update_state(reconstruction_loss)\nself.kl_loss_tracker.update_state(kl_loss)\n\nreturn {m.name: m.result() for m in self.metrics}\n\nvae = VAE(encoder, decoder)\nvae.compile(optimizer=\"adam\" )\nvae.fit(\n\ntrain,\n\nepochs=5,\n\nbatch_size=100\n\n@ This function describes what we would like returned what we call the VAE on a particular input\n\nimage.\n\n\u00ae@ This function describes one training step of the VAE, including the calculation of the loss function.\n\n\u00a9 A beta value of 500 is used in the reconstruction loss.\n\n\u00ae The total loss is the sum of the reconstruction loss and the KL divergence loss.\n\nGRADIENT TAPE\n\nTensorFlow\u2019s Gradient Tape is a mechanism that allows the computation of gradients of operations executed during a\nforward pass of a model. To use it, you need to wrap the code that performs the operations you want to differentiate\nina tf\u00a3.GradientTape() context. Once you have recorded the operations, you can compute the gradient of the loss\nfunction with respect to some variables by calling tape. gradient() . The gradients can then be used to update the\nvariables with the optimizer.\n\nThis mechanism is useful for calculating the gradient of custom loss functions (as we have done here) and also for\ncreating custom training loops, as we shall see in Chapter 4.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.16,
                        "section_name": "Analysis of the Variational Autoencoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_16/1de27c53-e74f-473c-aea6-74142b683461.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Now that we have trained our VAE, we can use the encoder to encode the images in the test set and plot\nthe z_mean values in the latent space. We can also sample from a standard normal distribution to gener-\nate points in the latent space and use the decoder to decode these points back into pixel space to see how\nthe VAE performs.\n\nFigure 3-13 shows the structure of the new latent space, alongside some sampled points and their decod-\ned images. We can immediately see several changes in how the latent space is organized.\n\nehrAREa\n\n(34,3) (-29,3] (253) (243) [16.3] [13.3]\n\n4a-~-8t tas\n\n[-08,3) [-04,3] [01,3] [05,3] [09,3] [14,3]\n\naeiaaa\n\n(1.8,3) [22,3] [26,3] [-34,2.6] [-2.9, 26] [-2.5, 26]\n\n3 -2 4 0 1 2 3\n\nFigure 3-13. The new latent space: the black dots show the z_mean value of each encoded image, while blue dots show some sampled points in\nthe latent space (with their decoded images on the right)\n\nFirstly, the KL divergence loss term ensures that the z_mean and z_log var values of the encoded im-\nages never stray too far from a standard normal distribution. Secondly, there are not so many poorly\nformed images as the latent space is now much more continuous, due to fact that the encoder is now sto-\n\nchastic, rather than deterministic.\n\nFinally, by coloring points in the latent space by clothing type (Figure 3-14), we can see that there is no\npreferential treatment of any one type. The righthand plot shows the space transformed into p-values\u2014\nwe can see that each color is approximately equally represented. Again, it\u2019s important to remember that\nthe labels were not used at all during training; the VAE has learned the various forms of clothing by itself\n\nin order to help minimize reconstruction loss.\n\n3\u00b002 \u00ab9 ~\u00ab0 1 2 3 0.0 02 04 06 08 10\n\nFigure 3-14. The latent space of the VAE colored by clothing type\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.17,
                        "section_name": "Exploring the Latent Space",
                        "section_path": "./screenshots-images-2/chapter_3/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_17/647045bc-4d04-4ca6-8de8-93bdd5c82d88.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Exploring the Latent Space\n\nSo far, all of our work on autoencoders and variational autoencoders has been limited to a latent space\nwith two dimensions. This has helped us to visualize the inner workings of a VAE on the page and under-\nstand why the small tweaks that we made to the architecture of the autoencoder helped transform it into\n\na more powerful class of network that can be used for generative modeling.\n\nLet\u2019s now turn our attention to a more complex dataset and see the amazing things that variational au-\ntoencoders can achieve when we increase the dimensionality of the latent space.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/03_vae/03_faces/vae_faces.ipynb\nin the book repository.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.18,
                        "section_name": "The CelebA Dataset",
                        "section_path": "./screenshots-images-2/chapter_3/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_18/28e37552-5774-447c-994c-0a121162f0ac.png",
                            "./screenshots-images-2/chapter_3/section_18/986ba439-99d3-4c97-a155-0581af3aed1e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The CelebA Dataset\n\nWe shall be using the CelebFaces Attributes (CelebA) dataset to train our next variational autoencoder.\n\nThis is a collection of over 200,000 color images of celebrity faces, each annotated with various labels\n(e.g., wearing hat, smiling, etc.). A few examples are shown in Figure 3-15.\n\n3\nFigure 3-15. Some examples from the CelebA dataset (source: Liu et al,, 2015)\n\nOf course, we don\u2019t need the labels to train the VAE, but these will be useful later when we start exploring\nhow these features are captured in the multidimensional latent space. Once our VAE is trained, we can\nsample from the latent space to generate new examples of celebrity faces.\n\nThe CelebA dataset is also available through Kaggle, so you can download the dataset by running the\nKaggle dataset downloader script in the book repository, as shown in Example 3-14. This will save the im-\n\nages and accompanying metadata locally to the /data folder.\n\nExample 3-14. Downloading the CelebA dataset\n\nbash scripts/download_kaggle_data.sh jessicali9530 celeba-dataset\n\nWe use the Keras function image_dataset_from_directory to create a TensorFlow Dataset pointed at\nthe directory where the images are stored, as shown in Example 3-15. This allows us to read batches of\nimages into memory only when required (e.g., during training), so that we can work with large datasets\nand not worry about having to fit the entire dataset into memory. It also resizes the images to 32 x 32, in-\n\nterpolating between pixel values.\n\nExample 3-15. Preprocessing the CelebA dataset\n\ntrain_data = utils.image_dataset_from_directory/(\n\"/app/data/celeba-dataset/img_align_celeba/img_align_celeba\",\nlabels=None,\ncolor_mode=\"rgb\",\nimage_size=(32, 32),\nbatch_size=128,\nshuffle=True,\nseed=42,\ninterpolation=\"bilinear\",\n\nThe original data is scaled in the range [0, 255] to denote the pixel intensity, which we rescale to the range\n\n[0, 1] as shown in Example 3-16.\n\nExample 3-16. Preprocessing the CelebA dataset\n\ndef preprocess(img):\nimg = tf.cast(img, \"float32\") / 255.0\n\nreturn img\n\ntrain = train_data.map(lambda x: preprocess(x) )\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.19,
                        "section_name": "Training the Variational Autoencoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_19/cd68cfd8-79fc-4d49-b188-4f8883eb6660.png",
                            "./screenshots-images-2/chapter_3/section_19/476bbd93-c503-44e3-a4a5-c6e39fac6bf2.png",
                            "./screenshots-images-2/chapter_3/section_19/d671dd7d-878a-4fcd-9a59-847818a7f423.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the Variational Autoencoder\n\nThe network architecture for the faces model is similar to the Fashion-MNIST example, with a few slight\ndifferences:\n\n\u00a2 Our data now has three input channels (RGB) instead of one (grayscale). This means we need to\nchange the number of channels in the final convolutional transpose layer of the decoder to 3.\n\nWe shall be using a latent space with 200 dimensions instead of 2. Since faces are much more com-\nplex than the Fashion-MNIST images, we increase the dimensionality of the latent space so that the\n\nnetwork can encode a satisfactory amount of detail from the images.\n\nThere are batch normalization layers after each convolutional layer to stabilize training. Even\nthough each batch takes a longer time to run, the number of batches required to reach the same loss\n\nis greatly reduced.\n\nWe increase the f factor for the KL divergence to 2,000. This is a parameter that requires tuning; for\nthis dataset and architecture this value was found to generate good results.\n\nThe full architectures of the encoder and decoder are shown in Tables 3-5 and 3-6, respectively.\n\nTable 3-5. Model summary of the VAE faces encoder\n\nLayer (type) Output shape Param # Connected to\nInputLayer (input) (None, 32, 32, 3) 0 ]\n\nConv2D (conv2d_1) (None, 16, 16,128) 3,584 input]\nBatchNormalization (bn_1) (None, 16, 16,128) 512 conv2d_1]\nLeakyReLU (Ir_1) (None, 16, 16,128) 0 bn_1]\nConv2D (conv2d_2) (None, 8, 8, 128) 147,584 Jr_1]\nBatchNormalization (bn_2) (None, 8, 8, 128) 512 conv2d_2]\nLeakyReLU (Ir_2) (None, 8, 8, 128) 0 bn_2]\nConv2D (conv2d_3) (None, 4, 4, 128) 147,584 r_2]\nBatchNormalization (bn_3) (None, 4, 4, 128) 512 conv2d_3]\nLeakyReLU (Ir_3) (None, 4, 4, 128) 0 bn_3]\nConv2D (conv2d_4) (None, 2, 2, 128) 147,584 Ir_3]\nBatchNormalization (bn_4) (None, 2, 2, 128) 512 conv2d_4]\nLeakyReLU (Ir_4) (None, 2, 2, 128) 0 bn_4]\nFlatten (flatten) (None, 512) 0 lr_4]\nDense (z_mean) (None, 200) 102,600 flatten]\n\n\nDense (z_mean) (None, 200) 102,600 [flatten]\n\nDense (z_log_var) (None, 200) 102,600 [flatten]\n\nSampling (z) (None, 200) 0 [z_mean, z_log_var]\nTotal params 653,584\n\nTrainable params 652,560\n\nNon-trainable params 1,024\n\nTable 3-6. Model summary of the VAE faces decoder\n\nLayer (type) Output shape Param #\nInputLayer (None, 200) 0\n\nDense (None, 512) 102,912\nBatchNormalization (None, 512) 2,048\nLeakyReLU (None, 512) 0\nReshape (None, 2, 2, 128) 0\n\nConv2DTranspose (None, 4, 4, 128) 147,584\nBatchNormalization (None, 4, 4, 128) 512\nLeakyReLU (None, 4, 4, 128) 0\nConv2DTranspose (None, 8, 8, 128) 147,584\nBatchNormalization (None, 8, 8, 128) 512\nLeakyReLU (None, 8, 8, 128) 0\nConv2DTranspose (None, 16, 16,128) 147,584\nBatchNormalization (None, 16, 16,128) 512\nLeakyReLU (None, 16, 16,128) 0\nConv2DTranspose (None, 32, 32,128) 147,584\nBatchNormalization (None, 32, 32,128) 512\nLeakyReLU (None, 32, 32,128) 0\n\nConv2DTranspose (None, 32, 32, 3) 3,459\n\nTotal params 700,803\nTrainable params 698,755\n\nNon-trainable params 2,048\n\nAfter around five epochs of training, our VAE should be able to produce novel images of celebrity faces!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.2,
                        "section_name": "Analysis of the Variational Autoencoder",
                        "section_path": "./screenshots-images-2/chapter_3/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_20/d521cec4-0e68-4bcc-86c3-b012ecdd0d31.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the Variational Autoencoder\n\nFirst, let\u2019s take a look at a sample of reconstructed faces. The top row in Figure 3-16 shows the original\nimages and the bottom row shows the reconstructions once they have passed through the encoder and\ndecoder.\n\nExample real faces\n\nReconstructions\n\nFigure 3-16. Reconstructed faces, after passing through the encoder and decoder\n\nWe can see that the VAE has successfully captured the key features of each face\u2014the angle of the head,\nthe hairstyle, the expression, etc. Some of the fine detail is missing, but it is important to remember that\nthe aim of building variational autoencoders isn\u2019t to achieve perfect reconstruction loss. Our end goal is\nto sample from the latent space in order to generate new faces.\n\nFor this to be possible we must check that the distribution of points in the latent space approximately re-\nsembles a multivariate standard normal distribution. If we see any dimensions that are significantly dif-\nferent from a standard normal distribution, we should probably reduce the reconstruction loss factor,\nsince the KL divergence term isn\u2019t having enough effect.\n\nThe first 50 dimensions in our latent space are shown in Figure 3-17. There aren\u2019t any distributions that\nstand out as being significantly different from the standard normal, so we can move on to generating\nsome faces!\n\nSmee\nana\u201d\nsa aan\nsai\nSa\nSai aa\naaa\n5 and\namd\namd\n\nFigure 3-17. Distributions of points for the first 50 dimensions in the latent space\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.21,
                        "section_name": "Generating New Faces",
                        "section_path": "./screenshots-images-2/chapter_3/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_21/389345a8-74b5-4951-b5a8-c93ce7136293.png",
                            "./screenshots-images-2/chapter_3/section_21/c8a7f462-d4dc-4f0e-991b-9086e50f4882.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generating New Faces\nTo generate new faces, we can use the code in Example 3-17.\n\nExample 3-17. Generating new faces from the latent space\n\ngrid_width, grid_height = (10,3)\nz_sample = np.random.normal(size=(grid_width * grid_height, 200)) \u00ae\n\nreconstructions = decoder.predict(z_sample) @\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_width * grid_height):\nax = fig.add_subplot(grid_height, grid_width, i + 1)\n\nax.axis(\"off\")\n\nax.imshow(reconstructions[i, :, :]) \u00ae\n\n@ Sample 30 points from a standard multivariate normal distribution with 200 dimensions.\n@ Decode the sampled points.\n\u00ae Plot the images!\n\nThe output is shown in Figure 3-18.\n\n| eke |\n\u00bb \u00a9 Gi\n\nFigure 3-18. New generated faces\n\nAmazingly, the VAE is able to take the set of points that we sampled from a standard normal distribution\nand convert each into a convincing image of a person\u2019s face. This is our first glimpse of the true power of\ngenerative models!\n\nNext, let\u2019s see if we can start to use the latent space to perform some interesting operations on generated\n\nimages.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.22,
                        "section_name": "Latent Space Arithmetic",
                        "section_path": "./screenshots-images-2/chapter_3/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_22/ddfa28b4-c125-453c-94de-cd0c097af818.png",
                            "./screenshots-images-2/chapter_3/section_22/5cba4e87-f32f-48de-bc85-7c377b7ee6d3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Latent Space Arithmetic\n\nOne benefit of mapping images into a lower-dimensional latent space is that we can perform arithmetic\non vectors in this latent space that has a visual analogue when decoded back into the original image\ndomain.\n\nFor example, suppose we want to take an image of somebody who looks sad and give them a smile. To do\nthis we first need to find a vector in the latent space that points in the direction of increasing smile.\nAdding this vector to the encoding of the original image in the latent space will give us a new point\nwhich, when decoded, should give us a more smiley version of the original image.\n\nSo how can we find the smile vector? Each image in the CelebA dataset is labeled with attributes, one of\nwhich is Smiling. If we take the average position of encoded images in the latent space with the at-\ntribute Smiling and subtract the average position of encoded images that do not have the attribute\nSmiling , we will obtain the vector that points in the direction of Smiling , which is exactly what we\nneed.\n\nConceptually, we are performing the following vector arithmetic in the latent space, where alpha isa\nfactor that determines how much of the feature vector is added or subtracted:\n\nz_new = z + alpha * feature_vector\n\nLet\u2019s see this in action. Figure 3-19 shows several images that have been encoded into the latent space. We\nthen add or subtract multiples of a certain vector (e.g., Smiling, Black_Hair, Eyeglasses, Young,\nMale, Blond_Hair ) to obtain different versions of the image, with only the relevant feature changed.\n\nSubtracting vector Adding vector\n\nOriginal\nimage\n\nFigure 3-19. Adding and subtracting features to and from faces\n\nIt is remarkable that even though we are moving the point a significantly large distance in the latent\nspace, the core image remains approximately the same, except for the one feature that we want to ma-\nnipulate. This demonstrates the power of variational autoencoders for capturing and adjusting high-level\nfeatures in images.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.23,
                        "section_name": "Morphing Between Faces",
                        "section_path": "./screenshots-images-2/chapter_3/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_23/97c37b6a-ddcc-44f0-9540-4ebc1d6946a7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Morphing Between Faces\n\nWe can use a similar idea to morph between two faces. Imagine two points in the latent space, A and B,\nthat represent two images. If you started at point A and walked toward point B in a straight line, decoding\neach point on the line as you went, you would see a gradual transition from the starting face to the end\n\nface.\n\nMathematically, we are traversing a straight line, which can be described by the following equation:\n\nz_new = z_A * (1- alpha) + z_B * alpha\n\nHere, alpha is a number between 0 and 1 that determines how far along the line we are, away from\npoint A.\n\nFigure 3-20 shows this process in action. We take two images, encode them into the latent space, and then\ndecode points along the straight line between them at regular intervals.\n\nFigure 3-20. Morphing between two faces\n\nIt is worth noting the smoothness of the transition\u2014even where there are multiple features to change si-\nmultaneously (e.g., removal of glasses, hair color, gender), the VAE manages to achieve this fluidly, show-\ning that the latent space of the VAE is truly a continuous space that can be traversed and explored to gen-\nerate a multitude of different human faces.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 3.24,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_3/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_24/6837f141-3eda-4906-bbc1-50b1a40f320d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter we have seen how variational autoencoders are a powerful tool in the generative model-\ning toolbox. We started by exploring how plain autoencoders can be used to map high-dimensional im-\nages into a low-dimensional latent space, so that high-level features can be extracted from the individual-\nly uninformative pixels. However, we quickly found that there were some drawbacks to using plain au-\ntoencoders as a generative model\u2014sampling from the learned latent space was problematic, for example.\n\nVariational autoencoders solve these problems by introducing randomness into the model and constrain-\ning how points in the latent space are distributed. We saw that with a few minor adjustments, we can\ntransform our autoencoder into a variational autoencoder, thus giving it the power to be a true genera-\ntive model.\n\nFinally, we applied our new technique to the problem of face generation and saw how we can simply de-\ncode points from a standard normal distribution to generate new faces. Moreover, by performing vector\narithmetic within the latent space, we can achieve some amazing effects, such as face morphing and fea-\n\nture manipulation.\n\nIn the next chapter, we shall explore a different kind of model that remains a popular choice for genera-\ntive image modeling: the generative adversarial network.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 4,
                "chapter_name": "Chapter 4. Generative Adversarial Networks",
                "chapter_path": "./screenshots-images-2/chapter_4",
                "sections": [
                    {
                        "section_id": 4.1,
                        "section_name": "Chapter 4. Generative Adversarial Networks",
                        "section_path": "./screenshots-images-2/chapter_4/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_1/3f2fa590-3857-4578-ac02-c85ac84c5654.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 4. Generative Adversarial Networks\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\ne Learn about the architectural design of a generative adversarial network (GAN).\n\ne Build and train a deep convolutional GAN (DCGAN) from scratch using Keras.\n\n\u00a9 Use the DCGAN to generate new images.\n\n\u00a2 Understand some of the common problems faced when training a DCGAN.\n\n\u00a2 Learn how the Wasserstein GAN (WGAN) architecture addresses these problems.\n\n\u00a9 Understand additional enhancements that can be made to the WGAN, such as incorporating a gradi-\nent penalty (GP) term into the loss function.\n\n\u00a2 Build a WGAN-GP from scratch using Keras.\n\n\u00a9 Use the WGAN-GP to generate faces.\n\ne Learn how a conditional GAN (CGAN) gives you the ability to condition generated output on a given\nlabel.\n\n\u00a2 Build and train a CGAN in Keras and use it to manipulate a generated image.\n\nIn 2014, Ian Goodfellow et al. presented a paper entitled \u201cGenerative Adversarial Nets\u201d* at the Neural\nInformation Processing Systems conference (NeurIPS) in Montreal. The introduction of generative adver-\nsarial networks (or GANs, as they are more commonly known) is now regarded as a key turning point in\nthe history of generative modeling, as the core ideas presented in this paper have spawned some of the\nmost successful and impressive generative models ever created.\n\nThis chapter will first lay out the theoretical underpinning of GANs, then we will see how to build our\nown GAN using Keras.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_4/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_2/c1d0ef56-8d52-4b46-870c-ae6034cd725a.png",
                            "./screenshots-images-2/chapter_4/section_2/84f819b4-0bfd-4f6d-8ed0-7e0b5944b282.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Let\u2019s start with a short story to illustrate some of the fundamental concepts used in the GAN training\n\nprocess.\n\nBRICKKI BRICKS AND THE FORGERS\nIt\u2019s your first day at your new job as head of quality control for Brickki, a company that specializes in pro-\nducing high-quality building blocks of all shapes and sizes ( ).\n\nFigure 4-1. The production line of a company making bricks of many different shapes and sizes (created with )\n\nYou are immediately alerted to a problem with some of the items coming off the production line. A com-\npetitor has started to make counterfeit copies of Brickki bricks and has found a way to mix them into the\nbags received by your customers. You decide to become an expert at telling the difference between the\ncounterfeit bricks and the real thing, so that you can intercept the forged bricks on the production line\nbefore they are given to customers. Over time, by listening to customer feedback, you gradually become\nmore adept at spotting the fakes.\n\nThe forgers are not happy about this\u2014they react to your improved detection abilities by making some\nchanges to their forgery process so that now, the difference between the real bricks and the fakes is even\n\nharder for you to spot.\n\nNot one to give up, you retrain yourself to identify the more sophisticated fakes and try to keep one step\nahead of the forgers. This process continues, with the forgers iteratively updating their brick creation\ntechnologies while you try to become increasingly more accomplished at intercepting their fakes.\n\nWith every week that passes, it becomes more and more difficult to tell the difference between the real\nBrickki bricks and those created by the forgers. It seems that this simple game of cat and mouse is enough\nto drive significant improvement in both the quality of the forgery and the quality of the detection.\n\nThe story of Brickki bricks and the forgers describes the training process of a generative adversarial\nnetwork.\n\nA GAN is a battle between two adversaries, the generator and the discriminator. The generator tries to\nconvert random noise into observations that look as if they have been sampled from the original dataset,\nand the discriminator tries to predict whether an observation comes from the original dataset or is one of\nthe generator\u2019s forgeries. Examples of the inputs and outputs to the two networks are shown in Figure 4-\n2.\n\nGEO BOBO R>\n\nRandom noise Generated image\n\n01\nPrediction for how likely\n\nImage (either real or fake) the image is to be real\n\nFigure 4-2. Inputs and outputs of the two networks in a GAN\n\nAt the start of the process, the generator outputs noisy images and the discriminator predicts randomly.\nThe key to GANs lies in how we alternate the training of the two networks, so that as the generator be-\ncomes more adept at fooling the discriminator, the discriminator must adapt in order to maintain its abil-\nity to correctly identify which observations are fake. This drives the generator to find new ways to fool\nthe discriminator, and so the cycle continues.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.3,
                        "section_name": "Deep Convolutional GAN (DCGAN)",
                        "section_path": "./screenshots-images-2/chapter_4/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_3/9f7090b5-0285-4123-bb86-b097fb2c1079.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Deep Convolutional GAN (DCGAN)\n\nTo see this in action, let\u2019s start building our first GAN in Keras, to generate pictures of bricks.\n\nWe will be closely following one of the first major papers on GANs, \u201cUnsupervised Representation\nLearning with Deep Convolutional Generative Adversarial Networks.\u201d In this 2015 paper, the authors\nshow how to build a deep convolutional GAN to generate realistic images from a variety of datasets. They\nalso introduce several changes that significantly improve the quality of the generated images.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/04_gan/01_dcgan/dcgan.ipynb in\nthe book repository.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.4,
                        "section_name": "The Bricks Dataset",
                        "section_path": "./screenshots-images-2/chapter_4/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_4/ba1a506c-f82d-495d-9a4d-aae76f378828.png",
                            "./screenshots-images-2/chapter_4/section_4/62fce5ea-125d-497e-9781-9fb5dd5d7987.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Bricks Dataset\n\nFirst, you'll need to download the training data. We'll be using the Images of LEGO Bricks dataset that is\n\navailable through Kaggle. This is a computer-rendered collection of 40,000 photographic images of 50 dif-\nferent toy bricks, taken from multiple angles. Some example images of Brickki products are shown in\nFigure 4-3.\n\n6#o~~rv#&BSO\n\nFigure 4-3. Examples of images from the Bricks dataset\n\nYou can download the dataset by running the Kaggle dataset downloader script in the book repository, as\nshown in Example 4-1. This will save the images and accompanying metadata locally to the /data folder.\n\nExample 4-1. Downloading the Bricks dataset\n\nbash scripts/download_kaggle_data.sh joosthazelzet lego-brick-images\n\nWe use the Keras function image_dataset_from_directory to create a TensorFlow Dataset pointed at\nthe directory where the images are stored, as shown in Example 4-2. This allows us to read batches of im-\nages into memory only when required (e.g., during training), so that we can work with large datasets and\nnot worry about having to fit the entire dataset into memory. It also resizes the images to 64 x 64, interpo-\nlating between pixel values.\n\nExample 4-2. Creating a TensorFlow Dataset from image files in a directory\n\ntrain_data = utils.image_dataset_from_directory/(\n\"/app/data/lego-brick-images/dataset/\",\nlabels=None,\ncolor_mode=\"grayscale\",\nimage_size=(64, 64),\nbatch_size=128,\nshuffle=True,\nseed=42,\ninterpolation=\"bilinear\",\n\nThe original data is scaled in the range [0, 255] to denote the pixel intensity. When training GANs we\nrescale the data to the range [-1, 1] so that we can use the tanh activation function on the final layer of\nthe generator, which tends to provide stronger gradients than the sigmoid function (Example 4-3).\n\nExample 4-3. Preprocessing the Bricks dataset\n\ndef preprocess(img):\nimg = (tf.cast(img, \"float32\") - 127.5) / 127.5\nreturn img\n\ntrain = train_data.map(lambda x: preprocess(x) )\n\nLet\u2019s now take a look at how we build the discriminator.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.5,
                        "section_name": "The Discriminator",
                        "section_path": "./screenshots-images-2/chapter_4/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_5/18e1ea79-6326-4955-9f3e-04f4774763ac.png",
                            "./screenshots-images-2/chapter_4/section_5/e3e45aef-88b6-410e-ba0a-29fe2eb055dd.png",
                            "./screenshots-images-2/chapter_4/section_5/2e3e5afd-8caf-4c13-b167-ea9ef88dc558.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Discriminator\n\nThe goal of the discriminator is to predict if an image is real or fake. This is a supervised image classifica-\ntion problem, so we can use a similar architecture to those we worked with in Chapter 2: stacked convo-\nlutional layers, with a single output node.\n\nThe full architecture of the discriminator we will be building is shown in Table 4-1.\n\nTable 4-1. Model summary of the discriminator\n\nLayer (type) Output shape Param #\nInputLayer (None, 64, 64, 1) 0\nConv2D (None, 32, 32,64) 1,024\nLeakyReLU (None, 32, 32,64) 0\nDropout (None, 32, 32,64) 0\nConv2D (None, 16, 16,128) 131,072\nBatchNormalization (None, 16, 16,128) 512\nLeakyReLU (None, 16, 16,128) 0\nDropout (None, 16, 16,128) 0\nConv2D (None, 8, 8, 256) 524,288\nBatchNormalization (None, 8, 8, 256) 1,024\nLeakyReLU (None, 8, 8, 256) 0\nDropout (None, 8, 8, 256) 0\nConv2D (None, 4, 4, 512) 2,097,152\nBatchNormalization (None, 4, 4, 512) 2,048\nLeakyReLU (None, 4, 4, 512) 0\nDropout (None, 4, 4, 512) 0\nConv2D (None, 1, 1, 1) 8,192\nFlatten (None, 1) 0\n\nTotal params 2,765,312\n\nTrainable params 2,763,520\n\nNon-trainable params 1,792\n\nThe Keras code to build the discriminator is provided in Example 4-4.\n\nExample 4-4. The discriminator\n\ndiscriminator_input = layers.Input(shape=(64, 64, 1)) @\nx = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias = False)\ndiscriminator_input\n\nlayers .LeakyReLU(0.2) (x)\n= layers.Dropout(0.3) (x)\n\nx eM\n\nlayers .Conv2D(\n128, kernel_size=4, strides=2, padding=\"same\", use_bias = False\n\nx)\n\nlayers .BatchNormalization(momentum = 0.9) (x)\nlayers .LeakyReLU(0.2) (x)\nlayers .Dropout(0.3) (x)\n\nx x MM\n\"\n\nlayers .Conv2D(\n256, kernel_size=4, strides=2, padding=\"same\", use_bias = False\n\n\u00ae\n\n= layers.BatchNormalization(momentum = 0.9) (x)\nlayers .LeakyReLU(0.2) (x)\n= layers.Dropout(0.3) (x)\n\nx x MM\n\"\n\n= layers.Conv2D(\n512, kernel_size=4, strides=2, padding=\"same\", use_bias = False\n\n\u00ae\n\n= layers.BatchNormalization(momentum = 0.9) (x)\nlayers .LeakyReLU(0.2) (x)\n= layers.Dropout(0.3) (x)\n\nx x MM\n\"\n\n= layers.Conv2D(\ni,\nkernel_size=4,\nstrides=1,\npadding=\"valid\",\nuse_bias = False,\nactivation = 'sigmoid'\n)(x)\n\ndiscriminator_output = layers.Flatten()(x) \u00ae\n\ndiscriminator = models.Model(discriminator_input, discriminator_output) @\n\n@ Define the Input layer of the discriminator (the image).\n\n@ Stack Conv2D layers on top of each other, with BatchNormalization, LeakyReLU activation,\nand Dropout layers sandwiched in between.\n\n\u00ae Flatten the last convolutional layer\u2014by this point, the shape of the tensor is 1 x 1 x 1, so there is no\n\nneed for a final Dense layer.\n\n\u00ae Flatten the last convolutional layer\u2014by this point, the shape of the tensor is 1 x 1 x 1, so there is no\nneed for a final Dense layer.\n\n\u00a9 The Keras model that defines the discriminator\u2014a model that takes an input image and outputs a\nsingle number between 0 and 1.\n\nNotice how we use a stride of 2 in some of the Conv2D layers to reduce the spatial shape of the tensor as\nit passes through the network (64 in the original image, then 32, 16, 8, 4, and finally 1), while increasing\nthe number of channels (1 in the grayscale input image, then 64, 128, 256, and finally 512), before collaps-\ning to a single prediction.\n\nWe use a sigmoid activation on the final Conv2D layer to output a number between 0 and 1.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.6,
                        "section_name": "The Generator",
                        "section_path": "./screenshots-images-2/chapter_4/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_6/fd02ab1a-f22d-4368-92e7-07571012b599.png",
                            "./screenshots-images-2/chapter_4/section_6/1fafdc4f-37cf-434f-ad66-d0b141764867.png",
                            "./screenshots-images-2/chapter_4/section_6/b7e1e4c4-b152-47f8-b989-758e94ba9985.png",
                            "./screenshots-images-2/chapter_4/section_6/36381327-8803-4c3d-ab03-f45c13526894.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Generator\n\nNow let\u2019s build the generator. The input to the generator will be a vector drawn from a multivariate stan-\ndard normal distribution. The output is an image of the same size as an image in the original training\ndata.\n\nThis description may remind you of the decoder in a variational autoencoder. In fact, the generator of a\nGAN fulfills exactly the same purpose as the decoder of a VAE: converting a vector in the latent space to\nan image. The concept of mapping from a latent space back to the original domain is very common in\ngenerative modeling, as it gives us the ability to manipulate vectors in the latent space to change high-lev-\nel features of images in the original domain.\n\nThe architecture of the generator we will be building is shown in Table 4-2.\n\nTable 4-2. Model summary of the generator\n\nLayer (type) Output shape Param #\nInputLayer (None, 100) 0\nReshape (None, 1, 1, 100) 0\n\nConv2DTranspose (None, 4, 4, 512) 819,200\nBatchNormalization (None, 4, 4, 512) 2,048\nReLU (None, 4, 4, 512) 0\nConv2DTranspose (None, 8, 8, 256) 2,097,152\nBatchNormalization (None, 8, 8, 256) 1,024\nReLU (None, 8, 8, 256) 0\nConv2DTranspose (None, 16, 16,128) 524,288\nBatchNormalization (None, 16, 16,128) 512\nReLU (None, 16, 16,128) 0\nConv2DTranspose (None, 32, 32,64) 131,072\nBatchNormalization (None, 32, 32,64) 256\nReLU (None, 32, 32,64) 0\n\nConv2DTranspose (None, 64, 64, 1) 1,024\n\nTotal params 3,576,576\nTrainable params 3,574,656\n\nNon-trainable params 1,920\n\nThe code for building the generator is given in Example 4-5.\n\nExample 4-5. The generator\n\ngenerator input = layers.Input(shape=(100,)) @\nx = layers.Reshape((1, 1, 100))(generator_input) @\nx = layers.Conv2DTranspose(\n512, kernel_size=4, strides=1, padding=\"valid\", use_bias = False\n)(x) \u00ae\nx = layers.BatchNormalization(momentum=0 .9) (x)\nlayers .LeakyReLU(0.2) (x)\nx = layers.Conv2DTranspose(\n256, kernel_size=4, strides=2, padding=\"same\", use_bias = False\n\n*\n\"\n\nx = layers.BatchNormalization(momentum=0.9) (x)\nx = layers.LeakyReLU(0.2) (x)\nx = layers.Conv2DTranspose(\n128, kernel_size=4, strides=2, padding=\"same\", use_bias = False\n\nx = layers.BatchNormalization(momentum=0.9) (x)\nx = layers.LeakyReLU(0.2) (x)\nx = layers.Conv2DTranspose(\n64, kernel_size=4, strides=2, padding=\"same\", use_bias = False\n)(x)\nx = layers.BatchNormalization(momentum=0.9) (x)\nx = layers.LeakyReLU(0.2) (x)\ngenerator output = layers.Conv2DTranspose(\ni,\nkernel_size=4,\n\nstrides=2,\n\npadding=\"same\",\n\nuse_bias = False,\n\nactivation = 'tanh'\n)(x) @\n\ngenerator = models.Model(generator_input, generator_output) \u00a9\n\n@ Define the Input layer of the generator\u2014a vector of length 100.\n\n@ Weusea Reshape layer to give a 1 x 1 x 100 tensor, so that we can start applying convolutional\n\ntranspose operations.\n\n\u00a9 We pass this through four Conv2DTranspose layers, with BatchNormalization and\n\nLeakyReLU layers sandwiched in between.\n\n\u00a9 The final Cconv2DTranspose layer uses a tanh activation function to transform the output to the\nrange [-1, 1], to match the original image domain.\n\n\u00a9 The final Conv2DTranspose layer uses a tanh activation function to transform the output to the\nrange [-1, 1], to match the original image domain.\n\n\u00a9 The Keras model that defines the generator\u2014a model that accepts a vector of length 100 and out-\nputs a tensor of shape [64, 64, 1].\n\nNotice how we use a stride of 2 in some of the Conv2DTranspose layers to increase the spatial shape of\nthe tensor as it passes through the network (1 in the original vector, then 4, 8, 16, 32, and finally 64),\nwhile decreasing the number of channels (512 then 256, 128, 64, and finally 1 to match the grayscale\noutput).\n\nUPSAMPLING VERSUS CONV2DTRANSPOSE\nAn alternative to using Conv2DTranspose layers is to instead use an UpSampling2D layer followed by\nanormal Conv2D layer with stride 1, as shown in Example 4-6.\n\nExample 4-6. Upsampling example\n\n*\n\"\n\nlayers .UpSampling2D(size = 2) (x)\n\n*\n\"\n\nlayers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\") (x)\n\nThe UpSampling2D layer simply repeats each row and column of its input in order to double the size.\nThe Conv2D layer with stride 1 then performs the convolution operation. It is a similar idea to convolu-\ntional transpose, but instead of filling the gaps between pixels with zeros, upsampling just repeats the ex-\nisting pixel values.\n\nIt has been shown that the Conv2DTranspose method can lead to artifacts, or small checkerboard pat-\nterns in the output image (see Figure 4-4) that spoil the quality of the output. However, they are still used\n\nin many of the most impressive GANs in the literature and have proven to be a powerful tool in the deep\n\nRadford et al., 2015 Salimans et al., 2016 Donahue et al., 2019 Dumoulin et al., 2016\n\nlearning practitioner\u2019s toolbox.\n\n4\n\na\nFigure 4-4. Artifacts when using convolutional transpose layers (source: Odena et al., 2016)\n\nBoth of these methods\u2014 UpSampling2D + Conv2D and Conv2DTranspose \u2014are acceptable ways to\ntransform back to the original image domain. It really is a case of testing both methods in your own prob-\nlem setting and seeing which produces better results.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.7,
                        "section_name": "Training the DCGAN",
                        "section_path": "./screenshots-images-2/chapter_4/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_7/b1dc598e-b1e6-4ec0-a656-e8b2c82bafb4.png",
                            "./screenshots-images-2/chapter_4/section_7/97c31c70-897d-4ef9-970f-0923f011f223.png",
                            "./screenshots-images-2/chapter_4/section_7/ff038709-2344-4224-9c71-ce172c3f1d75.png",
                            "./screenshots-images-2/chapter_4/section_7/42f38b9c-d9d1-411b-9fb8-21ba97ceb148.png",
                            "./screenshots-images-2/chapter_4/section_7/99d2a482-371d-4fe2-868f-65a3065336da.png",
                            "./screenshots-images-2/chapter_4/section_7/50bbc8ce-3011-4060-8393-be072741a0ab.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the DCGAN\n\nAs we have seen, the architectures of the generator and discriminator in a DCGAN are very simple and\nnot so different from the VAE models that we looked at in Chapter 3. The key to understanding GANs lies\nin understanding the training process for the generator and discriminator.\n\nWe can train the discriminator by creating a training set where some of the images are real observations\nfrom the training set and some are fake outputs from the generator. We then treat this as a supervised\nlearning problem, where the labels are 1 for the real images and 0 for the fake images, with binary cross-\nentropy as the loss function.\n\nHow should we train the generator? We need to find a way of scoring each generated image so that it can\noptimize toward high-scoring images. Luckily, we have a discriminator that does exactly that! We can\ngenerate a batch of images and pass these through the discriminator to get a score for each image. The\nloss function for the generator is then simply the binary cross-entropy between these probabilities and a\nvector of ones, because we want to train the generator to produce images that the discriminator thinks\nare real.\n\nCrucially, we must alternate the training of these two networks, making sure that we only update the\nweights of one network at a time. For example, during the generator training process, only the genera-\ntor\u2019s weights are updated. If we allowed the discriminator\u2019s weights to change as well, the discriminator\nwould just adjust so that it is more likely to predict the generated images to be real, which is not the de-\nsired outcome. We want generated images to be predicted close to 1 (real) because the generator is\nstrong, not because the discriminator is weak.\n\nA diagram of the training process for the discriminator and generator is shown in Figure 4-5.\n\nDiscriminator training process\n\nTraining\n\nbatch (fake)\n\nTraining\nbatch (real)\n\nTraining\nbatch (fake)\n\nFigure 4-5. Training the DCGAN\u2014gray boxes indicate that the weights are frozen during training\n\nKeras provides us with the ability to create a custom train_step function to implement this logic.\nExample 4-7 shows the full DcGAN model class.\n\nExample 4-7. Compiling the DCGAN\n\nclass DCGAN(models.Model):\ndef _ init__(self, discriminator, generator, latent_dim):\nsuper(DCGAN, self). init ()\nself.discriminator = discriminator\nself.generator = generator\nself.latent_dim = latent_dim\n\ndef compile(self, d_optimizer, g optimizer):\nsuper(DCGAN, self) .compile()\nself.loss_fn = losses.BinaryCrossentropy() @\nself.d_optimizer = d_optimizer\nself.g optimizer = g_ optimizer\nself.d_loss_metric = metrics.Mean(name=\"d_loss\")\n\nself.g_loss_metric = metrics.Mean(name=\"g_loss\")\n\ndef metrics(self):\nreturn [self.d_loss_ metric, self.g_loss_metric]\n\ndef train_step(self, real_images):\nbatch_size = tf.shape(real_images) [0]\nrandom_latent_vectors = tf.random.normal (\nshape=(batch_size, self.latent_dim)\n\nim)\n\nwith tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\ngenerated _images = self.generator(\nrandom_latent_vectors, training = True\n) 9\nreal_predictions = self.discriminator(real_images, training = True) @\nfake_predictions = self.discriminator(\ngenerated_images, training = True\n\n)e\n\nreal_labels = tf.ones_like(real_predictions)\nreal_noisy labels = real_labels + 0.1 * t\u00a3.random.uniform(\ntf.shape(real_predictions)\n\n)\nfake_labels = tf.zeros_like(fake_predictions)\n\nfake_noisy labels = fake_labels - 0.1 * tf.random.uniform(\n\ntf.shape(fake_predictions)\n\nd_real_loss = self.loss_fn(real_noisy_ labels, real_predictions)\nd_fake_loss = self.loss_fn(fake_noisy_ labels, fake_predictions)\n\nd_loss = (d_real_loss + d_fake_loss) / 2.0 \u00ae\n\ng_loss = self.loss_fn(real_labels, fake_predictions) @\n\ng_loss = self.loss_fn(real_labels, fake_predictions) @\n\ngradients_of_discriminator = disc_tape.gradient(\nd_loss, self.discriminator.trainable_ variables\n\n)\ngradients_of_generator = gen_tape.gradient(\ng_loss, self.generator.trainable variables\n\nself.d_optimizer.apply_gradients/(\n\nzip(gradients of discriminator, discriminator.trainable variables)\n) 9\nself.g optimizer.apply_ gradients (\n\nzip(gradients of generator, generator.trainable_ variables)\n\nself.d_loss_metric.update_state(d_loss)\nself.g_loss_metric.update_state(g_loss)\n\nreturn {m.name: m.result() for m in self.metrics}\n\ndcgan = DCGAN(\ndiscriminator=discriminator, generator=generator, latent_dim=100\n\ndcegan.compile(\nd_optimizer=optimizers .Adam(\nlearning_rate=0.0002, beta_l = 0.5, beta_2 = 0.999\nye\n\ng_optimizer=optimizers .Adam(\nlearning_rate=0.0002, beta_l = 0.5, beta_2 = 0.999\nye\n\ndegan.fit(train, epochs=300)\n\n@ The loss function for the generator and discriminator is BinaryCrossentropy.\n\n@ Totrain the network, first sample a batch of vectors from a multivariate standard normal\ndistribution.\n\n\u00a9 Next, pass these through the generator to produce a batch of generated images.\n\n@\u00ae Now ask the discriminator to predict the realness of the batch of real images...\n\n\u00a9 ...and the batch of generated images.\n\n\u00ae The discriminator loss is the average binary cross-entropy across both the real images (with label\n1) and the fake images (with label 0).\n\n@ The generator loss is the binary cross-entropy between the discriminator predictions for the gener-\nated images and a label of 1.\n\n\u00ae Update the weights of the discriminator and generator separately.\n\nThe discriminator and generator are constantly fighting for dominance, which can make the DCGAN\ntraining process unstable. Ideally, the training process will find an equilibrium that allows the generator\nto learn meaningful information from the discriminator and the quality of the images will start to im-\nprove. After enough epochs, the discriminator tends to end up dominating, as shown in Figure 4-6, but\nthis may not be a problem as the generator may have already learned to produce sufficiently high-quality\nimages by this point.\n\nepoch _g_acc\ntag: epoch g acc\n\n0.2\n016\n012\n\n0.08\n0.04\n\n0 20 40 60 80 100 120 140 160\n\nwsbepoc ds epee xc\noss | i Lt\n0.96\n045 0.96\n025 088!\n084\n0.25 os |\n\n0 20 40 60 80 100120 140160 0 20 40 60 80 100 120 140160\n\nFigure 4-6. Loss and accuracy of the discriminator and generator during training\n\nADDING NOISE TO THE LABELS\n\nA useful trick when training GANs is to add a small amount of random noise to the training labels. This helps to im-\nprove the stability of the training process and sharpen the generated images. This label smoothing acts as way to\ntame the discriminator, so that it is presented with a more challenging task and doesn\u2019t overpower the generator.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.8,
                        "section_name": "Analysis of the DCGAN",
                        "section_path": "./screenshots-images-2/chapter_4/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_8/b4dd6693-eef1-4fb2-980c-8d36e95814bb.png",
                            "./screenshots-images-2/chapter_4/section_8/cedcfe87-d677-4254-a9e9-c1d9e3827ea1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the DCGAN\n\nBy observing images produced by the generator at specific epochs during training (Figure 4-7), it is clear\nthat the generator is becoming increasingly adept at producing images that could have been drawn from\nthe training set.\n\na ie ce ee ee\npoh > \u2122  & aged v\nmoro B\u00ae Pw} | a swf @\ner Sir a eee ae\n\neons o Qe ff + > wo HH\n\nFigure 4-7. Output from the generator at specific epochs during training\n\nIt is somewhat miraculous that a neural network is able to convert random noise into something mean-\ningful. It is worth remembering that we haven\u2019t provided the model with any additional features beyond\nthe raw pixels, so it has to work out high-level concepts such as how to draw shadows, cuboids, and cir-\ncles entirely by itself.\n\nAnother requirement of a successful generative model is that it doesn\u2019t only reproduce images from the\ntraining set. To test this, we can find the image from the training set that is closest to a particular generat-\ned example. A good measure for distance is the L1 distance, defined as:\n\ndef compare_images(imgl, img2):\n\nreturn np.mean(np.abs(imgl - img2))\n\nFigure 4-8 shows the closest observations in the training set for a selection of generated images. We can.\nsee that while there is some degree of similarity between the generated images and the training set, they\nare not identical. This shows that the generator has understood these high-level features and can gener-\nate examples that are distinct from those it has already seen.\n\nGenerated images\n\n\u00ab @ a &\u00ae &\n\nve 42\n\n\u00a9 ff #\u00a9 \u00a9 w@\n\n#\n\nClosest training set examples\n|\n\n\u2018=e &\n\u00a2 \u00a9 * \u00ab e@\n\nFigure 4-8. Closest matches of generated images from the training set\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.9,
                        "section_name": "GAN Training: Tips and Tricks",
                        "section_path": "./screenshots-images-2/chapter_4/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_9/12657827-79f5-4a5b-a513-2ca409e8d0b6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "GAN Training: Tips and Tricks\n\nWhile GANs are a major breakthrough for generative modeling, they are also notoriously difficult to\ntrain. We will explore some of the most common problems and challenges encountered when training\nGANs in this section, alongside potential solutions. In the next section, we will look at some more funda-\nmental adjustments to the GAN framework that we can make to remedy many of these problems.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.1,
                        "section_name": "Discriminator overpowers the generator",
                        "section_path": "./screenshots-images-2/chapter_4/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_10/18aab99d-b6b8-4984-8de0-36017dd29677.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Discriminator overpowers the generator\n\nIf the discriminator becomes too strong, the signal from the loss function becomes too weak to drive any\n\nmeaningful improvements in the generator. In the worst-case scenario, the discriminator perfectly learns\n\nto separate real images from fake images and the gradients vanish completely, leading to no training\n\nwhatsoever, as can be seen in Figure 4-9.\n\nEooch 1/300\nSaved to . /output/generated_1mg_800.p09\n\nEpoch 2/308\nSaves to ./output/generated_img_001.p09\n\ns-esee-8-\n\nEpoch 3/200\nSaved to ./output/generated_1mg_002.509\n\nFigure 4-9. Example output when the discriminator overpowers the generator\n\nIf you find your discriminator loss function collapsing, you need to find ways to weaken the discrimina-\n\ntor. Try the following suggestions:\n\nIncrease the rate parameter of the Dropout layers in the discriminator to dampen the amount of\ninformation that flows through the network.\n\nReduce the learning rate of the discriminator.\n\nReduce the number of convolutional filters in the discriminator.\n\nAdd noise to the labels when training the discriminator.\n\nFlip the labels of some images at random when training the discriminator.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.11,
                        "section_name": "Generator overpowers the discriminator",
                        "section_path": "./screenshots-images-2/chapter_4/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_11/5d07be28-c8ac-4b9f-b9e0-4cb073a0b623.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Generator overpowers the discriminator\n\nIf the discriminator is not powerful enough, the generator will find ways to easily trick the discriminator\nwith a small sample of nearly identical images. This is known as mode collapse.\n\nFor example, suppose we were to train the generator over several batches without updating the discrimi-\nnator in between. The generator would be inclined to find a single observation (also known as a mode)\nthat always fools the discriminator and would start to map every point in the latent input space to this\nimage. Moreover, the gradients of the loss function would collapse to near 0, so it wouldn\u2019t be able to re-\ncover from this state.\n\nEven if we then tried to retrain the discriminator to stop it being fooled by this one point, the generator\nwould simply find another mode that fools the discriminator, since it has already become numb to its in-\n\nput and therefore has no incentive to diversify its output.\n\nThe effect of mode collapse can be seen in Figure 4-10.\n\nEpoch 11/300\nSaved to ./eutput/generated_1ag_819.p09\n\nFigure 4-10. Example of mode collapse when the generator overpowers the discriminator\n\nIf you find that your generator is suffering from mode collapse, you can try strengthening the discrimina-\ntor using the opposite suggestions to those listed in the previous section. Also, you can try reducing the\nlearning rate of both networks and increasing the batch size.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.12,
                        "section_name": "Uninformative loss",
                        "section_path": "./screenshots-images-2/chapter_4/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_12/be41e13a-4bae-4ef9-9333-220b6fbfd0ba.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Uninformative loss\n\nSince the deep learning model is compiled to minimize the loss function, it would be natural to think that\nthe smaller the loss function of the generator, the better the quality of the images produced. However,\nsince the generator is only graded against the current discriminator and the discriminator is constantly\nimproving, we cannot compare the loss function evaluated at different points in the training process.\nIndeed, in Figure 4-6, the loss function of the generator actually increases over time, even though the\nquality of the images is clearly improving. This lack of correlation between the generator loss and image\nquality sometimes makes GAN training difficult to monitor.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.13,
                        "section_name": "Hyperparameters",
                        "section_path": "./screenshots-images-2/chapter_4/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_13/36002d0b-ae33-448e-a494-700bb0c1410d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Hyperparameters\n\nAs we have seen, even with simple GANs, there are a large number of hyperparameters to tune. As well\nas the overall architecture of both the discriminator and the generator, there are the parameters that\ngovern batch normalization, dropout, learning rate, activation layers, convolutional filters, kernel size,\nstriding, batch size, and latent space size to consider. GANs are highly sensitive to very slight changes in\nall of these parameters, and finding a set of parameters that works is often a case of educated trial and er-\nror, rather than following an established set of guidelines.\n\nThis is why it is important to understand the inner workings of the GAN and know how to interpret the\nloss function\u2014so that you can identify sensible adjustments to the hyperparameters that might improve\nthe stability of the model.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.14,
                        "section_name": "Tackling GAN challenges",
                        "section_path": "./screenshots-images-2/chapter_4/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_14/1d029bf6-758f-400e-9a7a-4d4746024872.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tackling GAN challenges\n\nIn recent years, several key advancements have drastically improved the overall stability of GAN models\nand diminished the likelihood of some of the problems listed earlier, such as mode collapse.\n\nIn the remainder of this chapter we shall examine the Wasserstein GAN with Gradient Penalty (WGAN-\nGP), which makes several key adjustments to the GAN framework we have explored thus far to improve\nthe stability and quality of the image generation process.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.15,
                        "section_name": "Wasserstein GAN with Gradient Penalty (WGAN-GP)",
                        "section_path": "./screenshots-images-2/chapter_4/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_15/d0de6cbe-bcc6-4d81-93ac-6e9acaccedd2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Wasserstein GAN with Gradient Penalty (WGAN-GP)\n\nIn this section we will build a WGAN-GP to generate faces from the CelebA dataset that we utilized in\nChapter 3.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/04_gan/02_wgan_gp/wgan_g-\np.ipynb in the book repository.\n\nThe code has been adapted from the excellent WGAN-GP tutorial created by Aakash Kumar Nain, available on the\nKeras website.\n\n4\nThe Wasserstein GAN (WGAN), introduced in a 2017 paper by Arjovsky et al., was one of the first big\nsteps toward stabilizing GAN training. With a few changes, the authors were able to show how to train\nGANs that have the following two properties (quoted from the paper):\n\ne\u00a2 A meaningful loss metric that correlates with the generator\u2019s convergence and sample quality\n\u00a2 Improved stability of the optimization process\n\nSpecifically, the paper introduces the Wasserstein loss function for both the discriminator and the genera-\ntor. Using this loss function instead of binary cross-entropy results in a more stable convergence of the\nGAN.\n\nIn this section we'll define the Wasserstein loss function and then see what other changes we need to\nmake to the model architecture and training process to incorporate our new loss function.\n\nYou can find the full model class in the Jupyter notebook located at chapter05/wgan-gp/faces/train.ipynb in\nthe book repository.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.16,
                        "section_name": "Wasserstein Loss",
                        "section_path": "./screenshots-images-2/chapter_4/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_16/b270af75-98f5-4b7c-b3e7-66aa8269cc62.png",
                            "./screenshots-images-2/chapter_4/section_16/1ca139d0-7c1d-4412-bc7f-a58f53e967ff.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Wasserstein Loss\n\nLet\u2019s first remind ourselves of the definition of binary cross-entropy loss\u2014the function that we are cur-\nrently using to train the discriminator and generator of the GAN (Equation 4-1).\n\nEquation 4-1. Binary cross-entropy loss\n\n-2 ys (yi log (pi) + (1 \u2014 ys) log (1 \u2014 pi)\ni=l\n\nTo train the GAN discriminator D, we calculate the loss when comparing predictions for real images\npi = D(x;) to the response y; = 1 and predictions for generated images p; = D (G (z;)) to the re-\nsponse y; = 0. Therefore, for the GAN discriminator, minimizing the loss function can be written as\nshown in Equation 4-2.\n\nEquation 4-2. GAN discriminator loss minimization\nmin ~ (Ex~px [log D(z)] + Ez~p, [log (1 ~ D(G(z)))))\n\nTo train the GAN generator G, we calculate the loss when comparing predictions for generated images\npi = D(G (z)) to the response y; = 1. Therefore, for the GAN generator, minimizing the loss function\ncan be written as shown in Equation 4-3.\n\nEquation 4-3. GAN generator loss minimization\nmin \u2014 (Ez~pz [log D(G(z))])\nNow let\u2019s compare this to the Wasserstein loss function.\n\nFirst, the Wasserstein loss requires that we use y; = 1 and y; = -1 as labels, rather than 1 and 0. We also\nremove the sigmoid activation from the final layer of the discriminator, so that predictions p; are no\nlonger constrained to fall in the range [0, 1] but instead can now be any number in the range (\u2014 oo, oo).\nFor this reason, the discriminator in a WGAN is usually referred to as a critic that outputs a score rather\nthan a probability.\n\nThe Wasserstein loss function is defined as follows:\n\nn\n\n-2 SY wi)\n\n1\n\nTo train the WGAN critic D, we calculate the loss when comparing predictions for real images\npi = D(zx;) to the response y; = 1 and predictions for generated images p; = D (G (z;)) to the re-\nsponse y; =\u20141. Therefore, for the WGAN critic, minimizing the loss function can be written as follows:\n\nmin ~ (Ex~px [D (#)] \u2014 E.~p, [D (G (2))))\n\nIn other words, the WGAN critic tries to maximize the difference between its predictions for real images\nand generated images.\n\nTo train the WGAN generator, we calculate the loss when comparing predictions for generated images\npi = D(G (z)) to the response y; = 1. Therefore, for the WGAN generator, minimizing the loss func-\ntion can be written as follows:\n\nmin \u2014 (E.~pz [D (G (z))])\n\nIn other words, the WGAN generator tries to produce images that are scored as highly as possible by the\ncritic (i.e., the critic is fooled into thinking they are real).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.17,
                        "section_name": "The Lipschitz Constraint",
                        "section_path": "./screenshots-images-2/chapter_4/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_17/a88b0347-2a07-4e75-8d81-5a7b1cc42ab0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Lipschitz Constraint\n\nIt may surprise you that we are now allowing the critic to output any number in the range (\u2014oo, oo),\nrather than applying a sigmoid function to restrict the output to the usual [0, 1] range. The Wasserstein\nloss can therefore be very large, which is unsettling\u2014usually, large numbers in neural networks are to be\navoided!\n\nIn fact, the authors of the WGAN paper show that for the Wasserstein loss function to work, we also need\nto place an additional constraint on the critic. Specifically, it is required that the critic is a 1-Lipschitz con-\ntinuous function. Let\u2019s pick this apart to understand what it means in more detail.\n\nThe critic is a function D that converts an image into a prediction. We say that this function is 1-Lipschitz\n\nif it satisfies the following inequality for any two input images, 7; and \u00a32:\n\n|D (@1) \u2014 D(@2)|\n\n<1\na1 \u2014 a9\n\nHere, |x1 \u2014 @4| is the average pixelwise absolute difference between two images and |D (x1) \u2014 D(22)|\nis the absolute difference between the critic predictions. Essentially, we require a limit on the rate at\nwhich the predictions of the critic can change between two images (i.e., the absolute value of the gradient\nmust be at most 1 everywhere). We can see this applied to a Lipschitz continuous 1D function in Figure 4-\n11\u2014at no point does the line enter the cone, wherever you place the cone on the line. In other words,\nthere is a limit on the rate at which the line can rise or fall at any point.\n\nFigure 4-11. A Lipschitz continuous function (source: Wikipedia)\n\nTIP\nFor those who want to delve deeper into the mathematical rationale behind why the Wasserstein loss only works\nwhen this constraint is enforced, Jonathan Hui offers an excellent explanation.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.18,
                        "section_name": "Enforcing the Lipschitz Constraint",
                        "section_path": "./screenshots-images-2/chapter_4/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_18/a7931b9f-98a2-4566-8582-3f5f7bb51808.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Enforcing the Lipschitz Constraint\n\nIn the original WGAN paper, the authors show how it is possible to enforce the Lipschitz constraint by\nclipping the weights of the critic to lie within a small range, [-0.01, 0.01], after each training batch.\n\nOne of the criticisms of this approach is that the capacity of the critic to learn is greatly diminished, since\nwe are clipping its weights. In fact, even in the original WGAN paper the authors write, \u201cWeight clipping\nis a clearly terrible way to enforce a Lipschitz constraint.\u201d A strong critic is pivotal to the success of a\nWGAN, since without accurate gradients, the generator cannot learn how to adapt its weights to produce\nbetter samples.\n\nTherefore, other researchers have looked for alternative ways to enforce the Lipschitz constraint and im-\nprove the capacity of the WGAN to learn complex features. One such method is the Wasserstein GAN with\nGradient Penalty.\n\nIn the paper introducing this variant, the authors show how the Lipschitz constraint can be enforced di-\nrectly by including a gradient penalty term in the loss function for the critic that penalizes the model if\nthe gradient norm deviates from 1. This results in a far more stable training process.\n\nIn the next section, we\u2019ll see how to build this extra term into the loss function for our critic.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.19,
                        "section_name": "The Gradient Penalty Loss",
                        "section_path": "./screenshots-images-2/chapter_4/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_19/2ad69fd7-d2a9-44b1-9f31-f661161a316b.png",
                            "./screenshots-images-2/chapter_4/section_19/882db8cc-af6b-4c7b-b643-7121eb14bbd0.png",
                            "./screenshots-images-2/chapter_4/section_19/0e9d4aaf-3b96-4115-ab9a-f9fb8e546e83.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Gradient Penalty Loss\n\nFigure 4-12 is a diagram of the training process for the critic of a WGAN-GP. If we compare this to the\noriginal discriminator training process from Figure 4-5, we can see that the key addition is the gradient\npenalty loss included as part of the overall loss function, alongside the Wasserstein loss from the real and\nfake images.\n\nFigure 4-12. The WGAN-GP critic training process\n\nThe gradient penalty loss measures the squared difference between the norm of the gradient of the pre-\ndictions with respect to the input images and 1. The model will naturally be inclined to find weights that\nensure the gradient penalty term is minimized, thereby encouraging the model to conform to the\n\nLipschitz constraint.\n\nIt is intractable to calculate this gradient everywhere during the training process, so instead the WGAN-\nGP evaluates the gradient at only a handful of points. To ensure a balanced mix, we use a set of interpo-\nlated images that lie at randomly chosen points along lines connecting the batch of real images to the\nbatch of fake images pairwise, as shown in Figure 4-13.\n\nO\nFake image @-@----- @ Real image\n\nInterpolated\nimage\n\nIn Example 4-8, we show how the gradient penalty is calculated in code.\n\nExample 4-8. The gradient penalty loss function\n\ndef gradient_penalty(self, batch_size, real_images, fake_images):\nalpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0) @\ndiff = fake_images - real_images\n\ninterpolated = real_images + alpha * diff @\nwith tf.GradientTape() as gp_tape:\ngp_tape.watch(interpolated)\npred = self.critic(interpolated, training=True) \u00ae\ngrads = gp_tape.gradient(pred, [interpolated])[0] \u00ae\nnorm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3])) \u00a9\ngp = tf.reduce_mean((norm - 1.0) ** 2) @\nreturn gp\n@ Each image in the batch gets a random number, between 0 and 1, stored as the vector alpha.\n\u00ae A set of interpolated images is calculated.\n\n\u00ae The critic is asked to score each of these interpolated images.\n\n\u00a9 The gradient of the predictions is calculated with respect to the input images.\n\n\u00a9 The L2 norm of this vector is calculated.\n\n\u00a9 The function returns the average squared distance between the L2 norm and 1.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.2,
                        "section_name": "Training the WGAN-GP",
                        "section_path": "./screenshots-images-2/chapter_4/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_20/515f5225-a23e-410a-a459-d87cad3f3889.png",
                            "./screenshots-images-2/chapter_4/section_20/5425eea2-4e8d-4a96-85b1-e5527f763689.png",
                            "./screenshots-images-2/chapter_4/section_20/b670633a-01f7-4d64-a622-3bde11f8cfa0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the WGAN-GP\n\nA key benefit of using the Wasserstein loss function is that we no longer need to worry about balancing\n\nthe training of the critic and the generator\u2014in fact, when using the Wasserstein loss, the critic must be\n\ntrained to convergence before updating the generator, to ensure that the gradients for the generator up-\n\ndate are accurate. This is in contrast to a standard GAN, where it is important not to let the discriminator\n\nget too strong.\n\nTherefore, with Wasserstein GANs, we can simply train the critic several times between generator up-\n\ndates, to ensure it is close to convergence. A typical ratio used is three to five critic updates per generator\n\nupdate.\n\nWe have now introduced both of the key concepts behind the WGAN-GP\u2014the Wasserstein loss and the\ngradient penalty term that is included in the critic loss function. The training step of the WGAN model\n\nthat incorporates all of these ideas is shown in Example 4-9.\n\nExample 4-9. Training the WGAN-GP\n\ndef train_step(self, real_images):\nbatch_size = tf.shape(real_images) [0]\n\nfor i in range(3): @\nrandom_latent_vectors = tf.random.normal (\nshape=(batch_size, self.latent_dim)\n\nwith tf.GradientTape() as tape:\nfake_images = self.generator(\nrandom_latent_vectors, training = True\n)\nfake_predictions = self.critic(fake_images, training\nreal_predictions = self.critic(real_images, training\n\nTrue)\nTrue)\n\nc_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(\n\nreal_predictions\nm2)\nc_gp = self.gradient_penalty(\nbatch_size, real_images, fake_images\n) 9\n\nc_loss = c_wass_loss + c_gp * self.gp_weight @\n\nc_gradient = tape.gradient(c_loss, self.critic.trainable variables)\n\nself.c_optimizer.apply_gradients(\nzip(c_gradient, self.critic.trainable variables)\n\n)e\n\nrandom_latent_vectors = tf.random.normal (\nshape=(batch_size, self.latent_dim)\n\n)\n\nwith tf.GradientTape() as tape:\nfake_images = self.generator(random_latent_vectors, training=True)\nfake_predictions = self.critic(fake_images, training=True)\n\ng_loss = -tf.reduce_mean(fake_predictions) \u00a9\ngen_gradient = tape.gradient(g_loss, self.generator.trainable variables)\nself.g optimizer.apply_gradients(\n\nzip(gen_gradient, self.generator.trainable variables)\n)@\nself.c_loss_metric.update_state(c_loss)\nself.c_wass_loss_metric.update_state(c_wass_loss)\nself.c_gp_metric.update_state(c_gp)\n\nself.g_loss_metric.update_state(g_loss)\n\nreturn {m.name: m.result() for m in self.metrics}\n\n@ Perform three critic updates.\n\n\u00ae Calculate the Wasserstein loss for the critic\u2014the difference between the average prediction for the\nfake images and the real images.\n\n\u00ae Calculate the gradient penalty term (see Example 4-8).\n\n\u00a9 The critic loss function is a weighted sum of the Wasserstein loss and the gradient penalty.\n\u00a9 Update the weights of the critic.\n\n\u00ae Calculate the Wasserstein loss for the generator.\n\n@ Update the weights of the generator.\n\nBATCH NORMALIZATION IN A WGAN-GP\n\nOne last consideration we should note before training a WGAN-GP is that batch normalization shouldn\u2019t be used in\nthe critic. This is because batch normalization creates correlation between images in the same batch, which makes\nthe gradient penalty loss less effective. Experiments have shown that WGAN-GPs can still produce excellent results\neven without batch normalization in the critic.\n\n\nWe have now covered all of the key differences between a standard GAN and a WGAN-GP. To recap:\n\ne\u00a2 A WGAN-GP uses the Wasserstein loss.\n\n\u00a2 The WGAN-GP is trained using labels of 1 for real and -1 for fake.\n\u00a2 There is no sigmoid activation in the final layer of the critic.\n\n\u00a2 Include a gradient penalty term in the loss function for the critic.\n\u00a2 Train the critic multiple times for each update of the generator.\n\n\u00a2 There are no batch normalization layers in the critic.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.21,
                        "section_name": "Analysis of the WGAN-GP",
                        "section_path": "./screenshots-images-2/chapter_4/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_21/6d5b9f87-e3c0-4126-8edd-c4f5421fc8ff.png",
                            "./screenshots-images-2/chapter_4/section_21/71170918-386d-4a57-aaac-82641cf5a343.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the WGAN-GP\n\nLet\u2019s take a look at some example outputs from the generator, after 25 epochs of training (Figure 4-14).\n\ni\n4\nPy -\n\nFigure 4-14. WGAN-GP face examples\n\nThe model has learned the significant high-level attributes of a face, and there is no sign of mode\n\ncollapse.\n\nWe can also see how the loss functions of the model evolve over time (Figure 4-15)\u2014the loss functions of\n\nboth the critic and generator are highly stable and convergent.\n\nIf we compare the WGAN-GP output to the VAE output from the previous chapter, we can see that the\nGAN images are generally sharper\u2014especially the definition between the hair and the background. This\nis true in general; VAEs tend to produce softer images that blur color boundaries, whereas GANs are\n\nknown to produce sharper, more well-defined images.\n\nepoch_c_loss epoch _g loss\ntag: epoch_c_loss tag: epoch_g loss\n2!\n\u201c10\n4.\n418+\n0 5 0 5 20 2\nepoch_c_wass_loss epoch_c_gp\ntag: epoch_c_wass loss tag: epoch_c gp\n-4} + 0s\n8) | 04\n2! } 03\n-16 ' 02\n20, , 01\nm4 \u2014 0\n0 5 0 Bb MO 2 0 5 0 6 OO\n\nFigure 4-15. WGAN-GP loss curves: the critic loss (epoch_c_loss ) is broken down into the Wasserstein loss ( epoch_c_wass ) and the gradi-\nent penalty loss (epoch_c_gp)\n\nIt is also true that GANs are generally more difficult to train than VAEs and take longer to reach a satisfac-\ntory quality. However, many state-of-the-art generative models today are GAN-based, as the rewards for\ntraining large-scale GANs on GPUs over a longer period of time are significant.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.22,
                        "section_name": "Conditional GAN (CGAN)",
                        "section_path": "./screenshots-images-2/chapter_4/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_22/70869049-273b-4150-80d5-1f4701e3c665.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Conditional GAN (CGAN)\n\nSo far in this chapter, we have built GANs that are able to generate realistic images from a given training\nset. However, we haven\u2019t been able to control the type of image we would like to generate\u2014for example,\na male or female face, or a large or small brick. We can sample a random point from the latent space, but\nwe do not have the ability to easily understand what kind of image will be produced given the choice of\n\nlatent variable.\n\nIn the final part of this chapter we shall turn our attention to building a GAN where we are able to con-\ntrol the output\u2014a so called conditional GAN. This idea, first introduced in \u201cConditional Generative\n\n6\nAdversarial Nets\u201d by Mirza and Osindero in 2014, is a relatively simple extension to the GAN\n\narchitecture.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/04_gan/03_cgan/cgan.ipynb in\nthe book repository.\n\nThe code has been adapted from the excellent CGAN tutorial created by Sayak Paul, available on the Keras website.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.23,
                        "section_name": "CGAN Architecture",
                        "section_path": "./screenshots-images-2/chapter_4/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_23/2441c2a6-b388-45fe-a63b-15403cf27eab.png",
                            "./screenshots-images-2/chapter_4/section_23/bcb1e50e-701e-4256-98f0-6bfc0a3c6a30.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "CGAN Architecture\n\nIn this example, we will condition our CGAN on the blond hair attribute of the faces dataset. That is, we\nwill be able to explicitly specify whether we want to generate an image with blond hair or not. This label\nis provided as part of the CelebA dataset.\n\nThe high-level CGAN architecture is shown in Figure 4-16.\n\nG08\n\nRandom noise\n\n| | Generated image\n\nOne-hot\nencoded label\n\nImage (RGB)\n\n14\nRealness score\n\nOne-hot encoded\nlabel channels\n\nFigure 4-16. Inputs and outputs of the generator and critic in a CGAN\n\nThe key difference between a standard GAN and a CGAN is that in a CGAN we pass in extra information\nto the generator and critic relating to the label. In the generator, this is simply appended to the latent\nspace sample as a one-hot encoded vector. In the critic, we add the label information as extra channels to\nthe RGB image. We do this by repeating the one-hot encoded vector to fill the same shape as the input\nimages.\n\nCGANs work because the critic now has access to extra information regarding the content of the image,\nso the generator must ensure that its output agrees with the provided label, in order to keep fooling the\ncritic. If the generator produced perfect images that disagreed with the image label the critic would be\nable to tell that they were fake simply because the images and labels did not match.\n\nTIP\n\nIn our example, our one-hot encoded label will have length 2, because there are two classes (Blonde and Not Blond).\nHowever, you can have as many labels as you like\u2014for example, you could train a CGAN on the Fashion-MNIST\ndataset to output one of the 10 different fashion items, by incorporating a one-hot encoded label vector of length 10\ninto the input of the generator and 10 additional one-hot encoded label channels into the input of the critic.\n\nThe only change we need to make to the architecture is to concatenate the label information to the exist-\ning inputs of the generator and the critic, as shown in Example 4-10.\n\nExample 4-10. Input layers in the CGAN\n\ncritic_input = layers.Input(shape=(64, 64, 3)) @\nlabel_input = layers.Input(shape=(64, 64, 2))\n\nx = layers.Concatenate(axis = -1)([{critic_input, label_input])\n\ngenerator input = layers.Input(shape=(32,)) @\n\nlabel_input = layers.Input(shape=(2,) )\n\nx = layers.Concatenate(axis = -1)([generator_input, label_input])\nx = layers.Reshape((1,1, 34))(x)\n\n@ The image channels and label channels are passed in separately to the critic and concatenated.\n\n@ The latent vector and the label classes are passed in separately to the generator and concatenated\nbefore being reshaped.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.24,
                        "section_name": "Training the CGAN",
                        "section_path": "./screenshots-images-2/chapter_4/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_24/6b723208-efd9-4977-91ff-78c70ee0318e.png",
                            "./screenshots-images-2/chapter_4/section_24/14a1e988-39a1-4300-9f98-981506f755ca.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the CGAN\n\nWe must also make some changes to the train_step of the CGAN to match the new input formats of the\n\ngenerator and critic, as shown in Example 4-11.\n\nExample 4-11. The train_step of the CGAN\n\ndef train_step(self, data):\n\nreal_images, one_hot_labels = data @\n\nimage_one_hot_labels = one_hot_labels[:, None, None, :] @\nimage_one_hot_labels = tf.repeat(\n\nimage_one_hot_labels, repeats=64, axis = 1\n)\nimage_one_hot_labels = tf.repeat(\nimage_one_hot_labels, repeats=64, axis = 2\n\nbatch_size = tf.shape(real_images) [0]\n\nfor i in range(self.critic_steps):\nrandom_latent_vectors = tf.random.normal (\nshape=(batch_size, self.latent_dim)\n\nwith tf.GradientTape() as tape:\nfake_images = self.generator(\n[random_latent_vectors, one_hot_labels], training = True\n\n)\u00b0o\n\nfake_predictions = self.critic(\n\n[fake_images, image_one_hot_labels], training = True\n)9\nreal_predictions = self.critic(\n\n[real_images, image_one_hot_labels], training = True\n\nc_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(\nreal_predictions\n)\nc_gp = self.gradient_penalty(\nbatch_size, real_images, fake_images, image_one_hot_labels\n) 6\n\nc_loss = c_wass_loss + c_gp * self.gp_weight\n\nc_gradient = tape.gradient(c_loss, self.critic.trainable variables)\nself.c_optimizer.apply_gradients(\nzip(c_gradient, self.critic.trainable variables)\n\nrandom_latent_vectors = tf.random.normal (\nshape=(batch_size, self.latent_dim)\n\nwith tf.GradientTape() as tape:\nfake_images = self.generator(\n[random_latent_vectors, one_hot_labels], training=True\n) 9\nfake_predictions = self.critic(\n[fake_images, image_one_hot_labels], training=True\n)\n\ng_loss = -tf.reduce_mean(fake_predictions)\ngen_gradient = tape.gradient(g_loss, self.generator.trainable variables)\n\nself.g optimizer.apply_gradients(\nzip(gen_gradient, self.generator.trainable variables)\n\nThe images and labels are unpacked from the input data.\n\nThe one-hot encoded vectors are expanded to one-hot encoded images that have the same spatial\nsize as the input images (64 x 64).\n\nThe generator is now fed with a list of two inputs\u2014the random latent vectors and the one-hot en-\ncoded label vectors.\n\nThe critic is now fed with a list of two inputs\u2014the fake/real images and the one-hot encoded label\nchannels.\n\nThe gradient penalty function also requires the one-hot encoded label channels to be passed\nthrough as it uses the critic.\n\nThe changes made to the critic training step also apply to the generator training step.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.25,
                        "section_name": "Analysis of the CGAN",
                        "section_path": "./screenshots-images-2/chapter_4/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_25/1f167399-6653-435f-a8a6-af270232ef61.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the CGAN\n\nWe can control the CGAN output by passing a particular one-hot encoded label into the input of the gen-\nerator. For example, to generate a face with nonblond hair, we pass in the vector [1, 0].To generate a\nface with blond hair, we pass in the vector [0, 1].\n\nThe output from the CGAN can be seen in Figure 4-17. Here, we keep the random latent vectors the same\nacross the examples and change only the conditional label vector. It is clear that the CGAN has learned to\nuse the label vector to control only the hair color attribute of the images. It is impressive that the rest of\nthe image barely changes\u2014this is proof that GANs are able to organize points in the latent space in sucha\nway that individual features can be decoupled from each other.\n\nOg\n\nRandom noise vector\n\nO\n] a\n\nmit!\nog,\nOo\n+ Blond label vector |\na i\n\nFigure 4-17. Output from the CGAN when the Blond and Not Blond vectors are appended to the latent sample\n\nTIP\n\nIf labels are available for your dataset, it is generally a good idea to include them as input to your GAN even if you do\nnot necessarily need to condition the generated output on the label, as they tend to improve the quality of images\ngenerated. You can think of the labels as just a highly informative extension to the pixel input.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 4.26,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_4/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_26/6a2d375a-6af4-447c-bfaf-55ecc4cdde93.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter we explored three different generative adversarial network (GAN) models: the deep con-\nvolutional GAN (DCGAN), the more sophisticated Wasserstein GAN with Gradient Penalty (WGAN-GP),\nand the conditional GAN (CGAN).\n\nAll GANs are characterized by a generator versus discriminator (or critic) architecture, with the discrimi-\nnator trying to \u201cspot the difference\u201d between real and fake images and the generator aiming to fool the\ndiscriminator. By balancing how these two adversaries are trained, the GAN generator can gradually\nlearn how to produce similar observations to those in the training set.\n\nWe first saw how to train a DCGAN to generate images of toy bricks. It was able to learn how to realisti-\ncally represent 3D objects as images, including accurate representations of shadow, shape, and texture.\nWe also explored the different ways in which GAN training can fail, including mode collapse and vanish-\ning gradients.\n\nWe then explored how the Wasserstein loss function remedied many of these problems and made GAN\ntraining more predictable and reliable. The WGAN-GP places the 1-Lipschitz requirement at the heart of\nthe training process by including a term in the loss function to pull the gradient norm toward 1.\n\nWe applied the WGAN-GP to the problem of face generation and saw how by simply choosing points from\na standard normal distribution, we can generate new faces. This sampling process is very similar to a\nVAE, though the faces produced by a GAN are quite different\u2014often sharper, with greater distinction be-\ntween different parts of the image.\n\nFinally, we built a CGAN that allowed us to control the type of image that is generated. This works by\npassing in the label as input to the critic and generator, thereby giving the network the additional infor-\nmation it needs in order to condition the generated output on a given label.\n\nOverall, we have seen how the GAN framework is extremely flexible and able to be adapted to many in-\nteresting problem domains. In particular, GANs have driven significant progress in the field of image\ngeneration with many interesting extensions to the underlying framework, as we shall see in Chapter 10.\n\nIn the next chapter, we will explore a different family of generative model that is ideal for modeling se-\nquential data\u2014autoregressive models.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 5,
                "chapter_name": "Chapter 5. Autoregressive Models",
                "chapter_path": "./screenshots-images-2/chapter_5",
                "sections": [
                    {
                        "section_id": 5.1,
                        "section_name": "Chapter 5. Autoregressive Models",
                        "section_path": "./screenshots-images-2/chapter_5/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_1/9daf4086-2659-418a-83de-ffe1d3f6dbeb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 5. Autoregressive Models\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\n\u00a2 Learn why autoregressive models are well suited to generating sequential data such as text.\n\u00a2 Learn how to process and tokenize text data.\n\n\u00a2 Learn about the architectural design of recurrent neural networks (RNNs).\n\n\u00a2 Build and train a long short-term memory network (LSTM) from scratch using Keras.\n\nUse the LSTM to generate new text.\n\u00a2 Learn about other variations of RNNs, including gated recurrent units (GRUs) and bidirectional cells.\n\nUnderstand how image data can be treated as a sequence of pixels.\n\u00a2 Learn about the architectural design of a PixelCNN.\n\n\u00a2 Build a PixelCNN from scratch using Keras.\n\n\u00a9 Use the PixelCNN to generate images.\n\nSo far, we have explored two different families of generative models that have both involved latent vari-\nables\u2014variational autoencoders (VAEs) and generative adversarial networks (GANs). In both cases, a new\nvariable is introduced with a distribution that is easy to sample from and the model learns how to decode\nthis variable back into the original domain.\n\nWe will now turn our attention to autoregressive models\u2014a family of models that simplify the generative\nmodeling problem by treating it as a sequential process. Autoregressive models condition predictions on\nprevious values in the sequence, rather than on a latent random variable. Therefore, they attempt to ex-\nplicitly model the data-generating distribution rather than an approximation of it (as in the case of VAEs).\n\nIn this chapter we shall explore two different autoregressive models: long short-term memory networks\nand PixelCNN. We will apply the LSTM to text data and the PixelCNN to image data. We will cover another\n\nhighly successful autoregressive model, the Transformer, in detail in Chapter 9.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_5/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_2/939bf3cd-20d6-4ee8-8d03-2fc578d73185.png",
                            "./screenshots-images-2/chapter_5/section_2/8e380e14-53dd-44a2-aab5-6cf428da16a5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nTo understand how an LSTM works, we will first pay a visit to a strange prison, where the inmates have\n\nformed a literary society...\n\nTHE LITERARY SOCIETY FOR TROUBLESOME MISCREANTS\nEdward Sopp hated his job as a prison warden. He spent his days watching over the prisoners and had no\ntime to follow his true passion of writing short stories. He was running low on inspiration and needed to\n\nfind a way to generate new content.\n\nOne day, he came up with a brilliant idea that would allow him to produce new works of fiction in his\nstyle, while also keeping the inmates occupied\u2014he would get the inmates to collectively write the stories\nfor him! He branded the new society the Literary Society for Troublesome Miscreants, or LSTM (\n\n).\n\nFigure 5-1. A large cell of prisoners reading books (created with )\n\nThe prison is particularly strange because it only consists of one large cell, containing 256 prisoners. Each\nprisoner has an opinion on how Edward\u2019s current story should continue. Every day, Edward posts the lat-\nest word from his novel into the cell, and it is the job of the inmates to individually update their opinions\non the current state of the story, based on the new word and the opinions of the inmates from the previ-\nous day.\n\nThe prison is particularly strange because it only consists of one large cell, containing 256 prisoners. Each\nprisoner has an opinion on how Edward\u2019s current story should continue. Every day, Edward posts the lat-\nest word from his novel into the cell, and it is the job of the inmates to individually update their opinions\non the current state of the story, based on the new word and the opinions of the inmates from the previ-\nous day.\n\nEach prisoner uses a specific thought process to update their own opinion, which involves balancing in-\nformation from the new incoming word and other prisoners\u2019 opinions with their own prior beliefs. First,\nthey decide how much of yesterday\u2019s opinion they wish to forget, taking into account the information\nfrom the new word and the opinions of other prisoners in the cell. They also use this information to form\nnew thoughts and decide to what extent they want to mix these into the old beliefs that they have chosen\nto carry forward from the previous day. This then forms the prisoner\u2019s new opinion for the day.\n\nHowever, the prisoners are secretive and don\u2019t always tell their fellow inmates all of their opinions. They\neach also use the latest chosen word and the opinions of the other inmates to decide how much of their\nopinion they wish to disclose.\n\nWhen Edward wants the cell to generate the next word in the sequence, the prisoners each tell their dis-\nclosable opinions to the guard at the door, who combines this information to ultimately decide on the\nnext word to be appended to the end of the novel. This new word is then fed back into the cell, and the\nprocess continues until the full story is completed.\n\nTo train the inmates and the guard, Edward feeds short sequences of words that he has written previous-\nly into the cell and monitors whether the inmates\u2019 chosen next word is correct. He updates them on their\naccuracy, and gradually they begin to learn how to write stories in his own unique style.\n\nAfter many iterations of this process, Edward finds that the system has become quite accomplished at\ngenerating realistic-looking text. Satisfied with the results, he publishes a collection of the generated tales\nin his new book, entitled E. Sopp\u2019s Fables.\n\nThe story of Mr. Sopp and his crowdsourced fables is an analogy for one of the most notorious autore-\ngressive techniques for sequential data such as text: the long short-term memory network.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.3,
                        "section_name": "Long Short-Term Memory Network (LSTM)",
                        "section_path": "./screenshots-images-2/chapter_5/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_3/9c01c8a4-0ed4-4654-addf-04e9d9ec750d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Long Short-Term Memory Network (LSTM)\n\nAn LSTM is a particular type of recurrent neural network (RNN). RNNs contain a recurrent layer (or cell)\nthat is able to handle sequential data by making its own output at a particular timestep form part of the\n\ninput to the next timestep.\n\nWhen RNNs were first introduced, recurrent layers were very simple and consisted solely of a tanh oper-\nator that ensured that the information passed between timesteps was scaled between -1 and 1. However,\nthis approach was shown to suffer from the vanishing gradient problem and didn\u2019t scale well to long se-\n\nquences of data.\n\nLSTM cells were first introduced in 1997 in a paper by Sepp Hochreiter and Jiirgen Schmidhuber.* In the\npaper, the authors describe how LSTMs do not suffer from the same vanishing gradient problem experi-\nenced by vanilla RNNs and can be trained on sequences that are hundreds of timesteps long. Since then,\nthe LSTM architecture has been adapted and improved, and variations such as gated recurrent units (dis-\ncussed later in this chapter) are now widely utilized and available as layers in Keras.\n\nLSTMs have been applied to a wide range of problems involving sequential data, including time series\nforecasting, sentiment analysis, and audio classification. In this chapter we will be using LSTMs to tackle\nthe challenge of text generation.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/05_autoregressive/01_Istm/Ist-\nm.ipynb in the book repository.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.4,
                        "section_name": "The Recipes Dataset",
                        "section_path": "./screenshots-images-2/chapter_5/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_4/d4429d92-e983-4b5b-ae64-b2eb72aa7361.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Recipes Dataset\n\nWe'll be using the Epicurious Recipes dataset that is available through Kaggle. This is a set of over 20,000\nrecipes, with accompanying metadata such as nutritional information and ingredient lists.\n\nYou can download the dataset by running the Kaggle dataset downloader script in the book repository, as\nshown in Example 5-1. This will save the recipes and accompanying metadata locally to the /data folder.\n\nExample 5-1. Downloading the Epicurious Recipe dataset\n\nbash scripts/download_kaggle_data.sh hugodarwood epirecipes\n\nExample 5-2 shows how the data can be loaded and filtered so that only recipes with a title and a descrip-\ntion remain. An example of a recipe text string is given in Example 5-3.\n\nExample 5-2. Loading the data\n\nwith open('/app/data/epirecipes/full_format_recipes.json') as json_data:\nrecipe_data = json.load(json_data)\n\nfiltered_data = [\n\"Recipe for ' + x['title']+ ' | ' + ' '.join(x['directions'])\n\nfor x in recipe_data\n\nif 'title' in x\n\nand x['title'] is not None\n\nand '\u2018directions' in x\n\nand x['directions'] is not None\n\nExample 5-3. A text string from the Recipes dataset\n\ntcipe for Ham Persillade with Mustard Potato Salad and Mashed Peas | Chop enough\nirsley leaves to measure 1 tablespoon; reserve. Chop remaining leaves and stems\n\nid simmer with broth and garlic in a small saucepan, covered, 5 minutes.\n\nsanwhile, sprinkle gelatin over water in a medium bowl and let soften 1 minute.\n:rain broth through a fine-mesh sieve into bowl with gelatin and stir to dissolve.\ntason with salt and pepper. Set bowl in an ice bath and cool to room temperature,\n:irring. Toss ham with reserved parsley and divide among jars. Pour gelatin on top\nid chill until set, at least 1 hour. Whisk together mayonnaise, mustard, vinegar,\n'4 teaspoon salt, and 1/4 teaspoon pepper in a large bowl. Stir in celery,\n\u00bbrnichons, and potatoes. Pulse peas with marjoram, oil, 1/2 teaspoon pepper, and\n'4 teaspoon salt in a food processor to a coarse mash. Layer peas, then potato\nt.lad, over ham.\n\nBefore taking a look at how to build an LSTM network in Keras, we must first take a quick detour to un-\nderstand the structure of text data and how it is different from the image data that we have seen so far in\nthis book.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.5,
                        "section_name": "Working with Text Data",
                        "section_path": "./screenshots-images-2/chapter_5/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_5/322c8e6a-d6dc-4e3b-bdc0-d17016d55fe7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Working with Text Data\n\nThere are several key differences between text and image data that mean that many of the methods that\nwork well for image data are not so readily applicable to text data. In particular:\n\ne Text data is composed of discrete chunks (either characters or words), whereas pixels in an image\nare points in a continuous color spectrum. We can easily make a green pixel more blue, but it is not\nobvious how we should go about making the word cat more like the word dog, for example. This\nmeans we can easily apply backpropagation to image data, as we can calculate the gradient of our\nloss function with respect to individual pixels to establish the direction in which pixel colors should\nbe changed to minimize the loss. With discrete text data, we can\u2019t obviously apply backpropagation\nin the same way, so we need to find a way around this problem.\n\nText data has a time dimension but no spatial dimension, whereas image data has two spatial dimen-\nsions but no time dimension. The order of words is highly important in text data and words wouldn\u2019t\nmake sense in reverse, whereas images can usually be flipped without affecting the content.\nFurthermore, there are often long-term sequential dependencies between words that need to be cap-\ntured by the model: for example, the answer to a question or carrying forward the context of a pro-\nnoun. With image data, all pixels can be processed simultaneously.\n\nText data is highly sensitive to small changes in the individual units (words or characters). Image\ndata is generally less sensitive to changes in individual pixel units\u2014a picture of a house would still\nbe recognizable as a house even if some pixels were altered\u2014but with text data, changing even a\nfew words can drastically alter the meaning of the passage, or make it nonsensical. This makes it\nvery difficult to train a model to generate coherent text, as every word is vital to the overall meaning\nof the passage.\n\nText data has a rules-based grammatical structure, whereas image data doesn\u2019t follow set rules\nabout how the pixel values should be assigned. For example, it wouldn\u2019t make grammatical sense in\nany context to write \u201cThe cat sat on the having.\u201d There are also semantic rules that are extremely dif-\nficult to model; it wouldn\u2019t make sense to say \u201cI am in the beach,\u201d even though grammatically, there\nis nothing wrong with this statement.\n\nADVANCES IN TEXT-BASED GENERATIVE DEEP LEARNING\n\nUntil recently, most of the most sophisticated generative deep learning models have focused on image data, because\nmany of the challenges presented in the preceding list were beyond the reach of even the most advanced techniques.\nHowever, in the last five years astonishing progress has been made in the field of text-based generative deep learn-\ning, thanks to the introduction of the Transformer model architecture, which we will explore in Chapter 9.\n\nWith these points in mind, let\u2019s now take a look at the steps we need to take in order to get the text data\ninto the right shape to train an LSTM network.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.6,
                        "section_name": "Tokenization",
                        "section_path": "./screenshots-images-2/chapter_5/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_6/73dcb3f8-7e61-4f80-9268-155445d873cc.png",
                            "./screenshots-images-2/chapter_5/section_6/33cbd223-c5ad-4c92-a2b5-2f405351bc98.png",
                            "./screenshots-images-2/chapter_5/section_6/31e877c8-5e2c-4d24-93e8-651d9cdaa261.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tokenization\n\nThe first step is to clean up and tokenize the text. Tokenization is the process of splitting the text up into\n\nindividual units, such as words or characters.\n\nHow you tokenize your text will depend on what you are trying to achieve with your text generation\nmodel. There are pros and cons to using both word and character tokens, and your choice will affect how\nyou need to clean the text prior to modeling and the output from your model.\n\nIf you use word tokens:\n\ne All text can be converted to lowercase, to ensure capitalized words at the start of sentences are tok-\nenized the same way as the same words appearing in the middle of a sentence. In some cases, how-\never, this may not be desirable; for example, some proper nouns, such as names or places, may bene-\nfit from remaining capitalized so that they are tokenized independently.\n\n\u00a9 The text vocabulary (the set of distinct words in the training set) may be very large, with some words\nappearing very sparsely or perhaps only once. It may be wise to replace sparse words with a token\nfor unknown word, rather than including them as separate tokens, to reduce the number of weights\nthe neural network needs to learn.\n\nWords can be stemmed, meaning that they are reduced to their simplest form, so that different tens-\nes of a verb remained tokenized together. For example, browse, browsing, browses, and browsed\nwould all be stemmed to brows.\n\n\u00a2 You will need to either tokenize the punctuation, or remove it altogether.\n\nUsing word tokenization means that the model will never be able to predict words outside of the\n\ntraining vocabulary.\nIf you use character tokens:\n\n\u00a2 The model may generate sequences of characters that form new words outside of the training vocab-\nulary\u2014this may be desirable in some contexts, but not in others.\n\n* Capital letters can either be converted to their lowercase counterparts, or remain as separate tokens.\n\n\u00a2 The vocabulary is usually much smaller when using character tokenization. This is beneficial for\nmodel training speed as there are fewer weights to learn in the final output layer.\n\nFor this example, we\u2019ll use lowercase word tokenization, without word stemming. We\u2019ll also tokenize\npunctuation marks, as we would like the model to predict when it should end sentences or use commas,\n\nfor example.\nThe code in Example 5-4 cleans and tokenizes the text.\n\nExample 5-4. Tokenization\n\ndef pad_punctuation(s):\nSs = re.sub(f\"([{string.punctuation}])\", r' \\l ', s)\ns = re.sub(' +', ' ', s)\nreturn s\ntext_data = [pad_punctuation(x) for x in filtered_data] 0\n\ntext_ds = tf.data.Dataset.from_tensor_slices(text_data).batch(32).shuffle(1000) @\n\nvectorize layer = layers.TextVectorization( \u00ae\n\nstandardize = \u2018lower',\nmax_tokens = 10000,\noutput_mode = \"int\",\n\noutput_sequence_length = 200 + 1,\n\nvectorize_layer.adapt(text_ds) @\n\nvocab = vectorize_layer.get_vocabulary() \u00a9\n\n@ Pad the punctuation marks, to treat them as separate words.\n\u00ae@ Convert to a TensorFlow Dataset.\n\n\u00ae Create a Keras TextVectorization layer to convert text to lowercase, give the most prevalent\n10,000 words a corresponding integer token, and trim or pad the sequence to 201 tokens long.\n\n\u00a9 Apply the TextVectorization layer to the training data.\n\u00a9 The vocab variable stores a list of the word tokens.\n\nAn example of a recipe after tokenization is shown in Example 5-5. The sequence length that we use to\ntrain the model is a parameter of the training process. In this example we choose to use a sequence\nlength of 200, so we pad or clip the recipe to one more than this length, to allow us to create the target\nvariable (more on this in the next section). To achieve this desired length, the end of the vector is padded\nwith zeros.\n\nSTOP TOKENS\n\nThe 0 token is known as a the stop token, signifying that the text string has come to an end.\n\n\nExample 5-5. The recipe from Example 5-3 tokenized\n\n[ 26 16 557 1 8 298 335 189 41054 494 27 332 228\n\n235 262 5 594 11 133 22 311 2 332 45 262 4 671\n\n4 70 8 171 4 81 6 9 65 80 3 121 3.59\n\n12 2 299 3. 88 650 20 39 6 9 29\u00b0 21 4 67\n\n529 11 164 2 320 171 102 9 374 13 643 306 25 21\n\n8 650 4 42 5 931 2 63 8 24 4 33 2 114\n\n21 6 178 181 1245 4 60 5 140 112 3 48 2 117\n\n557 8 285 235 4 200 292 980 2 107 650 28 72 4\n\n108 10 114 3. 57 204 11 172 2 73 110 482 3 298\n\n3 190 3 11 23 32 142\u00b0 24 3 4 11 23 32 142\n\n33 6 9 30 21 2 42 6 353 3 3224 3 4 150\n\n2 437 494 8 1281 337 11.230 \u00ab15 \u00ab142,33 3\n\n11 23\u00ab-32:\u00ab142s 6 291 188 5 9 412 572\n\n230 494 3 46 335 189 20 557 2 0 0 0\n0 0 0 0]\n\noN A\nwow\n\nIn Example 5-6, we can see a subset of the list of tokens mapped to their respective indices. The layer re-\nserves the 0 token for padding (i.e., it is the stop token) and the 1 token for unknown words that fall\noutside the top 10,000 words (e.g., persillade). The other words are assigned tokens in order of frequency.\nThe number of words to include in the vocabulary is also a parameter of the training process. The more\nwords included, the fewer unknown tokens you will see in the text; however, your model will need to be\nlarger to accommodate the larger vocabulary size.\n\nExample 5-6. The vocabulary of the TextVectorization layer\n\n[UNK]\n\nand\n\nin\nthe\nwith\n\nCw oOI DH RWNHO\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.7,
                        "section_name": "Creating the Training Set",
                        "section_path": "./screenshots-images-2/chapter_5/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_7/440c2b41-f642-447b-be75-3e5eb9429e2e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Creating the Training Set\n\nOur LSTM will be trained to predict the next word in a sequence, given a sequence of words preceding\nthis point. For example, we could feed the model the tokens for grilled chicken with boiled and would ex-\npect the model to output a suitable next word (e.g., potatoes, rather than bananas).\n\nWe can therefore simply shift the entire sequence by one token in order to create our target variable.\nThe dataset generation step can be achieved with the code in Example 5-7.\n\nExample 5-7. Creating the training dataset\n\ndef prepare_inputs(text):\ntext = tf.expand_dims(text, -1)\ntokenized_sentences = vectorize_layer(text)\n:-1]\ny = tokenized_sentences[:, 1:]\n\nx = tokenized_sentences\n\nreturn x, y\n\ntrain_ds = text_ds.map(prepare inputs) O\n\n@\u00ae Create the training set consisting of recipe tokens (the input) and the same vector shifted by one to-\nken (the target).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.8,
                        "section_name": "The LSTM Architecture",
                        "section_path": "./screenshots-images-2/chapter_5/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_8/59088c19-45ae-4fdb-8c54-6b6ac12d41b5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The LSTM Architecture\n\nThe architecture of the overall LSTM model is shown in Table 5-1. The input to the model is a sequence of\ninteger tokens and the output is the probability of each word in the 10,000-word vocabulary appearing\nnext in the sequence. To understand how this works in detail, we need to introduce two new layer types,\n\nEmbedding and LSTM.\n\nTable 5-1. Model summary of the LSTM\n\nLayer (type) Output shape Param #\nInputLayer (None, None) 0\n\nEmbedding (None, None, 100) 1,000,000\n\nLSTM (None, None, 128) 117,248\nDense (None, None, 10000) 1,290,000\nTotal params 2,407,248\n\nTrainable params 2,407,248\n\nNon-trainable params 0\n\nTHE INPUT LAYER OF THE LSTM\n\nNotice that the Input layer does not need us to specify the sequence length in advance. Both the batch size and the\nsequence length are flexible (hence the (None, None) shape). This is because all downstream layers are agnostic to\nthe length of the sequence being passed through.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.9,
                        "section_name": "The Embedding Layer",
                        "section_path": "./screenshots-images-2/chapter_5/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_9/7280f62f-1320-41d3-a625-d7697a58f9f6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Embedding Layer\n\nAn embedding layer is essentially a lookup table that converts each integer token into a vector of length\nembedding_size, as shown in Figure 5-2. The lookup vectors are learned by the model as weights.\nTherefore, the number of weights learned by this layer is equal to the size of the vocabulary multiplied\nby the dimension of the embedding vector (i.e., 10,000 x 100 = 1,000,000).\n\nVocabulary size (10,000)\n\nEmbedding size (100)\n\nFigure 5-2. An embedding layer is a lookup table for each integer token\n\nWe embed each integer token into a continuous vector because it enables the model to learn a represen-\ntation for each word that is able to be updated through backpropagation. We could also just one-hot en-\ncode each input token, but using an embedding layer is preferred because it makes the embedding itself\ntrainable, thus giving the model more flexibility in deciding how to embed each token to improve its per-\n\nformance.\n\nTherefore, the Input layer passes a tensor of integer sequences of shape [batch_size, seq_length]\nto the Embedding layer, which outputs a tensor of shape [batch_size, seq_length,\nembedding_size] . This is then passed on to the Lst\u2122 layer (Figure 5-3).\n\n2 [064 [04 |\n2 fox [056 |\n56 [aa [oss || o22 | -025)\n=|3 [ose [05s || 0 | -045)\nS) 2 UA) [om [ose [om [55]\nZ\n2) \u00ab0 f(a [ox |. [oa [ 05]\n# Toa fi34_ [08 |. [01 | -010 |\ne [ose [ox |. | 063 | om |\n5 Foes [ os7 |. | os | -055\nsl [or [oa |. [om | om |\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.1,
                        "section_name": "The LSTM Layer",
                        "section_path": "./screenshots-images-2/chapter_5/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_10/8762a689-45d6-4dad-a51b-21a71d21790f.png",
                            "./screenshots-images-2/chapter_5/section_10/815a35a5-e052-4bee-bf72-4edc095f31a3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The LSTM Layer\n\nTo understand the LSTM layer, we must first look at how a general recurrent layer works.\n\nA recurrent layer has the special property of being able to process sequential input data z1,---, @,. It\nconsists of a cell that updates its hidden state, hz, as each element of the sequence 2; is passed through it,\none timestep at a time.\n\nThe hidden state is a vector with length equal to the number of units in the cell\u2014it can be thought of as\nthe cell\u2019s current understanding of the sequence. At timestep f, the cell uses the previous value of the hid-\nden state, hy_1, together with the data from the current timestep x; to produce an updated hidden state\nvector, h;. This recurrent process continues until the end of the sequence. Once the sequence is finished,\nthe layer outputs the final hidden state of the cell, h,,, which is then passed on to the next layer of the net-\nwork. This process is shown in Figure 5-4.\n\nRecurrent layer\n\nh\n\nn\n\nFigure 5-4. A simple diagram of a recurrent layer\n\nTo explain this in more detail, let\u2019s unroll the process so that we can see exactly how a single sequence is\nfed through the layer (Figure 5-5).\n\nCELL WEIGHTS\nIt\u2019s important to remember that all of the cells in this diagram share the same weights (as they are really the same\ncell). There is no difference between this diagram and Figure 5-4; it\u2019s just a different way of drawing the mechanics of\n\na recurrent layer.\n\n\nSequence (200)\n\na\nThe input to the cell is the embedded BERBEEREEEEE\nvector from the corresponding word\n\nin the sequence and the hidden state\n\nEmbedding size (100)\n\nThe hidden state\nis initialized.\n\n[0.34 -012 .. 054 -0.23] [-0.93 -072 .. 019 -0.61]\nunits (158) units (158)\nEach cell carries its \u201chidden\u201d state to the The final hidden state is output by\nnext timestep. This is a vector with the the layer. It is a vector with the same\nsame length as the number of units in a cell. length as the number of units ina cell.\n\nFigure 5-5. How a single sequence flows through a recurrent layer\n\nHere, we represent the recurrent process by drawing a copy of the cell at each timestep and show how\nthe hidden state is constantly being updated as it flows through the cells. We can clearly see how the pre-\nvious hidden state is blended with the current sequential data point (i.e., the current embedded word\nvector) to produce the next hidden state. The output from the layer is the final hidden state of the cell, af-\nter each word in the input sequence has been processed.\n\nWARNING\n\nThe fact that the output from the cell is called a hidden state is an unfortunate naming convention\u2014it\u2019s not really hid-\nden, and you shouldn\u2019t think of it as such. Indeed, the last hidden state is the overall output from the layer, and we\nwill be making use of the fact that we can access the hidden state at each individual timestep later in this chapter.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.11,
                        "section_name": "The LSTM Cell",
                        "section_path": "./screenshots-images-2/chapter_5/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_11/8c95fdad-6e52-4747-a906-59c9fe715f68.png",
                            "./screenshots-images-2/chapter_5/section_11/932b19ae-5f49-43df-91a1-8c3ead9dbc25.png",
                            "./screenshots-images-2/chapter_5/section_11/899224cd-1124-42e8-b61d-265ff9e3e488.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The LSTM Cell\n\nNow that we have seen how a generic recurrent layer works, let\u2019s take a look inside an individual LSTM\ncell.\n\nThe job of the LSTM cell is to output a new hidden state, h;, given its previous hidden state, hy_;, and the\ncurrent word embedding, 2;. To recap, the length of h, is equal to the number of units in the LSTM. This\nis a parameter that is set when you define the layer and has nothing to do with the length of the\nsequence.\n\nWARNING\n\nMake sure you do not confuse the term cell with unit. There is one cell in an LSTM layer that is defined by the num-\nber of units it contains, in the same way that the prisoner cell from our earlier story contained many prisoners. We\noften draw a recurrent layer as a chain of cells unrolled, as it helps to visualize how the hidden state is updated at\neach timestep.\n\nAn LSTM cell maintains a cell state, C;, which can be thought of as the cell\u2019s internal beliefs about the\ncurrent status of the sequence. This is distinct from the hidden state, h;, which is ultimately output by the\ncell after the final timestep. The cell state is the same length as the hidden state (the number of units in\nthe cell).\n\nLet\u2019s look more closely at a single cell and how the hidden state is updated (Figure 5-6).\nThe hidden state is updated in six steps:\n\n1. The hidden state of the previous timestep, h;_;, and the current word embedding, x;, are concate-\nnated and passed through the forget gate. This gate is simply a dense layer with weights matrix Wy,\nbias by, and a sigmoid activation function. The resulting vector, fi, has length equal to the number of\nunits in the cell and contains values between 0 and 1 that determine how much of the previous cell\nstate, C;_1, should be retained.\n\nN\n\nwe\n\nwa\n\nB f= 5% -thxdebp\n= o(W)- Lhe px) +b)\n\nG,=tanh(We- [hy px] + Bp)\nCe fer Coa tle\n0, = (Wo + [eX] + By)\nhy=0,+tanh(C,)\n\nFigure 5-6. An LSTM cell\n\n. The concatenated vector is also passed through an input gate that, like the forget gate, is a dense lay-\n\ner with weights matrix Wj, bias 6;, and a sigmoid activation function. The output from this gate, 7;,\nhas length equal to the number of units in the cell and contains values between 0 and 1 that deter-\nmine how much new information will be added to the previous cell state, Cy_1.\n\n. The concatenated vector is passed through a dense layer with weights matrix Wc, bias bc, anda\n\ntanh activation function to generate a vector Gh, that contains the new information that the cell\nwants to consider keeping. It also has length equal to the number of units in the cell and contains\nvalues between -1 and 1.\n\nf; and C;_; are multiplied element-wise and added to the element-wise multiplication of i; and (on\nThis represents forgetting parts of the previous cell state and then adding new relevant information\nto produce the updated cell state, Ci.\n\n. The concatenated vector is passed through an output gate: a dense layer with weights matrix W,,\n\nbias 6,, and a sigmoid activation. The resulting vector, o;, has length equal to the number of units in\nthe cell and stores values between 0 and 1 that determine how much of the updated cell state, C;, to\n\noutput from the cell.\n\n. 0, is multiplied element-wise with the updated cell state, C;, after a tanh activation has been applied\n\nto produce the new hidden state, hy.\n\nTHE KERAS LSTM LAYER\nAll of this complexity is wrapped up within the LsTm layer type in Keras, so you don\u2019t have to worry about imple-\nmenting it yourself!\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.12,
                        "section_name": "Training the LSTM",
                        "section_path": "./screenshots-images-2/chapter_5/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_12/afc0dab0-e02b-4cf5-9632-f229f6285a1c.png",
                            "./screenshots-images-2/chapter_5/section_12/47da990f-e5f6-4734-8e1a-964444f52716.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the LSTM\nThe code to build, compile, and train the LSTM is given in Example 5-8.\n\nExample 5-8. Building, compiling, and training the LSTM\n\ninputs = layers.Input(shape=(None,), dtype=\"int32\") @\n\nx = layers.Embedding(10000, 100)(inputs) @\n\nx = layers.LSTM(128, return_sequences=True)(x) \u00ae\n\noutputs = layers.Dense(10000, activation = 'softmax')(x) @\n\nistm = models.Model(inputs, outputs) \u00a9\n\nloss_fn = losses.SparseCategoricalCrossentropy()\nlistm.compile(\"adam\", loss_fn) \u00a9\nlistm.fit(train_ds, epochs=25) @\n\n@ The Input layer does not need us to specify the sequence length in advance (it can be flexible), so\nwe use None as a placeholder.\n\n@ The Embedding layer requires two parameters, the size of the vocabulary (10,000 tokens) and the\ndimensionality of the embedding vector (100).\n\n\u00ae The LSTM layers require us to specify the dimensionality of the hidden vector (128). We also choose\nto return the full sequence of hidden states, rather than just the hidden state at the final timestep.\n\n@ The Dense layer transforms the hidden states at each timestep into a vector of probabilities for\n\nthe next token.\n\n\u00a9 The overall Model predicts the next token, given an input sequence of tokens. It does this for each\ntoken in the sequence.\n\n\u00a9 The model is compiled with SparseCategoricalCrossentropy loss\u2014this is the same as categor-\nical cross-entropy, but is used when the labels are integers rather than one-hot encoded vectors.\n\n@ The model is fit to the training dataset.\n\nIn Figure 5-7 you can see the first few epochs of the LSTM training process\u2014notice how the example out-\nput becomes more comprehensible as the loss metric falls. Figure 5-8 shows the cross-entropy loss metric\nfalling throughout the training process.\n\nEpoch 1/25\n628/629 (:\ngenerated text\nrecipe for mold salad are high 8 pickled to fold cook the dish into and warm in baking reduced but halves beans\nand cut\n\n6] ~ ETA: @5 ~ loss: 4.4536\n\n629/629 (===\u2014\u2014\u2014=s\nEpoch 2/25\n628/629 (=nnmmmnn:\ngenerated text:\nrecipe for racks - up~don with herb fizz | serve checking thighs onto sanding butter and baking surface in a hea\nvy heavy large saucepan over blender ; stand overnight . (UNK] over moderate heat until very blended , garlic ,\nabout 8 minutes . cook airtight until cooked are soft seeds , about 1 45 minutes . sugar , until s is brow , 5\nto sliced , parmesan , until browned and add extract . wooden crumb to outside of out sheets . flatten and prehe\nated return to the paste . add in pecans oval and let transfer day \u00ab\n\n] = 295 43ms/step ~ loss: 4.4527\n\nmmnn>.] - ETA: @5 ~ loss: 3.2339\n\n629/629 (=nmmmmnn:\nEpoch 3/25\n629/629 (=eenemnemaennnnmnnnnnnensmames] - ETA: @5 - loss: 2.6229\n\ngenerated text:\n\nrecipe for grilled chicken | preheat oven to 400\u00b0f . cook in large 8 - caramel grinder or until desired are firm\n\u00bb about 6 minutes\n\nmmmmnn} - 305 48ms/step - loss: 3.2336\n\n629/629 [- |] = 275 42ms/step - loss: 2.6229\nEpoch 4/25\n629/629 [- ] - ETA: @s ~ loss: 2.3426\n\ngenerated text:\nrecipe for pizza salad with sweet red pepper and star fruit | combine all ingredients except lowest ingredients\nin a large skillet . working with batches and deglaze , cook until just cooked through , about 1 minute . meanwh\nile , boil potatoes and paprika in a Little oil over medium - high heat , stirring it just until crisp , about 3\nminutes . stir in bell pepper , onion and cooked paste and jalapefio until clams well after most - reggiano , abo\nut 5 minutes . transfer warm 2 tablespoons flesh of eggplants to medium bowl . serve \u00ab\n\nFigure 5-7. The first few epochs of the LSTM training process\n\nepoch_loss\ntag: epoch_loss\n\n4.5\n\n3.5\n\n25\n\n15\n\n0 2 4 6 8 10 12 14 16 18 20 22 24\noe\n\nFigure 5-8. The cross-entropy loss metric of the LSTM training process by epoch\n\nro\nLa\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.13,
                        "section_name": "Analysis of the LSTM",
                        "section_path": "./screenshots-images-2/chapter_5/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_13/4b406b07-334b-48dd-8f2e-daa246013578.png",
                            "./screenshots-images-2/chapter_5/section_13/24aa1bd0-98c6-4e9a-a4a5-83ec33409733.png",
                            "./screenshots-images-2/chapter_5/section_13/05321e84-2504-4beb-8d98-65eccbdc52b8.png",
                            "./screenshots-images-2/chapter_5/section_13/e8bddaf4-bbd3-43fe-bb44-1533b4d623fe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the LSTM\n\nNow that we have compiled and trained the LSTM, we can start to use it to generate long strings of text by\napplying the following process:\n\n1. Feed the network with an existing sequence of words and ask it to predict the following word.\n2. Append this word to the existing sequence and repeat.\n\nThe network will output a set of probabilities for each word that we can sample from. Therefore, we can\nmake the text generation stochastic, rather than deterministic. Moreover, we can introduce a temperature\nparameter to the sampling process to indicate how deterministic we would like the process to be.\n\nTHE TEMPERATURE PARAMETER\n\nA temperature close to 0 makes the sampling more deterministic (i.e., the word with the highest probability is very\nlikely to be chosen), whereas a temperature of 1 means each word is chosen with the probability output by the\nmodel.\n\nThis is achieved with the code in Example 5-9, which creates a callback function that can be used to gen-\nerate text at the end of each training epoch.\n\nExample 5-9. The TextGenerator callback function\n\nclass TextGenerator(callbacks.Callback) :\ndef init__(self, index_to_word, top_k=10):\nself.index_to_word = index_to_word\nself.word_to_index = {\nword: index for index, word in enumerate(index_to_word)\n\n=)\n\ndef sample _from(self, probs, temperature): @\nprobs = probs ** (1 / temperature)\nprobs = probs / np.sum(probs)\nreturn np.random.choice(len(probs), p=probs), probs\n\ndef generate(self, start_prompt, max_tokens, temperature):\nstart_tokens = [\nself.word_to_index.get(x, 1) for x in start_prompt.split()\n19\nsample token = None\ninfo = []\nwhile len(start_tokens) < max_tokens and sample _token != 0: @\nx = np.array([start_tokens])\ny = self.model.predict(x) \u00a9\nsample token, probs = self.sample_from(y[0][-1], temperature) \u00a9\ninfo.append({'prompt': start_prompt , \u2018word_probs': probs})\nstart_tokens.append(sample token) @\n\nstart_prompt = start_prompt + ' ' + self.index_to_word[sample_token]\n\nprint(f\"\\ngenerated text:\\n{start_prompt}\\n\")\nreturn info\n\ndef on_epoch_end(self, epoch, logs=None):\nself.generate(\"recipe for\", max_tokens = 100, temperature = 1.0)\n\n@ Create an inverse vocabulary mapping (from word to token).\n\u00ae This function updates the probabilities with a temperature scaling factor.\n\n\u00ae The start prompt is a string of words that you would like to give the model to start the generation\nprocess (for example, recipe for). The words are first converted to a list of tokens.\n\n@ The sequence is generated until it is max_tokens long or a stop token (0) is produced.\n\u00a9 The model outputs the probabilities of each word being next in the sequence.\n\n\u00ae The probabilities are passed through the sampler to output the next word, parameterized by\n\ntemperature.\n@ We append the new word to the prompt text, ready for the next iteration of the generative process.\n\nLet\u2019s take a look at this in action, at two different temperature values (Figure 5-9).\n\ngenerated text:\nrecipe for sour japanese potatoes julienne | in a bowl stir together the yeast nixture with the nilk and the\npeanut butter crunbs , the sour crean , and the butter mixture with a fork , gently fold in the prunes gently\nor until incorporated\u2019. lightly stir the oil and yeast until it just holds Soft peaks , but not runny , on bo\ntton of 2 7 - sided sheet of aluninun foil , top it with a round , and a pinch of each\u2019brownies into a goblet\n\u00bb or with the baking dish . serve each with\u2019 sorbet\n\ngenerated text:\nrecipe for grilled chicken with mustard - herb sauce | conbine first 6 ingredients in nedium bowl . add chick\nen to pot . add chicken and turn to coat . cover and refrigerate at least 1 hour and up to 1 day . preheat ov\nen to 450\u00b0f . place turkey on rock in roasting pan . roast until thernoneter inserted into thickest part of t\nhigh registers 175\u00b0F , about 1 hour longer . transfer to rack in center of oven and preheat to 45\u00b0F . brush\nchicken with oil . sprinkle with salt and pepper . roast until thernoneter inserted into\n\nFigure 5-9. Generated outputs at temperature = 1.0 and temperature = 0.2\n\nThere are a few things to note about these two passages. First, both are stylistically similar to a recipe\nfrom the original training set. They both open with a recipe title and contain generally grammatically cor-\nrect constructions. The difference is that the generated text with a temperature of 1.0 is more adventur-\nous and therefore less accurate than the example with a temperature of 0.2. Generating multiple samples\nwith a temperature of 1.0 will therefore lead to more variety, as the model is sampling from a probability\n\ndistribution with greater variance.\n\nTo demonstrate this, Figure 5-10 shows the top five tokens with the highest probabilities for a range of\n\nprompts, for both temperature values.\n\nPROMPT: recipe for roast PROMPT: recipe for roast\n67.54%\n\n1: recipe for chocolate ice cream |\n27.31%\n\n11.21%\n6.66%\n5.64%\n\nFigure 5-10. Distribution of word probabilities following various sequences, for temperature values of 1.0 and 0.2\n\nThe model is able to generate a suitable distribution for the next most likely word across a range of con-\ntexts. For example, even though the model was never told about parts of speech such as nouns, verbs, or\nnumbers, it is generally able to separate words into these classes and use them in a way that is grammati-\n\ncally correct.\n\nMoreover, the model is able to select an appropriate verb to begin the recipe instructions, depending on\nthe preceding title. For roasted vegetables, it selects preheat , prepare, heat, put, Or combine as\nthe most likely possibilities, whereas for ice cream it selects in, combine, stir, whisk, and mix. This\nshows that the model has some contextual understanding of the differences between recipes depending\non their ingredients.\n\nNotice also how the probabilities for the temperature = 0.2 examples are much more heavily weight-\ned toward the first choice token. This is the reason why there is generally less variety in generations\nwhen the temperature is lower.\n\nWhile our basic LSTM model is doing a great job at generating realistic text, it is clear that it still struggles\nto grasp some of the semantic meaning of the words that it is generating. It introduces ingredients that\nare not likely to work well together (for example, sour Japanese potatoes, pecan crumbs, and sorbet)! In\nsome cases, this may be desirable\u2014say, if we want our LSTM to generate interesting and unique patterns\nof words\u2014but in other cases, we will need our model to have a deeper understanding of the ways in\nwhich words can be grouped together and a longer memory of ideas introduced earlier in the text.\n\nIn the next section, we\u2019ll explore some of the ways that we can improve our basic LSTM network. In\nChapter 9, we\u2019ll take a look at a new kind of autoregressive model, the Transformer, which takes lan-\nguage modeling to the next level.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.14,
                        "section_name": "Recurrent Neural Network (RNN) Extensions",
                        "section_path": "./screenshots-images-2/chapter_5/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_14/88cc7f64-2985-4e97-a9d5-def87643fc4c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Recurrent Neural Network (RNN) Extensions\n\nThe model in the preceding section is a simple example of how an LSTM can be trained to learn how to\ngenerate text in a given style. In this section we will explore several extensions to this idea.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.15,
                        "section_name": "Stacked Recurrent Networks",
                        "section_path": "./screenshots-images-2/chapter_5/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_15/b9a264a0-22be-4bc7-bfc5-5e1fc2c59bd3.png",
                            "./screenshots-images-2/chapter_5/section_15/de6ad72e-95f2-4a7f-83ef-c1d949cc0290.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stacked Recurrent Networks\n\nThe network we just looked at contained a single LSTM layer, but we can also train networks with\nstacked LSTM layers, so that deeper features can be learned from the text.\n\nTo achieve this, we simply introduce another LSTM layer after the first. The second LSTM layer can then\nuse the hidden states from the first layer as its input data. This is shown in Figure 5-11, and the overall\nmodel architecture is shown in Table 5-2.\n\nFigure 5-11. Diagram of a multilayer RNN: g- denotes hidden states of the first layer and I, denotes hidden states of the second layer\nTable 5-2. Model summary of the stacked LSTM\nLayer (type) Output shape Param #\nInputLayer (None, None) 0\n\nEmbedding (None, None, 100) 1,000,000\n\nLSTM (None, None, 128) 117,248\nLSTM (None, None, 128) 131,584\nDense (None, None, 10000) 1,290,000\nTotal params 2,538,832\n\nTrainable params 2,538,832\n\nNon-trainable params 0\n\nThe code to build the stacked LSTM is given in Example 5-10.\n\nThe code to build the stacked LSTM is given in Example 5-10.\n\nExample 5-10. Building a stacked LSTM\n\ntext_in = layers.Input(shape = (None, ))\n\nembedding = layers.Embedding(total_words, embedding size) (text_in)\nx = layers.LSTM(n_units, return_sequences = True) (x)\n\nx = layers.LSTM(n_units, return_sequences = True) (x)\n\nprobabilites = layers.Dense(total_words, activation = \u2018softmax') (x)\nmodel = models.Model(text_in, probabilites)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.16,
                        "section_name": "Gated Recurrent Units",
                        "section_path": "./screenshots-images-2/chapter_5/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_16/f68c6a65-6648-41f1-9e34-3da339105b94.png",
                            "./screenshots-images-2/chapter_5/section_16/801625d9-0c9a-4fde-bdae-f018646eaabf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Gated Recurrent Units\n\n2\nAnother type of commonly used RNN layer is the gated recurrent unit (GRU). The key differences from\nthe LSTM unit are as follows:\n\n1. The forget and input gates are replaced by reset and update gates.\n2. There is no cell state or output gate, only a hidden state that is output from the cell.\n\nThe hidden state is updated in four steps, as illustrated in Figure 5-12.\n\n= OW, Lhe X4))\n\n= tanh(W-[r,* hy. X4))\n2,= 0(W,- [hy 1X)\n\nhy= (1-2) +hyy +2 *hy\n\nFigure 5-12. A single GRU cell\n\nThe process is as follows:\n\n. The hidden state of the previous timestep, h_1, and the current word embedding, x;, are concate-\nnated and used to create the reset gate. This gate is a dense layer, with weights matrix W, and a sig-\nmoid activation function. The resulting vector, 7;, has length equal to the number of units in the cell\nand stores values between 0 and 1 that determine how much of the previous hidden state, hy_1,\nshould be carried forward into the calculation for the new beliefs of the cell.\n\n. The reset gate is applied to the hidden state, hy_,, and concatenated with the current word embed-\nding, x,. This vector is then fed to a dense layer with weights matrix W and a tanh activation func-\ntion to generate a vector, he that stores the new beliefs of the cell. It has length equal to the number\nof units in the cell and stores values between -1 and 1.\n\n. The concatenation of the hidden state of the previous timestep, h;_;, and the current word embed-\nding, x;, are also used to create the update gate. This gate is a dense layer with weights matrix W,\nand a sigmoid activation. The resulting vector, z;, has length equal to the number of units in the cell\nand stores values between 0 and 1, which are used to determine how much of the new beliefs, hi to\nblend into the current hidden state, hi_1.\n\n. The new beliefs of the cell, hi and the current hidden state, hy_j, are blended in a proportion deter-\nmined by the update gate, z;, to produce the updated hidden state, h;, that is output from the cell.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.17,
                        "section_name": "Bidirectional Cells",
                        "section_path": "./screenshots-images-2/chapter_5/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_17/0a84d0e3-d3ef-44a5-bbf4-5a2298892ffc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Bidirectional Cells\n\nFor prediction problems where the entire text is available to the model at inference time, there is no rea-\nson to process the sequence only in the forward direction\u2014it could just as well be processed backward. A\nBidirectional layer takes advantage of this by storing two sets of hidden states: one that is produced\nas a result of the sequence being processed in the usual forward direction and another that is produced\nwhen the sequence is processed backward. This way, the layer can learn from information both preced-\n\ning and succeeding the given timestep.\n\nIn Keras, this is implemented as a wrapper around a recurrent layer, as shown in Example 5-11.\n\nExample 5-11. Building a bidirectional GRU layer\n\nlayer = layers.Bidirectional(layers.GRU(100))\n\nHIDDEN STATE\n\nThe hidden states in the resulting layer are vectors of length equal to double the number of units in the wrapped cell\n(a concatenation of the forward and backward hidden states). Thus, in this example the hidden states of the layer are\nvectors of length 200.\n\nSo far, we have only applied autoregressive models (LSTMs) to text data. In the next section, we will see\nhow autoregressive models can also be used to generate images.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.18,
                        "section_name": "PixelCNN",
                        "section_path": "./screenshots-images-2/chapter_5/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_18/0828e3f9-1164-4261-a386-9640236caefd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "PixelCNN\n\n3\nIn 2016, van den Oord et al. introduced a model that generates images pixel by pixel by predicting the\nlikelihood of the next pixel based on the pixels before it. The model is called PixelCNN, and it can be\ntrained to generate images autoregressively.\n\nThere are two new concepts that we need to introduce to understand the PixelCNN\u2014masked convolution-\n\nal layers and residual blocks.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at\nnotebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb in the book repository.\n\nThe code has been adapted from the excellent PixelCNN tutorial created by ADMoreau, available on the Keras\n\nwebsite.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.19,
                        "section_name": "Masked Convolutional Layers",
                        "section_path": "./screenshots-images-2/chapter_5/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_19/416f0710-fc11-46b7-b0a8-04b40eb8311c.png",
                            "./screenshots-images-2/chapter_5/section_19/62ebb7e2-4313-4859-a881-6b400d5ec6a3.png",
                            "./screenshots-images-2/chapter_5/section_19/2289a271-cf5a-450b-b545-c50ee7502546.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Masked Convolutional Layers\n\nAs we saw in Chapter 2, a convolutional layer can be used to extract features from an image by applying\na series of filters. The output of the layer at a particular pixel is a weighted sum of the filter weights mul-\ntiplied by the preceding layer values over a small square centered on the pixel. This method can detect\nedges and textures and, at deeper layers, shapes and higher-level features.\n\nWhilst convolutional layers are extremely useful for feature detection, they cannot directly be used in an\nautoregressive sense, because there is no ordering placed on the pixels. They rely on the fact that all pix-\nels are treated equally\u2014no pixel is treated as the start or end of the image. This is in contrast to the text\n\ndata that we have already seen in this chapter, where there is a clear ordering to the tokens so recurrent\n\nmodels such as LSTMs can be readily applied.\n\nFor us to be able to apply convolutional layers to image generation in an autoregressive sense, we must\nfirst place an ordering on the pixels and ensure that the filters are only able to see pixels that precede the\npixel in question. We can then generate images one pixel at a time, by applying convolutional filters to\nthe current image to predict the value of the next pixel from all preceding pixels.\n\nWe first need to choose an ordering for the pixels\u2014a sensible suggestion is to order the pixels from top\nleft to bottom right, moving first along the rows and then down the columns.\n\nWe then mask the convolutional filters so that the output of the layer at each pixel is only influenced by\npixel values that precede the pixel in question. This is achieved by multiplying a mask of ones and zeros\nwith the filter weights matrix, so that the values of any pixels that are after the target pixel are zeroed.\n\nThere are actually two different kinds of masks in a PixelCNN, as shown in Figure 5-13:\n\n\u00a9 Type A, where the value of the central pixel is masked\n\u00a9 Type B, where the value of the central pixel is not masked\n\n255\n\nFigure 5-13. Left: a convolutional filter mask; right: a mask applied to a set of pixels to predict the distribution of the central pixel value (source:\nvan den Oord et al, 2016)\n\nThe initial masked convolutional layer (i.e., the one that is applied directly to the input image) cannot use\nthe central pixel, because this is precisely the pixel we want the network to guess! However, subsequent\nlayers can use the central pixel because this will have been calculated only as a result of information\n\nfrom preceding pixels in the original input image.\nWe can see in Example 5-12 how a MaskedConvLayer can be built using Keras.\n\nExample 5-12. A MaskedConvLayer in Keras\n\nExample 5-12. A MaskedConvLayer in Keras\n\nclass MaskedConvLayer(layers.Layer):\ndef init__(self, mask_type, **kwargs):\nsuper(MaskedConvLayer, self). init _ ()\nself.mask_type = mask_type\n\nself.conv = layers.Conv2D(**kwargs) @\n\ndef build(self, input_shape):\nself.conv.build(input_shape)\nkernel_shape = self.conv.kernel.get_shape()\n\nself.mask = np.zeros(shape=kernel_shape) @\n\nself.mask[: kernel_shape[0] // 2, ...] = 1.00\nself.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.00\nif self.mask_type == \"B\":\n\nself.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.090\n\ndef call(self, inputs):\nself.conv.kernel.assign(self.conv.kernel * self.mask) \u00a9\nreturn self.conv(inputs)\n\n@ The MaskedConvLayer is based on the normal Conv2D layer.\n\n@ The mask is initialized with all zeros.\n\nThe pixels in the preceding rows are unmasked with ones.\n\nThe pixels in the preceding columns that are in the same row are unmasked with ones.\n\u00a9 Ifthe mask type is B, the central pixel is unmasked with a one.\n@ The mask is multiplied with the filter weights.\n\nNote that this simplified example assumes a grayscale image (i.e., with one channel). If we have color im-\nages, we'll have three color channels that we can also place an ordering on so that, for example, the red\nchannel precedes the blue channel, which precedes the green channel.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.2,
                        "section_name": "Residual Blocks",
                        "section_path": "./screenshots-images-2/chapter_5/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_20/76f9ae4e-a635-4598-9875-ae98719c219e.png",
                            "./screenshots-images-2/chapter_5/section_20/287c27ae-9de1-40d3-a4e1-9bfcd3e86ca7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Residual Blocks\n\nNow that we have seen how to mask the convolutional layer, we can start to build our PixelCNN. The core\nbuilding block that we will use is the residual block.\n\nA residual block is a set of layers where the output is added to the input before being passed on to the rest\nof the network. In other words, the input has a fast-track route to the output, without having to go\nthrough the intermediate layers\u2014this is called a skip connection. The rationale behind including a skip\nconnection is that if the optimal transformation is just to keep the input the same, this can be achieved by\nsimply zeroing the weights of the intermediate layers. Without the skip connection, the network would\nhave to find an identity mapping through the intermediate layers, which is much harder.\n\nA diagram of the residual block in our PixelCNN is shown in Figure 5-14.\n\nMaskedConv2D\n\nConv2D\n\nFigure 5-14. A PixelCNN residual block (the numbers of filters are next to the arrows and the filter sizes are next to the layers)\n\nWe can build a ResidualBlock using the code shown in Example 5-13.\n\nExample 5-13. A ResidualBlock\n\nclass ResidualBlock(layers.Layer):\n\ndef\n\ndef\n\n__init_ (self, filters, **kwargs):\nsuper(ResidualBlock, self).__init_(**kwargs)\nself.convl = layers.Conv2D(\n\nfilters=filters // 2, kernel_size=1, activation=\"relu\"\n)@o\nself.pixel_conv = MaskedConv2D(\n\nmask_type=\"B\",\n\nfilters=filters // 2,\n\nkernel_size=3,\n\nactivation=\"relu\",\n\npadding=\"same\",\n)9@\nself.conv2 = layers.Conv2D(\n\nfilters=filters, kernel _size=1, activation=\"relu\"\n\n:)\n\ncall(self, inputs):\n\nx = self.convl(inputs)\nx = self.pixel_conv(x)\nx = self.conv2(x)\n\nreturn layers.add([inputs, x]) @\n\n@ The initial conv2pD layer halves the number of channels.\n\n@ The Type B MaskedConv2D layer with kernel size of 3 only uses information from five pixels\u2014\n\nthree pixels in the row above the focus pixel, one to the left, and the focus pixel itself.\n\n\u00ae The final conv2pD layer doubles the number of channels to again match the input shape.\n\n\u00a9 The output from the convolutional layers is added to the input\u2014this is the skip connection.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.21,
                        "section_name": "Training the PixelCNN",
                        "section_path": "./screenshots-images-2/chapter_5/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_21/61023f49-4ab1-4542-9ccb-f35d512fc92e.png",
                            "./screenshots-images-2/chapter_5/section_21/8ae9454e-2ba7-410d-8fa1-00c1ea89f49d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the PixelCNN\n\nIn Example 5-14 we put together the whole PixelCNN network, approximately following the structure laid\nout in the original paper. In the original paper, the output layer is a 256-filter Conv2D layer, with soft-\nmax activation. In other words, the network tries to re-create its input by predicting the correct pixel val-\nues, a bit like an autoencoder. The difference is that the PixelCNN is constrained so that no information\nfrom earlier pixels can flow through to influence the prediction for each pixel, due to the way that net-\nwork is designed, using MaskedConv2D layers.\n\nA challenge with this approach is that the network has no way to understand that a pixel value of, say,\n200 is very close to a pixel value of 201. It must learn every pixel output value independently, which\nmeans training can be very slow, even for the simplest datasets. Therefore, in our implementation, we in-\nstead simplify the input so that each pixel can take only one of four values. This way, we can use a 4-filter\nConv2D output layer instead of 256.\n\nExample 5-14. The PixelCNN architecture\n\ninputs = layers.Input(shape=(16, 16, 1)) @\nx = MaskedConv2D(mask_type=\"A\"\n\n, filters=128\n\n, kernel_size=7\n\n, activation=\"relu\"\n\n, padding=\"same\") (inputs)@\n\nfor _ in range(5):\nx = ResidualBlock(filters=128)(x) \u00ae\n\nfor _ in range(2):\n\nx = MaskedConv2D(\nmask_type=\"B\",\nfilters=128,\nkernel_size=1,\nstrides=1,\nactivation=\"relu\",\npadding=\"valid\",\n\n)(x) @\n\nout = layers.Conv2D(\nfilters=4, kernel_size=1, strides=1, activation=\"softmax\", padding=\"valid\"\n)(x) \u00a9\n\npixel_cnn = models.Model(inputs, out) @\n\nadam = optimizers.Adam(learning rate=0.0005)\npixel_cnn.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\")\n\npixel_cnn.fit(\ninput_data\n, output_data\n1 batch _size=128\n, epochs=150\n)@\n\n@ The model Input isa grayscale image of size 16 x 16 x 1, with inputs scaled between 0 and 1.\n\n\u00ae@ The first Type A MaskedConv2D layer with a kernel size of 7 uses information from 24 pixels\u201421\npixels in the three rows above the focus pixel and 3 to the left (the focus pixel itself is not used).\n\n\u00ae Five ResidualBlock layer groups are stacked sequentially.\n\n@ Two Type B MaskedConv2D layers with a kernel size of 1 act as Dense layers across the number\nof channels for each pixel.\n\n\u00ae The final conv2D layer reduces the number of channels to four\u2014the number of pixel levels for\nthis example.\n\n@ The Model is built to accept an image and output an image of the same dimensions.\n\n@ Fit the model\u2014 input_data is scaled in the range [0, 1] (floats); output_data is scaled in the\nrange [0, 3] (integers).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.22,
                        "section_name": "Analysis of the PixelCNN",
                        "section_path": "./screenshots-images-2/chapter_5/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_22/1a4bdc25-7e6d-4089-bf25-7ee94b6ce199.png",
                            "./screenshots-images-2/chapter_5/section_22/72a8978d-0d10-4f96-9a29-76857c339b1a.png",
                            "./screenshots-images-2/chapter_5/section_22/f81158e9-19ed-4294-a1c4-b1a3f43ecf70.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the PixelCNN\n\nWe can train our PixelCNN on images from the Fashion-MNIST dataset that we encountered in Chapter 3.\nTo generate new images, we need to ask the model to predict the next pixel given all preceding pixels,\none pixel at a time. This is a very slow process compared to a model such as a variational autoencoder!\nFor a 32 x 32 grayscale image, we need to make 1,024 predictions sequentially using the model, compared\nto the single prediction that we need to make for a VAE. This is one of the major downsides to autoregres-\nsive models such as a PixelCNN\u2014they are slow to sample from, because of the sequential nature of the\nsampling process.\n\nFor this reason, we use an image size of 16 x 16, rather than 32 x 32, to speed up the generation of new\nimages. The generation callback class is shown in Example 5-15.\n\nExample 5-15. Generating new images using the PixelCNN\n\nclass ImageGenerator(callbacks.Callback):\ndef init (self, num_img):\n\nself.num_img = num_img\n\ndef sample from(self, probs, temperature):\nprobs = probs ** (1 / temperature)\nprobs = probs / np.sum(probs)\nreturn np.random.choice(len(probs), p=probs)\n\ndef generate(self, temperature):\ngenerated_images = np.zeros(\nshape=(self.num_img,) + (pixel_cnn.input_shape)[1:]\n)@o\n\nbatch, rows, cols, channels = generated_images.shape\n\nfor row in range(rows):\nfor col in range(cols):\nfor channel in range(channels):\nprobs = self.model.predict(generated_images) [\nt, row, col, :\n]@\ngenerated_images[:, row, col, channel] = [\nself.sample_from(x, temperature) for x in probs\n3)\ngenerated_images[:, row, col, channel] /= 4 @\nreturn generated_images\n\ndef on_epoch_end(self, epoch, logs=None):\ngenerated_images = self.generate(temperature = 1.0)\ndisplay (\ngenerated_images,\nsave_to = \"./output/generated_img_%03d.png\" % (epoch)\ns)\n\nimg_generator_callback = ImageGenerator(num_img=10)\n\n@ Start with a batch of empty images (all zeros).\n\n\u00ae Loop over the rows, columns, and channels of the current image, predicting the distribution of the\n\nnext pixel value.\n\u00a9 Sample a pixel level from the predicted distribution (for our example, a level in the range [0, 3]).\n\n\u00a9 Convert the pixel level to the range [0, 1] and overwrite the pixel value in the current image, ready\nfor the next iteration of the loop.\n\nIn Figure 5-15, we can see several images from the original training set, alongside images that have been\ngenerated by the PixelCNN.\n\nImages from the training set\n\naud\n\nGenerated images\n\na AP\nifAde#\n\nFigure 5-15. Example images from the training set and generated images created by the PixelCNN model\n\nThe model does a great job of re-creating the overall shape and style of the original images! It is quite\namazing that we can treat images as a series of tokens (pixel values) and apply autoregressive models\nsuch as a PixelCNN to produce realistic samples.\n\nAs mentioned previously, one of the downsides to autoregressive models is that they are slow to sample\nfrom, which is why a simple example of their application is presented in this book. However, as we shall\nsee in Chapter 10, more complex forms of autoregressive model can be applied to images to produce\nstate-of-the-art outputs. In such cases, the slow generation speed is a necessary price to pay in return for\nexceptional-quality outputs.\n\nSince the original paper was published, several improvements have been made to the architecture and\ntraining process of the PixelCNN. The following section introduces one of those changes\u2014using mixture\ndistributions\u2014and demonstrates how to train a PixelCNN model with this improvement using a built-in\nTensorFlow function.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.23,
                        "section_name": "Mixture Distributions\n\nFor our previous example, we reduced th",
                        "section_path": "./screenshots-images-2/chapter_5/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_23/bffe89b2-ee99-4815-81ed-8ecea6a0cc72.png",
                            "./screenshots-images-2/chapter_5/section_23/e51ebda3-9086-4646-9e23-4d0956869861.png",
                            "./screenshots-images-2/chapter_5/section_23/0848bdbe-3325-4f54-a501-8790a14e0943.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Mixture Distributions\n\nFor our previous example, we reduced the output of the PixelCNN to just 4 pixel levels to ensure the net-\nwork didn\u2019t have to learn a distribution over 256 independent pixel values, which would slow the train-\ning process. However, this is far from ideal\u2014for color images, we wouldn\u2019t want our canvas to be re-\n\nstricted to only a handful of possible colors.\n\nTo get around this problem, we can make the output of the network a mixture distribution, instead of a\nsoftmax over 256 discrete pixel values, following the ideas presented by Salimans et al\u2019 A mixture distri-\nbution is quite simply a mixture of two or more other probability distributions. For example, we could\nhave a mixture distribution of five logistic distributions, each with different parameters. The mixture dis-\ntribution also requires a discrete categorical distribution that denotes the probability of choosing each of\n\nthe distributions included in the mix. An example is shown in Figure 5-16.\n\n0.0200\n0.0175\n0.0150 05\nons)\n\n20100 | 03\n0.0075.\n0.0050\n\n0.0025 | =\n\n0.0000 . = :\n0 50 100 150 200 250\nx\n\np(x\n\nFigure 5-16. A mixture distribution of three normal distributions with different varameters\u2014the categorical distribution over the three normal\ndistributions is [0.5, 0.3, 0.2]\n\nTo sample from a mixture distribution, we first sample from the categorical distribution to choose a par-\nticular subdistribution and then sample from this in the usual way. This way, we can create complex dis-\ntributions with relatively few parameters. For example, the mixture distribution in Figure 5-16 only re-\nquires eight parameters\u2014two for the categorical distribution and a mean and variance for each of the\nthree normal distributions. This is compared to the 255 parameters that would define a categorical distri-\n\nbution over the entire pixel range.\n\nConveniently, the TensorFlow Probability library provides a function that allows us to create a PixelCNN\nwith mixture distribution output in a single line. Example 5-16 illustrates how to build a PixelCNN using\n\nthis function.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook in notebooks/05_autoregressive/03_pixelcnn_md/pixel-\n\ncnn_md.ipynb in the book repository.\n\nExample 5-16. Building a PixelCNN using the TensorFlow function\n\nimport tensorflow_probability as tfp\n\ndist = tfp.distributions.PixelCNN(\nimage_shape=(32, 32, 1),\nnum_resnet=1,\nnum_hierarchies=2,\nnum_filters=32,\nnum_logistic_mix=5,\ndropout_p=.3,\n\n)o\n\nimage_input = layers.Input(shape=(32, 32, 1)) @\n\nlog_prob = dist.log_prob(image_input)\n\nmodel = models.Model(inputs=image_input, outputs=log prob) \u00ae\n\nmodel .add_loss(-tf.reduce_mean(log_prob)) @\n\n@ Define the PixelCNN as a distribution\u2014i.e., the output layer is a mixture distribution made up of\nfive logistic distributions.\n\n\u00ae@ The input is a grayscale image of size 32 x 32 x 1.\n\n\u00a9 The Model takes a grayscale image as input and outputs the log-likelihood of the image under the\nmixture distribution calculated by the PixelCNN.\n\n\u00a9 The loss function is the mean negative log-likelihood over the batch of input images.\n\nThe model is trained in the same way as before, but this time accepting integer pixel values as input, in\nthe range [0, 255]. Outputs can be generated from the distribution using the sample function, as shown\nin Example 5-17.\n\nExample 5-17. Sampling from the PixelCNN mixture distribution\n\ndist.sample(10) .numpy()\n\nExample generated images are shown in Figure 5-17. The difference from our previous examples is that\nnow the full range of pixel values is being utilized.\n\n1a8\n@\u00a3& ff <A\n\nFigure 5-17. Outputs from the PixelCNN using a mixture distribution output\n\nft\na\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 5.24,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_5/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_24/a677a8ca-f7a9-4698-b32f-0a42849ba750.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter we have seen how autoregressive models such as recurrent neural networks can be ap-\nplied to generate text sequences that mimic a particular style of writing, and also how a PixelCNN can\ngenerate images in a sequential fashion, one pixel at a time.\n\nWe explored two different types of recurrent layers\u2014long short-term memory (LSTM) and gated recur-\nrent unit (GRU)\u2014and saw how these cells can be stacked or made bidirectional to form more complex\nnetwork architectures. We built an LSTM to generate realistic recipes using Keras and saw how to manip-\nulate the temperature of the sampling process to increase or decrease the randomness of the output.\n\nWe also saw how images can be generated in an autoregressive manner, using a PixelCNN. We built a\nPixelCNN from scratch using Keras, coding the masked convolutional layers and residual blocks to allow\ninformation to flow through the network so that only preceding pixels could be used to generate the cur-\nrent pixel. Finally, we discussed how the TensorFlow Probability library provides a standalone\nPixelcnn function that implements a mixture distribution as the output layer, allowing us to further\nimprove the learning process.\n\nIn the next chapter we will explore another generative modeling family that explicitly models the data-\ngenerating distribution\u2014normalizing flow models.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 6,
                "chapter_name": "Chapter 6. Normalizing Flow Models",
                "chapter_path": "./screenshots-images-2/chapter_6",
                "sections": [
                    {
                        "section_id": 6.1,
                        "section_name": "Chapter 6. Normalizing Flow Models",
                        "section_path": "./screenshots-images-2/chapter_6/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_1/87b68169-8c33-4406-87f3-a328ac315907.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 6. Normalizing Flow Models\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\n\u00a2 Learn how normalizing flow models utilize the change of variables equation.\n\nSee how the Jacobian determinant plays a vital role in our ability to compute an explicit density\nfunction.\n\nUnderstand how we can restrict the form of the Jacobian using coupling layers.\n\nSee how the neural network is designed to be invertible.\n\n\u00a2 Build a RealNVP model\u2014a particular example of a normalizing flow to generate points in 2D.\n\n\u00a9 Use the RealNVP model to generate new points that appear to have been drawn from the data\ndistribution.\n\n\u00a2 Learn about two key extensions of the RealNVP model, GLOW and FFJORD.\n\nSo far, we have discussed three families of generative models: variational autoencoders, generative ad-\nversarial networks, and autoregressive models. Each presents a different way to address the challenge of\nmodeling the distribution p(z), either by introducing a latent variable that can be easily sampled (and\ntransformed using the decoder in VAEs or generator in GANs), or by tractably modeling the distribution\nas a function of the values of preceding elements (autoregressive models).\n\nIn this chapter, we will cover a new family of generative models\u2014normalizing flow models. As we shall\nsee, normalizing flows share similarities with both autoregressive models and variational autoencoders.\nLike autoregressive models, normalizing flows are able to explicitly and tractably model the data-gener-\nating distribution p(z). Like VAEs, normalizing flows attempt to map the data into a simpler distribution,\nsuch as a Gaussian distribution. The key difference is that normalizing flows place a constraint on the\nform of the mapping function, so that it is invertible and can therefore be used to generate new data\npoints.\n\nWe will dig into this definition in detail in the first section of this chapter before implementing a normal-\nizing flow model called RealNVP using Keras. We will also see how normalizing flows can be extended to\ncreate more powerful models, such as GLOW and FFJORD.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_6/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_2/c063a622-f8fa-441d-9833-4baca2e4844f.png",
                            "./screenshots-images-2/chapter_6/section_2/8decadf3-48c5-4494-b553-0a5e7f588886.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nWe will begin with a short story to illustrate the key concepts behind normalizing flows.\n\nJACOB AND THE F.L.O.W. MACHINE\n\nUpon visiting a small village, you notice a mysterious-looking shop with a sign above the door that says\nJACOB'S. Intrigued, you cautiously enter and ask the old man standing behind the counter what he sells\n\u00a2 ).\n\nFigure 6-1. Inside a steampunk shop, with a large metallic bell (created with )\n\nHe replies that he offers a service for digitizing paintings, with a difference. After a brief moment rum-\nmaging around the back of the shop, he brings out a silver box, embossed with the letters F.L.O.W. He\ntells you that this stands for Finding Likenesses Of Watercolors, which approximately describes what the\nmachine does. You decide to give the machine a try.\n\nYou come back the next day and hand the shopkeeper a set of your favorite paintings, and he passes them\nthrough the machine. The F.L.O.W. machine begins to hum and whistle and after a while outputs a set of\nnumbers that appear randomly generated. The shopkeeper hands you the list and begins to walk to the\ntill to calculate how much you owe him for the digitization process and the F.L.O.W. box. Distinctly unim-\npressed, you ask the shopkeeper what you should do with this long list of numbers, and how you can get\nyour favorite paintings back.\n\nThe shopkeeper rolls his eyes, as if the answer should be obvious. He walks back to the machine and\npasses in the long list of numbers, this time from the opposite side. You hear the machine whir again and\nwait, puzzled, until finally your original paintings drop out from where they entered.\n\nRelieved to finally have your paintings back, you decide that it might be best to just store them in the attic\ninstead. However, before you have a chance to leave, the shopkeeper ushers you across to a different cor-\nner of the shop, where a giant bell hangs from the rafters. He hits the bell curve with a huge stick, send-\ning vibrations around the store.\n\nInstantly, the F.L.O.W. machine under your arm begins to hiss and whirr in reverse, as if a new set of\nnumbers had just been passed in. After a few moments, more beautiful watercolor paintings begin to fall\nout of the F.L.O.W. machine, but they are not the same as the ones you originally digitized. They resemble\nthe style and form of your original set of paintings, but each one is completely unique!\n\nYou ask the shopkeeper how this incredible device works. He explains that the magic lies in the fact that\nhe has developed a special process that ensures the transformation is extremely fast and simple to calcu-\nlate while still being sophisticated enough to convert the vibrations produced by the bell into the complex\npatterns and shapes present in the paintings.\n\nRealizing the potential of this contraption, you hurriedly pay for the device and exit the store, happy that\nyou now have a way to generate new paintings in your favorite style, simply by visiting the shop, chiming\nthe bell, and waiting for your F.L.O.W. machine to work its magic!\n\nThe story of Jacob and the F.L.O.W. machine is a depiction of a normalizing flow model. Let\u2019s now explore\nthe theory of normalizing flows in more detail, before we implement a practical example using Keras.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.3,
                        "section_name": "Normalizing Flows",
                        "section_path": "./screenshots-images-2/chapter_6/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_3/75958a43-662e-4a7a-a0fe-d8503f9d0be5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Normalizing Flows\n\nThe motivation of normalizing flow models is similar to that of variational autoencoders, which we ex-\nplored in Chapter 3. To recap, in a variational autoencoder, we learn an encoder mapping function be-\ntween a complex distribution and a much simpler distribution that we can sample from. We then also\nlearn a decoder mapping function from the simpler distribution to the complex distribution, so that we\ncan generate a new data point by sampling a point z from the simpler distribution and applying the\nlearned transformation. Probabilistically speaking, the decoder models p(zx|z) but the encoder is only an\napproximation q(z|zx) of the true p(z|x)\u2014the encoder and decoder are two completely distinct neural\nnetworks.\n\nIna normalizing flow model, the decoding function is designed to be the exact inverse of the encoding\nfunction and quick to calculate, giving normalizing flows the property of tractability. However, neural\nnetworks are not by default invertible functions. This raises the question of how we can create an invert-\nible process that converts between a complex distribution (such as the data generation distribution of a\nset of watercolor paintings) and a much simpler distribution (such as a bell-shaped Gaussian distribu-\ntion) while still making use of the flexibility and power of deep learning.\n\nTo answer this question, we first need to understand a technique known as change of variables. For this\nsection, we will work with a simple example in just two dimensions, so that you can see exactly how nor-\nmalizing flows work in fine detail. More complex examples are just extensions of the basic techniques\npresented here.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.4,
                        "section_name": "Change of Variables",
                        "section_path": "./screenshots-images-2/chapter_6/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_4/157751dd-f2a4-403a-89a3-2977ba3aea84.png",
                            "./screenshots-images-2/chapter_6/section_4/7b07888f-e046-425a-a90c-d0402f47c9e6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Change of Variables\n\nSuppose we have a probability distribution px (x) defined over a rectangle X in two dimensions (\nBS (x1, x2)), as shown in Figure 6-2.\n\nX\n\n- Xa\np,X)=\n\nFigure 6-2. A probability distribution px (x) defined over two dimensions, shown in 2D (left) and 3D (right)\n\nThis function integrates to 1 over the domain of the distribution (i.e., x;in the range [1, 4] and 2 in the\nrange [0, 2]), so it represents a well-defined probability distribution. We can write this as follows:\n\n2 4\n[ [ px (x)dx;dz, =1\n0 1\n\nLet\u2019s say that we want to shift and scale this distribution so that it is instead defined over a unit square Z.\nWe can achieve this by defining a new variable z = (21, z2) and a function f that maps each point in X\nto exactly one point in Z as follows:\n\nz= f(x)\n\u2014 @-1\n\n3\n\n_ *\n\naa >\n\nNote that this function is invertible. That is, there is a function g that maps every z back to its correspond-\ning &. This is essential for a change of variables, as otherwise we cannot consistently map backward and\nforward between the two spaces. We can find g simply by rearranging the equations that define f, as\nshown in Figure 6-3.\n\n2\n\n20 5,\n\nX=g(Z)\n\n4% X=3Z41 ty) 1 Z\nX=22,\n\nFigure 6-3. Changing variables between X and Z\n\nWe now need to see how the change of variables from X to Z affects the probability distribution px (a).\nWe can do this by plugging the equations that define g into px (a) to transform it into a function pz (z)\nthat is defined in terms of z:\n\n(821 + 1) \u2014 1) (222)\n\npz(z) = 9\n\n22122\n3\n\nHowever, if we now integrate pz (z) over the unit square, we can see that we have a problem!\n\na 22122 1\ndzdzz = \u2014\n[ [ 3 6\n\nThe transformed function pz (z) is now no longer a valid probability distribution, because it only inte-\n\ngrates to 1/6. If we want to transform our complex probability distribution over the data into a simpler\ndistribution that we can sample from, we must ensure that it integrates to 1.\n\nThe missing factor of 6 is due to the fact that the domain of our transformed probability distribution is six\ntimes smaller than the original domain\u2014the original rectangle X had area 6, and this has been com-\npressed into a unit square Z that only has area 1. Therefore, we need to multiply the new probability dis-\ntribution by a normalization factor that is equal to the relative change in area (or volume in higher\ndimensions).\n\nLuckily, there is a way to calculate this volume change for a given transformation\u2014it is the absolute val-\nue of the Jacobian determinant of the transformation. Let\u2019s unpack that!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.5,
                        "section_name": "The Jacobian Determinant",
                        "section_path": "./screenshots-images-2/chapter_6/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_5/97096f47-1f27-48d9-9bd9-c641a4a39967.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Jacobian Determinant\n\nThe Jacobian of a function z = f(a) is the matrix of its first-order partial derivatives, as shown here:\n\nOn, \u201c2\nOx, Oey,\nOz\ndaa\nOZm . OZm\nOx, OLn,\n\nThe best way to explain this is with our example. If we take the partial derivative of z; with respect to 21,\nwe obtain i. If we take the partial derivative of z; with respect to x2, we obtain 0. Similarly, if we take\nthe partial derivative of z2 with respect to x1, we obtain 0. Lastly, if we take the partial derivative of zz\n\nwith respect to \u00a32, we obtain $.\n\nTherefore, the Jacobian matrix for our function f(z) is as follows:\n\n)\n\nThe determinant is only defined for square matrices and is equal to the signed volume of the paral-\n\nOo ol\nwhe Oo\n\nlelepiped created by applying the transformation represented by the matrix to the unit (hyper)cube. In\ntwo dimensions, this is therefore just the signed area of the parallelogram created by applying the trans-\n\nformation represented by the matrix to the unit square.\n\nThere is a general formula for calculating the determinant of a matrix with n dimensions, which runs in\n6 (n\u00b0) time. For our example, we only need the formula for two dimensions, which is simply as follows:\n\nact\u00ae \u00b0) <ad\u20140\net = ad \u2014 0c\ned\n\n1\n3\n\nfactor of 1/6 that we need to ensure that the probability distribution after transformation still integrates\ntol!\n\nTherefore, for our example, the determinant of the Jacobian is > x + -0x0= + This is the scaling\n\nTIP\nBy definition, the determinant is signed\u2014that is, it can be negative. Therefore we need to take the absolute value of\nthe Jacobian determinant in order to obtain the relative change of volume.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.6,
                        "section_name": "The Change of Variables Equation",
                        "section_path": "./screenshots-images-2/chapter_6/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_6/bbef3b68-dfcf-45dd-9876-944a01b205f4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Change of Variables Equation\n\nWe can now write down a single equation that describes the process for changing variables between X\nand Z. This is known as the change of variables equation (Equation 6-1).\n\nEquation 6-1. The change of variables equation\n\ndz\npx (2) = pa (2) det)\n\nHow does this help us build a generative model? The key is understanding that if pz (z) is a simple distri-\nbution from which we can easily sample (e.g., a Gaussian), then in theory, all we need to do is find an ap-\npropriate invertible function f(z) that can map from the data X into Z and the corresponding inverse\nfunction g(z) that can be used to map a sampled z back to a point a in the original domain. We can use\nthe preceding equation involving the Jacobian determinant to find an exact, tractable formula for the\ndata distribution p(a).\n\nHowever, there are two major issues when applying this in practice that we first need to address!\n\nFirstly, calculating the determinant of a high-dimensional matrix is computationally extremely expensive\n\u2014specifically, itis @ (n\u00b0). This is completely impractical to implement in practice, as even small 32 x 32-\npixel grayscale images have 1,024 dimensions.\n\nSecondly, it is not immediately obvious how we should go about calculating the invertible function f(x).\nWe could use a neural network to find some function f(x) but we cannot necessarily invert this network\n\u2014neural networks only work in one direction!\n\nTo solve these two problems, we need to use a special neural network architecture that ensures that the\nchange of variables function f is invertible and has a determinant that is easy to calculate.\n\nWe shall see how to do this in the following section using a technique called real-valued non-volume pre-\nserving (RealNVP) transformations.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.7,
                        "section_name": "RealNVP",
                        "section_path": "./screenshots-images-2/chapter_6/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_7/2072a08d-5c23-4c25-9470-085078856b45.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "RealNVP\n\n1\nRealNVP was first introduced by Dinh et al. in 2017. In this paper the authors show how to construct a\nneural network that can transform a complex data distribution into a simple Gaussian, while also pos-\nsessing the desired properties of being invertible and having a Jacobian that can be easily calculated.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/06_normflow/01_realnvp/real-\n\nnvp.ipynb in the book repository.\n\nThe code has been adapted from the excellent RealNVP tutorial created by Mandolini Giorgio Maria et al. available\non the Keras website.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.8,
                        "section_name": "The Two Moons Dataset",
                        "section_path": "./screenshots-images-2/chapter_6/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_8/a402c600-341b-478c-bbeb-9cdb09dc6b32.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Two Moons Dataset\n\nThe dataset we will use for this example is created by the make_moons function from the Python library\nsklearn . This creates a noisy dataset of points in 2D that resemble two crescents, as shown in Figure 6-\n4.\n\n15\n1.0\n0.5\n0.0\n-0.5\n-1.0\n\u201c15\n\n-2.0 bd\n20 15 40 -05 00 O05 10 15 20\n\nFigure 6-4. The two moons dataset in two dimensions\n\nThe code for creating this dataset is given in Example 6-1.\n\nExample 6-1. Creating a moons dataset\n\ndata = datasets.make_moons(3000, noise=0.05)[0].astype(\"float32\") @\nnorm = layers.Normalization()\nnorm. adapt (data)\n\nnormalized data = norm(data) @\n\n@ Make a noisy, unnormalized moons dataset of 3,000 points.\n@ Normalize the dataset to have mean 0 and standard deviation 1.\n\nWe will build a RealNVP model that can generate points in 2D that follow a similar distribution to the two\nmoons dataset. Whilst this is a very simple example, it will help us understand how a normalizing flow\nmodel works in practice, in fine detail.\n\nFirst, however, we need to introduce a new type of layer, called a coupling layer.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.9,
                        "section_name": "Coupling Layers",
                        "section_path": "./screenshots-images-2/chapter_6/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_9/0123b520-a6c6-4507-a66f-5fae124ad77a.png",
                            "./screenshots-images-2/chapter_6/section_9/1a250384-5e0f-4b84-bde4-4acb16c2bcef.png",
                            "./screenshots-images-2/chapter_6/section_9/63b084a4-506e-403a-bc89-ea7d0ecc0397.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Coupling Layers\n\nA coupling layer produces a scale and translation factor for each element of its input. In other words, it\nproduces two tensors that are exactly the same size as the input, one for the scale factor and one for the\ntranslation factor, as shown in Figure 6-5.\n\nCoupling layer\n\nFigure 6-5. A coupling layer outputs two tensors that are the same shape as the input: a scaling factor (s) and a translation factor (t)\n\nTo build a custom Coupling layer for our simple example, we can stack Dense layers to create the\nscale output and a different set of Dense layers to create the translation output, as shown in Example 6-\n2.\n\nTIP\n\nFor images, Coupling layer blocks use Conv2D layers instead of Dense layers.\n\nExample 6-2. A Coupling layer in Keras\n\ndef Coupling():\ninput_layer = layers.Input(shape=2) @\n\ns_layer_1 = layers.Dense(\n\n256, activation=\"relu\", kernel_regularizer=regularizers.12(0.01)\n)(input_layer) @\ns_layer_2 = layers.Dense(\n\n256, activation=\"relu\", kernel_regularizer=regularizers.12(0.01)\n) (s_layer_1)\ns_layer_3 = layers.Dense(\n\n256, activation=\"relu\", kernel_regularizer=regularizers.12(0.01)\n) (s_layer_2)\ns_layer_4 = layers.Dense(\n\n256, activation=\"relu\", kernel_regularizer=regularizers.12(0.01)\n) (s_layer_3)\ns_layer_5 = layers.Dense(\n\n2, activation=\"tanh\", kernel_regularizer=regularizers.12(0.01)\n)(s_layer_4) \u00ae\n\nt_layer_1 = layers .Dense(\n\n256, activation=\"relu\", kernel_regularizer=regularizers.12(0.01)\n)(input_layer) @\nt_layer_2 = layers.Dense(\n\n256, activation=\"relu\", kernel_regularizer=regularizers.12(0.01)\n) (t_layer_1)\nt_layer_3 = layers .Dense(\n\n256, activation=\"relu\", kernel_regularizer=regularizers.12(0.01)\n) (t_layer_2)\nt_layer_4 = layers.Dense(\n\n256, activation=\"relu\", kernel_regularizer=regularizers.12(0.01)\n) (t_layer_3)\nt_layer_5 = layers .Dense(\n\n2, activation=\"linear\", kernel_regularizer=regularizers.12(0.01)\n)(t_layer_4) \u00a9\n\nreturn models.Model(inputs=input_layer, outputs=[s_layer 5, t_layer_5]) @\n\n@ The input to the Coupling layer block in our example has two dimensions.\n\n\u00ae@ The scaling stream is a stack of Dense layers of size 256.\n\n\u00ae The final scaling layer is of size 2 and has tanh activation.\n\n\u00ae The translation stream is a stack of Dense layers of size 256.\n\n\u00a9 The final translation layer is of size 2 and has linear activation.\n\n@ The Coupling layer is constructed as a Keras Model with two outputs (the scaling and transla-\ntion factors).\n\nNotice how the number of channels is temporarily increased to allow for a more complex representation\nto be learned, before being collapsed back down to the same number of channels as the input. In the orig-\ninal paper, the authors also use regularizers on each layer to penalize large weights.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.1,
                        "section_name": "Passing data through a coupling layer",
                        "section_path": "./screenshots-images-2/chapter_6/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_10/da6b154d-2bf1-451c-a43c-2ccc7ca9b3a4.png",
                            "./screenshots-images-2/chapter_6/section_10/f0765931-516f-452f-ac72-052239fd4084.png",
                            "./screenshots-images-2/chapter_6/section_10/6b3f852b-aed3-4f7a-85d5-01601c54f502.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Passing data through a coupling layer\n\nThe architecture of a coupling layer is not particularly interesting\u2014what makes it unique is the way the\ninput data is masked and transformed as it is fed through the layer, as shown in Figure 6-6.\n\nFigure 6-6. The process of transforming the input \u00ab through a coupling layer\n\nNotice how only the first d dimensions of the data are fed through to the first coupling layer\u2014the remain-\ning D \u2014 d dimensions are completely masked (i.e., set to zero). In our simple example with D = 2, choos-\ning d = 1 means that instead of the coupling layer seeing two values, (1, \u00a32), the layer sees (x1, 0).\n\nThe outputs from the layer are the scale and translation factors. These are again masked, but this time\nwith the inverse mask to previously, so that only the second halves are let through\u2014i.e., in our example,\nwe obtain (0, s2) and (0, t2). These are then applied element-wise to the second half of the input x2 and\nthe first half of the input 7 is simply passed straight through, without being updated at all. In summary,\nfor a vector with dimension D where d < D, the update equations are as follows:\n\nZid = Tid\n\nZd+1:D Lay1p \u00a9 exp 8(21.4)) + t (ra)\n\nYou may be wondering why we go to the trouble of building a layer that masks so much information. The\nanswer is clear if we investigate the structure of the Jacobian matrix of this function:\n\na it 0\n\nOe \u2014 diag(exp [s (1.4)])\n\nThe top-left d x d submatrix is simply the identity matrix, because z.q = 21.4. These elements are\npassed straight through without being updated. The top-right submatrix is therefore 0, because 21.4 is not\ndependent on #441:p.\n\nThe bottom-left submatrix is complex, and we do not seek to simplify this. The bottom-right submatrix is\nsimply a diagonal matrix, filled with the elements of exp(s (w1.)), because 2q+1:p is linearly dependent\non %4+1:p and the gradient is dependent only on the scaling factor (not on the translation factor).\n\nFigure 6-7 shows a diagram of this matrix form, where only the nonzero elements are filled in with color.\n\nNotice how there are no nonzero elements above the diagonal\u2014for this reason, this matrix form is called\nlower triangular. Now we see the benefit of structuring the matrix in this way\u2014the determinant of a low-\ner-triangular matrix is just equal to the product of the diagonal elements. In other words, the determi-\nnant is not dependent on any of the complex derivatives in the bottom-left submatrix!\n\nX1:d Xd+1:D\nY A\n\n21:4 4\n\nZd+1:D 7\n\nFigure 6-7. The Jacobian matrix of the transformation\u2014a lower triangular matrix, with determinant equal to the product of the elements along\nthe diagonal\n\nTherefore, we can write the determinant of this matrix as follows:\n\ndet (J) = exp > s(e1.4);\ni\nThis is easily computable, which was one of the two original goals of building a normalizing flow model.\n\nThe other goal was that the function must be easily invertible. We can see that this is true as we can write\ndown the invertible function just by rearranging the forward equations, as follows:\n\nTd = 2d\n(Zay1:0 \u2014 t (@1:a)) \u00a9 exp \u2014 8(21:2))\n\nBq+1:D\n\nThe equivalent diagram is shown in Figure 6-8.\n\nx = g(z)\n\nXd+1:D\n\nFigure 6-8. The inverse function x = g(z)\n\nWe now have almost everything we need to build our RealNVP model. However, there is one issue that\nstill remains\u2014how should we update the first d elements of the input? Currently they are left completely\nunchanged by the model!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.11,
                        "section_name": "Stacking coupling layers",
                        "section_path": "./screenshots-images-2/chapter_6/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_11/c8b17035-3d4b-4723-af5d-6b6d68631a12.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stacking coupling layers\n\nTo resolve this problem, we can use a really simple trick. If we stack coupling layers on top of each other\nbut alternate the masking pattern, the layers that are left unchanged by one layer will be updated in the\nnext. This architecture has the added benefit of being able to learn more complex representations of the\ndata, as it is a deeper neural network.\n\nThe Jacobian of this composition of coupling layers will still be simple to compute, because linear algebra\ntells us that the determinant of a matrix product is the product of the determinants. Similarly, the inverse\nof the composition of two functions is just the composition of the inverses, as shown in the following\nequations:\n\ndet(A-B) =  det(A) det(B)\n(fo\u00b0 fa)? falofy*\n\nTherefore, if we stack coupling layers, flipping the masking each time, we can build a neural network\nthat is able to transform the whole input tensor, while retaining the essential properties of having a sim-\nple Jacobian determinant and being invertible. Figure 6-9 shows the overall structure.\n\nFigure 6-9. Stacking coupling layers, alternating the masking with each layer\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.12,
                        "section_name": "Training the RealNVP Model",
                        "section_path": "./screenshots-images-2/chapter_6/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_12/c89bf058-5acb-412e-886b-ea590f25b0a5.png",
                            "./screenshots-images-2/chapter_6/section_12/97c51706-efe5-4027-80a4-784b6dc54332.png",
                            "./screenshots-images-2/chapter_6/section_12/54b63218-e612-4d2b-94ae-1d11d4ceaf3d.png",
                            "./screenshots-images-2/chapter_6/section_12/19621f92-304d-488d-9282-a2dfbdba1119.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the RealNVP Model\n\nNow that we have built the RealNVP model, we can train it to learn the complex distribution of the two\nmoons dataset. Remember, we want to minimize the negative log-likelihood of the data under the model\n\u2014 log px (a). Using Equation 6-1, we can write this as follows:\n\na.\n\u2014 log px (x) = \u2014 log pz (z)\u2014 log det =)|\n\nWe choose the target output distribution pz (z) of the forward process f to be a standard Gaussian, be-\ncause we can easily sample from this distribution. We can then transform a point sampled from the\nGaussian back into the original image domain by applying the inverse process g, as shown in Figure 6-10.\n\nfi fr fs fs\n\n& a 82 L 8 a 84 G\nx~p,(x) z~p,(2)\n\n> +\n\nFigure 6-10. Transforming between the complex distribution px (x) and a simple Gaussian pz (2) in 1D (middle row) and 2D (bottom row)\n\nExample 6-3 shows how to build a RealNVP network, as a custom Keras Model .\n\nExample 6-3. Building the RealNVP model in Keras\n\nclass RealNVP(models.Model):\ndef init__(self, input_dim, coupling layers, coupling_dim, regularization):\nsuper(RealNVP, self). init ()\nself.coupling layers = coupling layers\nself.distribution = tfp.distributions.MultivariateNormalDiag(\nloc=[0.0, 0.0], scale _diag=[1.0, 1.0]\n)@o\nself.masks = np.array(\n{[0, 1], [1, 0]] * (coupling_layers // 2), dtype=\"float32\"\n)9@\nself.loss_tracker = metrics.Mean(name=\"loss\")\nself.layers_list = [\nCoupling(input_dim, coupling dim, regularization)\nfor i in range(coupling_layers)\n19\n\ndef metrics(self):\n\nreturn [self.loss_tracker]\n\ndef call(self, x, training=True):\n\nlog_det_inv = 0\n\ndirection = 1\n\nif training:\ndirection = -1\n\nfor i in range(self.coupling_layers)[::direction]: @\nx_masked = x * self.masks[i]\nreversed_mask = 1 - self.masks[i]\ns, t = self.layers_list[i](x_masked)\ns *= reversed_mask\n\nt *= reversed_mask\ngate = (direction - 1) / 2\nx = (\n\nreversed_mask\n* (x * t\u00a3.exp(direction * s) + direction * t * tf.exp(gate * s))\n+ x_masked\n) 6\nlog_det_inv += gate * tf.reduce_sum(s, axis = 1) \u00a9\nreturn x, log det_inv\n\ndef log_loss(self, x):\ny, logdet = self(x)\nlog_likelihood = self.distribution.log prob(y) + logdet @\nreturn -tf.reduce_mean(log_likelihood)\n\ndef train_step(self, data):\nwith tf.GradientTape() as tape:\nloss = self.log_loss(data)\ng = tape.gradient(loss, self.trainable variables)\nself.optimizer.apply gradients(zip(g, self.trainable variables) )\nself.loss_tracker.update_state(loss)\n\nreturn {\"loss\": self.loss_tracker.result()}\n\ndef test_step(self, data):\nloss = self.log_loss(data)\nself.loss_tracker.update_state(loss)\n\nreturn {\"loss\": self.loss_tracker.result()}\n\nmodel = RealNVP(\ninput_dim = 2\n, coupling layers= 6\n, coupling dim = 256\n, vegularization = 0.01\n\nmodel .compile(optimizer=optimizers.Adam(learning_ rate=0.0001))\n\nmodel .fit(\nnormalized_data\n, batch_size=256\n, epochs=300\n\nThe target distribution is a standard 2D Gaussian.\n\nHere, we create the alternating mask pattern.\n\nAlist of Coupling layers that define the RealNVP network.\n\nIn the main call function of the network, we loop over the Coupling layers. If\ntraining=True , then we move forward through the layers (i.e., from data to latent space). If\ntraining=False, then we move backward through the layers (i.e., from latent space to data).\n\nThis line describes both the forward and backward equations dependent on the direction (try\nplugging in direction = -1 and direction = 1 to prove this to yourself!).\n\nThe log determinant of the Jacobian, which we need to calculate the loss function, is simply the\nsum of the scaling factors.\n\nThe loss function is the negative sum of the log probability of the transformed data, under our tar-\nget Gaussian distribution and the log determinant of the Jacobian.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.13,
                        "section_name": "Analysis of the RealNVP Model",
                        "section_path": "./screenshots-images-2/chapter_6/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_13/80e60d30-6d13-41d4-9cbd-94aa751d72b2.png",
                            "./screenshots-images-2/chapter_6/section_13/81533edd-91bb-4150-aad7-ce37a479bef0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the RealNVP Model\n\nOnce the model is trained, we can use it to transform the training set into the latent space (using the for-\nward direction, f) and, more importantly, to transform a sampled point in the latent space into a point\nthat looks like it could have been sampled from the original data distribution (using the backward direc-\ntion, g).\n\nFigure 6-11 shows the output from the network before any learning has taken place\u2014the forward and\nbackward directions just pass information straight through with hardly any transformation.\n\nData space X f(x)\n\nx1\nLatent space Z\n\n\u2018 Gi\n\nGQ\n\nFigure 6-11. The RealNVP model inputs (left) and outputs (right) before training, for the forward process (top) and the reverse process (bottom)\n\nAfter training (Figure 6-12), the forward process is able to convert the points from the training set into a\ndistribution that resembles a Gaussian. Likewise, the backward process can take points sampled from a\nGaussian distribution and map them back to a distribution that resembles the original data.\n\nData space X\n\nFigure 6-12. The RealNVP model inputs (left) and outputs (right) after training, for the forward process (top) and the reverse process (bottom)\n\nThe loss curve for the training process is shown in Figure 6-13.\n\nepoch_loss\ntag: epoch_loss\n\n23\n\n21\n\n19\n\n17\n\n0 50 100 150 200 250 300\n\nco)\n\nFigure 6-13. The loss curve for the RealNVP training process\n\nThis completes our discussion of RealNVP, a specific case of a normalizing flow generative model. In the\nnext section, we\u2019ll cover some modern normalizing flow models that extend the ideas introduced in the\nRealNVP paper.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.14,
                        "section_name": "Other Normalizing Flow Models",
                        "section_path": "./screenshots-images-2/chapter_6/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_14/1fe6ebe4-f4be-4978-ba5c-f892204bda18.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Other Normalizing Flow Models\n\nTwo other successful and important normalizing flow models are GLOW and FFJORD. The following sec-\ntions describe the key advancements they made.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.15,
                        "section_name": "GLOW",
                        "section_path": "./screenshots-images-2/chapter_6/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_15/3c616b48-ab67-4db3-bec3-eb56fb9f7e51.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "GLOW\n\nPresented at NeurIPS 2018, GLOW was one of the first models to demonstrate the ability of normalizing\nflows to generate high-quality samples and produce a meaningful latent space that can be traversed to\nmanipulate samples. The key step was to replace the reverse masking setup with invertible 1 x 1 convolu-\ntional layers. For example, with RealNVP applied to images, the ordering of the channels is flipped after\neach step, to ensure that the network gets the chance to transform all of the input. In GLOW a 1 x 1 convo-\nlution is applied instead, which effectively acts as a general method to produce any permutation of the\nchannels that the model desires. The authors show that even with this addition, the distribution as a\nwhole remains tractable, with determinants and inverses that are easy to compute at scale.\n\nFigure 6-14. Random samples from the GLOW model (source: 7)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.16,
                        "section_name": "FFIORD",
                        "section_path": "./screenshots-images-2/chapter_6/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_16/1fd9a60d-e68b-4104-b1f8-bde4259f1e68.png",
                            "./screenshots-images-2/chapter_6/section_16/5aee40ea-2481-4513-8b94-6e2bde1aba77.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "FEJORD\n\nRealNVP and GLOW are discrete time normalizing flows\u2014that is, they transform the input through a dis-\ncrete set of coupling layers. FFJORD (Free-Form Continuous Dynamics for Scalable Reversible Generative\nModels), presented at ICLR 2019, shows how it is possible to model the transformation as a continuous\ntime process (i.e., by taking the limit as the number of steps in the flow tends to infinity and the step size\ntends to zero). In this case, the dynamics are modeled using an ordinary differential equation (ODE)\nwhose parameters are produced by a neural network ( fg). A black-box solver is used to solve the ODE at\ntime \u00a2,\u2014ie., to find z; given some initial point 29 sampled from a Gaussian at to, as described by the fol-\nlowing equations:\n\nzo ~ P20)\nPO = fy(e(,0)\n\nA diagram of the transformation process is shown in Figure 6-15.\n\nWa\n\np(Z(to))\n\nZ\n\nFigure 6-15. FFJORD models the transformation between the data distribution and a standard Gaussian vig an ordinary differential equation,\nparameterized by a neural network (source: Will Grathwohl et al, 2018)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 6.17,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_6/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_17/e5713cf5-20c7-4e2a-8abd-bd84c9b7c6fc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter we explored normalizing flow models such as RealNVP, GLOW, and FFJORD.\n\nA normalizing flow model is an invertible function defined by a neural network that allows us to directly\nmodel the data density via a change of variables. In the general case, the change of variables equation re-\nquires us to calculate a highly complex Jacobian determinant, which is impractical for all but the simplest\nof examples.\n\nTo sidestep this issue, the RealNVP model restricts the form of the neural network, such that it adheres to\nthe two essential criteria: it is invertible and has a Jacobian determinant that is easy to compute.\n\nIt does this through stacking coupling layers, which produce scale and translation factors at each step.\nImportantly, the coupling layer masks the data as it flows through the network, in a way that ensures that\nthe Jacobian is lower triangular and therefore has a simple-to-compute determinant. Full visibility of the\ninput data is achieved through flipping the masks at each layer.\n\nBy design, the scale and translation operations can be easily inverted, so that once the model is trained it\nis possible to run data through the network in reverse. This means that we can target the forward trans-\nformation process toward a standard Gaussian, which we can easily sample from. We can then run the\n\nsampled points backward through the network to generate new observations.\n\nThe RealNVP paper also shows how it is possible to apply this technique to images, by using convolutions\ninside the coupling layers, rather than densely connected layers. The GLOW paper extended this idea to\nremove the necessity for any hardcoded permutation of the masks. The FFJORD model introduced the\nconcept of continuous time normalizing flows, by modeling the transformation process as an ODE de-\nfined by a neural network.\n\nOverall, we have seen how normalizing flows are a powerful generative modeling family that can pro-\nduce high-quality samples, while maintaining the ability to tractably describe the data density function.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 7,
                "chapter_name": "Chapter 7. Energy-Based Models",
                "chapter_path": "./screenshots-images-2/chapter_7",
                "sections": [
                    {
                        "section_id": 7.1,
                        "section_name": "Chapter 7. Energy-Based Models",
                        "section_path": "./screenshots-images-2/chapter_7/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_1/14f6fcb2-1bf3-4d5f-aa4b-d80891b04239.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 7. Energy-Based Models\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\n\u00a2 Understand how to formulate a deep energy-based model (EBM).\n\n\u00a2 See how to sample from an EBM using Langevin dynamics.\n\n\u00a2 Train your own EBM using contrastive divergence.\n\ne Analyze the EBM, including viewing snapshots of the Langevin dynamics sampling process.\n\u00a2 Learn about other types of EBM, such as restricted Boltzmann machines.\n\nEnergy-based models are a broad class of generative model that borrow a key idea from modeling physi-\ncal systems\u2014namely, that the probability of an event can be expressed using a Boltzmann distribution, a\nspecific function that normalizes a real-valued energy function between 0 and 1. This distribution was\noriginally formulated in 1868 by Ludwig Boltzmann, who used it to describe gases in thermal\nequilibrium.\n\nIn this chapter, we will see how we can use this idea to train a generative model that can be used to pro-\nduce images of handwritten digits. We will explore several new concepts, including contrastive diver-\ngence for training the EBM and Langevin dynamics for sampling.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_7/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_2/a31491b2-5c65-43bc-82bc-2df1220ed8a9.png",
                            "./screenshots-images-2/chapter_7/section_2/82c44db2-4a17-497d-a566-9f99d5ae20cb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nWe will begin with a short story to illustrate the key concepts behind energy-based models.\n\nTHE LONG-AU-VIN RUNNING CLUB\n\nDiane Mixx was head coach of the long-distance running team in the fictional French town of Long-au-\nVin. She was well known for her exceptional abilities as a trainer and had acquired a reputation for be-\ning able to turn even the most mediocre of athletes into world-class runners (, ).\n\nFigure 7-1. A running coach training some elite athletes (created with 1)\n\nHer methods were based around assessing the energy levels of each athlete. Over years of working with\nathletes of all abilities, she had developed an incredibly accurate sense of just how much energy a partic-\nular athlete had left after a race, just by looking at them. The lower an athlete\u2019s energy level, the better\u2014\n\nelite athletes always gave everything they had during the race!\n\nTo keep her skills sharp, she regularly trained herself by measuring the contrast between her energy\nsensing abilities on known elite athletes and the best athletes from her club. She ensured that the diver-\ngence between her predictions for these two groups was as large as possible, so that people would take\nher seriously if she said that she had found a true elite athlete within her club.\n\nTo keep her skills sharp, she regularly trained herself by measuring the contrast between her energy\nsensing abilities on known elite athletes and the best athletes from her club. She ensured that the diver-\ngence between her predictions for these two groups was as large as possible, so that people would take\nher seriously if she said that she had found a true elite athlete within her club.\n\nThe real magic was her ability to convert a mediocre runner into a top-class runner. The process was sim-\nple\u2014she measured the current energy level of the athlete and worked out the optimal set of adjustments\nthe athlete needed to make to improve their performance next time. Then, after making these adjust-\nments, she measured the athlete\u2019s energy level again, looking for it to be slightly lower than before, ex-\nplaining the improved performance on the track. This process of assessing the optimal adjustments and\ntaking a small step in the right direction would continue until eventually the athlete was indistinguish-\nable from a world-class runner.\n\nAfter many years Diane retired from coaching and published a book on her methods for generating elite\nathletes\u2014a system she branded the \u201cLong-au-Vin, Diane Mixx\u201d technique.\n\nThe story of Diane Mixx and the Long-au-Vin running club captures the key ideas behind energy-based\nmodeling. Let\u2019s now explore the theory in more detail, before we implement a practical example using\nKeras.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.3,
                        "section_name": "Energy-Based Models",
                        "section_path": "./screenshots-images-2/chapter_7/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_3/275cb489-b204-4506-8887-011802656a64.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Energy-Based Models\n\nEnergy-based models attempt to model the true data-generating distribution using a Boltzmann distribu-\ntion (Equation 7-1) where E(z) is know as the energy function (or score) of an observation x.\n\nEquation 7-1. Boltzmann distribution\n\nIn practice, this amounts to training a neural network E(z) to output low scores for likely observations\n(so px is close to 1) and high scores for unlikely observations (so px is close to 0).\n\nThere are two challenges with modeling the data in this way. Firstly, it is not clear how we should use our\nmodel for sampling new observations\u2014we can use it to generate a score given an observation, but how\n\ndo we generate an observation that has a low score (i.e., a plausible observation)?\n\nSecondly, the normalizing denominator of Equation 7-1 contains an integral that is intractable for all but\nthe simplest of problems. If we cannot calculate this integral, then we cannot use maximum likelihood es-\ntimation to train the model, as this requires that px is a valid probability distribution.\n\nThe key idea behind an energy-based model is that we can use approximation techniques to ensure we\nnever need to calculate the intractable denominator. This is in contrast to, say, a normalizing flow, where\nwe go to great lengths to ensure that the transformations that we apply to our standard Gaussian distrib-\nution do not change the fact that the output is still a valid probability distribution.\n\nWe sidestep the tricky intractable denominator problem by using a technique called contrastive diver-\ngence (for training) and a technique called Langevin dynamics (for sampling), following the ideas from\nDu and Mordatch\u2019s 2019 paper \u201cImplicit Generation and Modeling with Energy-Based Models.\u201d* We shall\nexplore these techniques in detail while building our own EBM later in the chapter.\n\nFirst, let\u2019s get set up with a dataset and design a simple neural network that will represent our real-val-\nued energy function E(z).\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/07_ebm/01_ebm/ebm.ipynb in the\nbook repository.\n\nThe code is adapted from the excellent tutorial on deep energy-based generative models by Phillip Lippe.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.4,
                        "section_name": "The MNIST Dataset",
                        "section_path": "./screenshots-images-2/chapter_7/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_4/aa0228bf-e04b-4e07-8a6c-61cf3706bd8e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The MNIST Dataset\n\nWe'll be using the standard MNIST dataset, consisting of grayscale images of handwritten digits. Some ex-\n\nample images from the dataset are shown in Figure 7-2.\n\nThe dataset comes prepackaged with TensorFlow, so it can be downloaded as shown in Example 7-1.\n\nFigure 7-2. Examples of images from the MNIST dataset\n\nExample 7-1. Loading the MNIST dataset\n\nfrom tensorflow.keras import datasets\n(x_train, _), (x_test, _) = datasets.mnist.load_data()\n\nAs usual, we'll scale the pixel values to the range [-1, 1] and add some padding to make the images 32 x 32\npixels in size. We also convert it to a TensorFlow Dataset, as shown in Example 7-2.\n\nExample 7-2. Preprocessing the MNIST dataset\n\ndef preprocess(imgs):\nimgs = (imgs.astype(\"float32\") - 127.5) / 127.5\nimgs = np.pad(imgs , ((0,0), (2,2), (2,2)), constant_values= -1.0)\nimgs = np.expand_dims(imgs, -1)\nreturn imgs\n\nx_train = preprocess(x_train)\n\nx_test = preprocess(x_test)\n\nx_train = tf.data.Dataset.from_tensor_slices(x_train) .batch(128)\nx_test = tf.data.Dataset.from_tensor_slices(x_test).batch(128)\n\nNow that we have our dataset, we can build the neural network that will represent our energy function\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.5,
                        "section_name": "The Energy Function",
                        "section_path": "./screenshots-images-2/chapter_7/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_5/c7653d48-9fad-476d-a68d-b7ff19cd6f09.png",
                            "./screenshots-images-2/chapter_7/section_5/fdccbdd0-3dd5-4783-9007-7e0fc5675355.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Energy Function\n\nThe energy function Ey (x) is a neural network with parameters @ that can transform an input image x\ninto a scalar value. Throughout this network, we make use of an activation function called swish, as de-\nscribed in the following sidebar.\n\nSWISH ACTIVATION\n2\nSwish is an alternative to ReLU that was introduced by Google in 2017 and is defined as follows:\n\nx\n\nswish (x) = x - sigmoid (x) = =a\n\nSwish is visually similar to ReLU, with the key difference being that it is smooth, which helps to alleviate\nthe vanishing gradient problem. This is particularly important for energy-based models. A plot of the\nswish function is shown in Figure 7-3.\n\nswish(x) 4\n\nFigure 7-3. The swish activation function\n\nThe network is a set of stacked Conv2D layers that gradually reduce the size of the image while increas-\ning the number of channels. The final layer is a single fully connected unit with linear activation, so the\nnetwork can output values in the range (\u2014oo, oo). The code to build it is given in Example 7-3.\n\nExample 7-3. Building the energy function E(<) neural network\n\nExample 7-3. Building the energy function E(<) neural network\n\nebm_input = layers.Input(shape=(32, 32, 1))\n\nx = layers.Conv2D(\n\n16, kernel_size=5, strides=2, padding=\"same\", activation = activations.swish\n\n)(ebm_input) \u00ae\nx = layers.Conv2D(\n\n32, kernel_size=3, strides=2, padding=\"same\", activation = activations.swish\n\n)(x)\n\nx = layers.Conv2D(\n\n64, kernel_size=3, strides=2, padding=\"same\", activation = activations.swish\n\n)(x)\n\nx = layers.Conv2D(\n\n64, kernel_size=3, strides=2, padding=\"same\", activation = activations.swish\n\n)(x)\n\nx = layers.Flatten() (x)\n\nx = layers.Dense(64, activation\nebm_output = layers.Dense(1) (x)\n\nmodel = models.Model(ebm_input,\n\n= activations.swish) (x)\n2)\nebm_output) \u00a9\n\n@ The energy function is a set of stacked Conv2D layers, with swish activation.\n\n\u00ae The final layer is a single fully connected unit, with a linear activation function.\n\n\u00ae AKeras Model that converts the input image into a scalar energy value.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.6,
                        "section_name": "Sampling Using Langevin Dynamics",
                        "section_path": "./screenshots-images-2/chapter_7/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_6/32959da0-fd9a-404f-af5d-37abd8bc7b57.png",
                            "./screenshots-images-2/chapter_7/section_6/d30df9e8-228e-4242-adf5-d13e4df06e58.png",
                            "./screenshots-images-2/chapter_7/section_6/230a7875-63b8-48e1-89b0-be277a81b903.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Sampling Using Langevin Dynamics\n\nThe energy function only outputs a score for a given input\u2014how can we use this function to generate\nnew samples that have a low energy score?\n\nWe will use a technique called Langevin dynamics, which makes use of the fact that we can compute the\ngradient of the energy function with respect to its input. If we start from a random point in the sample\nspace and take small steps in the opposite direction of the calculated gradient, we will gradually reduce\nthe energy function. If our neural network is trained correctly, then the random noise should transform\ninto an image that resembles an observation from the training set before our eyes!\n\nSTOCHASTIC GRADIENT LANGEVIN DYNAMICS\n\nImportantly, we must also add a small amount of random noise to the input as we travel across the sample space;\notherwise, there is a risk of falling into local minima. The technique is therefore known as stochastic gradient\nLangevin dynamics.\n\nWe can visualize this gradient descent as shown in Figure 7-4, for a two-dimensional space with the ener-\ngy function value on the third dimension. The path is a noisy descent downhill, following the negative\ngradient of the energy function E(x) with respect to the input x. In the MNIST image dataset, we have\n1,024 pixels so are navigating a 1,024-dimensional space, but the same principles apply!\n\nFigure 7-4. Gradient descent using Langevin dynamics\n\nIt is worth noting the difference between this kind of gradient descent and the kind of gradient descent\nwe normally use to train a neural network.\n\nWhen training a neural network, we calculate the gradient of the loss function with respect to the para-\nmeters of the network (i.e., the weights) using backpropagation. Then we update the parameters a small\namount in the direction of the negative gradient, so that over many iterations, we gradually minimize the\nloss.\n\nWith Langevin dynamics, we keep the neural network weights fixed and calculate the gradient of the out-\nput with respect to the input. Then we update the input a small amount in the direction of the negative\ngradient, so that over many iterations, we gradually minimize the output (the energy score).\n\nBoth processes utilize the same idea (gradient descent), but are applied to different functions and with re-\nspect to different entities.\n\nFormally, Langevin dynamics can be described by the following equation:\n\na\u2019 =a*1 _w,Eg (c*) tw\n\nwhere w ~ -V(0,\u00a2) and ae ~ UAL. 7 is the step size hyperparameter that must be tuned\u2014too large\nand the steps jump over minima, too small and the algorithm will be too slow to converge.\n\nTIP\n\n\u00ab2\u00b0 ~ &Y-1,1) is the uniform distribution on the range [-1, 1].\n\nWe can code up our Langevin sampling function as illustrated in Example 7-4.\n\nExample 7-4. The Langevin sampling function\n\ndef generate _samples(model, inp _imgs, steps, step_size, noise):\nimgs_per_step = []\nfor _ in range(steps): @\ninp_imgs += tf.random.normal(inp_imgs.shape, mean = 0, stddev = noise) @\ninp_imgs = tf.clip_by value(inp_imgs, -1.0, 1.0)\nwith tf.GradientTape() as tape:\ntape.watch(inp_imgs)\nout_score = -model(inp_imgs) \u00a9\ngrads = tape.gradient(out_score, inp_imgs) \u00ae\ngrads = tf.clip by value(grads, -0.03, 0.03)\ninp_imgs += -step size * grads \u00a9\ninp_imgs = tf.clip_by value(inp_imgs, -1.0, 1.0)\nreturn inp_imgs\n\n@ Loop over given number of steps.\n\n@ Adda small amount of noise to the image.\n\n\u00ae Pass the image through the model to obtain the energy score.\n\n\u00ae Calculate the gradient of the output with respect to the input.\n\n\u00ae Adda small amount of the gradient to the input image.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.7,
                        "section_name": "Training with Contrastive Divergence",
                        "section_path": "./screenshots-images-2/chapter_7/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_7/14120591-14cb-4b2e-ae1e-b1ed1129e82e.png",
                            "./screenshots-images-2/chapter_7/section_7/793abd0f-774c-4061-92f3-2b7c188e11e6.png",
                            "./screenshots-images-2/chapter_7/section_7/24024413-0e24-4232-b625-b15c6a29a733.png",
                            "./screenshots-images-2/chapter_7/section_7/5bb8ebf3-f734-436f-9493-1180bc4a8339.png",
                            "./screenshots-images-2/chapter_7/section_7/9df2f3ee-9e34-4638-a015-e99834ab2a20.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training with Contrastive Divergence\n\nNow that we know how to sample a novel low-energy point from the sample space, let\u2019s turn our atten-\ntion to training the model.\n\nWe cannot apply maximum likelihood estimation, because the energy function does not output a proba-\nbility; it outputs a score that does not integrate to 1 across the sample space. Instead, we will apply a tech-\nnique first proposed in 2002 by Geoffrey Hinton, called contrastive divergence, for training unnormalized\nscoring models\n\nThe value that we want to minimize (as always) is the negative log-likelihood of the data:\n\nL = \u2014Ezndata log po (x)]\n\nWhen pg (x) has the form of a Boltzmann distribution, with energy function Eg (x), it can be shown that\n\nthe gradient of this value can be written as follows (Oliver Woodford\u2019s \u201cNotes on Contrastive Divergence\u201d\n5\n\nfor the full derivation):\n\nVo! = EBen~dataV oo (x)] \u2014 Ex~moaet \u00a5eEo (x)]\n\nThis intuitively makes a lot of sense\u2014we want to train the model to output large negative energy scores\nfor real observations and large positive energy scores for generated fake observations so that the contrast\nbetween these two extremes is as large as possible.\n\nIn other words, we can calculate the difference between the energy scores of real and fake samples and\nuse this as our loss function.\n\nTo calculate the energy scores of fake samples, we would need to be able to sample exactly from the dis-\ntribution pg (x), which isn\u2019t possible due to the intractable denominator. Instead, we can use our\nLangevin sampling procedure to generate a set of observations with low energy scores. The process\nwould need to run for infinitely many steps to produce a perfect sample (which is obviously impractical),\nso instead we run for some small number of steps, on the assumption that this is good enough to produce\na meaningful loss function.\n\nWe also maintain a buffer of samples from previous iterations, so that we can use this as the starting\npoint for the next batch, rather than pure random noise. The code to produce the sampling buffer is\n\nshown in Example 7-5.\n\nExample 7-5. The Buffer\n\nclass Buffer:\ndef _init_ (self, model):\nsuper().__init_ ()\nself.model = model\nself.examples = [\ntf.random.uniform(shape = (1, 32, 32, 1)) * 2-1\nfor _ in range(128)\n\n10\n\ndef sample _new_exmps(self, steps, step_size, noise):\nn_new = np.random.binomial(128, 0.05) @\nrand_imgs = (\ntf.random.uniform((n_new, 32, 32, 1)) * 2-1\n)\nold_imgs = tf.concat(\nrandom.choices(self.examples, k=128-n_new), axis=0\n)9\ninp_imgs = tf.concat([rand_imgs, old_imgs], axis=0)\ninp_imgs = generate_samples(\nself.model, inp_imgs, steps=steps, step_size=step size, noise = noise\n) 9\nself.examples = tf.split(inp_imgs, 128, axis = 0) + self.examples \u00a9\nself.examples = self.examples[:8192]\n\nreturn inp_imgs\n\n@ The sampling buffer is initialized with a batch of random noise.\n\n\u00ae On average, 5% of observations are generated from scratch (i.e., random noise) each time.\n\u00a9 The rest are pulled at random from the existing buffer.\n\n\u00a9 The observations are concatenated and run through the Langevin sampler.\n\n\u00a9 The resulting sample is added to the buffer, which is trimmed to a max length of 8,192\nobservations.\n\nFigure 7-5 shows one training step of contrastive divergence. The scores of real observations are pushed\ndown by the algorithm and the scores of fake observations are pulled up, without caring about normaliz-\n\ning these scores after each step.\n\nE(x)| E(x)\nFake observation\nContrastive\ndivergence step\nReal observation\nx x\n\nFigure 7-5. One step of contrastive divergence\n\nWe can code up the training step of the contrastive divergence algorithm within a custom Keras model as\nshown in Example 7-6.\n\nExample 7-6. EBM trained using contrastive divergence\n\nclass EBM(models.Model):\ndef init (self):\nsuper(EBM, self). init  ()\nself.model = model\nself.buffer = Buffer(self.model)\nself.alpha = 0.1\nself.loss_metric = metrics.Mean(name=\"loss\")\nself.reg_loss metric = metrics.Mean(name=\"reg\")\n\nself.cdiv_loss_metric = metrics.Mean(nam cdiv\")\n\nself.real_out_metric = metrics .Mean(nam\u00e9 real\")\n\nself.fake_out_metric = metrics.Mean(name=\"fake\")\n\n@property\ndef metrics(self):\nreturn [\n\nself.loss_metric,\nself.reg_ loss metric,\nself.cdiv_loss_ metric,\nself.real_out_metric,\nself.fake_out_metric\n\ndef train_step(self, real_imgs):\nreal_imgs += t\u00a3.random.normal(\nshape=tf.shape(real_imgs), mean = 0, stddev = 0.005\n)@o\nreal_imgs = tf.clip_by value(real_imgs, -1.0, 1.0)\nfake_imgs = self.buffer.sample_new_exmps(\nsteps=60, step size=10, noise = 0.005\n)9@\n\ninp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\nwith tf.GradientTape() as training_tape:\nreal_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0) \u00a9\ncdiv_loss = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(\nreal_out, axis = 0\n)9\nreg_loss = self.alpha * tf.reduce_mean(\nreal_out ** 2 + fake out ** 2, axis = 0\n) 6\nloss = reg loss + cdiv_loss\ngrads = training_tape.gradient(loss, self.model.trainable variables) @\nself.optimizer.apply_gradients(\nzip(grads, self.model.trainable_ variables)\n)\nself.loss_metric.update_state(loss)\nself.reg_ loss metric.update_state(reg_ loss)\nself.cdiv_loss_metric.update_state(cdiv_loss)\nself.real_out_metric.update_state(tf.reduce_mean(real_out, axis = 0))\n\n0))\n\nself.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis\nreturn {m.name: m.result() for m in self.metrics}\n\ndef test_step(self, real_imgs): @\nbatch_size = real_imgs.shape[0]\nfake_imgs = tf.random.uniform((batch_size, 32, 32, 1)) * 2-1\ninp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\nreal_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0)\ncdiv = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(\nreal_out, axis = 0\n)\nself.cdiv_loss_metric.update_state(cdiv)\nself.real_out_metric.update_state(tf.reduce_mean(real_out, axis = 0))\n\n0))\n\nself.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis\nreturn {m.name: m.result() for m in self.metrics[2:]}\n\nebm = EBM()\nebm.compile(optimizer=optimizers.Adam(learning rate=0.0001), run_eagerly=True)\nebm.fit(x_train, epochs=60, validation_data = x_test,)\n\nA small amount of random noise is added to the real images, to avoid the model overfitting to the\ntraining set.\n\nA set of fake images are sampled from the buffer.\n\nThe real and fake images are run through the model to produce real and fake scores.\n\nThe contrastive divergence loss is simply the difference between the scores of real and fake\nobservations.\n\nA regularization loss is added to avoid the scores becoming too large.\n\nGradients of the loss function with respect to the weights of the network are calculated for\nbackpropagation.\n\nThe test_step is used during validation and calculates the contrastive divergence between the\nscores of a set of random noise and data from the training set. It can be used as a measure for how\nwell the model is training (see the following section).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.8,
                        "section_name": "Analysis of the Energy-Based Model",
                        "section_path": "./screenshots-images-2/chapter_7/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_8/52a920d4-c887-4fd1-96f9-1990dca24eae.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the Energy-Based Model\n\nThe loss curves and supporting metrics from the training process are shown in Figure 7-6.\n\nepoch _loss epoch_cdiv epoch_reg\ntag: epoch_loss tag: epoch_cdiv tag: epoch_reg\n\n0 \u00a7 1015 0 5 ma was \u00a9 \u00a7 10 15 20 25 90 95 a0 45\n\nepoch_real epoch_fake\ntag: epoch_real tag: epoch_fake\n& Ologs/train\n& Ologs/validation\n\n0 5 0% ms ws OS \u00a9 5 10 15 2 25 9 35 \u00a9 45\n\nFigure 7-6. Loss curves and metrics for the training process of the EBM\n\nFirstly, notice that the loss calculated during the training step is approximately constant and small across\nepochs. While the model is constantly improving, so is the quality of generated images in the buffer that\nit is required to compare against real images from the training set, so we shouldn\u2019t expect the training\nloss to fall significantly.\n\nTherefore, to judge model performance, we also set up a validation process that doesn\u2019t sample from the\nbuffer, but instead scores a sample of random noise and compares this against the scores of examples\nfrom the training set. If the model is improving, we should see that the contrastive divergence falls over\nthe epochs (i.e., it is getting better at distinguishing random noise from real images), as can be seen in\n\nGenerating new samples from the EBM is simply a case of running the Langevin sampler for a large num-\nber of steps, from a standing start (random noise), as shown in Example 7-7. The observation is forced\ndownhill, following the gradients of the scoring function with respect to the input, so that out of the noise,\na plausible observation appears.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.9,
                        "section_name": "Other Energy-Based Models",
                        "section_path": "./screenshots-images-2/chapter_7/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_9/927f9150-bd39-400a-8a3c-358ca50fd6f2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Other Energy-Based Models\n\nIn the previous example we made use of a deep EBM trained using contrastive divergence with a\nLangevin dynamics sampler. However, early EBM models did not make use of Langevin sampling, but in-\nstead relied on other techniques and architectures.\n\n6\nOne of the earliest examples of an EBM was the Boltzmann machine. This is a fully connected, undirected\nneural network, where binary units are either visible (v) or hidden (h). The energy of a given configura-\ntion of the network is defined as follows:\n\nEq (v,h) = -Zu\"le +hTIh+v\u2122Wh)\n\nwhere W, L, J are the weights matrices that are learned by the model. Training is achieved by con-\ntrastive divergence, but using Gibbs sampling to alternate between the visible and hidden layers until an\nequilibrium is found. In practice this is very slow and not scalable to large numbers of hidden units.\n\nTIP\n\nSee Jessica Stringham\u2019s blog post \u201cGibbs Sampling in Python\u201d for an excellent simple example of Gibbs sampling.\n\nAn extension to this model, the restricted Boltzmann machine (RBM), removes the connections between\nunits of the same type, therefore creating a two-layer bipartite graph. This allows RBMs to be stacked into\ndeep belief networks to model more complex distributions. However, modeling high-dimensional data\nwith RBMs remains impractical, due to the fact that Gibbs sampling with long mixing times is still\nrequired.\n\nIt was only in the late 2000s that EBMs were shown to have potential for modeling more high-dimension-\nal datasets and a framework for building deep EBMs was established\u201d Langevin dynamics became the\npreferred sampling method for EBMs, which later evolved into a training technique known as score\nmatching. This further developed into a model class known as Denoising Diffusion Probabilistic Models,\nwhich power state-of-the-art generative models such as DALL.E 2 and ImageGen. We will explore diffu-\nsion models in more detail in Chapter 8.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 7.1,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_7/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_7/section_10/4f6a765c-19b0-4d75-8ab6-be20dbc0e378.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nEnergy-based models are a class of generative model that make use of an energy scoring function\u2014a\nneural network that is trained to output low scores for real observations and high scores for generated\nobservations. Calculating the probability distribution given by this score function would require normal-\nizing by an intractable denominator. EBMs avoid this problem by utilizing two tricks: contrastive diver-\ngence for training the network and Langevin dynamics for sampling new observations.\n\nThe energy function is trained by minimizing the difference between the generated sample scores and\nthe scores of the training data, a technique known as contrastive divergence. This can be shown to be\nequivalent to minimizing the negative log-likelihood, as required by maximum likelihood estimation, but\ndoes not require us to calculate the intractable normalizing denominator. In practice, we approximate\nthe sampling process for the fake samples to ensure the algorithm remains efficient.\n\nSampling of deep EBMs is achieved through Langevin dynamics, a technique that uses the gradient of the\nscore with respect to the input image to gradually transform random noise into a plausible observation\nby updating the input in small steps, following the gradient downhill. This improves upon earlier meth-\nods such as Gibbs sampling, which is utilized by restricted Boltzmann machines.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 8,
                "chapter_name": "Chapter 8. Diffusion Models",
                "chapter_path": "./screenshots-images-2/chapter_8",
                "sections": [
                    {
                        "section_id": 8.1,
                        "section_name": "Chapter 8. Diffusion Models",
                        "section_path": "./screenshots-images-2/chapter_8/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_1/ac3014e2-7456-447c-94e6-d66ded7c6fa1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "CHAPTER GOALS\n\nIn this chapter you will:\n\n\u00a2 Learn the underlying principles and components that define a diffusion model.\n\n\u00a9 See how the forward process is used to add noise to the training set of images.\n\n\u00a9 Understand the reparameterization trick and why it is important.\n\n\u00a2 Explore different forms of forward diffusion scheduling.\n\n\u00a9 Understand the reverse diffusion process and how it relates to the forward noising process.\n\n\u00a2 Explore the architecture of the U-Net, which is used to parameterize the reverse diffusion process.\n\ne Build your own denoising diffusion model (DDM) using Keras to generate images of flowers.\n\n\u00a2 Sample new images of flowers from your model.\n\n\u00a2 Explore the effect of the number of diffusion steps on image quality and interpolate between two im-\nages in the latent space.\n\nAlongside GANs, diffusion models are one of the most influential and impactful generative modeling\ntechniques for image generation to have been introduced over the last decade. Across many benchmarks,\ndiffusion models now outperform previously state-of-the-art GANs and are quickly becoming the go-to\nchoice for generative modeling practitioners, particularly for visual domains (e.g., OpenAl\u2019s DALL.E 2 and\nGoogle\u2019s ImageGen for text-to-image generation). Recently, there has been an explosion of diffusion mod-\nels being applied across wide range of tasks, reminiscent of the GAN proliferation that took place be-\ntween 2017-2020.\n\nMany of the core ideas that underpin diffusion models share similarities with earlier types of generative\nmodels that we have already explored in this book (e.g., denoising autoencoders, energy-based models).\nIndeed, the name diffusion takes inspiration from the well-studied property of thermodynamic diffusion:\nan important link was made between this purely physical field and deep learning in 2015.\n\nImportant progress was also being made in the field of score-based generative models,\u201d a branch of en-\nergy-based modeling that directly estimates the gradient of the log distribution (also known as the score\nfunction) in order to train the model, as an alternative to using contrastive divergence. In particular,\nYang Song and Stefano Ermon used multiple scales of noise perturbations applied to the raw data to en-\nsure the model\u2014a noise conditional score network (NCSN)\u2014performs well on regions of low data density.\n\nThe breakthrough diffusion model paper came in the summer of 2020. Standing on the shoulders of ear-\nlier works, the paper uncovers a deep connection between diffusion models and score-based generative\nmodels, and the authors use this fact to train a diffusion model that can rival GANs across several\ndatasets, called the Denoising Diffusion Probabilistic Model (DDPM).\n\nThis chapter will walk through the theoretical requirements for understanding how a denoising diffusion\nmodel works. You will then learn how to build your own denoising diffusion model using Keras.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_8/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_2/bdb51839-8b09-4f48-ac79-6a94bc294fa6.png",
                            "./screenshots-images-2/chapter_8/section_2/866691cd-030d-421d-be3c-1c0e7afadf67.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nTo help explain the key ideas that underpin diffusion models, let\u2019s begin with a short story!\n\nDIFFUSETV\n\nYou are standing in an electronics store that sells television sets. However, this store is clearly very differ-\nent from ones you have visited in the past. Instead of a wide variety of different brands, there are hun-\ndreds of identical copies of the same TV connected together in sequence, stretching into the back of the\nshop as far as you can see. What\u2019s more, the first few TV sets appear to be showing nothing but random\nstatic noise (Figure 8-1).\n\nThe shopkeeper comes over to ask if you need assistance. Confused, you ask her about the odd setup. She\nexplains that this is the new DiffuseTV model that is set to revolutionize the entertainment industry and\nimmediately starts telling you how it works, while walking deeper into the shop, alongside the line of TVs.\n\nFigure 8-1. A long line of connected television sets stretching out along an aisle of a shop (created with Midjourney)\n\nShe explains that during the manufacturing process, the DiffuseTV is exposed to thousands of images of\nprevious TV shows\u2014but each of those images has been gradually corrupted with random static, until it is\nindistinguishable from pure random noise. The TVs are then designed to undo the random noise, in small\nsteps, essentially trying to predict what the images looked like before the noise was added. You can see\nthat as you walk further into the shop the images on each television set are indeed slightly clearer than\nthe last.\n\nYou eventually reach the end of the long line of televisions, where you can see a perfect picture on the\nlast set. While this is certainly clever technology, you are curious to understand how this is useful to the\nviewer. The shopkeeper continues with her explanation.\n\nInstead of choosing a channel to watch, the viewer chooses a random initial configuration of static. Every\nconfiguration will lead to a different output image, and in some models can even be guided by a text\nprompt that you choose to input. Unlike a normal TV, with a limited range of channels to watch, the\nDiffuseTV gives the viewer unlimited choice and freedom to generate whatever they would like to appear\non the screen!\n\nYou purchase a DiffuseTV right away and are relieved to hear that the long line of TVs in the shop is for\ndemonstration purposes only, so you won\u2019t have to also buy a warehouse to store your new device!\n\nThe DiffuseTV story describes the general idea behind a diffusion model. Now let\u2019s dive into the technical-\nities of how we build such a model using Keras.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.3,
                        "section_name": "Denoising Diffusion Models (DDM)",
                        "section_path": "./screenshots-images-2/chapter_8/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_3/72f39ddf-8eb5-4ef1-b065-8caedf9a858f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Denoising Diffusion Models (DDM)\n\nThe core idea behind a denoising diffusion model is simple\u2014we train a deep learning model to denoise\nan image over a series of very small steps. If we start from pure random noise, in theory we should be\nable to keep applying the model until we obtain an image that looks as if it were drawn from the training\n\nset. What\u2019s amazing is that this simple concept works so well in practice!\n\nLet\u2019s first get set up with a dataset and then walk through the forward (noising) and backward (denois-\n\ning) diffusion processes.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/08_diffusion/01_ddm/ddm.ipynb\nin the book repository.\n\nThe code is adapted from the excellent tutorial on denoising diffusion implicit models created by Andras B\u00e9res avail-\nable on the Keras website.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.4,
                        "section_name": "The Flowers Dataset",
                        "section_path": "./screenshots-images-2/chapter_8/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_4/d05c88c9-2607-4044-8abc-9a9c984ef996.png",
                            "./screenshots-images-2/chapter_8/section_4/4c6392b6-87ca-4694-9845-450521887e6a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Flowers Dataset\n\nWe'll be using the Oxford 102 Flower dataset that is available through Kaggle. This is a set of over 8,000\ncolor images of a variety of flowers.\n\nYou can download the dataset by running the Kaggle dataset downloader script in the book repository, as\nshown in Example 8-1. This will save the flower images to the /data folder.\n\nExample 8-1. Downloading the Oxford 102 Flower dataset\n\nbash scripts/download_kaggle_data.sh nunenuh pytorch-challange-flower-dataset\n\nAs usual, we'll load the images in using the Keras image_dataset_from_directory function, resize the\nimages to 64 = 64 pixels, and scale the pixel values to the range [0, 1]. We'll also repeat the dataset five\ntimes to increase the epoch length and batch the data into groups of 64 images, as shown in Example 8-2.\n\nExample 8-2. Loading the Oxford 102 Flower dataset\n\ntrain_data = utils.image_dataset_from_directory/(\n\"/app/data/pytorch-challange-flower-dataset/dataset\",\nlabels=None,\nimage_size=(64, 64),\nbatch_size=None,\nshuffle=True,\nseed=42,\ninterpolation=\"bilinear\",\n\n)9\n\ndef preprocess(img):\nimg = tf.cast(img, \"float32\") / 255.0\n\nreturn img\n\ntrain = train_data.map(lambda x: preprocess(x)) @\ntrain = train.repeat(5) \u00ae\n\ntrain = train.batch(64, drop _remainder=True) @\n\n@ Load dataset (when required during training) using the Keras image_dataset_from_directory\n\nfunction.\n\n\u00ae@ Scale the pixel values to the range [0, 1].\n\n\u00ae Repeat the dataset five times.\n\n\u00a9 Batch the dataset into groups of 64 images.\n\nExample images from the dataset are shown in Figure 8-2.\n\n\u00a9\n\nra\n\nway\n\nFigure 8-2. Example images from the Oxford 102 Flower dataset\n\nNow that we have our dataset we can explore how we should add noise to the images, using a forward\ndiffusion process.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.5,
                        "section_name": "The Forward Diffusion Process",
                        "section_path": "./screenshots-images-2/chapter_8/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_5/30a61573-21f0-4382-8cef-8d8b43a02420.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Forward Diffusion Process\n\nSuppose we have an image Xo that we want to corrupt gradually over a large number of steps (say,\nT = 1,000), so that eventually it is indistinguishable from standard Gaussian noise (ie., x7 should have\nzero mean and unit variance). How should we go about doing this?\n\nWe can define a function g that adds a small amount of Gaussian noise with variance (; to an image X;\u20141\n\nto generate a new image x;. If we keep applying this function, we will generate a sequence of progres-\nsively noisier images (x9, ..., x7), as shown in Figure 8-3.\n\nf.3- fa = ~\nxo *td\n\nFigure 8-3. The forward diffusion process q\n\nxt\n\nWe can write this update process mathematically as follows (here, \u20ac;_; is a standard Gaussian with zero\nmean and unit variance):\n\nx= V1\u2014 Bix + V Bea\n\nNote that we also scale the input image x;_1, to ensure that the variance of the output image x; remains\nconstant over time. This way, if we normalize our original image xy to have zero mean and unit variance,\nthen xr will approximate a standard Gaussian distribution for large enough T, by induction, as follows.\n\nIf we assume that X;_; has zero mean and unit variance then J1\u2014 Bxi1 will have variance 1 \u2014 6;\nand y/B;\u20ac;_1 will have variance (;, using the rule that Var (aX) = a?Var (X). Adding these together,\nwe obtain a new distribution x; with zero mean and variance 1 \u2014 8; + 8; = 1, using the rule that\nVar(X + Y) = Var(X) + Var(Y) for independent X and Y. Therefore, if xo is normalized to a zero\nmean and unit variance, then we guarantee that this is also true for all x;, including the final image x7,\nwhich will approximate a standard Gaussian distribution. This is exactly what we need, as we want to be\nable to easily sample x7 and then apply a reverse diffusion process through our trained neural network\nmodel!\n\nIn other words, our forward noising process q can also be written as follows:\n\n9 (x2|X1-1) = Y (x: vi1- Bixt1, BL)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.6,
                        "section_name": "The Reparameterization Trick",
                        "section_path": "./screenshots-images-2/chapter_8/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_6/eb93d3aa-9465-46d4-9b6f-6ff83212c46e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Reparameterization Trick\n\nIt would also be useful to be able to jump straight from an image x9 to any noised version of the image x;\nwithout having to go through \u00a2 applications of g. Luckily, there is a reparameterization trick that we can\nuse to do this.\n\nIf we define a; = 1 \u2014 6; and a = [];_, aj, then we can write the following:\n\nx = JeixiitJ/1\u2014 ae\n= Vardy 1X12 + V1 \u2014 aray_i\u20ac\n\n= Vaixy + V1 \u2014 aye\n\nNote that the second line uses the fact that we can add two Gaussians to obtain a new Gaussian. We there-\nfore have a way to jump from the original image Xo to any step of the forward diffusion process x;.\nMoreover, we can define the diffusion schedule using the @; values, instead of the original /; values,\nwith the interpretation that @; is the variance due to the signal (the original image, x9) and 1 \u2014 @; is the\nvariance due to the noise (\u20ac).\n\nThe forward diffusion process q can therefore also be written as follows:\n\na (0) = 4 (x05 Veo, (1 \u2014 JE)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.7,
                        "section_name": "Diffusion Schedules",
                        "section_path": "./screenshots-images-2/chapter_8/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_7/4c101a83-f94d-47b8-90e6-f7dc80f6ba8e.png",
                            "./screenshots-images-2/chapter_8/section_7/2273125f-cca5-4bb4-bf6a-559bef2c06c8.png",
                            "./screenshots-images-2/chapter_8/section_7/0009dbbe-80dc-45b4-8d04-594d986e8b94.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Diffusion Schedules\n\nNotice that we are also free to choose a different (; at each timestep\u2014they don\u2019t all have be the same.\nHow the /; (or @;) values change with t is called the diffusion schedule.\n\nIn the original paper (Ho et al., 2020), the authors chose a linear diffusion schedule for B,\u2014that is, 8; in-\ncreases linearly with \u00a2, from $8; = 0.0001 to Br = 0.02. This ensures that in the early stages of the noising\nprocess we take smaller noising steps than in the later stages, when the image is already very noisy.\n\nWe can code up a linear diffusion schedule as shown in Example 8-3.\n\nExample 8-3. The linear diffusion schedule\n\ndef linear_diffusion_schedule(diffusion_times):\nmin_rate = 0.0001\nmax_rate = 0.02\nbetas = min_rate + tf.convert_to_tensor(diffusion_ times) * (max_rate - min_rai\nalphas = 1 - betas\nalpha_bars = tf.math.cumprod(alphas)\nsignal_rates = alpha_bars\nnoise_rates = 1 - alpha_bars\nreturn noise rates, signal_rates\n\nT = 1000\n\ndiffusion_times = [x/T for x in range(T)] @\n\nlinear_noise rates, linear_signal_rates = linear_diffusion_schedule(\ndiffusion_times\n\n)\n\n@ The diffusion times are equally spaced steps between 0 and 1.\n\u00ae@ The linear diffusion schedule is applied to the diffusion times to produce the noise and signal rates.\n\nIna later paper it was found that a cosine diffusion schedule outperformed the linear schedule from the\n5\noriginal paper. A cosine schedule defines the following values of Q;:\n\n@ =cos\u201d tn\nOO AT 2\n\nThe updated equation is therefore as follows (using the trigonometric identity cos\u201d (a)+ sin? (x) = 1):\n\ntom + si t o@\nx; =cos ( \u2014- \u2014 s oes\n2 =c0s | +> |xot sin | 7\u00b0 > Je\n\nThis equation is a simplified version of the actual cosine diffusion schedule used in the paper. The au-\nthors also add an offset term and scaling to prevent the noising steps from being too small at the begin-\nning of the diffusion process. We can code up the cosine and offset cosine diffusion schedules as shown in\nExample 8-4.\n\nExample 8-4. The cosine and offset cosine diffusion schedules\n\ndef cosine _diffusion_schedule(diffusion_times): \u00ae\nsignal_rates = tf.cos(diffusion_times * math.pi / 2)\nnoise rates = tf.sin(diffusion_times * math.pi / 2)\nreturn noise rates, signal_rates\n\ndef offset_cosine_diffusion_schedule(diffusion_times): @\nmin_signal_rate = 0.02\nmax_signal_rate = 0.95\nstart_angle = tf.acos(max_signal_rate)\nend_angle = tf.acos(min_signal_rate)\n\ndiffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n\nsignal_rates = tf.cos(diffusion_angles)\nnoise rates = tf.sin(diffusion_angles)\n\nreturn noise rates, signal_rates\n\n@ The pure cosine diffusion schedule (without offset or rescaling).\n\n\u00ae The offset cosine diffusion schedule that we will be using, which adjusts the schedule to ensure the\nnoising steps are not too small at the start of the noising process.\n\nWe can compute the @; values for each t to show how much signal (@;) and noise (1 \u2014 @;) is let through\n\nat each stage of the process for the linear, cosine, and offset cosine diffusion schedules, as shown in\nFigure 8-4.\n\nSignal Noise\n10 1.0 \u2014linear\n\u2014cosine\n\n08. 0.81 \u2014offset.cosine\nF 06 06\n3 4\n04 'Fo4\n\n02 02\n\n00. 00.\n\n00 O02 03 06 O08 10 00 02 03 06 O8 10\n\nvT tT\n\nFigure 8-4, The signal and noise at each step of the noising process, for the linear, cosine, and offset cosine diffusion schedules\n\nNotice how the noise level ramps up more slowly in the cosine diffusion schedule. A cosine diffusion\nschedule adds noise to the image more gradually than a linear diffusion schedule, which improves train-\ning efficiency and generation quality. This can also be seen in images that have been corrupted by the lin-\near and cosine schedules (Figure 8-5).\n\nlinear 3 \u00a3E \u00a3 eC\ncosine 3 E \u00a3 \u00a3\n\nFigure 8-5. An image being corrupted by the linear (top) and cosine (bottom) diffusion schedules, at equally spaced values of t from 0 to T\n(source: Ho et al., 2020)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.8,
                        "section_name": "The Reverse Diffusion Process",
                        "section_path": "./screenshots-images-2/chapter_8/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_8/e1ff1343-4a4e-4a80-a3af-a761406a420e.png",
                            "./screenshots-images-2/chapter_8/section_8/0ffb76f7-fb7f-43be-8ef7-89c849cc50ba.png",
                            "./screenshots-images-2/chapter_8/section_8/526ebb4f-d591-49d4-b514-e2d432876f87.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Reverse Diffusion Process\n\nNow let\u2019s look at the reverse diffusion process. To recap, we are looking to build a neural network\n\nPo (X+-1|X,) that can undo the noising process\u2014that is, approximate the reverse distribution g(x;\u20141|x:).\nIf we can do this, we can sample random noise from -# (0, I) and then apply the reverse diffusion\nprocess multiple times in order to generate a novel image. This is visualized in Figure 8-6.\n\nxo xt *T\n\nFigure 8-6. The reverse diffusion process po. (X\u00bb-1|Xt) tries to undo the noise produced by the forward diffusion process\n\nThere are many similarities between the reverse diffusion process and the decoder of a variational au-\ntoencoder. In both, we aim to transform random noise into meaningful output using a neural network.\nThe difference between diffusion models and VAEs is that in a VAE the forward process (converting im-\nages to noise) is part of the model (i.e., it is learned), whereas in a diffusion model it is unparameterized.\n\nTherefore, it makes sense to apply the same loss function as in a variational autoencoder. The original\nDDPM paper derives the exact form of this loss function and shows that it can be optimized by training a\nnetwork \u20acg to predict the noise \u20ac that has been added to a given image xg at timestep t.\n\nIn other words, we sample an image x9 and transform it by \u00a2 noising steps to get the image\n\nx, = Xo + V1 \u2014 aye. We give this new image and the noising rate @ to the neural network and ask\nit to predict e, taking a gradient step against the squared error between the prediction \u20acg (x;) and the\n\ntrue \u20ac.\n\nWe'll take a look at the structure of the neural network in the next section. It is worth noting here that the\ndiffusion model actually maintains two copies of the network: one that is actively trained used gradient\ndescent and another (the EMA network) that is an exponential moving average (EMA) of the weights of\nthe actively trained network over previous training steps. The EMA network is not as susceptible to short-\nterm fluctuations and spikes in the training process, making it more robust for generation than the ac-\ntively trained network. We therefore use the EMA network whenever we want to produce generated out-\nput from the network.\n\nThe training process for the model is shown in Figure 8-7.\n\nAlgorithm 1 Training\n\n: repeat\nxo ~ q(Xo)\nt ~ Uniform({1,...,T})\ne~ N(0,1)\n: Take gradient descent step on\nVo |le \u2014 \u20aco(Vaexo + VI \u2014 axe, t)||\u201d\n6: until converged\n\nUbwWNe\n\nFigure 8-7. The training process for a denoising diffusion model (source: Ho et al, 2020)\n\nIn Keras, we can code up this training step as illustrated in Example 8-5.\n\nExample 8-5. The train_step function of the Keras diffusion model\n\nclass DiffusionModel (models .Model) :\ndef init (self):\nsuper().__init_ ()\nself.normalizer = layers.Normalization()\nself.network = unet\nself.ema_network = models.clone_model(self.network)\nself.diffusion_schedule = cosine _diffusion_schedule\n\ndef denoise(self, noisy_images, noise_rates, signal_rates, training):\nif training:\nnetwork = self.network\nelse:\nnetwork = self.ema_network\npred_noises = network(\n[noisy_images, noise_rates**2], training=training\n)\n\npred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\nreturn pred_noises, pred_images\n\ndef train_step(self, images):\nimages = self.normalizer(images, training=True) @\nnoises = tf.random.normal(shape=tf.shape(images)) @\nbatch_size = tf.shape(images) [0]\ndiffusion_times = tf.random.uniform(\nshape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n\n)\n\nnoisy images = signal_rates * images + noise rates * noises \u00a9\nwith tf.GradientTape() as tape:\n\npred_noises, pred_images = self.denoise(\n\nnoisy images, noise_rates, signal_rates, training=True\n\n) 9\n\nnoise_loss = self.loss(noises, pred_noises) @\ngradients = tape.gradient(noise_loss, self.network.trainable weights)\nself.optimizer.apply gradients(\n\nzip(gradients, self.network.trainable weights)\n) 9\n\nself.noise_loss_tracker.update_state(noise_loss)\n\nfor weight, ema_weight in zip(\nself.network.weights, self.ema_network.weights\n\nema_weight.assign(0.999 * ema_weight + (1 - 0.999) * weight) \u00a9\n\nreturn {m.name: m.result() for m in self.metrics}\n\nWe first normalize the batch of images to have zero mean and unit variance.\n\nNext, we sample noise to match the shape of the input images.\n\nWe also sample random diffusion times...\n\n..and use these to generate the noise and signal rates according to the cosine diffusion schedule.\n\nThen we apply the signal and noise weightings to the input images to generate the noisy images.\n\nNext, we denoise the noisy images by asking the network to predict the noise and then undoing the\nnoising operation, using the provided noise_rates and signal_rates.\n\nWe can then calculate the loss (mean absolute error) between the predicted noise and the true\n\nnoise...\n\n..and take a gradient step against this loss function.\n\nThe EMA network weights are updated to a weighted average of the existing EMA weights and the\ntrained network weights after the gradient step.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.9,
                        "section_name": "The U-Net Denoising Model",
                        "section_path": "./screenshots-images-2/chapter_8/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_9/e99c1ee9-197b-40e6-a8b8-bdbc9a6bd4b5.png",
                            "./screenshots-images-2/chapter_8/section_9/c90d2743-f4a3-40d9-a2ad-b3f1b24481f3.png",
                            "./screenshots-images-2/chapter_8/section_9/c33aa6f8-ee65-4278-b7e1-e39e6b8a3588.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The U-Net Denoising Model\n\nNow that we have seen the kind of neural network that we need to build (one that predicts the noise\nadded to a given image), we can look at the architecture that makes this possible.\n\nThe authors of the DDPM paper used a type of architecture known as a U-Net. A diagram of this network\nis shown in Figure 8-8, explicitly showing the shape of the tensor as it passes through the network.\n\nNoise variance Noisy image\n\n64x 64\u00ab3\n\n1x1x1 0.34\n\n64x64 x32\n64x 64 x32 \u2014\u2014> 64 x 64x64\n64x64 x32\n\nDownBlock 64x 64x32 \u2014\u2014> 6464 \u00ab96 UpBlock\n323232 64x 64x64\n32 32x64\n32x 32x64. \u2014\u2014> 32x32 128\n32 32x64\n\nDownBlock | 5) 3x64 \u2014\u2014\u00bb32x32~160|  UPBlock\n16 \u00ab16 x 64 32*32\u00ab96\n\n16 \u00ab16 x96\n16 \u00ab16x96 \u2014\u2014>16~16 \u00ab192\n16 x16 x96\n16 x16 x96 \u2014\u2014+ 16 x 16 x 224\n8x8x96 16 x 16 x 128\n\nPredicted noise\n1x1x32\n\n64 x 64 x 32 64 x 64 x 32\n\nUpBlock\n\nOG) ee\n\nResidualBlock ResidualBlock\n8x8x128 8x8x 128\n\nFigure 8-8. U-Net architecture diagram\n\nIna similar manner to a variational autoencoder, a U-Net consists of two halves: the downsampling half,\nwhere input images are compressed spatially but expanded channel-wise, and the upsampling half,\nwhere representations are expanded spatially while the number of channels is reduced. However, unlike\nina VAE, there are also skip connections between equivalent spatially shaped layers in the upsampling\nand downsampling parts of the network. A VAE is sequential; data flows through the network from input\nto output, one layer after another. A U-Net is different, because the skip connections allow information to\nshortcut parts of the network and flow through to later layers.\n\nA U-Net is particularly useful when we want the output to have the same shape as the input. In our diffu-\nsion model example, we want to predict the noise added to an image, which has exactly the same shape\nas the image itself, so a U-Net is the natural choice for the network architecture.\n\nFirst let\u2019s take a look at the code that builds this U-Net in Keras, shown in Example 8-6.\n\nExample 8-6. A U-Net model in Keras\n\nnoisy _images = layers.Input(shape=(64, 64, 3)) @\n\nx = layers.Conv2D(32, kernel_size=1)(noisy_images) @\n\nnoise_variances = layers.Input(shape=(1, 1, 1)) \u00ae\n\nnoise_embedding = layers.Lambda(sinusoidal_embedding) (noise_variances) \u00ae\n\nnoise_embedding = layers.UpSampling2D(size=64, interpolation=\"nearest\" ) (\nnoise_embedding\n\n)\u00ae6\n\nx = layers.Concatenate()([x, noise_embedding]) \u00a9\nskips = [] @\n\nx = DownBlock(32, block_depth = 2)([x, skips]) \u00ae\nx = DownBlock(64, block_depth = 2)([x, skips])\n\nx = DownBlock(96, block_depth = 2)([x, skips])\n\nx = ResidualBlock(128)(x) \u00ae\nx = ResidualBlock(128) (x)\n\nx = UpBlock(96, block_depth\nx = UpBlock(64, block_depth\nx = UpBlock(32, block_depth\n\n2)([x, skips]) \u00ae\n2)(x, skips])\n2)(x, skips])\n\n*\n\"\n\nlayers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x) \u00ae\n\nunet = models.Model({noisy_images, noise_variances], x, name=\"unet\") \u00ae\n\n@ The first input to the U-Net is the image that we wish to denoise.\n\n\u00ae This image is passed through a Conv2D layer to increase the number of channels.\n\n\u00ae The second input to the U-Net is the noise variance (a scalar).\n\n\u00ae This is encoded using a sinusoidal embedding.\n\n\u00a9 This embedding is copied across spatial dimensions to match the size of the input image.\n\n\u00a9 The two input streams are concatenated across channels.\n\n\u00a9 This embedding is copied across spatial dimensions to match the size of the input image.\n\n\u00a9 The two input streams are concatenated across channels.\n\n@ The skips list will hold the output from the DownBlock layers that we wish to connect to\nUpBlock layers downstream.\n\n\u00ae The tensor is passed through a series of DownBlock layers that reduce the size of the image, while\nincreasing the number of channels.\n\n\u00a9 The tensor is then passed through two ResidualBlock layers that hold the image size and num-\nber of channels constant.\n\n@\u00ae Next, the tensor is passed through a series of UpBlock layers that increase the size of the image,\nwhile decreasing the number of channels. The skip connections incorporate output from the earli-\n\ner DownBlock layers.\n\n\u00ae The final Conv2D layer reduces the number of channels to three (RGB).\n\n\u00ae The U-Net is a Keras Model that takes the noisy images and noise variances as input and outputs a\npredicted noise map.\n\nTo understand the U-Net in detail, we need to explore four more concepts: the sinusoidal embedding of\nthe noise variance, the ResidualBlock , the DownBlock, and the UpBlock.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.1,
                        "section_name": "Sinusoidal embedding",
                        "section_path": "./screenshots-images-2/chapter_8/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_10/95d41fbf-99be-4f1d-918d-9352d4684581.png",
                            "./screenshots-images-2/chapter_8/section_10/5a8272f8-be1e-4ac2-92ac-58f51729499a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Sinusoidal embedding\n\n6\nSinusoidal embedding was first introduced in a paper by Vaswani et al. We will be using an adaptation of\nthat original idea as utilized in Mildenhall et al.\u2019s paper titled \u201cNeRF: Representing Scenes as Neural\n\n1\nRadiance Fields for View Synthesis.\u201d\n\nThe idea is that we want to be able to convert a scalar value (the noise variance) into a distinct higher-di-\nmensional vector that is able to provide a more complex representation, for use downstream in the net-\nwork. The original paper used this idea to encode the discrete position of words in a sentence into vec-\ntors; the NeRF paper extends this idea to continuous values.\n\nSpecifically, a scalar value x is encoded as shown in the following equation:\n\n(2) = (sin (2me%fx), +++, sin (2ne!P92), cos (2ne%a), +++, C08 (2ne!\"-YFa))\n\nwhere we choose L = 16 to be half the size of our desired noise embedding length and f = Sahoo) to be\nthe maximum scaling factor for the frequencies.\nThis produces the embedding pattern shown in Figure 8-9.\n\ni=\n\n[=]\n\nB\n\nc\n\na\n\n\u00a3\n\nuT\n\noD\n\n&\n\n=\n\nas\n\n3\n\n[=\n\nwe\n\n04 0.5\nNoise variance\n-1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50 075 1.00\nEmbedding value\n\nFigure 8-9. The pattern of sinusoidal embeddings for noise variances from 0 to 1\n\nWe can code this sinusoidal embedding function as shown in Example 8-7. This converts a single noise\nvariance scalar value into a vector of length 32.\n\nExample 8-7. The sinusoidal_embedding function that encodes the noise variance\n\nExample 8-7. The sinusoidal_embedding function that encodes the noise variance\n\ndef sinusoidal_embedding(x):\nfrequencies = tf.exp(\ntf.linspace(\ntf.math.log(1.0),\ntf.math.1o0g(1000.0),\n16,\n\n)\n\nangular_speeds = 2.0 * math.pi * frequencies\nembeddings = tf.concat(\n\n[tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n)\n\nreturn embeddings\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.11,
                        "section_name": "ResidualBlock",
                        "section_path": "./screenshots-images-2/chapter_8/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_11/08cdfa9e-eb97-465b-a66e-57f105786840.png",
                            "./screenshots-images-2/chapter_8/section_11/d32fe8b9-e61a-4806-af1c-f2acc4072301.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "ResidualBlock\n\nBoth the DownBlock and the UpBlock contain ResidualBlock layers, so let\u2019s start with these. We al-\nready explored residual blocks in Chapter 5, when we built a PixelCNN, but we will recap here for\ncompleteness.\n\nA residual block is a group of layers that contains a skip connection that adds the input to the output.\nResidual blocks help us to build deeper networks that can learn more complex patterns without suffering\nas greatly from vanishing gradient and degradation problems. The vanishing gradient problem is the as-\nsertion that as the network gets deeper, the gradient propagated through deeper layers is tiny and there-\nfore learning is very slow. The degradation problem is the fact that as neural networks become deeper,\nthey are not necessarily as accurate as their shallower counterparts\u2014accuracy seems to become saturat-\ned at a certain depth and then degrade rapidly.\n\nDEGRADATION\n\nThe degradation problem is somewhat counterintuitive, but observed in practice as the deeper layers must at least\nlearn the identity mapping, which is not trivial\u2014especially considering other problems deeper networks face, such\nas the vanishing gradient problem.\n\nThe solution, first introduced in the ResNet paper by He et al. in 2015,\" is very simple. By including a skip\nconnection highway around the main weighted layers, the block has the option to bypass the complex\nweight updates and simply pass through the identity mapping. This allows the network to be trained to\ngreat depth without sacrificing gradient size or network accuracy.\n\nA diagram of a ResidualBlock is shown in Figure 8-10. Note that in some residual blocks, we also in-\nclude an extra Conv2D layer with kernel size 1 on the skip connection, to bring the number of channels\nin line with the rest of the block.\n\nResidualBlock\n\nFigure 8-10. The ResidualBlock in the U-Net\n\nWe can code a ResidualBlock in Keras as shown in Example 8-8.\n\nExample 8-8. Code for the ResidualBlock in the U-Net\n\ndef ResidualBlock(width):\ndef apply(x):\ninput_width = x.shape[3]\nif input_width == width: 0\nresidual = x\nelse:\nresidual = layers.Conv2D(width, kernel_size=1) (x)\n\nlayers .BatchNormalization(center=False, scale=False)(x) @\n\nlayers .Conv2D(\n\nwidth, kernel _size=3, padding=\"same\", activation=activations.swish\n)(x) \u00ae\n\nx = layers.Conv2D(width, kernel_size=3, padding=\"same\" ) (x)\n\nx = layers.Add()([x, residual]) @\n\nreturn x\n\nreturn apply\n\n@ Check if the number of channels in the input matches the number of channels that we would like\nthe block to output. If not, include an extra Conv2D layer on the skip connection to bring the num-\nber of channels in line with the rest of the block.\n\n@ Apply a BatchNormalization layer.\n\u00ae Apply two Conv2D layers.\n\n\u00a9 Add the original block input to the output to provide the final output from the block.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.12,
                        "section_name": "DownBlocks and UpBlocks",
                        "section_path": "./screenshots-images-2/chapter_8/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_12/c755677e-a2e8-4247-a095-66cef2804faa.png",
                            "./screenshots-images-2/chapter_8/section_12/4031cac0-511c-44fd-b8b8-3666d2a90206.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "DownBlocks and UpBlocks\n\nEach successive DownBlock increases the number of channels via block_depth (=2 in our example)\nResidualBlock s, while also applying a final AveragePooling2D layer in order to halve the size of the\nimage. Each ResidualBlock is added to a list for use later by the UpBlock layers as skip connections\nacross the U-Net.\n\nAn UpBlock first applies an UpSampling2D layer that doubles the size of the image, through bilinear\ninterpolation. Each successive UpBlock decreases the number of channels via block_depth (=2)\nResidualBlock s, while also concatenating the outputs from the DownBlock s through skip connections\nacross the U-Net. A diagram of this process is shown in Figure 8-11.\n\nDownBlock UpBlock\nResidualBlock\nResidualBlock\n\nResidualBlock\nResidualBlock\nAveragePooling2D i UpSampling2D\n\nFigure 8-11. The DownBlock and corresponding UpBlock in the U-Net\n\nWe can code the DownBlock and UpBlock using Keras as illustrated in Example 8-9.\n\nExample 8-9. Code for the DownBlock and UpBlock in the U-Net model\n\ndef DownBlock(width, block_depth):\ndef apply(x):\n\nx, skips = x\n\nfor _ in range(block_depth):\nx = ResidualBlock(width)(x) @\nskips.append(x) @\n\nx = layers.AveragePooling2D(pool_size=2)(x) \u00a9\n\nreturn x\n\nreturn apply\n\ndef UpBlock(width, block_depth):\ndef apply(x):\nx, skips = x\nx = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x) @\nfor _ in range(block_depth):\nx = layers.Concatenate()([{x, skips.pop()]) @\nx = ResidualBlock(width)(x) \u00a9\nreturn x\n\nreturn apply\n\n@ The DownBlock increases the number of channels in the image using a ResidualBlock ofa giv-\nen width...\n\n@ ...each of which are saved to a list (skips ) for use later by the UpBlocks.\n\n\u00ae A final AveragePooling2D layer reduces the dimensionality of the image by half.\n\n\u00a9 The UpBlock begins with an UpSampling2D layer that doubles the size of the image.\n\n\u00a9 The output from a DownBlock layer is glued to the current output using a Concatenate layer.\n\n@ A ResidualBlock is used to reduce the number of channels in the image as it passes through the\n\nUpBlock.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.13,
                        "section_name": "Training the Diffusion Model",
                        "section_path": "./screenshots-images-2/chapter_8/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_13/e1919b97-1e52-48e1-a69d-84d0fef25d03.png",
                            "./screenshots-images-2/chapter_8/section_13/bb732f95-6382-43eb-b73c-6d1da39f9cbe.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the Diffusion Model\n\nWe now have all the components in place to train our denoising diffusion model! Example 8-10 creates,\ncompiles, and fits the diffusion model.\n\nExample 8-10. Code for training the DiffusionModel\n\nmodel = DiffusionModel() @\n\nmodel .compile(\noptimizer=optimizers.experimental.AdamW(learning rate=le-3, weight_decay=le-4\nloss=losses.mean_absolute_error,\n\n)@\nmodel.normalizer.adapt(train) \u00ae\n\nmodel. fit(\ntrain,\nepochs=50,\n)o\n\n@ Instantiate the model.\n\n\u00ae Compile the model, using the AdamW optimizer (similar to Adam but with weight decay, which\nhelps stabilize the training process) and mean absolute error loss function.\n\n\u00ae Calculate the normalization statistics using the training set.\n\u00a9 Fit the model over 50 epochs.\n\nThe loss curve (noise mean absolute error [MAE]) is shown in Figure 8-12.\n\n0 5 10 15 20 25 30 35 40 45 50\n\nFigure 8-12. The noise mean absolute error loss curve, by epoch\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.14,
                        "section_name": "Sampling from the Denoising Diffusion Model",
                        "section_path": "./screenshots-images-2/chapter_8/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_14/7e73d16f-0673-4557-9c04-b0bb49da2974.png",
                            "./screenshots-images-2/chapter_8/section_14/2566dcac-df35-4e57-b4f1-05b93540454b.png",
                            "./screenshots-images-2/chapter_8/section_14/1dd78941-d7ca-47ff-8eea-095397032860.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Sampling from the Denoising Diffusion Model\n\nIn order to sample images from our trained model, we need to apply the reverse diffusion process\u2014that\nis, we need to start with random noise and use the model to gradually undo the noise, until we are left\nwith a recognizable picture of a flower.\n\nWe must bear in mind that our model is trained to predict the total amount of noise that has been added\nto a given noisy image from the training set, not just the noise that was added at the last timestep of the\nnoising process. However, we do not want to undo the noise all in one go\u2014predicting an image from pure\nrandom noise in one shot is clearly not going to work! We would rather mimic the forward process and\nundo the predicted noise gradually over many small steps, to allow the model to adjust to its own\npredictions.\n\nTo achieve this, we can jump from 2; to x; in two steps\u2014first by using our model\u2019s noise prediction to\ncalculate an estimate for the original image xo and then by reapplying the predicted noise to this image,\nbut only over \u00a2 \u2014 1 timesteps, to produce x;+_1. This idea is shown in Figure 8-13.\n\n1. Predict the noise \u20ac, from x using the model and use this to estimate xq\n\n2. Reapply the predicted noise \u20ac\u00a2 but\nonly until time t-1, to estimate x.\n\nxo *t xt\nFigure 8-13. One step of the sampling process for our diffusion model\nIf we repeat this process over a number of steps, we\u2019ll eventually get back to an estimate for xo that has\nbeen guided gradually over many small steps. In fact, we are free to choose the number of steps we take,\n\nand crucially, it doesn\u2019t have to be the same as the large number of steps in the training noising process\n(Le., 1,000). It can be much smaller\u2014in this example we choose 20.\n\nThe following equation (Song et al., 2020) this process mathematically:\n\n= _{t) \u2014\u2014\nx, \u2014 VT \u2014 Greg\u2019 (x2) / @\nXt = VG _ ] +. 1-1-0? 6)\u2019 (x) + te\nt-1 1 Va V t-1\u2014 07+ 69\u00b0 (Xr) tet\n\na ; random noise\ndirection pointing to x,\n\npredicted xp\n\nLet\u2019s break this down. The first term inside the brackets on the righthand side of the equation is the esti-\nmated image 9, calculated using the noise predicted by our network \u00e9). We then scale this by the t \u2014 1\n\nsignal rate ,/@;_1 and reapply the predicted noise, but this time scaled by the t \u2014 1 noise rate\n\n4/ 1-a@ o?. Additional Gaussian random noise o;\u20ac; is also added, with the factors o; determining\nhow random we want our generation process to be.\n\nThe special case o; = 0 for all t corresponds to a type of model known as a Denoising Diffusion Implicit\nModel (DDIM), introduced by Song et al. in 2020. With a DDIM, the generation process is entirely deter-\nministic\u2014that is, the same random noise input will always give the same output. This is desirable as then\nwe have a well-defined mapping between samples from the latent space and the generated outputs in\npixel space.\n\nIn our example, we will implement a DDIM, thus making our generation process deterministic. The code\nfor the DDIM sampling process (reverse diffusion) is shown in Example 8-11.\n\nExample 8-11. Sampling from the diffusion model\n\nclass DiffusionModel(models.Model):\n\ndef reverse diffusion(self, initial_noise, diffusion_steps):\nnum_images = initial_noise.shape[0]\nstep_size = 1.0 / diffusion_steps\ncurrent_images = initial_noise\nfor step in range(diffusion_steps): @\ndiffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step size @\nnoise rates, signal_rates = self.diffusion_schedule(diffusion_times) (\npred_noises, pred_images = self.denoise(\ncurrent_images, noise rates, signal_rates, training=False\n)9\nnext_diffusion_times = diffusion_times - step size \u00a9\nnext_noise_rates, next_signal_rates = self.diffusion_schedule(\nnext_diffusion_times\n) 9\ncurrent_images = (\nnext_signal_rates * pred_images + next_noise rates * pred_noises\n)@\n\nreturn pred_images @\n\nLook over a fixed number of steps (e.g., 20).\n\nThe diffusion times are all set to 1 (i.e., at the start of the reverse diffusion process).\n\nThe noise and signal rates are calculated according to the diffusion schedule.\n\nThe U-Net is used to predict the noise, allowing us to calculate the denoised image estimate.\n\nThe diffusion times are reduced by one step.\n\nThe new noise and signal rates are calculated.\n\nThe t-1 images are calculated by reapplying the predicted noise to the predicted image, according\ntothe t-1 diffusion schedule rates.\n\nAfter 20 steps, the final x9 predicted images are returned.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.15,
                        "section_name": "Analysis of the Diffusion Model",
                        "section_path": "./screenshots-images-2/chapter_8/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_15/aef05b8b-9954-476b-bae9-dc602bf4ed93.png",
                            "./screenshots-images-2/chapter_8/section_15/71a33997-cd7f-4b41-a814-ee0ae8edc51d.png",
                            "./screenshots-images-2/chapter_8/section_15/24e5230b-a8fc-4c26-83f6-45befd3815b3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the Diffusion Model\n\nWe'll now take a look at three different ways that we can use our trained model: for generation of new\nimages, testing how the number of reverse diffusion steps affects quality, and interpolating between two\nimages in the latent space.\n\nGenerating images\n\nIn order to produce samples from our trained model, we can simply run the reverse diffusion process,\nensuring that we denormalize the output at the end (i.e., take the pixel values back into the range [0, 1]).\nWe can achieve this using the code in Example 8-12 inside the DiffusionModel class.\n\nExample 8-12. Generating images using the diffusion model\n\nclass DiffusionModel(models.Model):\n\ndef denormalize(self, images):\nimages = self.normalizer.mean + images * self.normalizer.variance**0.5 @\nreturn tf.clip by value(images, 0.0, 1.0)\n\ndef generate(self, num_images, diffusion_steps):\ninitial_noise = tf.random.normal(shape=(num_images, 64, 64, 3)) @\ngenerated_images = self.reverse_diffusion(initial_noise, diffusion_steps)\ngenerated_images = self.denormalize(generated_images) \u00a9\nreturn generated_images\n\n@ Generate some initial noise maps.\n@ Apply the reverse diffusion process.\n\n\u00ae The images output by the network will have mean zero and unit variance, so we need to denormal-\nize by reapplying the mean and variance calculated from the training data.\n\nIn Figure 8-14 we can observe some samples from the diffusion model at different epochs of the training\n\nprocess.\n\nFigure 8-14. Samples from the diffusion model at different epochs of the training process\n\nAdjusting the number of diffusion steps\n\nWe can also test to see how adjusting the number of diffusion steps in the reverse process affects image\nquality. Intuitively, the more steps taken by the process, the higher the quality of the image generation.\n\nWe can see in that the quality of the generations does indeed improve with the number of dif-\nfusion steps. With one giant leap from the initial sampled noise, the model can only predict a hazy blob of\ncolor. With more steps, the model is able to refine and sharpen its generations. However, the time taken\nto generate the images scales linearly with the number of diffusion steps, so there is a trade-off. There is\nminimal improvement between 20 and 100 diffusion steps, so we choose 20 as a reasonable compromise\n\nbetween quality and speed in this example.\n\nDiffusion\nsteps\n\nia\n\n100\n\ni\n\nRaa\nPe\n\nRe fe Hs Rt Ht Bt al\npefefep ste].\na aaa\nclelelslelels|\nPEPPY\nPep ef epet eels\nHABE\n\nFigure 8-15. Image quality improves with the number of diffusion steps\n\nInterpolating between images\n\nLastly, as we have seen previously with variational autoencoders, we can interpolate between points in\nthe Gaussian latent space in order to smoothly transition between images in pixel space. Here we choose\nto use a form of spherical interpolation that ensures that the variance remains constant while blending\nthe two Gaussian noise maps together. Specifically, the initial noise map at each step is given by\n\nasin (St) + bcos ($t), where t ranges smoothly from 0 to 1 and a and 6 are the two randomly sam-\n\npled Gaussian noise tensors that we wish to interpolate between.\n\nThe resulting images are shown in Figure 8-16.\n\nA B\n1\n\nFigure 8-16. Interpolating between images using the denoising diffusion model\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 8.16,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_8/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_8/section_16/fe2eee38-b510-4f82-a346-71a47c0c746c.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter we have explored one of the most exciting and promising areas of generative modeling in\nrecent times: diffusion models. In particular, we implemented the ideas from a key paper on generative\ndiffusion models (Ho et al., 2020) that introduced the original Denoising Diffusion Probabilistic Model\n(DDPM). We then extended this with the ideas from the Denoising Diffusion Implicit Model (DDIM) paper\nto make the generation process fully deterministic.\n\nWe have seen how diffusion models are formed of a forward diffusion process and a reverse diffusion\nprocess. The forward diffusion process adds noise to the training data through a series of small steps,\nwhile the reverse diffusion process consists of a model that tries to predict the noise added.\n\nWe make use of a reparameterization trick in order to calculate the noised images at any step of the for-\nward process without having to go through multiple noising steps. We have seen how the chosen sched-\nule of parameters used to add noise to the data plays an important part in the overall success of the\nmodel.\n\nThe reverse diffusion process is parameterized by a U-Net that tries to predict the noise at each timestep,\ngiven the noised image and the noise rate at that step. A U-Net consists of DownBlock s that increase the\nnumber of channels while reducing the size of the image and UpBlock s that decrease the number of\nchannels while increasing the size. The noise rate is encoded using sinusoidal embedding.\n\nSampling from the diffusion model is conducted over a series of steps. The U-Net is used to predict the\nnoise added to a given noised image, which is then used to calculate an estimate for the original image.\nThe predicted noise is then reapplied using a smaller noise rate. This process is repeated over a series of\nsteps (which may be significantly smaller than the number of steps used during training), starting from a\nrandom point sampled from a standard Gaussian noise distribution, to obtain the final generation.\n\nWe saw how increasing the number of diffusion steps in the reverse process improves the image genera-\ntion quality, at the expense of speed. We also performed latent space arithmetic in order to interpolate\nbetween two images.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 9,
                "chapter_name": "Chapter 9. Transformers",
                "chapter_path": "./screenshots-images-2/chapter_9",
                "sections": [
                    {
                        "section_id": 9.1,
                        "section_name": "Chapter 9. Transformers",
                        "section_path": "./screenshots-images-2/chapter_9/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_1/f1acb0d1-d1f6-46b5-8bcc-d085f5ee9954.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 9. Transformers\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\nLearn about the origins of GPT, a powerful decoder Transformer model for text generation.\n\nLearn conceptually how an attention mechanism mimics our way of attaching more importance to\nsome words in a sentence than others.\n\nDelve into how the attention mechanism works from first principles, including how queries, keys,\nand values are created and manipulated.\n\nSee the importance of causal masking for text generation tasks.\n\nUnderstand how attention heads can be grouped into a multihead attention layer.\n\nSee how multihead attention layers form one part of a Transformer block that also includes layer\nnormalization and skip connections.\n\nCreate positional encodings that capture the position of each token as well as the word token\nembedding.\n\nBuild a GPT model in Keras to generate the text contained in wine reviews.\n\nAnalyze the output from the GPT model, including interrogating the attention scores to inspect\nwhere the model is looking.\n\nLearn about the different types of Transformers, including examples of the types of tasks that can be\ntackled by each and descriptions of the most famous state-of-the-art implementations.\n\nUnderstand how encoder-decoder architectures work, like Google\u2019s T5 model.\n\nExplore the training process behind OpenAl\u2019s ChatGPT.\n\nWe saw in Chapter 5 how we can build generative models on text data using recurrent neural networks\n\n(RNNs), such as LSTMs and GRUs. These autoregressive models process sequential data one token at a\n\ntime, constantly updating a hidden vector that captures the current latent representation of the input.\n\nThe RNN can be designed to predict the next word in a sequence by applying a dense layer and softmax\n\nactivation over the hidden vector. This was considered the most sophisticated way to generatively pro-\n\nduce text until 2017, when one paper changed the landscape of text generation forever.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_9/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_2/488d7433-e984-4f84-b40b-f4c3c6d3ba94.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nThe Google Brain paper, confidently entitled \u201cAttention Is All You Need,\u201d is famous for popularizing the\nconcept of attention\u2014a mechanism that now powers most state-of-the-art text generation models.\n\nThe authors show how it is possible to create powerful neural networks called Transformers for sequen-\ntial modeling that do not require complex recurrent or convolutional architectures but instead only rely\non attention mechanisms. This approach overcomes a key downside to the RNN approach, which is that it\nis challenging to parallelize, as it must process sequences one token as a time. Transformers are highly\nparalellizable, allowing them to be trained on massive datasets.\n\nIn this chapter, we are going to delve into how modern text generation models make use of the\nTransformer architecture to reach state-of-the-art performance on text generation challenges. In particu-\nlar, we will explore a type of autoregressive model known as the generative pre-trained transformer\n(GPT), which powers OpenAl\u2019s GPT-4 model, widely considered to be the current state of the art for text\n\ngeneration.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.3,
                        "section_name": "GPT",
                        "section_path": "./screenshots-images-2/chapter_9/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_3/fa340a1a-bd96-4abe-9b57-c48ffefda0de.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "GPT\n\nOpenAI introduced GPT in June 2018, in the paper \u201cImproving Language Understanding by Generative\n2\nPre-Training,\u201d almost exactly a year after the appearance of the original Transformer paper.\n\nIn this paper, the authors show how a Transformer architecture can be trained on a huge amount of text\ndata to predict the next word in a sequence and then subsequently fine-tuned to specific downstream\ntasks.\n\nThe pre-training process of GPT involves training the model on a large corpus of text called BookCorpus\n(4.5 GB of text from 7,000 unpublished books of different genres). During pre-training, the model is\ntrained to predict the next word in a sequence given the previous words. This process is known as lan-\nguage modeling and is used to teach the model to understand the structure and patterns of natural\nlanguage.\n\nAfter pre-training, the GPT model can be fine-tuned for a specific task by providing it with a smaller, task-\nspecific dataset. Fine-tuning involves adjusting the parameters of the model to better fit the task at hand.\nFor example, the model can be fine-tuned for tasks such as classification, similarity scoring, or question\n\nanswering.\n\nThe GPT architecture has since been improved and extended by OpenAI with the release of subsequent\nmodels such as GPT-2, GPT-3, GPT-3.5, and GPT-4. These models are trained on larger datasets and have\nlarger capacities, so they can generate more complex and coherent text. The GPT models have been wide-\nly adopted by researchers and industry practitioners and have contributed to significant advancements\nin natural language processing tasks.\n\nIn this chapter, we will build our own variation of the original GPT model, trained on less data, but still\n\nutilizing the same components and underlying principles.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/09_transformer/01_gpt/gpt.ipynb\nin the book repository.\n\nThe code is adapted from the excellent GPT tutorial created by Apoorv Nandan available on the Keras website.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.4,
                        "section_name": "The Wine Reviews Dataset",
                        "section_path": "./screenshots-images-2/chapter_9/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_4/77c75b06-18e1-4c17-b5ba-3d0179defc0f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Wine Reviews Dataset\n\nWe'll be using the Wine Reviews dataset that is available through Kaggle. This is a set of over 130,000 re-\nviews of wines, with accompanying metadata such as description and price.\n\nYou can download the dataset by running the Kaggle dataset downloader script in the book repository, as\n\nshown in Example 9-1. This will save the wine reviews and accompanying metadata locally to the /data\nfolder.\n\nExample 9-1. Downloading the Wine Reviews dataset\n\nbash scripts/download_kaggle_data.sh zynicide wine-reviews\n\nThe data preparation steps are identical to the steps used in Chapter 5 for preparing data for input into\nan LSTM, so we will not repeat them in detail here. The steps, as shown in Figure 9-1, are as follows:\n\n1. Load the data and create a list of text string descriptions of each wine.\n\n2. Pad punctuation with spaces, so that each punctuation mark is treated as a separate word.\n\n3. Pass the strings through a Text Vectorization layer that tokenizes the data and pads/clips each\nstring to a fixed length.\n\n4. Create a training set where the inputs are the tokenized text strings and the outputs to predict are\nthe same strings shifted by one token.\n\nText with punctuation padded Tokenized text Training observation\n\nwine review 1 US + California + (720 220 2 29.2 @ 62235 | input\nPinot Noir 1 Oak and earth 5243 4145 453 634 26 9 497\nintermingle around robust aromas |\u2014\u2014\u2014P| 499 667 17 12 142 14 2214 43\n\n25 248 32 8 223 14 2213 948 4\n596 17 987 3 15 75 237 3 64 14\n82 97 5 74 2633 17 198 49 5\n12577400000000000\nooo0c0000000)\n\ntexture\u00bb\n\nOutput: shift by one token\n\nFigure 9-1. Data processing for the Transformer\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.5,
                        "section_name": "Attention",
                        "section_path": "./screenshots-images-2/chapter_9/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_5/073e3d80-4858-4117-a40e-cda5f63df022.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Attention\n\nThe first step to understanding how GPT works is to understand how the attention mechanism works.\nThis mechanism is what makes the Transformer architecture unique and distinct from recurrent ap-\nproaches to language modeling. When we have developed a solid understanding of attention, we will\n\nthen see how it is used within Transformer architectures such as GPT.\n\nWhen you write, the choice that you make for the next word in the sentence is influenced by other words\nthat you have already written. For example, suppose you start a sentence as follows:\n\nThe pink elephant tried to get into the car but it was too\n\nClearly, the next word should be something synonymous with big. How do we know this?\n\nCertain other words in the sentence are important for helping us to make our decision. For example, the\nfact that it is an elephant, rather than a sloth, means that we prefer big rather than slow. If it were a\nswimming pool, rather than a car, we might choose scared as a possible alternative to big. Lastly, the ac-\ntion of getting into the car implies that size is the problem\u2014if the elephant was trying to squash the car\n\ninstead, we might choose fast as the final word, with it now referring to the car.\n\nOther words in the sentence are not important at all. For example, the fact that the elephant is pink has\nno influence on our choice of final word. Equally, the minor words in the sentence (the, but, it, etc.) give\nthe sentence grammatical form, but here aren\u2019t important to determine the required adjective.\n\nIn other words, we are paying attention to certain words in the sentence and largely ignoring others.\nWouldn\u2019t it be great if our model could do the same thing?\n\nAn attention mechanism (also know as an attention head) in a Transformer is designed to do exactly this.\nIt is able to decide where in the input it wants to pull information from, in order to efficiently extract use-\nful information without being clouded by irrelevant details. This makes it highly adaptable to a range of\n\ncircumstances, as it can decide where it wants to look for information at inference time.\n\nIn contrast, a recurrent layer tries to build up a generic hidden state that captures an overall representa-\ntion of the input at each timestep. A weakness of this approach is that many of the words that have al-\nready been incorporated into the hidden vector will not be directly relevant to the immediate task at\nhand (e.g., predicting the next word), as we have just seen. Attention heads do not suffer from this prob-\nlem, because they can pick and choose how to combine information from nearby words, depending on\nthe context.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.6,
                        "section_name": "Queries, Keys, and Values",
                        "section_path": "./screenshots-images-2/chapter_9/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_6/1dba4be6-2dee-4c87-9b2b-e8d0bd836e2b.png",
                            "./screenshots-images-2/chapter_9/section_6/e6d3f5ec-b646-4323-9214-c6a977cf5233.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Queries, Keys, and Values\n\nSo how does an attention head decide where it wants to look for information? Before we get into the de-\ntails, let\u2019s explore how it works at a high level, using our pink elephant example.\n\nImagine that we want to predict what follows the word too. To help with this task, other preceding words\nchime in with their opinions, but their contributions are weighted by how confident they are in their own\nexpertise in predicting words that follow too. For example, the word elephant might confidently con-\ntribute that it is more likely to be a word related to size or loudness, whereas the word was doesn\u2019t have\nmuch to offer to narrow down the possibilities.\n\nIn other words, we can think of an attention head as a kind of information retrieval system, where a\nquery (\u201cWhat word follows too?\u201d) is made into a key/value store (other words in the sentence) and the re-\n\nsulting output is a sum of the values, weighted by the resonance between the query and each key.\n\nWe will now walk through the process in detail (Figure 9-2), again with reference to our pink elephant\nsentence.\n\nEmbedding\n\ntoo 84\n4 cy\ne\ntoaquery T\nH softmax|\n1 (7\nt voce\n| Attention\nEmbedding Key | weights Embedding\nthe 10 70.01 J 10 the\n4\nelephant 235->[ n> > 0 2 J<-235 elephant\ntried 58 .05 J< se tried\n\n01 I+ 3to\n\nJe 19 get\nJe 59 into\nJ+ 10 the\n\n}\u00a2-198 car\n\n-ha-- aaa\ni\n\n:\nget 19\ninto 59\nthe 10\ncar 198\u00b0\n\nL}\nL}\ne\n\u00b0\n0.09\n0.12\n0.01\n0.20\niL}\n\u00b0\niL}\ni}\n\nbut 9 i+ 9 but\nit7 i 7it\n\nwas 18 J 18 was\n\ntoo 84 i+ 84 too\n\noO Contextualized value vector\n\nFigure 9-2. The mechanics of an attention head\n\nThe query (Q) can be thought of as a representation of the current task at hand (e.g., \u201cWhat word follows\ntoo?\u201d). In this example, it is derived from the embedding of the word too, by passing it through a weights\nmatrix Wg to change the dimensionality of the vector from d, to dy.\n\nThe key vectors (K) are representations of each word in the sentence\u2014you can think of these as descrip-\ntions of the kinds of prediction tasks that each word can help with. They are derived in a similar fashion\nto the query, by passing each embedding through a weights matrix Wx to change the dimensionality of\n\neach vector from d, to d;. Notice that the keys and the query are the same length (d;).\n\nInside the attention head, each key is compared to the query using a dot product between each pair of\nvectors (QK*). This is why the keys and the query have to be the same length. The higher this number is\nfor a particular key/query pair, the more the key resonates with the query, so it is allowed to make more\nof a contribution to the output of the attention head. The resulting vector is scaled by Vd, to keep the\nvariance of the vector sum stable (approximately equal to 1), and a softmax is applied to ensure the con-\ntributions sum to 1. This is a vector of attention weights.\n\nThe value vectors (V) are also representations of the words in the sentence\u2014you can think of these as the\nunweighted contributions of each word. They are derived by passing each embedding through a weights\nmatrix Wy to change the dimensionality of each vector from d, to d,. Notice that the value vectors do\nnot necessarily have to have the same length as the keys and query (but often do, for simplicity).\n\nThe value vectors are multiplied by the attention weights to give the attention for a given Q, K, and V, as\nshown in Equation 9-1.\n\nEquation 9-1. Attention equation\n\nAttention (Q, K,V) ft (2)v\nention = sojtmar\n\ni vay\nTo obtain the final output vector from the attention head, the attention is summed to give a vector of\nlength d,. This context vector captures a blended opinion from words in the sentence on the task of pre-\ndicting what word follows too.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.7,
                        "section_name": "Multihead Attention",
                        "section_path": "./screenshots-images-2/chapter_9/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_7/c66fc78d-d3c9-40fb-9b5d-c486ad3046a7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Multihead Attention\n\nThere\u2019s no reason to stop at just one attention head! In Keras, we can build a MultiHeadAttention lay-\ner that concatenates the output from multiple attention heads, allowing each to learn a distinct attention\nmechanism so that the layer as a whole can learn more complex relationships.\n\nThe concatenated outputs are passed through one final weights matrix Wo to project the vector into the\ndesired output dimension, which in our case is the same as the input dimension of the query (d,), so that\nthe layers can be stacked sequentially on top of each other.\n\nFigure 9-3 shows how the output from a Mult iHeadAttention layer is constructed. In Keras we can\nsimply write the line shown in Example 9-2 to create such a layer.\n\nExample 9-2. Creating a MultiHeadAttention layer in Keras\n\nlayers .MultiHeadAttention/(\nnum_heads = 4, @\nkey_dim = 128, @\nvalue_dim = 64, \u00ae\noutput_shape = 256 @\n)\n\n@ This multihead attention layer has four heads.\n@ The keys (and query) are vectors of length 128.\n\u00ae The values (and therefore also the output from each head) are vectors of length 64.\n\n\u00a9 The output vector has length 256.\n\nConcatenate\n\nAttention head\n\nFigure 9-3. A multihead attention layer with four heads\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.8,
                        "section_name": "Causal Masking",
                        "section_path": "./screenshots-images-2/chapter_9/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_8/f82ddad9-d580-4d58-abe4-80831c0a307f.png",
                            "./screenshots-images-2/chapter_9/section_8/6504da36-299a-4f33-8ca3-e5fa98c2d159.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Causal Masking\n\nSo far, we have assumed that the query input to our attention head is a single vector. However, for effi-\nciency during training, we would ideally like the attention layer to be able to operate on every word in\nthe input at once, predicting for each what the subsequent word will be. In other words, we want our\nGPT model to be able to handle a group of query vectors in parallel (i.e., a matrix).\n\nYou might think that we can just batch the vectors together into a matrix and let linear algebra handle\nthe rest. This is true, but we need one extra step\u2014we need to apply a mask to the query/key dot product,\nto avoid information from future words leaking through. This is known as causal masking and is shown\n\noI [i\nok TS r=fetnf oof\nelephant [Ian]\n\nto\n\nin Figure 9-4.\n\nelephant\n\npink\ntried\ninto\n\no\n=f\ni=}\n\nBEBEREBE\ne[ele[slelsls|\n\nget\n\ninto\n\nELL\n\ncar\nbut\n\nFigure 9-4. Matrix calculation of the attention scores for a batch of input queries, using a causal attention mask to hide keys that are not avail-\nable to the query (because they come later in the sentence)\n\nWithout this mask, our GPT model would be able to perfectly guess the next word in the sentence, be-\ncause it would be using the key from the word itself as a feature! The code for creating a causal mask is\nshown in Example 9-3, and the resulting numpy array (transposed to match the diagram) is shown in\n\nExample 9-3. The causal mask function\n\ndef causal_attention_mask(batch_size, n_dest, n_src, dtype):\ni = tf.range(n_dest)[:, None]\ntf.range(n_src)\n\nm= i >= j - n_sre + n_dest\nmask = tf.cast(m, dtype)\nmask = tf.reshape(mask, [1, n_dest, n_src])\nmult = tf.concat(\n[tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n)\n\nreturn tf.tile(mask, mult)\n\nnp.transpose(causal_attention_mask(1, 10, 10, dtype = tf.int32)[0])\n\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n{(\u00ae, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n(0, \u00ae, 1, 1, 1, 1, 1, 1, 1, 1],\n(0, \u00ae, \u00ae, 1, 1, 1, 1, 1, 1, 1],\n{0, \u00ae, \u00ae, \u00ae, 1, 1, 1, 1, 1, 1],\n{0, \u00ae, \u00ae, \u00ae, \u00ae, 1, 1, 1, 1, 1],\n(0, 0, \u00ae, 0, 0, \u00ae, 1, 1, 1, 1],\n(0, 0, \u00ae, 0, 0, \u00ae, @, 1, 1, 1],\n[0, 0, \u00ae, 0, 0, 0, @, \u00ae, 1, 1],\n\n(0, @, @, \u00ae, \u00ae, @, \u00ae, @, 0, 1]], dtype=int32)\n\nFigure 9-5. The causal mask as a numpy array\u20141 means unmasked and 0 means masked\n\nTIP\nCausal masking is only required in decoder Transformers such as GPT, where the task is to sequentially generate to-\nkens given previous tokens. Masking out future tokens during training is therefore essential.\n\nOther flavors of Transformer (e.g., encoder Transformers) do not need causal masking, because they are not trained\nto predict the next token. For example Google\u2019s BERT predicts masked words within a given sentence, so it can use\n\n3\ncontext from both before and after the word in question.\n\nWe will explore the different types of Transformers in more detail at the end of the chapter.\n\nThis concludes our explanation of the multihead attention mechanism that is present in all Transformers.\nIt is remarkable that the learnable parameters of such an influential layer consist of nothing more than\nthree densely connected weights matrices for each attention head Wa, Wr, Wy) and one further\nweights matrix to reshape the output (Wo). There are no convolutions or recurrent mechanisms at all in\na multihead attention layer!\n\nNext, we shall take a step back and see how the multihead attention layer forms just one part of a larger\n\ncomponent known as a Transformer block.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.9,
                        "section_name": "The Transformer Block",
                        "section_path": "./screenshots-images-2/chapter_9/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_9/4ec29f3e-198b-474e-9b03-647edd48798e.png",
                            "./screenshots-images-2/chapter_9/section_9/6524f2d1-905f-4ec7-ac62-6f4bef7b3ccc.png",
                            "./screenshots-images-2/chapter_9/section_9/35aba228-78ea-4721-958e-ceca5affa08f.png",
                            "./screenshots-images-2/chapter_9/section_9/2837e1a0-44e3-4017-a9cb-6f01a7c1c16d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A Transformer block is a single component within a Transformer that applies some skip connections,\nfeed-forward (dense) layers, and normalization around the multihead attention layer. A diagram of a\nTransformer block is shown in Figure 9-6.\n\nTransformer block\n\nLayer normalization\n\nFeed-forward layers\n\nLayer normalization\n\nFigure 9-6. A Transformer block\n\nFirstly, notice how the query is passed around the multihead attention layer to be added to the output\u2014\nthis is a skip connection and is common in modern deep learning architectures. It means we can build\nvery deep neural networks that do not suffer as much from the vanishing gradient problem, because the\nskip connection provides a gradient-free highway that allows the network to transfer information for-\nward uninterrupted.\n\nSecondly, layer normalization is used in the Transformer block to provide stability to the training process.\nWe have already seen the batch normalization layer in action throughout this book, where the output\nfrom each channel is normalized to have a mean of 0 and standard deviation of 1. The normalization sta-\ntistics are calculated across the batch and spatial dimensions.\n\nIn contrast, layer normalization in a Transformer block normalizes each position of each sequence in the\nbatch by calculating the normalizing statistics across the channels. It is the complete opposite of batch\nnormalization, in terms of how the normalization statistics are calculated. A diagram showing the differ-\nence between batch normalization and layer normalization is shown in Figure 9-7.\n\nLayer normalization Batch normalization\n\nBatch\ndimension\n\nFigure 9-7. Layer normalization versus batch normalization\u2014the normalizgtion statistics are calculated across the red cells (source: Sheng et\nal, 20207\n\nLAYER NORMALIZATION VERSUS BATCH NORMALIZATION\n\nLayer normalization was used in the original GPT paper and is commonly used for text-based tasks to avoid creating\nnormalization dependencies across sequences in the batch. However, recent work such as Shen et al.s challenges this\nassumption, showing that with some tweaks a form of batch normalization can still be used within Transformers,\noutperforming more traditional layer normalization.\n\nLastly, a set of feed-forward (i.e., densely connected) layers is included in the Transformer block, to allow\nthe component to extract higher-level features as we go deeper into the network.\n\nA Keras implementation of a Transformer block is shown in Example 9-4.\n\nExample 9-4. A TransformerBlock layer in Keras\n\nclass TransformerBlock(layers.Layer):\ndef init__(self, num_heads, key dim, embed_dim, ff_dim, dropout_rate=0.1): \u00a2\nsuper(TransformerBlock, self). init_ ()\nself.num_heads = num_heads\nself.key dim = key dim\nself.embed_dim = embed_dim\nself.ff_dim = ff_dim\nself.dropout_rate = dropout_rate\nself.attn = layers.MultiHeadAttention(\nnum_heads, key_dim, output_shape = embed_dim\n)\nself.dropout_1 = layers.Dropout(self.dropout_rate)\nself.1ln_1 = layers.LayerNormalization(epsilon=1le-6)\nself.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\nself.ffn_2 = layers.Dense(self.embed_dim)\nself.dropout_2 = layers.Dropout(self.dropout_rate)\nself.1ln_2 = layers.LayerNormalization(epsilon=le-6)\n\ndef call(self, inputs):\n\ninput_shape = tf.shape(inputs)\n\nbatch_size = input_shape[0]\n\nseq_len = input_shape[1]\n\ncausal_mask = causal_attention_mask(\nbatch_size, seq_len, seq_len, tf.bool\n\n)9@\n\nattention_output, attention_scores = self.attn(\ninputs,\ninputs,\nattention_mask=causal_mask,\nreturn_attention_scores=True\n\n)9\n\nattention_output = self.dropout_1(attention_output)\n\noutl = self.1n_l(inputs + attention_output) @\n\nffn_l = self.ffn_l(outl) \u00a9\n\nffn_2 = self.ffn_2(ffn_1)\n\nffn_output = self.dropout_2(ffn_2)\n\nreturn (self.1n_2(outl + \u00a3fn_output), attention_scores) \u00a9\n\n@ The sublayers that make up the TransformerBlock layer are defined within the initialization\nfunction.\n\n\u00ae The causal mask is created to hide future keys from the query.\n\n\u00a9 The multihead attention layer is created, with the attention masks specified.\n\n\u00a9 The first add and normalization layer.\n\n\u00a9 The feed-forward layers.\n\n\u00a9 The feed-forward layers.\n\n@ The second add and normalization layer.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.1,
                        "section_name": "Positional Encoding",
                        "section_path": "./screenshots-images-2/chapter_9/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_10/5dd770f1-5f15-4db0-9672-5632da01757f.png",
                            "./screenshots-images-2/chapter_9/section_10/52e65dfe-4951-4188-92b4-43424f3a7aad.png",
                            "./screenshots-images-2/chapter_9/section_10/c9eb2509-7a8b-422c-aba4-447b1e1eea02.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Positional Encoding\n\nThere is one final step to cover before we can put everything together to train our GPT model. You may\nhave noticed that in the multihead attention layer, there is nothing that cares about the ordering of the\nkeys. The dot product between each key and the query is calculated in parallel, not sequentially, like ina\nrecurrent neural network. This is a strength (because of the parallelization efficiency gains) but also a\nproblem, because we clearly need the attention layer to be able to predict different outputs for the fol-\nlowing two sentences:\n\ne The dog looked at the boy and... (barked?)\ne The boy looked at the dog and ... (smiled?)\n\nTo solve this problem, we use a technique called positional encoding when creating the inputs to the ini-\ntial Transformer block. Instead of only encoding each token using a token embedding, we also encode the\nposition of the token, using a position embedding.\n\nThe token embedding is created using a standard Embedding layer to convert each token into a learned\nvector. We can create the positional embedding in the same way, using a standard Embedding layer to\nconvert each integer position into a learned vector.\n\nTIP\nWhile GPT uses an Embedding layer to embed the position, the original Transformer paper used trigonometric func-\ntions\u2014we\u2019ll cover this alternative in Chapter 11, when we explore music generation.\n\nTo construct the joint token\u2014-position encoding, the token embedding is added to the positional embed-\nding, as shown in Figure 9-8. This way, the meaning and position of each word in the sequence are cap-\ntured in a single vector.\n\nSequence length (n)\n\nPositional\nencoding\n\nToken and\npositional\n\nencoding\n\nFigure 9-8. The token embeddings are added to the positional embeddings to give the token position encoding\n\nThe code that defines our TokenAndPositionEmbedding layer is shown in Example 9-5.\n\nExample 9-5. The TokenAndPositionEmbedding layer\n\nclass TokenAndPositionEmbedding(layers.Layer):\n\ndef __ init__(self, maxlen, vocab_size, embed_dim):\nsuper (TokenAndPositionEmbedding, self). init ()\nself.maxlen = maxlen\nself.vocab_size =vocab_size\nself.embed_dim = embed_dim\nself.token_emb = layers.Embedding(\n\ninput_dim=vocab_size, output_dim=embed_dim\n\n)@o\nself.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) @\n\ndef call(self, x):\nmaxlen = tf.shape(x)[-1]\npositions = tf.range(start=0, limit=maxlen, delta=1)\npositions = self.pos_emb(positions)\nx = self.token_emb(x)\n\nreturn x + positions \u00ae\n\n@ The tokens are embedded using an Embedding layer.\n\n@ The positions of the tokens are also embedded using an Embedding layer.\n\n\u00a9 The output from the layer is the sum of the token and position embeddings.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.11,
                        "section_name": "Training GPT",
                        "section_path": "./screenshots-images-2/chapter_9/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_11/5b4acf64-ea59-4bd9-ae6d-3aeaa94c2967.png",
                            "./screenshots-images-2/chapter_9/section_11/d3ddb148-f7e2-4df6-bf75-c566794df13b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training GPT\n\nNow we are ready to build and train our GPT model! To put everything together, we need to pass our in-\nput text through the token and position embedding layer, then through our Transformer block. The final\noutput of the network is a simple Dense layer with softmax activation over the number of words in the\nvocabulary.\n\nTIP\n\nFor simplicity, we will use just one Transformer block, rather than the 12 in the paper.\n\nThe overall architecture is shown in Figure 9-9 and the equivalent code is provided in Example 9-6.\n\nText and positional embedding\n\nthe pink elephant tried to get into the car but it was too\n\nFigure 9-9. The simplified GPT model architecture\n\nExample 9-6. A GPT model in Keras\n\nMAX_LEN = 80\nVOCAB_SIZE = 10000\nEMBEDDING DIM = 256\nN_HEADS = 2\n\nKEY_DIM = 256\nFEED_FORWARD_DIM = 256\n\ninputs = layers.Input(shape=(None,), dtype=tf.int32) @\nx = TokenAndPositionEmbedding(MAX_LEN, VOCAB SIZE, EMBEDDING _DIM)(inputs) @\nx, attention_scores = TransformerBlock(\n\nN_HEADS, KEY_DIM, EMBEDDING DIM, FEED_FORWARD_DIM\n)(x) \u00ae\noutputs = layers.Dense(VOCAB_SIZE, activation = 'softmax')(x) @\ngpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores]) \u00a9\ngpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None]) @\ngpt.fit(train_ds, epochs=5)\n\nThe input is padded (with zeros).\n\n@ The text is encoded using a TokenAndPositionEmbedding layer.\n\n\u00ae The encoding is passed through a TransformerBlock.\n\n\u00a9 The transformed output is passed through a Dense layer with softmax activation to predict a dis-\ntribution over the subsequent word.\n\n\u00a9 The Model takes a sequence of word tokens as input and outputs the predicted subsequent word\ndistribution. The output from the Transformer block is also returned so that we can inspect how\nthe model is directing its attention.\n\n\u00a9 The model is compiled with SparseCategoricalCrossentropy loss over the predicted word\ndistribution.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.12,
                        "section_name": "Analysis of GPT",
                        "section_path": "./screenshots-images-2/chapter_9/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_12/c8d13d1d-5a6c-4752-80fa-1266d720df2a.png",
                            "./screenshots-images-2/chapter_9/section_12/018295f4-8a3d-4f4a-b511-a10f20404515.png",
                            "./screenshots-images-2/chapter_9/section_12/a8bcaa3c-4967-434c-936d-c6b11954063d.png",
                            "./screenshots-images-2/chapter_9/section_12/b0265767-9cdf-4464-a6ee-7aa7be1374ba.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of GPT\n\nNow that we have compiled and trained our GPT model, we can start to use it to generate long strings of\ntext. We can also interrogate the attention weights that are output from the TransformerBlock, to un-\nderstand where the Transformer is looking for information at different points in the generation process.\n\nGenerating text\nWe can generate new text by applying the following process:\n\n1. Feed the network with an existing sequence of words and ask it to predict the following word.\n\n2. Append this word to the existing sequence and repeat.\n\nThe network will output a set of probabilities for each word that we can sample from, so we can make\nthe text generation stochastic, rather than deterministic.\n\nWe will use the same TextGenerator class introduced in Chapter 5 for LSTM text generation, including\nthe temperature parameter that specifies how deterministic we would like the sampling process to be.\nLet\u2019s take a look at this in action, at two different temperature values (Figure 9-10).\n\nGenerated te\u00bb\nwine review : us : washington : chenin blanc : a light , mediun ~ bodied wine , this light - bodied expressi\n\u2018on is not a lot of enjoyment . it \u2019 s simple with butter and vanilla flavors that frane mixed with expressiv\ne fruit . it \u2019 s juicy and tangy with a lenon lingers on the finish .\n\nGenerated ter\nwine review : italy : piednont : nebbiolo : this opens with aronas of french oak , menthol and a whiff of to\nast . the straightforward palate offers red cherry , black raspberry jan and a hint of star anise alongside\nfirm but rather fleeting tannins , drink through 2016 .\n\nFigure 9-10. Generated outputs at temperature = 1.0 and temperature = 0.5.\n\nThere are a few things to note about these two passages. First, both are stylistically similar to a wine re-\nview from the original training set. They both open with the region and type of wine, and the wine type\nstays consistent throughout the passage (for example, it doesn\u2019t switch color halfway through). As we saw\nin Chapter 5, the generated text with temperature 1.0 is more adventurous and therefore less accurate\nthan the example with temperature 0.5. Generating multiple samples with temperature 1.0 will therefore\nlead to more variety as the model is sampling from a probability distribution with greater variance.\n\nViewing the attention scores\n\nWe can also ask the model to tell us how much attention is being placed on each word, when deciding on\n\nthe next word in the sentence. The TransformerBlock outputs the attention weights for each head,\n\nwhich are a softmax distribution over the preceding words in the sentence.\n\nTo demonstrate this, Figure 9-11 shows the top five tokens with the highest probabilities for three differ-\nent input prompts, as well as the average attention across both heads, against each preceding word. The\n\npreceding words are colored according to their attention score, averaged across the two attention heads.\n\nDarker blue indicates more attention is being placed on the word.\n\nwine review : Germany :\npfalz: 51.53%\nmosel: 41.21%\nrheingau: 4.27%\nrheinhessen: 2.16%\nfranken: 0.44%\n\nwine review : germany : rheingau : fiesling : this iS a ripe , full - bodied\n\nriesling:\nPy 27.78%\nwine:\n\nand: 4.58%\nyet: 1.33%\n\n46.56%\n\n16.88%\n\nwine review : germany : rheingau : riesling : this is a ripe , full - bodied riesling\n\nwith a touch of residual sugar | it ' \u00a7 a Slightly\nsweet: 94.23%\noily: 1.25%\nviscous: 1.09%\nbitter: 0.88%\n0.66%\n\nhoneyed:\n\nFigure 9-11. Distribution of word probabilities following various sequences\n\nIn the first example, the model attends closely to the country (germany) in order to decide on the word\nthat relates to the region. This makes sense! To pick a region, it needs to take lots of information from the\nwords that relate to the country, to ensure they match. It doesn\u2019t need to pay as much attention to the first\ntwo tokens (wine review) because they don\u2019t hold any useful information regarding the region.\n\nIn the second example, it needs to refer back to the grape (riesling), so it pays attention to the first time\nthat it was mentioned. It can pull this information by directly attending to the word, no matter how far\nback it is in the sentence (within the upper limit of 80 words). Notice that this is very different from a re-\ncurrent neural network, which relies on a hidden state to maintain all interesting information over the\nlength of the sequence so that it can be drawn upon if required\u2014a much less efficient approach.\n\nThe final sequence shows an example of how our GPT model can choose an appropriate adjective based\non a combination of information. Here the attention is again on the grape (riesling), but also on the fact\nthat it contains residual sugar. As Riesling is typically a sweet wine, and sugar is already mentioned, it\nmakes sense that it should be described as slightly sweet rather than slightly earthy, for example.\n\nIt is incredibly informative to be able to interrogate the network in this way, to understand exactly where\nit is pulling information from in order to make accurate decisions about each subsequent word. I highly\nrecommend playing around with the input prompts to see if you can get the model to attend to words re-\nally far back in the sentence, to convince yourself of the power of attention-based models over more tra-\nditional recurrent models!\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.13,
                        "section_name": "Other Transformers",
                        "section_path": "./screenshots-images-2/chapter_9/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_13/862fb89d-9dd1-4f0f-9025-f678652606c0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Other Transformers\n\nOur GPT model is a decoder Transformer\u2014it generates a text string one token at a time and uses causal\nmasking to only attend to previous words in the input string. There are also encoder Transformers, which\ndo not use causal masking\u2014instead, they attend to the entire input string in order to extract a meaning-\nful contextual representation of the input. For other tasks, such as language translation, there are also en-\ncoder-decoder Transformers that can translate from one text string to another; this type of model contains\nboth encoder Transformer blocks and decoder Transformer blocks.\n\nTable 9-1 summarizes the three types of Transformers, with the best examples of each architecture and\n\ntypical use cases.\n\nTable 9-1. The three Transformer architectures\n\nType Examples Use cases\nEncoder BERT (Google) Sentence classification, named entity recognition, extractive question\nanswering\nEncoder- TS (Google) Summarization, translation, question answering\ndecoder\nDecoder GPT-3 Text generation\n(OpenAl)\n\nA well-known example of an encoder Transformer is the Bidirectional Encoder Representations from\nTransformers (BERT) model, developed by Google (Devlin et al., 2018) that predicts missing words from a\nsentence, given context from both before and after the missing word in all layers.\n\nENCODER TRANSFORMERS\n\nEncoder Transformers are typically used for tasks that require an understanding of the input as a whole, such as sen-\ntence classification, named entity recognition, and extractive question answering. They are not used for text genera-\ntion tasks, so we will not explore them in detail in this book\u2014see Lewis Tunstall et al.\u2019s Natural Language Processing\nwith Transformers (O'Reilly) for more information.\n\nIn the following sections we will explore how encoder-decoder transformers work and discuss extensions\nof the original GPT model architecture released by OpenAl, including ChatGPT, which has been specifical-\nly designed for conversational applications.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.14,
                        "section_name": "TS",
                        "section_path": "./screenshots-images-2/chapter_9/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_14/7e936cbe-8ee4-4f50-a967-dd11c94762c4.png",
                            "./screenshots-images-2/chapter_9/section_14/0da9242c-7303-4121-85b0-61280b11ec1a.png",
                            "./screenshots-images-2/chapter_9/section_14/a29cabb2-93b9-45cf-bc56-5d5c5153ffa9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "TS\n\nAn example of a modern Transformer that uses the encoder-decoder structure is the T5 model from.\nGoogle. This model reframes a range of tasks into a text-to-text framework, including translation, linguis-\ntic acceptability, sentence similarity, and document summarization, as shown in Figure 9-12.\n\ntranslate Engl erman: That is good. Das ist gut\n\n\u201cstsb sentencel: The rhino grazed on the grass.\n\nsentence2: A rhino is grazing in a field.\u201d\n\n\u201csummarize: state authorities dispatched emergency crews\ntuesday to survey the damage after an onslaught of severe\nweather in mississippi...\u201d\n\n\u201csix people hospitalized after a\nstorm in attala county\n\nFigure 9-12. Examples of how TS reframes a range of tasks into a text-to-text framework, including translation, linguistic acceptability, sentence\nsimilarity, and document summarization (source: Raffel et al., 2019)\n\nThe T5 model architecture closely matches the encoder-decoder architecture used in the original\nTransformer paper, shown in Figure 9-13. The key difference is that TS is trained on an enormous 750 GB\ncorpus of text (the Colossal Clean Crawled Corpus, or C4), whereas the original Transformer paper was fo-\ncused only on language translation, so it was trained on 1.4 GB of English-German sentence pairs.\n\nOutput probabilities\n\nLinear\n\nL! Feed forward\n\nNx\n\nPositional encoding ACD Positional encoding\n\nInput embedding Output embedding\n\nInputs Outputs\n(shifted right)\n\nFigure 9-13. An encoder-decoder Transformer model: each gray box is a Transformer block (source: Vaswani et al., 2017)\n\nMuch of this diagram is already familiar to us\u2014we can see the Transformer blocks being repeated and\npositional embedding being used to capture the ordering of the input sequences. The two key differences\nbetween this model and the GPT model that we built earlier in the chapter are as follows:\n\n\u00a9 On the lefthand side, a set of encoder Transformer blocks encode the sequence to be translated.\nNotice that there is no causal masking on the attention layer. This is because we are not generating\nfurther text to extend the sequence to be translated; we just want to learn a good representation of\nthe sequence as a whole that can be fed to the decoder. Therefore, the attention layers in the encoder\ncan be completely unmasked to capture all the cross-dependencies between words, no matter the\norder.\n\nOn the righthand side, a set of decoder Transformer blocks generate the translated text. The initial\n\nattention layer is self-referential (i.e., the key, value, and query come from the same input) and causal\nmasking is used to ensure information from future tokens is not leaked to the current word to be\npredicted. However, we can then see that the subsequent attention layer pulls the key and value\nfrom the encoder, leaving only the query passed through from the decoder itself. This is called cross-\nreferential attention and means that the decoder can attend to the encoder representation of the in-\nput sequence to be translated. This is how the decoder knows what meaning the translation needs to\n\nconvey!\n\nFigure 9-14 shows an example of cross-referential attention. Two attention heads of the decoder layer are\nable to work together to provide the correct German translation for the word the, when used in the con-\ntext of the street. In German, there are three definite articles (der, die, das) depending on the gender of\nthe noun, but the Transformer knows to choose die because one attention head is able to attend to the\nword street (a feminine word in German), while another attends to the word to translate (the).\n\nAttention:\n\nInput - Output\n\nStraBe_\n\nnicht_\nwas_ weil_\ntoo_ es_\ntire Zu_\nd mu\n\nFigure 9-14. An example of how one attention head attends to the word \u201cthe\u201d and another attends to the word \u201cstreet\u201d in order to correctly\ntranslate the word \u201cthe\u201d to the German word \u201cdie\u201d as the feminine definite article of \u201cStrage\u201d\n\nTIP\n\nThis example is from the Tensor2Tensor GitHub repository, which contains a Colab notebook that allows you to play\n\naround with a trained encoder-decoder Transformer model and see how the attention mechanisms of the encoder\nand decoder impact the translation of a given sentence into German.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.15,
                        "section_name": "GPT-3 and GPT-4",
                        "section_path": "./screenshots-images-2/chapter_9/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_15/d4b34d37-0334-4763-9569-9ba38f7a6904.png",
                            "./screenshots-images-2/chapter_9/section_15/650f78ea-8aa5-4177-b9b5-d347841b0e94.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "GPT-3 and GPT-4\n\nSince the original 2018 publication of GPT, OpenAI has released multiple updated versions that improve\nupon the original model, as shown in Table 9-2.\n\nTable 9-2. The evolution of OpenAl\u2019s GPT collection of models\n\nWord\nAttention . Context ..\nModel Date Layers embedding | #parameters Training da\nheads . window\nsize\nGPT Jun 2018 12 12 768 512 120,000,000 BookCorpus\n4.5 GB of tex\nfrom unpub\n\nlished books\n\nGPT-2, Feb 2019 48 48 1,600 1,024 1,500,000,000 WebText: 40\nGB of text fr\noutbound\nReddit links\n\nGPT-3, May 2020 96 96 12,888 2,048 175,000,000,000 CommonCra\nWebText,\nEnglish\nWikipedia,\nbook corpor\nand others: :\nGB\n\nGPT-4 Mar 2023 - - - - 2 -\n\nThe model architecture of GPT-3 is fairly similar to the original GPT model, except it is much larger and\ntrained on much more data. At the time of writing, GPT-4 is in limited beta\u2014OpenAI has not publicly re-\nleased details of the model\u2019s structure and size, though we do know that it is able to accept images as in-\nput, so crosses over into being a multimodal model for the first time. The model weights of GPT-3 and\nGPT-4 are not open source, though the models are available through a commercial tool and API.\n\nGPT-3 can also be fine-tuned to your own training data\u2014this allows you to provide multiple examples of\n\nhow it should react to a given style of prompt by physically updating the weights of the network. In many\ncases this may not be necessary, as GPT-3 can be told how to react to a given style of prompt simply by\nproviding a few examples in the prompt itself (this is known as few-shot learning). The benefit of fine-tun-\ning is that you do not need to provide these examples as part of every single input prompt, saving costs in\nthe long run.\n\nAn example of the output from GPT-3, given a system prompt sentence, is shown in Figure 9-15.\n\nWrite me a short story about a unicorn who defeats an evil dragon.\n\nFigure 9-15. An example of how GPT-3 can extend a given system prompt\n\nLanguage models such as GPT benefit hugely from scaling\u2014both in terms of number of model weights\nand dataset size. The ceiling of large language model capability has yet to be reached, with researchers\ncontinuing to push the boundaries of what is possible with increasingly larger models and datasets.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.16,
                        "section_name": "ChatGPT",
                        "section_path": "./screenshots-images-2/chapter_9/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_16/716952b3-b974-41a4-b525-7287267904a3.png",
                            "./screenshots-images-2/chapter_9/section_16/ecb90f03-d535-4d14-94b5-cbbc2f4ea1f1.png",
                            "./screenshots-images-2/chapter_9/section_16/92ef5f7d-42f9-4f55-a776-9f89413a2604.png",
                            "./screenshots-images-2/chapter_9/section_16/20fcf6ad-02cc-47cc-9c44-78691424147e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "ChatGPT\n\nA few months before the beta release of GPT-4, OpenAI announced ChatGPT\u2014a tool that allows users to\ninteract with their suite of large language models through a conversational interface. The original release\nin November 2022 was powered by GPT-3.5, a version of the model that was more powerful that GPT-3\nand was fine-tuned to conversational responses.\n\nExample dialogue is shown in Figure 9-16. Notice how the agent is able to maintain state between inputs,\nunderstanding that the attention mentioned in the second question refers to attention in the context of\nTransformers, rather than a person\u2019s ability to focus.\n\nfos | What is a Transformer model and how is it used in generative deep learning? Can you give\na short ~100 word summary?\n\n[GA Transtormer model is a type of deep learning model used in natural language o>\nprocessing tasks such as language translation and text generation. It utilizes an attention\nmechanism to enable the model to focus on different parts of the input sequence during\nprocessing, allowing it to capture long-range dependencies and better model context.\n\nIn generative deep learning, a Transformer model can be used as a language model to\ngenerate new text that is similar to the input data it was trained on. This can be achieved\nby using the model's output at each time step as input to the next time step, and sampling\nfrom the output probability distribution to generate the next word in the sequence. With\nsufficient training data and appropriate tuning of hyperparameters, Transformer models\ncan produce high-quality, coherent text that is indistinguishable from text written by\nhumans.\n\nWhat's attention? Reply in the form of a limerick.\n\niS} Attention's a powerful tool oO?\nFor deep learning models to rule\nItlets them decide\nWhich parts to abide\n\u2018And focus on features that are cool\n\nIf a Transformer were an animal, what would it be and why?\n\n[J] ta Transformer were an animal, it would likely be a chameleon. oo?\n\nLike a Transformer, a chameleon has the ability to adapt and change its appearance to\nblend in with its environment, allowing it to be more effective in different situations.\nSimilarly, a Transformer model can adapt its attention to focus on different parts of input\nsequences based on the specific task it is performing, allowing it to excel at a wide range\nof natural language processing tasks.\n\nIn addition, both chameleons and Transformers have a reputation for being able to \"blend\nin\" seamlessly with their surroundings, making them difficult to detect. In the case of\nTransformers, this is often because the generated text is so coherent and similar to\nhuman-written text that it can be difficult to distinguish between the two.\n\nAt the time of writing, there is no official paper that describes how ChatGPT works in detail, but from the\nofficial blog post we know that it uses a technique called reinforcement learning from human feedback\n(RLHF) to fine-tune the GPT-3.5 model. This technique was also used in the ChatGPT group\u2019s earlier pa-\nper\u00ae that introduced the InstructGPT model, a fine-tuned GPT-3 model that is specifically designed to\nmore accurately follow written instructions.\n\nThe training process for ChatGPT is as follows:\n1. Supervised fine-tuning: Collect a demonstration dataset of conversational inputs (prompts) and de-\n\nsired outputs that have been written by humans. This is used to fine-tune the underlying language\nmodel (GPT-3.5) using supervised learning.\n\nN\n\n. Reward modeling: Present a human labeler with examples of prompts and several sampled model\noutputs and ask them to rank the outputs from best to worst. Train a reward model that predicts the\nscore given to each output, given the conversation history.\n\nwo\n\n. Reinforcement learning: Treat the conversation as a reinforcement learning environment where the\npolicy is the underlying language model, initialized to the fine-tuned model from step 1. Given the\ncurrent state (the conversation history) the policy outputs an action (a sequence of tokens), which is\nscored by the reward model trained in step 2. A reinforcement learning algorithm\u2014proximal policy\noptimization (PPO)\u2014can then be trained to maximize the reward, by adjusting the weights of the\nlanguage model.\n\nREINFORCEMENT LEARNING\nFor an introduction to reinforcement learning see Chapter 12, where we explore how generative models can be used\n\nin a reinforcement learning setting.\n\nThe RLHF process is shown in Figure 9-17.\n\nStep1_ Step 2 .\nCollect demonstration data and Collect comparison data and train a\ntrain a supervised policy reward model\n\nExplain reinforcement\n\nExplain reinforcement leaming toa six-year-old\n\nlearningto a six-year-\n\nExplain\nrewards...\npunishments\n\ntoteach.\n\n\u2018We give treats and\noutput behavior {punishments to teach..\n\nThis data is\nused to fine-\n\nStep3\nOptimize a policy against the reward model using the PPO reinforcement\nlearning algorithm\n\nAnew prompt is sampled from the dataset\nThe PPO modelis initialized from the supervised policy\n\nTheol gnetean ca\n\nThe reward model calculates a reward for the output\n\nThe reward is used to update the policy using PPO\n\nFigure 9-17. The reinforcement learning from human feedback fine-tuning process used in ChatGPT (source: OpenAL\n\nWhile ChatGPT still has many limitations (such as sometimes \u201challucinating\u201d factually incorrect informa-\ntion), it is a powerful example of how Transformers can be used to build generative models that can pro-\nduce complex, long-ranging, and novel output that is often indistinguishable from human-generated text.\nThe progress made thus far by models like ChatGPT serves as a testament to the potential of AI and its\ntransformative impact on the world.\n\nMoreover, it is evident that Al-driven communication and interaction will continue to rapidly evolve in\nthe future. Projects like Visual ChatGPT\u201d are now combining the linguistic power of ChatGPT with visual\nfoundation models such as Stable Diffusion, enabling users to interact with ChatGPT not only through\ntext, but also images. The fusion of linguistic and visual capabilities in projects like Visual ChatGPT and\nGPT-4 have the potential to herald a new era in human-computer interaction.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 9.17,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_9/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_9/section_17/f9f1d9c0-b23a-43e1-9d48-9240dd1b1f6a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter, we explored the Transformer model architecture and built a version of GPT\u2014a model for\n\nstate-of-the-art text generation.\n\nGPT makes use of a mechanism known as attention, which removes the need for recurrent layers (e.g.,\nLSTMs). It works like an information retrieval system, utilizing queries, keys, and values to decide how\nmuch information it wants to extract from each input token.\n\nAttention heads can be grouped together to form what is known as a multihead attention layer. These are\nthen wrapped up inside a Transformer block, which includes layer normalization and skip connections\naround the attention layer. Transformer blocks can be stacked to create very deep neural networks.\n\nCausal masking is used to ensure that GPT cannot leak information from downstream tokens into the cur-\nrent prediction. Also, a technique known as positional encoding is used to ensure that the ordering of the\ninput sequence is not lost, but instead is baked into the input alongside the traditional word embedding.\n\nWhen analyzing the output from GPT, we saw it was possible not only to generate new text passages, but\nalso to interrogate the attention layer of the network to understand where in the sentence it is looking to\ngather information to improve its prediction. GPT can access information at a distance without loss of sig-\nnal, because the attention scores are calculated in parallel and do not rely on a hidden state that is car-\nried through the network sequentially, as is the case with recurrent neural networks.\n\nWe saw how there are three families of Transformers (encoder, decoder, and encoder-decoder) and the\ndifferent tasks that can be accomplished with each. Finally, we explored the structure and training\nprocess of other large language models such as Google\u2019s T5 and OpenAl\u2019s ChatGPT.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 10,
                "chapter_name": "Chapter 10. Advanced GANs",
                "chapter_path": "./screenshots-images-2/chapter_10",
                "sections": [
                    {
                        "section_id": 10.1,
                        "section_name": "Chapter 10. Advanced GANs",
                        "section_path": "./screenshots-images-2/chapter_10/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_1/72970f4f-9240-4294-91d8-d98804d9f40f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 10. Advanced GANs\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\nSee how a ProGAN model progressively trains a GAN to generate high-resolution images.\nUnderstand how ProGAN was adapted to build StyleGAN, a high-performing GAN for image\nsynthesis.\n\nExplore how StyleGAN was adjusted to create StyleGAN2, a state-of-the-art model that improves fur-\nther upon the original work.\n\nLearn about the key contributions of these models, including progressive training, adaptive instance\nnormalization, weight modulation and demodulation, and path length regularization.\n\nWalk through the architecture of the Self-Attention GAN (SAGAN), which incorporates the attention\nmechanism into the GAN framework.\n\nSee how BigGAN expands upon the ideas in the SAGAN paper to produce high-quality images.\n\nLearn how VQ-GAN uses a codebook to encode images into a discrete sequence of tokens that can be\nmodeled using a Transformer.\n\nSee how ViT VQ-GAN adapts the VQ-GAN architecture to use Transformers instead of convolutional\nlayers in the encoder and decoder.\n\nChapter 4 introduced generative adversarial networks (GANS), a class of generative model that has pro-\n\nduced state-of-the-art results across a wide variety of image generation tasks. The flexibility in the model\n\narchitecture and training process has led academics and deep learning practitioners to find new ways to\n\ndesign and train GANs, leading to many different advanced flavors of the architecture that we shall ex-\n\nplore in this chapter.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_10/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_2/0ee9fe01-aebc-4f03-931c-f0b422d1bfc7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nExplaining all GAN developments and their repercussions in detail could easily fill another book. The\nGAN Zoo repository on GitHub contains over 500 distinct examples of GANs with linked papers, ranging\nfrom ABC-GAN to ZipNet-GAN!\n\nIn this chapter we will cover the main GANs that have been influential in the field, including a detailed\nexplanation of the model architecture and training process for each.\n\nWe will first explore three important models from NVIDIA that have pushed the boundaries of image\ngeneration: ProGAN, StyleGAN, and StyleGAN2. We will analyze each of these models in enough detail to\nunderstand the fundamental concepts that underpin the architectures and see how they have each built\non ideas from earlier papers.\n\nWe will also explore two other important GAN architectures that incorporate attention: the Self-Attention\nGAN (SAGAN) and BigGAN, which built on many of the ideas in the SAGAN paper. We have already seen\nthe power of the attention mechanism in the context of Transformers in Chapter 9.\n\nLastly, we will cover VQ-GAN and ViT VQ-GAN, which incorporate a blend of ideas from variational au-\n\ntoencoders, Transformers, and GANs. VQ-GAN is a key component of Google\u2019s state-of-the-art text-to-im-\n1\n\nage generation model Muse. We will explore so-called multimodal models in more detail in Chapter 13.\n\nTRAINING YOUR OWN MODELS\n\nFor conciseness I have chosen not to include code to directly build these models in the code repository for this book,\nbut instead will point to publicly available implementations where possible, so that you can train your own versions\nif you wish.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.3,
                        "section_name": "ProGAN",
                        "section_path": "./screenshots-images-2/chapter_10/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_3/dcd5ad61-376c-4691-8178-d633fe13fad5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "ProGAN\n\n2\nProGAN is a technique developed by NVIDIA Labs in 2017 to improve both the speed and stability of\nGAN training. Instead of immediately training a GAN on full-resolution images, the ProGAN paper sug-\ngests first training the generator and discriminator on low-resolution images of, say, 4 x 4 pixels and then\n\nincrementally adding layers throughout the training process to increase the resolution.\n\nLet\u2019s take a look at the concept of progressive training in more detail.\n\nTRAINING YOUR OWN PROGAN\nThere is an excellent tutorial by Bharath K on training your own ProGAN using Keras available on the Paperspace\nblog. Bear in mind that training a ProGAN to achieve the results from the paper requires a significant amount of\n\ncomputing power.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.4,
                        "section_name": "Progressive Training",
                        "section_path": "./screenshots-images-2/chapter_10/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_4/a130b085-366f-4aeb-a638-e9e0cbcaf7a7.png",
                            "./screenshots-images-2/chapter_10/section_4/bb026eba-2b42-4158-b698-fca9e0a45b92.png",
                            "./screenshots-images-2/chapter_10/section_4/2e571fb7-1f08-474c-a217-f2e1d1d0ab4e.png",
                            "./screenshots-images-2/chapter_10/section_4/afad7335-5902-4f40-87d9-ec2ae2144676.png",
                            "./screenshots-images-2/chapter_10/section_4/4bc4fa24-0de7-4b3f-8c81-1cc4c7844a22.png",
                            "./screenshots-images-2/chapter_10/section_4/869337a3-deda-4246-9954-61452837ff79.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Progressive Training\n\nAs always with GANs, we build two independent networks, the generator and discriminator, with a fight\n\nfor dominance taking place during the training process.\n\nIna normal GAN, the generator always outputs full-resolution images, even in the early stages of train-\ning. It is reasonable to think that this strategy might not be optimal\u2014the generator might be slow to learn\nhigh-level structures in the early stages of training, because it is immediately operating over complex,\nhigh-resolution images. Wouldn\u2019t it be better to first train a lightweight GAN to output accurate low-reso-\nlution images and then see if we can build on this to gradually increase the resolution?\n\nThis simple idea leads us to progressive training, one of the key contributions of the ProGAN paper. The\nProGAN is trained in stages, starting with a training set that has been condensed down to 4 x 4~pixel im-\n\nages using interpolation, as shown in Figure 10-1.\n\n\u201cQRZ at IFoo\na\n\n~ SAAT Ie\n\n\u00abtb BASS iee\n\n~ ABR BASE Ie\n\nABR BAA Ieee\n\nFigure 10-1. Images in the dataset can be compressed to lower resolution using interpolation\n\nWe can then initially train the generator to transform a latent input noise vector z (say, of length 512) into\nan image of shape 4 x 4 x 3. The matching discriminator will need to transform an input image of size 4 x\n4x 3 into a single scalar prediction. The network architectures for this first step are shown in Figure 10-2.\n\nThe blue box in the generator represents the convolutional layer that converts the set of feature maps\ninto an RGB image ( torGB ), and the blue box in the discriminator represents the convolutional layer\nthat converts the RGB images into a set of feature maps ( fromRGB ).\n\nGenerator Discriminator\n\nZ\n\n1x1%512\n\nfromRGB} filters = 512\n\n4x4x512 4x4x512\n\n4x4x5]2 1*1x512\n\nFigure 10-2. The generator and discriminator architectures for the first stage of the ProGAN training process\n\nIn the paper, the authors train this pair of networks until the discriminator has seen 800,000 real images.\nWe now need to understand how the generator and discriminator are expanded to work with 8 x 8-pixel\nimages.\n\nTo expand the generator and discriminator, we need to blend in additional layers. This is managed in two\n\nphases, transition and stabilization, as shown in Figure 10-3.\n\nExisting 4 x 4 generator Transition Stabilization\nZ Z\n\n4x4x512 8x8x512\n\n4x4x3\n\nFigure 10-3. The ProGAN generator training process, expanding the network from 4 x 4 images to 8 x 8 (dotted lines represent the rest of the net-\nwork, not shown)\n\nLet\u2019s first look at the generator. During the transition phase, new upsampling and convolutional layers\nare appended to the existing network, with a residual connection set up to maintain the output from the\nexisting trained toRGB layer. Crucially, the new layers are initially masked using a parameter a that is\ngradually increased from 0 to 1 throughout the transition phase to allow more of the new toRGB output\nthrough and less of the existing toRGB layer. This is to avoid a shock to the network as the new layers\n\ntake over.\n\nEventually, there is no flow through the old torGB layer and the network enters the stabilization phase\u2014\na further period of training where the network can fine-tune the output, without any flow through the\nold torGB layer.\n\nThe discriminator uses a similar process, as shown in Figure 10-4.\n\nStabilization\n\nExisting 4 = 4 discriminator Transition\n\n4x4x512 8x8x512\n\n8x8 x512\n\nv\n\nFigure 10-4. The ProGAN discriminator training process, expanding the network from 4 \u00ab 4 images to 8 x 8 (dotted lines represent the rest of the\nnetwork, not shown)\n\nHere, we need to blend in additional downscaling and convolutional layers. Again, the layers are injected\ninto the network\u2014this time at the start of the network, just after the input image. The existing fromRGB\nlayer is connected via a residual connection and gradually phased out as the new layers take over during\nthe transition phase. The stabilization phase allows the discriminator to fine-tune using the new layers.\n\nAll transition and stabilization phases last until the discriminator has been shown 800,000 real images.\nNote that even through the network is trained progressively, no layers are frozen. Throughout the train-\ning process, all layers remain fully trainable.\n\nThis process continues, growing the GAN from 4 x 4 images to 8 x 8, then 16 x 16, 32 x 32, and so on, until\n\nit reaches full resolution (1,024 x 1,024), as shown in Figure 10-5.\n\nLatent Latent Latent\n\n[  }\n\n1024 x 1024\na\n+ *Reals\n\n1024 = 1024\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nD Training progresses\n\nFigure 10-5. The ProGAN training mechanism, and some example generated faces (source: Karras et al, 2017\n\nThe overall structure of the generator and discriminator after the full progressive training process is\ncomplete is shown in Figure 10-6.\n\n[Generator Act. \u2018Output shape Params Act. Output shape Params\nLatent vector - -\u2018Si2x 1 x T = image = 3 x 1024 x 108 =\nCony 4 x 4 tRelU =SI2x 4 x 4 42M Conv1 x 1 \u2018LReLU 16 x 1024 cy\nCony 3 x 3 RelU  SI2x 4 x 4 24M. Conv 3 x 3 \u2018LReLU x 1024 23k\n\u2018Upsample - S12 x x - Cony 3 x 3 \u2018LReLU 32 x 1024 x 1024 4.6k\nConv 3 x 3 Rhu =SI2x 8 x 8 24M Downsample - 32_x S12 _x S12 -\nCony 3 x 3 LRU SI2x 8 x 8 24M. Conv 3 x 3 \u2018LReLU 32 x S12 x S12 92k\n\n| Upsample = 3i2x 16 x 16 = Conv 3 x 3 LRU 64 x S12 x SIZ 18k\nConw3x3 LRU SI2x 16 x 16 24M Downsample = __ 64 x 256 x 256 =\nConv3x 3 iRelU SI2x 16 x 16 24M Conv3 x 3 TReLU 64 x 256 x 256 37k\nUpsample = Sizx 32 x 32 = Conv 3 x 3 LReLU 128 x 256 x 256 74k\nConw3x3 LRU S12x 32 x 32 24M Downsample = 128 x 128 x 128 =\nConv3 x3 LReLU S12 x_32_x_32 24M [Cons x 5 TRU 128 x 128 x 128 Tad\nUpsample = Six 6 x OF = Conv 3 x 3 LReLU 256. 128 x 128 295k\nConv3x3 LRU 256 64 x 64 = L2M Downsample - 256 x_64 x 64 =\nConv3 x3 _LReLU _256x_64 x 64 590K Conv 3 x 3 TRL 256x 64 x 64 350K\n\u2018Upsample - 236 x 128 x 128 = Conv 3 x 3 LRU SI2x 64 x 64 12M\nConv 3 x 3 LRU = 128 x 128 x 128 295k Downsample - 512 x_32_x_32 -\nConv3 x3 LReLU 128 x 128 x 128 148k Conv x 5 TRU 512% 32x 32 74M\n\u2018Upsample - 128 x 256 x 256 - Cony 3 x 3 LRU =5S12x 32 x 32 24M\nConv3 x3  LRLU 64x 256 x 256 74k Downsample - _512x_ 16 x 16 =\nConv 3 x 3 LReLU 64 x 256 x 256 37% Conv 3 x 3 RLU S12 16 x 16 24M\nUpsample = 6 x 512 x SI2 = Conv 3 x 3 LRU 512x 16 x 16 24M\nConv3x3 LRU 32 x S12 x 512 18k Downsample = __5I2x_ 8 x 8 =\nConv3 x3 ike 32 x $12 x 512 9.2K [eons se3 TRU 512x 8 x 8 7am\nUpsample = 32 x 1024 x 1024 = Conv 3 x 3 tke 512x 8 x 8 24M\nCony 3 x 3 \u2018LReLU 16 x 1024 x 1024 4.6k Downsample Si2x 4 x 4 -\nCony 3 x 3 \u2018LReLU 16 x 1024 x 1024 23k Minibatch stddev = sx Tx F\nConv 1x1 Tinear__3_x 1024 x 1024 31 Conv 3 x 3 LReLU  SI2x 4 x 4\n\u2018Total trainable parameters 2M Conv 4 x 4 Rey SI2X 1 x 1\n\nFully-connected _linear 1 x 1 x 1\n\u201cTotal trainable parameters\n\n\nFigure 10-6. The ProGAN generator and discriminator used to generate 1,024 x 1,024-pixel CelebA faces (source: Karras et al. 2018;\n\nThe paper also makes several other important contributions, namely minibatch standard deviation,\nequalized learning rates, and pixelwise normalization, which are described briefly in the following\nsections.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.5,
                        "section_name": "Minibatch standard deviation",
                        "section_path": "./screenshots-images-2/chapter_10/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_5/16d2a9c4-afd7-47cf-9d20-9a86092cc7da.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Minibatch standard deviation\n\nThe minibatch standard deviation layer is an extra layer in the discriminator that appends the standard\ndeviation of the feature values, averaged across all pixels and across the minibatch as an additional (con-\nstant) feature. This helps to ensure the generator creates more variety in its output\u2014if variety is low\nacross the minibatch, then the standard deviation will be small, and the discriminator can use this fea-\nture to distinguish the fake batches from the real batches! Therefore, the generator is incentivized to en-\nsure it generates a similar amount of variety as is present in the real training data.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.6,
                        "section_name": "Equalized learning rates",
                        "section_path": "./screenshots-images-2/chapter_10/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_6/aaad3b5d-48d2-46f6-ae04-b4a70b1205b2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Equalized learning rates\n\nAll dense and convolutional layers in ProGAN use equalized learning rates. Usually, weights in a neural\nnetwork are initialized using a method such as He initialization\u2014a Gaussian distribution where the stan-\ndard deviation is scaled to be inversely proportional to the square root of the number of inputs to the lay-\ner. This way, layers with a greater number of inputs will be initialized with weights that have a smaller\ndeviation from zero, which generally improves the stability of the training process.\n\nThe authors of the ProGAN paper found that this was causing problems when used in combination with\nmodern optimizers such as Adam or RMSProp. These methods normalize the gradient update for each\nweight, so that the size of the update is independent of the scale (magnitude) of the weight. However, this\nmeans that weights with a larger dynamic range (i.e., layers with fewer inputs) will take comparatively\nlonger to adjust than weights with a smaller dynamic range (i.e., layers with more inputs). It was found\nthat this causes an imbalance between the speed of training of the different layers of the generator and\ndiscriminator in ProGAN, so they used equalized learning rates to solve this problem.\n\nIn ProGAN, weights are initialized using a simple standard Gaussian, regardless of the number of inputs\nto the layer. The normalization is applied dynamically, as part of the call to the layer, rather than only at\ninitialization. This way, the optimizer sees each weight as having approximately the same dynamic range,\nso it applies the same learning rate. It is only when the layer is called that the weight is scaled by the fac-\ntor from the He initializer.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.7,
                        "section_name": "Pixelwise normalization",
                        "section_path": "./screenshots-images-2/chapter_10/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_7/720b20a3-1bb8-4f9a-b7a2-1f88e28af2fb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Pixelwise normalization\n\nLastly, in ProGAN pixelwise normalization is used in the generator, rather than batch normalization. This\nnormalizes the feature vector in each pixel to a unit length and helps to prevent the signal from spiraling\nout of control as it propagates through the network. The pixelwise normalization layer has no trainable\nweights.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.8,
                        "section_name": "Outputs",
                        "section_path": "./screenshots-images-2/chapter_10/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_8/399e010d-5286-40b8-af53-8266d9d5e92d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Outputs\n\nIn addition to the CelebA dataset, ProGAN was also applied to images from the Large-scale Scene\nUnderstanding (LSUN) dataset with excellent results, as shown in . This demonstrated the\npower of ProGAN over earlier GAN architectures and paved the way for future iterations such as\nStyleGAN and StyleGAN2, which we shall explore in the next sections.\n\n: \u2014 . a\nPOTTEDPLANT HORSE SOFA BUS CHURCHOUTDOOR = BICYCLE TVMONITOR\n\nFigure 10-7. Generated examples from a ProGAN trained progressively on the LSUN dataset at 256 * 256 resolution (source:\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.9,
                        "section_name": "StyleGAN",
                        "section_path": "./screenshots-images-2/chapter_10/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_9/7a7a57e1-77e8-4b80-9c5f-b4c27fb08504.png",
                            "./screenshots-images-2/chapter_10/section_9/31d1ada3-6c7b-4175-b535-649b6a7f7cb6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "StyleGAN\n\n3\nStyleGAN is a GAN architecture from 2018 that builds on the earlier ideas in the ProGAN paper. In fact,\nthe discriminator is identical; only the generator is changed.\n\nOften when training GANs it is difficult to separate out vectors in the latent space corresponding to high-\nlevel attributes\u2014they are frequently entangled, meaning that adjusting an image in the latent space to\ngive a face more freckles, for example, might also inadvertently change the background color. While\nProGAN generates fantastically realistic images, it is no exception to this general rule. We would ideally\nlike to have full control of the style of the image, and this requires a disentangled separation of features\nin the latent space.\n\nStyleGAN achieves this by explicitly injecting style vectors into the network at different points: some that\ncontrol high-level features (e.g., face orientation) and some that control low-level details (e.g., the way the\nhair falls across the forehead).\n\nThe overall architecture of the StyleGAN generator is shown in Figure 10-8. Let\u2019s walk through this archi-\ntecture step by step, starting with the mapping network.\n\nLatent z \u20ac Z Synthesis network g\n\nFigure 10-8. The StyleGAN generator architecture (source: Karras et al,.2018)\n\nTRAINING YOUR OWN STYLEGAN\n\nThere is an excellent tutorial by Soon-Yau Cheong on training your own StyleGAN using Keras available on the Keras\nwebsite. Bear in mind that training a StyleGAN to achieve the results from the paper requires a significant amount of\n\ncomputing power.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.1,
                        "section_name": "The Mapping Network",
                        "section_path": "./screenshots-images-2/chapter_10/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_10/2b2fe0b5-886e-4b6a-856d-e7d947e5cca3.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Mapping Network\n\nThe mapping network f is a simple feed-forward network that converts the input noise z \u20ac & into a dif-\nferent latent space w \u20ac W. This gives the generator the opportunity to disentangle the noisy input vector\ninto distinct factors of variation, which can be easily picked up by the downstream style-generating\nlayers.\n\nThe point of doing this is to separate out the process of choosing a style for the image (the mapping net-\nwork) from the generation of an image with a given style (the synthesis network).\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.11,
                        "section_name": "The Synthesis Network",
                        "section_path": "./screenshots-images-2/chapter_10/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_11/9c077cb2-b8a7-4d31-8143-a45f57262d7f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Synthesis Network\n\nThe synthesis network is the generator of the actual image with a given style, as provided by the mapping\nnetwork. As can be seen from Figure 10-8, the style vector w is injected into the synthesis network at dif-\nferent points, each time via a differently densely connected layer A;, which generates two vectors: a bias\nvector y,,; and a scaling vector y,;. These vectors define the specific style that should be injected at this\npoint in the network\u2014that is, they tell the synthesis network how to adjust the feature maps to move the\ngenerated image in the direction of the specified style.\n\nThis adjustment is achieved through adaptive instance normalization (AdaIN) layers.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.12,
                        "section_name": "Adaptive instance normalization",
                        "section_path": "./screenshots-images-2/chapter_10/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_12/a286d2af-20c2-4bf8-869b-08b447ad09de.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Adaptive instance normalization\n\nAn AdaIN layer is a type of neural network layer that adjusts the mean and variance of each feature map\nx; with a reference style bias yy, and scale ys,;, respectively\u201d Both vectors are of length equal to the\nnumber of channels output from the preceding convolutional layer in the synthesis network. The equa-\ntion for adaptive instance normalization is as follows:\n\nxi = HC\n\nAdalN (xi, ) = Ys ta)\n\n+ Ybi\n\nThe adaptive instance normalization layers ensure that the style vectors that are injected into each layer\nonly affect features at that layer, by preventing any style information from leaking through between lay-\ners. The authors show that this results in the latent vectors w being significantly more disentangled than\nthe original z vectors.\n\nSince the synthesis network is based on the ProGAN architecture, it is trained progressively. The style vec-\ntors at earlier layers in the synthesis network (when the resolution of the image is lowest\u20144 x 4, 8 x 8)\nwill affect coarser features than those later in the network (64 x 64 to 1,024 x 1,024-pixel resolution). This\nmeans that not only do we have complete control over the generated image through the latent vector w,\nbut we can also switch the w vector at different points in the synthesis network to change the style at a\nvariety of levels of detail.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.13,
                        "section_name": "Style mixing",
                        "section_path": "./screenshots-images-2/chapter_10/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_13/a6cea6a7-27b5-402f-bb50-20ea6f00462a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Style mixing\n\nThe authors use a trick known as style mixing to ensure that the generator cannot utilize correlations be-\ntween adjacent styles during training (i.e., the styles injected at each layer are as disentangled as\npossible). Instead of sampling only a single latent vector z, two are sampled (z1, 22), corresponding to\ntwo style vectors (w1, W2). Then, at each layer, either (w1 or w2) is chosen at random, to break any\npossible correlation between the vectors.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.14,
                        "section_name": "Stochastic variation",
                        "section_path": "./screenshots-images-2/chapter_10/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_14/63ba825f-f94f-4b0b-9e8f-757b4022af46.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stochastic variation\n\nThe synthesizer network adds noise (passed through a learned broadcasting layer B) after each convolu-\ntion to account for stochastic details such as the placement of individual hairs, or the background behind\nthe face. Again, the depth at which the noise is injected affects the coarseness of the impact on the image.\n\nThis also means that the initial input to the synthesis network can simply be a learned constant, rather\nthan additional noise. There is enough stochasticity already present in the style inputs and the noise in-\nputs to generate sufficient variation in the images.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.15,
                        "section_name": "Outputs from StyleGAN",
                        "section_path": "./screenshots-images-2/chapter_10/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_15/023febd0-896b-43e7-a231-de067d8226bd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Outputs from StyleGAN\n\nshows StyleGAN in action.\n\nSource A\n\nCoarse styles from source B\n\na\n@\n2\nS\nfo)\na\niS\n2\nar\n2\nFJ\n&\n2\n>\n=\n=]\nco\n=\n(=\nR=\n@\ni=\ncz\n\nFigure 10-9. Merging styles between two generated images at different levels of detail (source: )\n\nHere, two images, source A and source B, are generated from two different w vectors. To generate a\nmerged image, the source A w vector is passed through the synthesis network but, at some point,\nswitched for the source B w vector. If this switch happens early on (4 x 4 or 8 x 8 resolution), coarse styles\nsuch as pose, face shape, and glasses from source B are carried across onto source A. However, if the\nswitch happens later, only fine-grained detail is carried across from source B, such as colors and mi-\ncrostructure of the face, while the coarse features from source A are preserved.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.16,
                        "section_name": "StyleGAN2",
                        "section_path": "./screenshots-images-2/chapter_10/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_16/7d6780be-dbc2-44db-85d0-3a643699578e.png",
                            "./screenshots-images-2/chapter_10/section_16/60a94953-fbea-4ab0-9e84-21023f370ea4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "StyleGAN2\n\nThe final contribution in this chain of important GAN papers is StyleGAN2. This builds further upon the\nStyleGAN architecture, with some key changes that improve the quality of the generated output. In partic-\nular, StyleGAN2 generations do not suffer as greatly from artifacts\u2014water droplet-like areas of the image\nthat were found to be caused by the adaptive instance normalization layers in StyleGAN, as shown in\n\nFigure 10-10. An artifact in a StyleGAN-generated image of a face (source: \u00bb)\n\nBoth the generator and the discriminator in StyleGAN2 are different from the StyleGAN. In the next sec-\ntions we will explore the key differences between the architectures.\n\n\nTRAINING YOUR OWN STYLEGAN2\nThe official code for training your own StyleGAN using TensorFlow is available on GitHub. Bear in mind that training\na StyleGAN2 to achieve the results from the paper requires a significant amount of computing power.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.17,
                        "section_name": "Weight Modulation and Demodulation",
                        "section_path": "./screenshots-images-2/chapter_10/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_17/1fb46c76-c2c8-4de1-835a-80a41688435c.png",
                            "./screenshots-images-2/chapter_10/section_17/4c3b467b-e1f2-42fc-9eac-c00cbee996a4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Weight Modulation and Demodulation\n\nThe artifact problem is solved by removing the AdaIN layers in the generator and replacing them with\nweight modulation and demodulation steps, as shown in Figure 10-11. w represents the weights of the\nconvolutional layer, which are directly updated by the modulation and demodulation steps in StyleGAN2\nat runtime. In comparison, the AdaIN layers of StyleGAN operate on the image tensor as it flows through\nthe network.\n\nThe AdaIN layer in StyleGAN is simply an instance normalization followed by style modulation (scaling\nand bias). The idea in StyleGANZ is to apply style modulation and normalization (demodulation) directly\nto the weights of the convolutional layers at runtime, rather than the output from the convolutional lay-\ners, as shown in Figure 10-11. The authors show how this removes the artifact issue while retaining con-\ntrol of the image style.\n\nStyleGAN style block StyleGAN2 style block\n\n3\n\nie IN\n\nMod\n\nDemod\n\nMod\n\nDemod\n\nFigure 10-11. A comparison between the StyleGAN and StyleGAN2 style blocks\n\nIn StyleGAN2, each dense layer A outputs a single style vector s;, where 7 indexes the number of input\nchannels in the corresponding convolutional layer. This style vector is then applied to the weights of the\nconvolutional layer as follows:\n\nEND ONE AD DEEN OE\n\nWijk = Si Wijk\n\nHere, j indexes the output channels of the layer and k indexes the spatial dimensions. This is the modula-\n\ntion step of the process.\n\nThen, we need to normalize the weights so that they again have a unit standard deviation, to ensure sta-\n\nbility in the training process. This is the demodulation step:\n\nwv. = \u2014\niisk 2\nVv Dik ijk +e\n\nwhere \u20ac is a small constant value that prevents division by zero.\nIn the paper, the authors show how this simple change is enough to prevent water-droplet artifacts, while\n\nretaining control over the generated images via the style vectors and ensuring the quality of the output\n\nremains high.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.18,
                        "section_name": "Path Length Regularization",
                        "section_path": "./screenshots-images-2/chapter_10/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_18/a4fa57c5-57bb-411b-b10c-f5b5b03f3967.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Path Length Regularization\n\nAnother change made to the StyleGAN architecture is the inclusion of an additional penalty term in the\nloss function\u2014this is known as path length regularization.\n\nWe would like the latent space to be as smooth and uniform as possible, so that a fixed-size step in the la-\ntent space in any direction results in a fixed-magnitude change in the image.\n\nTo encourage this property, StyleGAN2 aims to minimize the following term, alongside the usual\nWasserstein loss with gradient penalty:\n\n2\nEyyJiylly \u2014 a)\n\nHere, w is a set of style vectors created by the mapping network, y is a set of noisy images drawn from\n\ni\n\nAN (0,1), and J, = ae is the Jacobian of the generator network with respect to the style vectors.\n\nThe term J Lyll 2 Measures the magnitude of the images y after transformation by the gradients given in\nthe Jacobian. We want this to be close to a constant a, which is calculated dynamically as the exponential\nmoving average of J) yl| , as the training progresses.\n\nThe authors find that this additional term makes exploring the latent space more reliable and consistent.\nMoreover, the regularization terms in the loss function are only applied once every 16 minibatches, for\nefficiency. This technique, called lazy regularization, does not cause a measurable drop in performance.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.19,
                        "section_name": "No Progressive Growing",
                        "section_path": "./screenshots-images-2/chapter_10/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_19/ece17459-e8a5-4e60-94bc-822d9e614ce0.png",
                            "./screenshots-images-2/chapter_10/section_19/748e0ae0-e2fd-4314-98df-27267b5ad246.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "No Progressive Growing\n\nAnother major update is in how StyleGAN2 is trained. Rather than adopting the usual progressive train-\ning mechanism, StyleGAN2 utilizes skip connections in the generator and residual connections in the dis-\ncriminator to train the entire network as one. It no longer requires different resolutions to be trained in-\ndependently and blended as part of the training process.\n\nFigure 10-12 shows the generator and discriminator blocks in StyleGAN2.\n\nGenerator block Discriminator block\n\nMod\n\nDemod\n\nMod\n\nDemod\n\ntoRGB\n\nFigure 10-12. The generator and discriminator blocks in StyleGAN2\n\nThe crucial property that we would like to be able to preserve is that the StyleGAN2 starts by learning\nlow-resolution features and gradually refines the output as training progresses. The authors show that\nthis property is indeed preserved using this architecture. Each network benefits from refining the convo-\nlutional weights in the lower-resolution layers in the earlier stages of training, with the skip and residual\nconnections used to pass the output through the higher-resolution layers mostly unaffected. As training\nprogresses, the higher-resolution layers begin to dominate, as the generator discovers more intricate\nways to improve the realism of the images in order to fool the discriminator. This process is demonstrat-\ned in Figure 10-13.\n\n80%\n\n1024 x 1024\n\n60%\n\nContribution\n40%\n\n20%\n\n0%.\n\n10 5 20 2\nTraining progress (million images)\n\nFigure 10-13. The contribution of each resolution layer to the output of the generator, by training time (adapted from Karras et al,.2019\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.2,
                        "section_name": "Outputs from StyleGAN2",
                        "section_path": "./screenshots-images-2/chapter_10/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_20/1b16d941-1a66-41e8-b3e8-2c0a7077e20a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Outputs from StyleGAN2\n\nSome examples of StyleGAN2 output are shown in . To date, the StyleGAN2 architecture (and\nscaled variations such as StyleGAN-XL ) remain state of the art for image generation on datasets such as\nFlickr-Faces-HQ (FFHQ) and CIFAR-10, according to the benchmarking website\n\nFigure 10-14. Uncurated StyleGAN2 output for the FFHQ face dataset and LSUN car dataset (source: y)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.21,
                        "section_name": "Other Important GANs",
                        "section_path": "./screenshots-images-2/chapter_10/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_21/21180101-465e-452d-9ddc-ef7cb2af69e5.png",
                            "./screenshots-images-2/chapter_10/section_21/cdca55ec-7f94-4305-a2c6-f7756a5ef9a7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Other Important GANs\n\nIn this section, we will explore two more architectures that have also contributed significantly to the d\nvelopment of GANs\u2014SAGAN and BigGAN.\n\nOther Important GANs\n\nIn this section, we will explore two more architectures that have also contributed significantly to the de-\nvelopment of GANs\u2014SAGAN and BigGAN.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.22,
                        "section_name": "Self-Attention GAN (SAGAN)",
                        "section_path": "./screenshots-images-2/chapter_10/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_22/ad2c56ba-616a-4c45-9cf5-9464be2bf102.png",
                            "./screenshots-images-2/chapter_10/section_22/fc0975b1-57ca-47ec-aaf7-bbdc31a9a14b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Self-Attention GAN (SAGAN)\nThe Self-Attention GAN (SAGAN)\u201d is a key development for GANs as it shows how the attention mecha-\nnism that powers sequential models such as the Transformer can also be incorporated into GAN-based\n\nmodels for image generation. Figure 10-15 shows the self-attention mechanism from the paper introduc-\ning this architecture.\n\n1x conv\nfo, Transpose\nAttention map\nConvolution Softmax\nfeature maps (x) 1\nconv Self-attention\n[|s 0, feature maps (0)\n\n1*1conv\n\n[> hi)\n\nFigure 10-15. The self-attention mechanism within the SAGAN model (source: Zhang et al., 2018,\n\nThe problem with GAN-based models that do not incorporate attention is that convolutional feature maps\nare only able to process information locally. Connecting pixel information from one side of an image to\nthe other requires multiple convolutional layers that reduce the size of the image, while increasing the\nnumber of channels. Precise positional information is reduced throughout this process in favor of captur-\ning higher-level features, making it computationally inefficient for the model to learn long-range depen-\ndencies between distantly connected pixels. SAGAN solves this problem by incorporating the attention\nmechanism that we explored earlier in this chapter into the GAN. The effect of this inclusion is shown in\n\nFigure 10-16.\n.\nee\n\nFigure 10-16. A SAGAN-generated image of a bird (leftmost cell) and the attention maps of the final attention-based generator layer for the pixels\ncovered by the three colored dots (rightmost cells) (source: Zhang et al., 2018\n\nThe red dot is a pixel that is part of the bird\u2019s body, and so attention naturally falls on the surrounding\nbody cells. The green dot is part of the background, and here the attention actually falls on the other side\nof the bird\u2019s head, on other background pixels. The blue dot is part of the bird\u2019s long tail and so attention\nfalls on other tail pixels, some of which are distant from the blue dot. It would be difficult to maintain this\nlong-range dependency for pixels without attention, especially for long, thin structures in the image (such\nas the tail in this case).\n\nTRAINING YOUR OWN SAGAN\n\nThe official code for training your own SAGAN using TensorFlow is available on GitHub. Bear in mind that training a\nSAGAN to achieve the results from the paper requires a significant amount of computing power.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.23,
                        "section_name": "BigGAN",
                        "section_path": "./screenshots-images-2/chapter_10/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_23/be28ffbc-ac7e-4aab-8111-22e2f5aef1b7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "BigGAN\n\n8\nBigGAN, developed at DeepMind, extends the ideas from the SAGAN paper. Figure 10-17 shows some of\nthe images generated by BigGAN, trained on the ImageNet dataset at 128 x 128 resolution.\n\nFigure 10-17. Examples of images generated by BigGAN (source: Brock et al., 2018\n\nAs well as some incremental changes to the base SAGAN model, there are also several innovations out-\nlined in the paper that take the model to the next level of sophistication. One such innovation is the so-\ncalled truncation trick. This is where the latent distribution used for sampling is different from the\n\nz~ MN(0,1) distribution used during training. Specifically, the distribution used during sampling is a\ntruncated normal distribution (resampling values of z that have magnitude greater than a certain thresh-\nold). The smaller the truncation threshold, the greater the believability of generated samples, at the ex-\npense of reduced variability. This concept is shown in Figure 10-18.\n\nFigure 10-18. The truncation trick: from left to right, the threshold is set to 2, 1, 0.5, and 0.04 (source: Brock et al. 2018)\n\nAlso, as the name suggests, BigGAN is an improvement over SAGAN in part simply by being bigger.\nBigGAN uses a batch size of 2,048\u20148 times larger than the batch size of 256 used in SAGAN\u2014and a chan-\nnel size that is increased by 50% in each layer. However, BigGAN additionally shows that SAGAN can be\nimproved structurally by the inclusion of a shared embedding, by orthogonal regularization, and by in-\ncorporating the latent vector z into each layer of the generator, rather than just the initial layer.\n\nFor a full description of the innovations introduced by BigGAN, I recommend reading the original paper\nand accompanying presentation material.\n\nUSING BIGGAN\n\nA tutorial for generating images using a pre-trained BigGAN is available on the TensorFlow website.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.24,
                        "section_name": "VQ-GAN",
                        "section_path": "./screenshots-images-2/chapter_10/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_24/5fe64cdc-18c9-4236-a94e-b85ce44275fc.png",
                            "./screenshots-images-2/chapter_10/section_24/8335d9b1-012b-46b0-aba3-82ad972e9d6f.png",
                            "./screenshots-images-2/chapter_10/section_24/13181e8c-e9d6-4c27-b0b9-fe538e31239d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "VQ-GAN\n\nAnother important type of GAN is the Vector Quantized GAN (VQ-GAN), introduced in 2020. This model\narchitecture builds upon an idea introduced in the 2017 paper \u201cNeural Discrete Representation\nLearning\u201d\u201d\u2014namely, that the representations learned by a VAE can be discrete, rather than continuous.\nThis new type of model, the Vector Quantized VAE (VQ-VAE), was shown to generate high-quality images\nwhile avoiding some of the issues often seen with traditional continuous latent space VAEs, such as poste-\nrior collapse (where the learned latent space becomes uninformative due to an overly powerful decoder).\n\nTIP\n\nThe first version of DALL.E, a text-to-image model released by OpenAl in 2021 (see Chapter 13), utilized a VAE witha\ndiscrete latent space, similar to VQ-VAE.\n\nBy a discrete latent space, we mean a learned list of vectors (the codebook), each associated with a corre-\nsponding index. The job of the encoder in a VQ-VAE is to collapse the input image to a smaller grid of vec-\ntors that can then be compared to the codebook. The closest codebook vector to each grid square vector\n(by Euclidean distance) is then taken forward to be decoded by the decoder, as shown in Figure 10-19.\nThe codebook is a list of learned vectors of length d (the embedding size) that matches the number of\nchannels in the output of the encoder and input to the decoder. For example, e; is a vector that can be in-\nterpreted as background.\n\n\u20ac1 & 65 & bs Os\n\nAlignment and commitment loss\n\nReconstruction loss\n\nFigure 10-19. A diagram of a VQ-VAE\n\nThe codebook can be thought of as a set of learned discrete concepts that are shared by the encoder and\ndecoder in order to describe the contents of a given image. The VQ-VAE must find a way to make this set\nof discrete concepts as informative as possible so that the encoder can accurately label each grid square\nwith a particular code vector that is meaningful to the decoder. The loss function for a VQ-VAE is there-\nfore the reconstruction loss added to two terms (alignment and commitment loss) that ensure that the\noutput vectors from the encoder are as close as possible to vectors in the codebook. These terms replace\nthe the KL divergence term between the encoded distribution and the standard Gaussian prior in a typi-\ncal VAE.\n\nHowever, this architecture poses a question\u2014how do we sample novel code grids to pass to the decoder\nto generate new images? Clearly, using a uniform prior (picking each code with equal probability for\neach grid square) will not work. For example in the MNIST dataset, the top-left grid square is highly likely\nto be coded as background, whereas grid squares toward the center of the image are not as likely to be\ncoded as such. To solve this problem, the authors used another model, an autoregressive PixelCNN (see\nChapter 5), to predict the next code vector in the grid, given previous code vectors. In other words, the\nprior is learned by the model, rather than static as in the case of the vanilla VAE.\n\nTRAINING YOUR OWN VQ-VAE\n\nThere is an excellent tutorial by Sayak Paul on training your own VQ-VAE using Keras available on the Keras website.\n\nThe VQ-GAN paper details several key changes to the VQ-VAE architecture, as shown in Figure 10-20.\n\nAdversarial loss\nreal/fake\n(patches)\n\n\u20ac1 er C3 ey es 6\n\nAlignment and commitment loss\n\nPerceptual loss\n\nFigure 10-20. A diagram of a VQ-GAN: the GAN discriminator helps to encourage the VAE to generate less blurry images through an additional\nadversarial loss term\n\nFirstly, as the name suggests, the authors include a GAN discriminator that tries to distinguish between\nthe output from the VAE decoder and real images, with an accompanying adversarial term in the loss\nfunction. GANs are known to produce sharper images than VAEs, so this addition improves the overall\nimage quality. Notice that despite the name, the VAE is still present in a VQ-GAN model\u2014the GAN discrim-\ninator is an additional component rather than a replacement of the VAE. The idea of combining a VAE\nwith a GAN discriminator (VAE-GAN) was first introduced by Larsen et al. in their 2015 paper.\"\n\nSecondly, the GAN discriminator predicts if small patches of the images are real or fake, rather than the\nentire image at once. This idea (PatchGAN) was applied in the successful pix2pix image-to-image model\nintroduced in 2016 by Isola et al? and was also successfully applied as part of CycleGAN; another im-\nage-to-image style transfer model. The PatchGAN discriminator outputs a prediction vector (a prediction\nfor each patch), rather than a single prediction for the overall image. The benefit of using a PatchGAN dis-\ncriminator is that the loss function can then measure how good the discriminator is at distinguishing im-\nages based on their style, rather than their content. Since each individual element of the discriminator\nprediction is based on a small square of the image, it must use the style of the patch, rather than its con-\ntent, to make its decision. This is useful as we know that VAEs produce images that are stylistically more\nblurry than real images, so the PatchGAN discriminator can encourage the VAE decoder to generate\nsharper images than it would naturally produce.\n\nThirdly, rather than use a single MSE reconstruction loss that compares the input image pixels with the\noutput pixels from the VAE decoder, VQ-GAN uses a perceptual loss term that calculates the difference be-\ntween feature maps at intermediate layers of the encoder and corresponding layers of the decoder. This\nidea is from the 2016 paper by Hou et al, where the authors show that this change to the loss function\nresults in more realistic image generations.\n\nLastly, instead of PixelCNN, a Transformer is used as the autoregressive part of the model, trained to gen-\nerate sequences of codes. The Transformer is trained in a separate phase, after the VQ-GAN has been fully\ntrained. Rather than use all previous tokens in a fully autoregressive manner, the authors choose to only\nuse tokens that fall within a sliding window around the token to be predicted. This ensures that the mod-\nel scales to larger images, which require a larger latent grid size and therefore more tokens to be generat-\ned by the Transformer.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.25,
                        "section_name": "ViT VQ-GAN",
                        "section_path": "./screenshots-images-2/chapter_10/section_25",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_25/069d8b9a-9c35-498d-b06b-f40c98f2f2aa.png",
                            "./screenshots-images-2/chapter_10/section_25/6b36c798-9e3f-4166-87e5-b805acdfc60f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "ViT VQ-GAN\n\nOne final extension to the VQ-GAN was made by Yu et al. in their 2021 paper entitled \u201cVector-Quantized\n15\n\nImage Modeling with Improved VQGAN.\u201d Here, the authors show how the convolutional encoder and\n\ndecoder of the VQ-GAN can be replaced with Transformers as shown in Figure 10-21.\n\nFor the encoder, the authors use a Vision Transformer (ViT). A ViT is a neural network architecture that\napplies the Transformer model, originally designed for natural language processing, to image data.\nInstead of using convolutional layers to extract features from an image, a ViT divides the image into a se-\n\nquence of patches, which are tokenized and then fed as input to an encoder Transformer.\n\nSpecifically, in the ViT VQ-GAN, the nonoverlapping input patches (each of size 8 x 8) are first flattened,\nthen projected into a low-dimensional embedding space, where positional embeddings are added. This\nsequence is then fed to a standard encoder Transformer and the resulting embeddings are quantized ac-\ncording to a learned codebook. These integer codes are then processed by a decoder Transformer model,\nwith the overall output being a sequence of patches that can be stitched back together to form the origi-\n\nnal image. The overall encoder-decoder model is trained end-to-end as an autoencoder.\n\nReconstruction\n\nStage 2: Vector-quantized\n\nStage I: Image quantization\nimage modeling\n\nFigure 10-21. A diagram of a ViT VQ-GAN: the GAN discriminator helps to encourage the VAE to generate less blurry images through an addi-\ntional adversarial loss term (source: Yu and Koh, 2022)\n\nAs with the original VQ-GAN model, the second phase of training involves using an autoregressive de-\ncoder Transformer to generate sequences of codes. Therefore in total, there are three Transformers in a\nViT VQ-GAN, in addition to the GAN discriminator and learned codebook. Examples of images generated\nby the ViT VQ-GAN from the paper are shown in\n\nFigure 10-22. Example images generated by a ViT VQ-GAN trained on ImageNet (source: )\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 10.26,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_10/section_26",
                        "images": [
                            "./screenshots-images-2/chapter_10/section_26/c78a19d4-6e72-4c59-aaa6-8b318cc00601.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter, we have taken a tour of some of the most important and influential GAN papers since\n2017. In particular, we have explored ProGAN, StyleGAN, StyleGAN2, SAGAN, BigGAN, VQ-GAN, and ViT\nVQ-GAN.\n\nWe started by exploring the concept of progressive training that was pioneered in the 2017 ProGAN pa-\nper. Several key changes were introduced in the 2018 StyleGAN paper that gave greater control over the\nimage output, such as the mapping network for creating a specific style vector and synthesis network\nthat allowed the style to be injected at different resolutions. Finally, StyleGAN2 replaced the adaptive in-\nstance normalization of StyleGAN with weight modulation and demodulation steps, alongside additional\nenhancements such as path regularization. The paper also showed how the desirable property of gradual\nresolution refinement could be retained without having to the train the network progressively.\n\nWe also saw how the concept of attention could be built into a GAN, with the introduction of SAGAN in\n2018. This allows the network to capture long-range dependencies, such as similar background colors\nover opposite sides of an image, without relying on deep convolutional maps to spread the information\nover the spatial dimensions of the image. BigGAN was an extension of this idea that made several key\nchanges and trained a larger network to improve the image quality further.\n\nIn the VQ-GAN paper, the authors show how several different types of generative models can be com-\nbined to great effect. Building on the original VQ-VAE paper that introduced the concept of a VAE with a\ndiscrete latent space, VQ-GAN additionally includes a discriminator that encourages the VAE to generate\nless blurry images through an additional adversarial loss term. An autoregressive Transformer is used to\nconstruct a novel sequence of code tokens that can be decoded by the VAE decoder to produce novel im-\nages. The ViT VQ-GAN paper extends this idea even further, by replacing the convolutional encoder and\ndecoder of VQ-GAN with Transformers.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 11,
                "chapter_name": "Chapter 11. Music Generation",
                "chapter_path": "./screenshots-images-2/chapter_11",
                "sections": [
                    {
                        "section_id": 11.1,
                        "section_name": "Chapter 11. Music Generation",
                        "section_path": "./screenshots-images-2/chapter_11/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_1/ff6c6f8d-d73e-4a1c-9818-6277a75fb56b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 11. Music Generation\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\nUnderstand how we can treat music generation as a sequence prediction problem, so we can apply\nautoregressive models such as Transformers.\n\nSee how to parse and tokenize MIDI files using the music21 package to create a training set.\n\u00a2 Learn how to use sine positional encoding.\n\n\u00a2 Train a music-generating Transformer, with multiple inputs and outputs to handle note and\nduration.\n\nUnderstand how to handle polyphonic music, including grid tokenization and event-based.\ntokenization.\n\n\u00a2 Train a MuseGAN model to generate multitrack music.\n\n\u00a9 Use the MuseGAN to adjust different properties of the generated bars.\n\nMusical composition is a complex and creative process that involves combining different musical ele-\nments such as melody, harmony, rhythm, and timbre. While this is traditionally seen as a uniquely hu-\nman activity, recent advancements have made it possible to generate music that both is pleasing to the\near and has long-term structure.\n\nOne of the most popular techniques for music generation is the Transformer, as music can be thought of\nas a sequence prediction problem. These models have been adapted to generate music by treating musi-\ncal notes as a sequence of tokens, similar to words in a sentence. The Transformer model learns to pre-\n\ndict the next note in the sequence based on the previous notes, resulting in a generated piece of music.\n\nMuseGAN takes a totally different approach to generating music. Unlike Transformers, which generate\nmusic note by note, MuseGAN generates entire musical tracks at once by treating music as an image, con-\nsisting of a pitch axis and a time axis. Moreover, MuseGAN separates out different musical components\nsuch as chords, style, melody, and groove so that they can be controlled independently.\n\nIn this chapter we will learn how to process music data and apply both a Transformer and MuseGAN to\ngenerate music that is stylistically similar to a given training set.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_11/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_2/0ca85c62-ae56-4bb3-97ce-af6768190e3a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nFor a machine to compose music that is pleasing to our ear, it must master many of the same technical\nchallenges that we saw in Chapter 9 in relation to text. In particular, our model must be able to learn\nfrom and re-create the sequential structure of music and be able to choose from a discrete set of possibili-\nties for subsequent notes.\n\nHowever, music generation presents additional challenges that are not present for text generation, name-\nly pitch and rhythm. Music is often polyphonic\u2014that is, there are several streams of notes played simul-\ntaneously on different instruments, which combine to create harmonies that are either dissonant (clash-\ning) or consonant (harmonious). Text generation only requires us to handle a single stream of text, in\ncontrast to the parallel streams of chords that are present in music.\n\nAlso, text generation can be handled one word at a time. Unlike text data, music is a multipart, interwo-\nven tapestry of sounds that are not necessarily delivered at the same time\u2014much of the interest that\nstems from listening to music is in the interplay between different rhythms across the ensemble. For ex-\nample, a guitarist might play a flurry of quicker notes while the pianist holds a longer sustained chord.\nTherefore, generating music note by note is complex, because we often do not want all the instruments to\n\nchange notes simultaneously.\n\nWe will start this chapter by simplifying the problem to focus on music generation for a single (mono-\nphonic) line of music. Many of the techniques from Chapter 9 for text generation can also be used for mu-\nsic generation, as the two tasks share many common themes. We will start by training a Transformer to\ngenerate music in the style of the J.S. Bach cello suites and see how the attention mechanism allows the\nmodel to focus on previous notes in order to determine the most natural subsequent note. We\u2019ll then\ntackle the task of polyphonic music generation and explore how we can deploy an architecture based\naround GANs to create music for multiple voices.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.3,
                        "section_name": "Transformers for Music Generation",
                        "section_path": "./screenshots-images-2/chapter_11/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_3/7a787a7c-deea-403d-beb1-8fb6d342092e.png",
                            "./screenshots-images-2/chapter_11/section_3/5b8ba3c9-2452-4553-98d0-1484833fabcb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Transformers for Music Generation\n\nThe model we will be building here is a decoder Transformer, taking inspiration from OpenAl\u2019s MuseNet,\n\nwhich also utilizes a decoder Transformer (similar to GPT-3) trained to predict the next note given a se-\nquence of previous notes.\n\nIn music generation tasks, the length of the sequence N grows large as the music progresses, and this\nmeans that the N x N attention matrix for each head becomes expensive to store and compute. We ide-\nally do not want to clip the input sequence to a short number of tokens, as we would like the model to\nconstruct the piece around a long-term structure and repeat motifs and phrases from several minutes\nago, as ahuman composer would.\n\nTo tackle this problem, MuseNet utilizes a form of Transformer known as a Sparse Transformer. Each out-\nput position in the attention matrix only computes weights for a subset of input positions, thereby reduc-\ning the computational complexity and memory required to train the model. MuseNet can therefore oper-\nate with full attention over 4,096 tokens and can learn long-term structure and melodic structure across a\nrange of styles. (See, for example, OpenAl\u2019s Chopin and Mozart recordings on SoundCloud.)\n\nTo see how the continuation of a musical phrase is often influenced by notes from several bars ago, take a\nlook at the opening bars of the Prelude to Bach\u2019s Cello Suite No. 1 (Figure 11-1).\n\neS\n\nFigure 11-1. The opening of Bach\u2019s Cello Suite No. 1 (Prelude)\n\nBARS\n\nBars (or measures) are small units of music that contain a fixed, small number of beats and are marked out by verti-\ncal lines that cross the staff. If you can count 1, 2, 1, 2 along to a piece of music, then there are two beats in each bar\nand you're probably listening to a march. If you can count 1, 2, 3, 1, 2, 3, then there are three beats to each bar and\nyou may be listening to a waltz.\n\nWhat note do you think comes next? Even if you have no musical training you may still be able to guess.\nIf you said G (the same as the very first note of the piece), then you\u2019d be correct. How did you know this?\nYou may have been able to see that every bar and half bar starts with the same note and used this infor-\nmation to inform your decision. We want our model to be able to perform the same trick\u2014in particular,\nwe want it to pay attention to a particular note from the previous half bar, when the previous low G was\nregistered. An attention-based model such as a Transformer will be able to incorporate this long-term\nlook-back without having to maintain a hidden state across many bars, as is the case with a recurrent\nneural network.\n\nWhat note do you think comes next? Even if you have no musical training you may still be able to guess.\nIf you said G (the same as the very first note of the piece), then you\u2019d be correct. How did you know this?\nYou may have been able to see that every bar and half bar starts with the same note and used this infor-\nmation to inform your decision. We want our model to be able to perform the same trick\u2014in particular,\nwe want it to pay attention to a particular note from the previous half bar, when the previous low G was\nregistered. An attention-based model such as a Transformer will be able to incorporate this long-term\nlook-back without having to maintain a hidden state across many bars, as is the case with a recurrent\nneural network.\n\nAnyone tackling the task of music generation must first have a basic understanding of musical theory. In\nthe next section we\u2019ll go through the essential knowledge required to read music and how we can repre-\nsent this numerically, in order to transform music into the input data required to train our Transformer.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at notebooks/11_music/01_transformer/trans-\nformer.ipynb in the book repository.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.4,
                        "section_name": "The Bach Cello Suite Dataset",
                        "section_path": "./screenshots-images-2/chapter_11/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_4/4de3b565-2604-45fa-ac4c-6742ff6dd2ac.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Bach Cello Suite Dataset\n\nThe raw dataset that we shall be using is a set of MIDI files for the Cello Suites by J.S. Bach. You can down-\nload the dataset by running the dataset downloader script in the book repository, as shown in\nExample 11-1. This will save the MIDI files locally to the /data folder.\n\nExample 11-1. Downloading the J.S. Bach Cello Suites dataset\n\nbash scripts/download_music_data.sh\n\nTo view and listen to the music generated by the model, you\u2019ll need some software that can produce mu-\nsical notation. MuseScore is a great tool for this purpose and can be downloaded for free.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.5,
                        "section_name": "Parsing MIDI Files",
                        "section_path": "./screenshots-images-2/chapter_11/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_5/ce3bacf1-6a4c-421d-bdec-1f558484ae57.png",
                            "./screenshots-images-2/chapter_11/section_5/e4dfb0b8-f3c9-4771-827b-59eae778fcb4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Parsing MIDI Files\n\nWe'll be using the Python library music21 to load the MIDI files into Python for processing. Example 11-\n2 shows how to load a MIDI file and visualize it (Figure 11-2), both as a score and as structured data.\n\n\u2018exanple_score. show()\n\nthe instrumentation, tempo, key, and time\nsignature of the piece.\n\n\u2018The midi file starts with some metadata around. \u2014\u2014\u2014\u2014+\n\u2018Soto Cello: Soto\n\nThis note starts on beat 4 of the piece (zero-indexed).\nthas duration of 1 beat (as the next note starts on\nbeat 5) and consists of a chord of low G, D, and 8.\nThis note starts on beat 6 of the piece. It has duration \u2014\u2014\u2014\u2014- @ {;\nof a quarter of a beat (as the next note starts on beat\n6.25) and consists of a single note ~ G.\n\nThis note starts a quarter of a beat before beat 8 of\nthe piece. It has duration of a quarter of a beat and\nconsists of a single note ~ high C.\n\n\u00a9 (8.75) emusie2i. chord. chord C4>\n\nFigure 11-2. Musical notation\n\nExample 11-2. Importing a MIDI file\n\nimport music21\n\nfile = \"/app/data/bach-cello/cs1-2all.mid\"\nexample score = music21.converter.parse(file) .chordify()\n\nOCTAVES\nThe number after each note name indicates the octave that the note is in\u2014since the note names (A to G) repeat, this is\nneeded to uniquely identify the pitch of the note. For example, G2 is an octave below G3.\n\n\nNow it\u2019s time to convert the scores into something that looks more like text! We start by looping over each\nscore and extracting the note and duration of each element in the piece into two separate text strings,\nwith elements separated by spaces. We encode the key and time signature of the piece as special symbols,\n\nwith zero duration.\n\nMONOPHONIC VERSUS POLYPHONIC MUSIC\n\nIn this first example, we will treat the music as monophonic (one single line), taking just the top note of any chords.\nSometimes we may wish to keep the parts separate to generate music that is polyphonic in nature. This presents addi-\ntional challenges that we shall tackle later on in this chapter.\n\nThe output from this process is shown in Figure 11-3\u2014compare this to Figure 11-2 so that you can see\nhow the raw music data has been transformed into the two strings.\n\nNotes string\n\nSTART G:major 4/4TS rest B3 B3 B3 A3 G3 F#3 G3 D3 E3 F#3 G\n3 A3 B3 C4 D4 B3 G3 F#3 G3 E3 D3 C3 B2 C3 D3 E3 F#3 G3 A3 B\n3 C4 A3 G3 F#3 G3 E3 F#3 G3 A2 D3 F#3 G3 A3 B3 C4 A3 BB...\n\nDuration string\n\n@.0 0.0 0.0 3.75 0.25 1.0 0.25 0.25 0.25 0.25 0.25 0.25 0.\n25 0.25 0.25 0.25 0.25 0.25 @.25 0.25 0.25 0.25 0.25 0.25\n@.25 @.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n@.25 @.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n@.25 @.25 0.25 ...\n\nFigure 11-3. Samples of the notes text string and the duration text string, corresponding to Figure 11-2\n\nThis looks a lot more like the text data that we have dealt with previously. The words are the note\u2014dura-\ntion combinations, and we should try to build a model that predicts the next note and duration, given a\nsequence of previous notes and durations. A key difference between music and text generation is that we\nneed to build a model that can handle the note and duration prediction simultaneously\u2014i.e., there are\ntwo streams of information that we need to handle, compared to the single streams of text that we saw in\n\nChapter 9.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.6,
                        "section_name": "Tokenization",
                        "section_path": "./screenshots-images-2/chapter_11/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_6/4bb4c297-24c4-497e-9666-1a536b23a4a3.png",
                            "./screenshots-images-2/chapter_11/section_6/0d6814d2-1480-4e8e-b962-7ae852b6c49e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tokenization\n\nTo create the dataset that will train the model, we first need to tokenize each note and duration, exactly\nas we did previously for each word in a text corpus. We can achieve this by using a\nTextVectorization layer, applied to the notes and durations separately, as shown in Example 11-3.\n\nExample 11-3. Tokenizing the notes and durations\n\ndef create_dataset(elements):\n\nds = (\ntf.data.Dataset.from_tensor_slices(elements)\n.-batch(BATCH_SIZE, drop remainder = True)\n.-shuffle(1000)\n\n)\n\nvectorize layer = layers.TextVectorization(\nstandardize = None, output_mode=\"int\"\n\n)\n\nvectorize_layer.adapt(ds)\n\nvocab = vectorize_layer.get_vocabulary()\n\nreturn ds, vectorize_ layer, vocab\n\nnotes_seq_ ds, notes_vectorize layer, notes_vocab = create_dataset(notes)\ndurations _seq_ds, durations_vectorize_layer, durations_vocab = create_dataset(\ndurations\n\n)\nseq_ds = tf.data.Dataset.zip((notes_seq_ds, durations_seq_ds))\n\nThe full parsing and tokenization process is shown in Figure 11-4.\n\nThe full parsing and tokenization process is shown in Figure 11-4.\n\nnote duration\nparsing string string\n\u2014\u2014 START 0.0\n\nG:major\n\n4/4TS\n\nG2\n\n03\n\n83\n\na3\n\n83\n\n03\n\n83\n\nD3\n\ntokenization\nnote duration\ntoken token\nNOTES_VOCAB: length = 59 DURATIONS_VOCAB: length = 24\nai 37 10 @:\n\n51 PH 1: [UNK]\n10 2: 0.25\n2 3: 0.5\n2 4: 1.0\n2 5: 1/3\n2 6: 0.75\n2 7: 12\n2 8: 1.5\n2 9: 1/6\n2\n\nFigure 11-4. Parsing the MIDI files and tokenizing the notes and durations\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.7,
                        "section_name": "Creating the Training Set",
                        "section_path": "./screenshots-images-2/chapter_11/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_7/41b9bcec-ffb9-433c-a133-30a29cff1b28.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Creating the Training Set\nThe final step of preprocessing is to create the training set that we will feed to our Transformer.\n\nWe do this by splitting both the note and duration strings into chunks of 50 elements, using a sliding win-\ndow technique. The output is simply the input window shifted by one note, so that the Transformer is\ntrained to predict the note and duration of the element one timestep into the future, given previous ele-\nments in the window. An example of this (using a sliding window of only four elements for demonstra-\ntion purposes) is shown in Figure 11-5.\n\nInput Output\nSTA\n\nFigure 11-5. The inputs and outputs for the musical Transformer model\u2014in this example, a sliding window of width 4 is used to create input\nchunks, which are then shifted by one element to create the target output\n\nThe architecture we will be using for our Transformer is the same as we used for text generation in\n\nChapter 9, with a few key differences.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.8,
                        "section_name": "Sine Position Encoding",
                        "section_path": "./screenshots-images-2/chapter_11/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_8/f42da947-5220-417a-af50-ce212fda1fe9.png",
                            "./screenshots-images-2/chapter_11/section_8/6f701433-02b2-41d1-aa14-681a4256acdd.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Sine Position Encoding\n\nFirstly, we will be introducing a different type of encoding for the token positions. In Chapter 9 we used a\nsimple Embedding layer to encode the position of each token, effectively mapping each integer position\nto a distinct vector that was learned by the model. We therefore needed to define a maximum length (NV)\nthat the sequence could be and train on this length of sequence. The downside to this approach is that it\nis then impossible to extrapolate to sequences that are longer than this maximum length. You would have\nto clip the input to the last N tokens, which isn\u2019t ideal if you are trying to generate long-form content.\n\nTo circumvent this problem, we can switch to using a different type of embedding called a sine position\nembedding. This is similar to the embedding that we used in Chapter 8 to encode the noise variances of\n\nthe diffusion model. Specifically, the following function is used to convert the position of the word (pos)\nin the input sequence into a unique vector of length d:\n\noo pos\nPB posi = sin (a)\n\n_ pos\nPEpos,2i+1 = COS (Gnome)\n\nFor small i, the wavelength of this function is short and therefore the function value changes rapidly\nalong the position axis. Larger values of 7 create a longer wavelength. Each position thus has its own\nunique encoding, which is a specific combination of the different wavelengths.\n\nTIP\nNotice that this embedding is defined for all possible position values. It is a deterministic function (i.e., it isn\u2019t learned\nby the model) that uses trigonometric functions to define a unique encoding for each possible position.\n\nThe Keras NLP module has a built-in layer that implements this embedding for us\u2014we can therefore de-\nfine our TokenAndPositionEmbedding layer as shown in Example 11-4.\n\nExample 11-4. Tokenizing the notes and durations\n\nExample 11-4. Tokenizing the notes and durations\n\nclass TokenAndPositionEmbedding(layers.Layer):\n\ndef\n\ndef\n\n__init__(self, vocab_size, embed_dim):\n\nsuper (TokenAndPositionEmbedding, self). init ()\n\nself.vocab_size = vocab_size\n\nself.embed_dim = embed_dim\n\nself.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_\u00ab\n\nself.pos_emb = keras_nlp.layers.SinePositionEncoding()\n\ncall(self, x):\n\nembedding = self.token_emb(x)\npositions = self.pos_emb(embedding)\nreturn embedding + positions\n\nFigure 11-6 shows how the two embeddings (token and position) are added to produce the overall embed-\n\nding for the sequence.\n\nPosition embedding\n\nFigure 11-6. The TokenAndPositionEmbedding layer adds the token embeddings to the sinusoidal position embeddings to produce the overall\n\nembedding for the sequence\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.9,
                        "section_name": "Multiple Inputs and Outputs",
                        "section_path": "./screenshots-images-2/chapter_11/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_9/98beae53-ef6b-49ef-b0ce-3b36187e5526.png",
                            "./screenshots-images-2/chapter_11/section_9/ff3728a9-0a9e-420e-9bfd-b0bd408dbb79.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Multiple Inputs and Outputs\n\nWe now have two input streams (notes and durations) and two output streams (predicted notes and dura-\ntions). We therefore need to adapt the architecture of our Transformer to cater for this.\n\nThere are many ways of handling the dual stream of inputs. We could create tokens that represent each\nnote-duration pair and then treat the sequence as a single stream of tokens. However, this has the down-\nside of not being able to represent note\u2014duration pairs that have not been seen in the training set (for ex-\nample, we may have seena G#2 note anda 1/3 duration independently, but never together, so there\nwould be no token for G#2:1/3.\n\nInstead, we choose to embed the note and duration tokens separately and then use a concatenation layer\nto create a single representation of the input that can be used by the downstream Transformer block.\nSimilarly, the output from the Transformer block is passed to two separate dense layers, which represent\nthe predicted note and duration probabilities. The overall architecture is shown in Figure 11-7. Layer out-\nput shapes are shown with batch size b and sequence length 1.\n\nStart 0.0\n\nG:major 0.0\n\n4/4TS 0.0\n\nG2 0.25\n\n03 0.25\n\nB3 0.25\n\nA3 0.25\n\n= B3 0.25 \u2014\n\nNote input D3 9.25 | _ Duration input\n(b.1] ne eee {b.1\n\n[b, 1, 128]\n\n{b, |, 128]\n\nD3\nText and positional embedding\n\n\u00ab25\nText and positional embedding\n\nTransformer block\n\nDense Dense\n\nNext note predictions Next duration predictions\n\nFigure 11-7. The architecture of the music-generating Transformer\n\nAn alternative approach would be to interleave the note and duration tokens into a single stream of input\nand let the model learn that the output should be a single stream where the note and duration tokens al-\nternate. This comes with the added complexity of ensuring that the output can still be parsed when the\nmodel has not yet learned how to interleave the tokens correctly.\n\nTIP\n\nThere is no right or wrong way to design your model\u2014part of the fun is experimenting with different setups and see-\ning which works best for you!\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.1,
                        "section_name": "Analysis of the Music-Generating Transformer",
                        "section_path": "./screenshots-images-2/chapter_11/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_10/b2ac0f12-163d-4e2f-ab95-cba7948daa67.png",
                            "./screenshots-images-2/chapter_11/section_10/ade16a88-a9da-443f-9f36-ab70228b999c.png",
                            "./screenshots-images-2/chapter_11/section_10/f9b292b4-6837-491f-a1f1-2cf0a3351500.png",
                            "./screenshots-images-2/chapter_11/section_10/317b8648-ddec-4c40-ba56-8ad7efec7c66.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the Music-Generating Transformer\n\nWe'll start by generating some music from scratch, by seeding the network with a START note token and\n0.0 duration token (i.e., we are telling the model to assume it is starting from the beginning of the\npiece). Then we can generate a musical passage using the same iterative technique we used in Chapter 9\n\nfor generating text sequences, as follows:\n\n1. Given the current sequence (of notes and durations), the model predicts two distributions, one for\nthe next note and one for the next duration.\n\n2. We sample from both of these distributions, using a temperature parameter to control how much\nvariation we would like in the sampling process.\n\n3. The chosen note and duration are appended to the respective input sequences.\n\n4. The process repeats with the new input sequences for as many elements as we wish to generate.\n\nFigure 11-8 shows examples of music generated from scratch by the model at various epochs of the train-\ning process. We use a temperature of 0.5 for the notes and durations.\n\nEpoch 20\n\nFigure 11-8. Some examples of passages generated by the model when seeded only with a START note token and 0.0 duration token\n\nMost of our analysis in this section will focus on the note predictions, rather than durations, as for Bach\u2019s\nCello Suites the harmonic intricacies are more difficult to capture and therefore more worthy of investi-\ngation. However, you can also apply the same analysis to the rhythmic predictions of the model, which\nmay be particularly relevant for other styles of music that you could use to train this model (such as a\ndrum track).\n\nThere are several points to note about the generated passages in Figure 11-8. First, see how the music is\nbecoming more sophisticated as training progresses. To begin with, the model plays it safe by sticking to\nthe same group of notes and rhythms. By epoch 10, the model has begun to generate small runs of notes,\nand by epoch 20 it is producing interesting rhythms and is firmly established in a set key (E > major).\n\nSecond, we can analyze the distribution of notes over time by plotting the predicted distribution at each\ntimestep as a heatmap. Figure 11-9 shows this heatmap for the example from epoch 20 in Figure 11-8.\n\n68\n66\n64.\n62\n\n0 10 20 30 40 50\nNote number\n\nFigure 11-9. The distribution of possible next notes over time (at epoch 20): the darker the square, the more certain the model is that the next\nnote is at this pitch\n\nAn interesting point to note here is that the model has clearly learned which notes belong to particular\nkeys, as there are gaps in the distribution at notes that do not belong to the key. For example, there is a\ngray gap along the row for note 54 (corresponding to G b/F #). This note is highly unlikely to appear in a\npiece of music in the key of E b major. The model establishes the key early on in the generation process,\nand as the piece progresses, the model chooses notes that are more likely to feature in that key by attend-\ning to the token that represents it.\n\nIt is also worth pointing out that the model has learned Bach\u2019s characteristic style of dropping to a low\nnote on the cello to end a phrase and bouncing back up again to start the next. See how around note 20,\nthe phrase ends on a low E } \u2014it is common in the Bach Cello Suites to then return to a higher, more\nsonorous range of the instrument for the start of next phrase, which is exactly what the model predicts.\nThere is a large gray gap between the low E b (pitch number 39) and the next note, which is predicted to\nbe around pitch number 50, rather than continuing to rumble around the depths of the instrument.\n\nLastly, we should check to see if our attention mechanism is working as expected. The horizontal axis in\nFigure 11-10 shows the generated sequence of notes; the vertical axis shows where the attention of the\nnetwork was aimed when predicting each note along the horizontal axis. The color of each square shows\nthe maximum attention weight across all heads at each point in the generated sequence. The darker the\nsquare, the more attention is being applied to this position in the sequence. For simplicity, we only show\nthe notes in this diagram, but the durations of each note are also being attended to by the network.\n\nWe can see that for the initial key signature, time signature, and rest, the network chose to place almost\nall of its attention on the START token. This makes sense, as these artifacts always appear at the start of a\npiece of music\u2014once the notes start flowing the START token essentially stops being attended to.\n\nAs we move beyond the initial few notes, we can see that the network places most attention on approxi-\nmately the last two to four notes and rarely places significant weight on notes more than four notes ago.\nAgain, this makes sense; there is probably enough information contained in the previous four notes to\nunderstand how the phrase might continue. Additionally, some notes attend more strongly back to the\nkey signature of D minor\u2014for example, the \u00a33 (7th note of the piece) and B-2 (B >-14th note of the\npiece). This is fascinating, because these are the exact notes that rely on the key of D minor to relieve any\nambiguity. The network must look back at the key signature in order to tell that there isa B b in the key\nsignature (rather than a B natural) but there isn\u2019t an E b in the key signature (E natural must be used\ninstead).\n\nThe generated passage\n\nSTART\n\nD:minor\n3/4TS\nrest\n\nAttention lookup\n\nPeete ty ye et fe\n\nFigure 11-10. The color of each square in the matrix indicates the amount of attention given to each position on the vertical axis, at the point of\npredicting the note on the horizontal axis\n\nThere are also examples of where the network has chosen to ignore a certain note or rest nearby, as it\ndoesn\u2019t add any additional information to its understanding of the phrase. For example, the penultimate\nnote (A2 ) is not particularly attentive to the B-2 three notes back, but is slightly more attentive to the\nA2 four notes back. It is more interesting for the model to look at the A2 that falls on the beat, rather\n\nthan the B-2 off the beat, which is just a passing note.\n\nRemember we haven\u2019t told the model anything about which notes are related or which notes belong to\nwhich key signatures\u2014it has worked this out for itself just by studying the music of J.S. Bach.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.11,
                        "section_name": "Tokenization of Polyphonic Music",
                        "section_path": "./screenshots-images-2/chapter_11/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_11/970a6264-2d4e-45fe-9509-33221ce97ff7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Tokenization of Polyphonic Music\n\nThe Transformer we\u2019ve been exploring in this section works well for single-line (monophonic) music, but\ncould it be adapted to multiline (polyphonic) music?\n\nThe challenge lies in how to represent the different lines of music as a single sequence of tokens. In the\nprevious section we decided to split the notes and durations of the notes into two distinct inputs and out-\nputs of the network, but we also saw that we could have interleaved these tokens into a single stream. We\ncan use the same idea to handle polyphonic music. Two different approaches will be introduced here:\ngrid tokenization and event-based tokenization, as discussed in the 2018 paper \u201cMusic Transformer:\nGenerating Music with Long-Term Structure.\u201d*\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.12,
                        "section_name": "Grid tokenization",
                        "section_path": "./screenshots-images-2/chapter_11/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_12/d36918f7-63c0-4fb8-b755-5d95b4fe0545.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Grid tokenization\n\nConsider the two bars of music from a J.S. Bach chorale in Figure 11-11. There are four distinct parts (so-\nprano [S], alto [A], tenor [T], bass [B]), written on different staffs.\n\nFigure 11-11. The first two bars of a J.S. Bach chorale\n\nWe can imagine drawing this music on a grid, where the y-axis represents the pitch of the note and the x-\naxis represents the number of 16th-notes (semiquavers) that have passed since the start of the piece. If\nthe grid square is filled, then there is a note playing at that point in time. All four parts are drawn on the\nsame grid. This grid is known as a piano roll because it resembles a physical roll of paper with holes\npunched into it, which was used as a recording mechanism before digital systems were invented.\n\nWe can serialize the grid into a stream of tokens by moving first through the four voices, then along the\ntimesteps in sequence. This produces a sequence of tokens $;, Ai, 71, Bi, $2, Ao, T2, Bo, ..., where the\nsubscript denotes the timestep, as shown in Figure 11-12.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.13,
                        "section_name": "Event-based tokenization",
                        "section_path": "./screenshots-images-2/chapter_11/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_13/37ff0671-03ee-4f93-9ce6-32ff16d9e437.png",
                            "./screenshots-images-2/chapter_11/section_13/9d523e90-012b-45c1-a641-f78deef344f7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Event-based tokenization\n\nA more flexible approach is to use event-based tokenization. This can be thought of as a vocabulary that\nliterally describes how the music is created as a sequence of events, using a rich set of tokens.\n\nFor example in Figure 11-13, we use three types of tokens:\n\n\u00a9 NOTE_oN<pitch> (start playing a note of a given pitch)\n\u00b0 NOTE_OFF<pitch> (stop playing a note of a given pitch)\n\u00a9 TIME_SHIFT<step> (shift forward in time by a given step)\n\nThis vocabulary can be used to create a sequence that describes the construction of the music as a set of\ninstructions.\n\nEvent tokenization\n\n[NOTE_ON<74>, NOTE_ON<70>, NOTE_ON<65>, NOTE_ON<S8>\nTIME_SHIFT<1.@>\n\nNOTE_OFF<74>, NOTE_OFF<65S>, NOTE_OFF<58>\n\nNOTE_ON<75>, NOTE_ON<S8>, NOTE_ON<SS>\n\nTIME_SHIFT<@.5>\n\nNOTE_OFF<5S8>, NOTE_ON<60>\n\nTIME_SHIFT<@.5>\n\nNOTE_OFF<75>, NOTE_OFF<7@>, NOTE_OFF<6@>, NOTE_OFF<5S>\nNOTE_ON<77>, NOTE_ON<69>, NOTE_ON<62>, NOTE_ON<S@>\nTIME_SHIFT<1.0>\n\nNOTE_OFF<69>, NOTE_OFF<5@>, NOTE_ON<7@>, NOTE_ON<S5S>\nTIME_SHIFT<@.5>\n\nNOTE_OFF<7@>, NOTE_ON<69>\n\nTIME_SHIFT<@.5>\n\nNOTE_OFF<77>, NOTE_OFF<69>, NOTE_OFF<62>, NOTE_OFF<55>\n\nFigure 11-13. An event tokenization for the first bar of the Bach chorale\n\nWe could easily incorporate other types of tokens into this vocabulary, to represent dynamic and tempo\nchanges for subsequent notes. This method also provides a way to generate triplets against a backdrop of\nquarter-notes, by separating the notes of the triplets with TIME_SHIFT<0.33> tokens. Overall, it isa\nmore expressive framework for tokenization, though it is also potentially more complex for the\nTransformer to learn inherent patterns in the training set music, as it is by definition less structured than\nthe grid method.\n\nTIP\n\nI encourage you to try implementing these polyphonic techniques and train a Transformer on the new tokenized\ndataset using all the knowledge you have built up so far in this book. I would also recommend checking our Dr.\nTristan Behrens\u2019s guide to music generation research, available on GitHub, which provides a comprehensive over-\n\nview of different papers on the topic of music generation using deep learning.\n\nIn the next section we will take a completely different approach to music generation, using GANs.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.14,
                        "section_name": "MuseGAN",
                        "section_path": "./screenshots-images-2/chapter_11/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_14/d35a21b1-0f5c-4829-a2b8-b3fe53f373a0.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "MuseGAN\n\nYou may have thought that the piano roll shown in Figure 11-12 looks a bit like a piece of modern art.\nThis begs the question\u2014could we in fact treat this piano roll as a picture and utilize image generation\nmethods instead of sequence generation techniques?\n\nAs we shall see, the answer to this question is yes, we can treat music generation directly as an image\ngeneration problem. This means that instead of using Transformers, we can apply the same convolution-\nal-based techniques that work so well for image generation problems\u2014in particular, GANs.\n\nMuseGAN was introduced in the 2017 paper \u201cMuseGAN: Multi-Track Sequential Generative Adversarial\nNetworks for Symbolic Music Generation and Accompaniment.\u201d The authors show how it is possible to\ntrain a model to generate polyphonic, multitrack, multibar music through a novel GAN framework.\nMoreover, they show how, by dividing up the responsibilities of the noise vectors that feed the generator,\nthey are able to maintain fine-grained control over the high-level temporal and track-based features of\nthe music.\n\nLet\u2019s start by introducing the the J.S. Bach chorale dataset.\n\nRUNNING THE CODE FOR THIS EXAMPLE\nThe code for this example can be found in the Jupyter notebook located at\nnotebooks/11_music/02_musegan/musegan.ipynb in the book repository.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.15,
                        "section_name": "The Bach Chorale Dataset",
                        "section_path": "./screenshots-images-2/chapter_11/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_15/17f28379-8ff1-41b6-96c8-5f62412c9937.png",
                            "./screenshots-images-2/chapter_11/section_15/8dc26e66-8068-4924-a5fb-8152fbb99e1b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Bach Chorale Dataset\n\nTo begin this project, you'll first need to download the MIDI files that we'll be using to train the MuseGAN.\nWe'll use a dataset of 229 J.S. Bach chorales for four voices.\n\nYou can download the dataset by running the Bach chorale dataset downloader script in the book reposi-\ntory, as shown in Example 11-5. This will save the MIDI files locally to the /data folder.\n\nExample 11-5. Downloading the Bach chorale dataset\n\nbash scripts/download_bach_chorale_data.sh\n\nThe dataset consists of an array of four numbers for each timestep: the MIDI note pitches of each of the\nfour voices. A timestep in this dataset is equal to a 16th note (a semiquaver). So, for example, in a single\nbar of 4 quarter (crotchet) beats, there are 16 timesteps. The dataset is automatically split into train, vali-\ndation, and test sets. We will be using the train dataset to train the MuseGAN.\n\nTo start, we need to get the data into the correct shape to feed the GAN. In this example we'll generate\ntwo bars of music, so we\u2019ll extract only the first two bars of each chorale. Each bar consists of 16\ntimesteps and there are a potential 84 pitches across the 4 voices.\n\nTIP\n\nVoices will be referred to as tracks from here on, to keep the terminology in line with the original paper.\n\nTherefore, the transformed data will have the following shape:\n\n[BATCH_SIZE, N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS]\n\nwhere:\n\nBATCH_SIZE = 64\nN_BARS = 2\nN_STEPS_PER_BAR = 16\nN_PITCHES = 84\nN_TRACKS = 4\n\nTo get the data into this shape, we one-hot encode the pitch numbers into a vector of length 84 and split\neach sequence of notes into two bars of 16 timesteps each. We are making the assumption here that each\nchorale in the dataset has four beats in each bar, which is reasonable, and even if this were not the case it\nwould not adversely affect the training of the model.\n\nFigure 11-14 shows how two bars of raw data are converted into the transformed piano roll dataset that\nwe will use to train the GAN.\n\nMusical score\n2bars\n16 timesteps\n136\nee \u2014\u2014\n\u2014\u2014 = \u2014 ,\ntracks\n\nSSS SS\n\n12345 6 7 8 9101112131415 16\n\nRaw data =) Transformed data\n\n4 tracks\n\n10., 65., 58.],\n10., 65.,\n70., 65.,\n70., 65.,\n70., 58.,\n70., 58.,\n70., 60.,\n70., 60.,\n69., 62.,\n69., 62.,\n69., 62.,\n69., 62.,\n70., 62.,\n10., 62.,\n69., 62.,\n69., 62.,\n67., 63.,\n67.1 63.,\n69., 63.,\n69., 63.,\n70., 65.,\n70., 65.,\n70+, 65.,\n10., 65.,\n69., 65.,\n[72., 69., 65.,\n[72., 69, 65.,\n\narray({(74\n(7\n\n2bars\n16 timesteps\n\n84 pitches\n\n[72., 69., 65., brand\n[72., 69., 65., 16 timesteps\n[72., 69., 65.,\n\n[72., 69., 65., 2bars\n\n(72., 69., 65.,\n\nFigure 11-14. Processing two bars of raw data into piano roll data that we can use to train the GAN\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.16,
                        "section_name": "The MuseGAN Generator",
                        "section_path": "./screenshots-images-2/chapter_11/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_16/438942f8-3279-453c-be7c-d42c2c5d8ae9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The MuseGAN Generator\n\nLike all GANs, MuseGAN consists of a generator and a critic. The generator tries to fool the critic with its\nmusical creations, and the critic tries to prevent this from happening by ensuring it is able to tell the dif-\nference between the generator\u2019s forged Bach chorales and the real thing.\n\nWhere MuseGAN differs is in the fact that the generator doesn\u2019t just accept a single noise vector as input,\nbut instead has four separate inputs, which correspond to four different characteristics of the music:\nchords, style, melody, and groove. By manipulating each of these inputs independently we can change\nhigh-level properties of the generated music.\n\nA high-level view of the generator is shown in Figure 11-15.\n\nTHE MUSEGAN GENERATOR\nNoise vector [32] 2bars\n\u2014\u00a2 Generated\nChords input Generated 2nd bar of pa\nthe 3rd track\nStyle input =\n2\u2014__,\nRr & Concatenate\nacrossall\nMelody input bars and tracks\ne/ \u2014\u2014\u2014>\n16 timesteps.\n\nFigure 11-15. High-level diagram of the MuseGAN generator\n\nThe diagram shows how the chords and melody inputs are first passed through a temporal network that\noutputs a tensor with one of the dimensions equal to the number of bars to be generated. The style and\ngroove inputs are not stretched temporally in this way, as they remain constant through the piece.\n\nThen, to generate a particular bar for a particular track, the relevant outputs from the chords, style,\nmelody, and groove parts of the network are concatenated to form a longer vector. This is then passed to\na bar generator, which ultimately outputs the specified bar for the specified track.\n\nBy concatenating the generated bars for all tracks, we create a score that can be compared with real\nscores by the critic.\n\nLet\u2019s first take a look at how to build a temporal network.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.17,
                        "section_name": "The temporal network",
                        "section_path": "./screenshots-images-2/chapter_11/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_17/02b38064-91aa-4a25-b490-9becf86dc2f5.png",
                            "./screenshots-images-2/chapter_11/section_17/01acb5ac-413d-4aec-960d-a6d45adbda37.png",
                            "./screenshots-images-2/chapter_11/section_17/7d5f7d80-bbcf-403a-a976-72ab3951a56f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The temporal network\n\nThe job of a temporal network\u2014a neural network consisting of convolutional transpose layers\u2014is to\ntransform a single input noise vector of length z_DIM = 32 into a different noise vector for every bar\n(also of length 32). The Keras code to build this is shown in Example 11-6.\n\nExample 11-6. Building the temporal network\n\ndef conv_t(x, f, k, s, a, p, bn):\n\nx = layers.Conv2DTranspose(\nfilters = f\n, kernel_size = k\n, padding = p\n, Strides =s\n, kernel_initializer = initializer\n)(x)\n\nif bn:\n\nx = layers.BatchNormalization(momentum = 0.9) (x)\n\nx = layers.Activation(a) (x)\nreturn x\n\ndef TemporalNetwork():\ninput_layer = layers.Input(shape=(Z_DIM,), name='temporal_input') O\nx = layers.Reshape([1,1,Z_DIM])(input_layer) @\n\nx = conv_t(\nx, \u00a3=1024, k=(2,1), s=(1,1), a = \u2018'relu', p = 'valid', bn = True\n)9\nx = conv_t(\nx, f\u00a3=Z_DIM, k=(N_BARS - 1,1), s=(1,1), a = \u2018relu', p = \u2018valid\u2019, bn = True\n\n)\noutput_layer = layers.Reshape([N_BARS, Z_DIM])(x) @\nreturn models.Model(input_layer, output_layer)\n\n@ The input to the temporal network is a vector of length 32 ( z_DIM).\n\n@ We reshape this vector to a 1 x 1 tensor with 32 channels, so that we can apply convolutional 2D\ntranspose operations to it.\n\n\u00ae We apply Conv2DTranspose layers to expand the size of the tensor along one axis, so that it is the\nsame length as N_BARS.\n\n@ We remove the unnecessary extra dimension with a Reshape layer.\n\nThe reason we use convolutional operations rather than requiring two independent vectors into the net-\nwork is because we would like the network to learn how one bar should follow on from another in a con-\nsistent way. Using a neural network to expand the input vector along the time axis means the model has a\nchance to learn how music flows across bars, rather than treating each bar as completely independent of\nthe last.\n\nChords, style, melody, and groove\nLet\u2019s now take a closer look at the four different inputs that feed the generator:\n\nChords\n\nThe chords input is a single noise vector of length z_D1M. This vector\u2019s job is to control the general\nprogression of the music over time, shared across tracks, so we use a TemporalNetwork to trans-\nform this single vector into a different latent vector for every bar. Note that while we call this input\nchords, it really could control anything about the music that changes per bar, such as general\nrhythmic style, without being specific to any particular track.\n\nStyle\nThe style input is also a vector of length z_D1M. This is carried forward without transformation, so\n\nit is the same across all bars and tracks. It can be thought of as the vector that controls the overall\nstyle of the piece (i.e., it affects all bars and tracks consistently).\n\nMelody\n\nThe melody input is an array of shape [N_TRACKS, Z_ DIM] \u2014that is, we provide the model with a\nrandom noise vector of length z_pIM for each track.\n\nEach of these vectors is passed through a track-specific TemporalNetwork , where the weights are\nnot shared between tracks. The output is a vector of length z_DIm for every bar of every track.\nThe model can therefore use these input vectors to fine-tune the content of every single bar and\n\ntrack independently.\n\nGroove\n\nThe groove input is also an array of shape [N_TRACKS, Z_DIM] \u2014a random noise vector of length\n\nZ_DIM for each track. Unlike the melody input, these vectors are not passed through the temporal\nnetwork but instead are fed straight through, just like the style vector. Therefore, each groove vec-\ntor will affect the overall properties of a track, across all bars.\n\nWe can summarize the responsibilities of each component of the MuseGAN generator as shown in\nTable 11-1.\n\nTable 11-1. Components of the MuseGAN generator\n\nOutput differs across bars? Output differs across parts?\n\nStyle xX x\nGroove x v\nChords v x\nMelody v v\n\nThe final piece of the MuseGAN generator is the bar generator\u2014let\u2019s see how we can use this to glue to-\ngether the outputs from the chord, style, melody, and groove components.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.18,
                        "section_name": "The bar generator",
                        "section_path": "./screenshots-images-2/chapter_11/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_18/b01a3e0a-194b-46a6-9fb9-e7bc0ee3c41f.png",
                            "./screenshots-images-2/chapter_11/section_18/0e2a79ab-3549-4107-9616-073fe645bbf7.png",
                            "./screenshots-images-2/chapter_11/section_18/99544174-0df3-43ed-aff4-99c0e9b18540.png",
                            "./screenshots-images-2/chapter_11/section_18/a2bc37e0-d7fb-4546-87b4-32072e005134.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The bar generator\n\nThe bar generator receives four latent vectors\u2014one from each of the chord, style, melody, and groove\ncomponents. These are concatenated to produce a vector of length 4 * Z_DIM as input. The output isa\npiano roll representation of a single bar for a single track\u2014i.e., a tensor of shape [1,\n\nn_steps_ per bar, n_pitches, 1].\n\nThe bar generator is just a neural network that uses convolutional transpose layers to expand the time\nand pitch dimensions of the input vector. We create one bar generator for every track, and weights are\nnot shared between tracks. The Keras code to build a BarGenerator is given in Example 11-7.\n\nExample 11-7. Building the BarGenerator\n\ndef BarGenerator():\n\ninput_layer = layers.Input(shape=(Z_ DIM * 4,), name='bar_generator_input') \u00ae@\n\nx = layers.Dense(1024)(input_layer) @\n\nx = layers.BatchNormalization(momentum = 0.9) (x)\n\nx = layers.Activation('relu' ) (x)\n\nx = layers.Reshape([2,1,512]) (x)\n\nxX = conv_t(x, \u00a3=512, k=(2,1), s=(2,1), a= 'relu', p = 'same', bn = True) \u00a9\nx = conv_t(x, f=256, k=(2,1), s=(2,1), a= \u2018relu', p = 'same', bn = True)\n\nx = conv_t(x, f=256, k=(2,1), s=(2,1), a= \u2018relu', p = 'same', bn = True)\n\nxX = conv_t(x, \u00a3=256, k=(1,7), s=(1,7), a= \u2018'relu', p = 'same', bn = True) @\nxX = conv_t(x, f=1, k=(1,12), s=(1,12), a= 'tanh', p = 'same', bn = False) \u00a9\n\noutput_layer = layers.Reshape([1, N_STEPS_PER_BAR , N_PITCHES ,1])(x) \u00a9\n\nreturn models.Model(input_layer, output_layer)\n\n@ The input to the bar generator is a vector of length 4 * Z DIM.\n\n\u00ae After passing it through a Dense layer, we reshape the tensor to prepare it for the convolutional\ntranspose operations.\n\nFirst we expand the tensor along the timestep axis...\n\n...then along the pitch axis.\n\nThe final layer has a tanh activation applied, as we will be using a WGAN-GP (which requires tanh\noutput activation) to train the network.\n\nThe tensor is reshaped to add two extra dimensions of size 1, to prepare it for concatenation with\nother bars and tracks.\n\nPutting it all together\n\nUltimately, the MuseGAN generator takes the four input noise tensors (chords, style, melody, and groove)\nand converts them into a multitrack, multibar score. The Keras code to build the MuseGAN generator is\n\nprovided in Example 11-8.\n\nExample 11-8. Building the MuseGAN generator\n\ndef Generator():\nchords_input = layers. Input(shape=(Z_DIM,), name='chords_input') @\nstyle input = layers.Input(shape=(Z_DIM,), name='style input\u2019)\nmelody input = layers.Input(shape=(N_TRACKS, Z DIM), name='melody input' )\ngroove_input = layers.Input(shape=(N_TRACKS, Z DIM), name='groove input\u2019)\n\nchords_tempNetwork = TemporalNetwork() @\nchords_over_time = chords_tempNetwork(chords_input)\n\nmelody _over_time = [None] * N_TRACKS\nmelody _tempNetwork = [None] * N_TRACKS\nfor track in range(N_TRACKS):\nmelody _tempNetwork[track] = TemporalNetwork() \u00ae\nmelody track = layers.Lambda(lambda x, track = track: x[:,track,:])(\nmelody_input\n)\nmelody _over_time[track] = melody_tempNetwork[track] (melody track)\n\nbarGen = [None] * N_TRACKS\nfor track in range(N_TRACKS):\n\nbarGen[track] = BarGenerator() @\n\nbars_output = [None] * N_BARS\n\nc = [None] * N_BARS\n\nfor bar in range(N_BARS): \u00ae\ntrack_output = [None] * N_TRACKS\n\nc[bar] = layers.Lambda(lambda x, bar = bar: x[:,bar,:])(chords_over_time)\n\ns = style_input\n\nfor track in range(N_TRACKS):\n\nm = layers.Lambda(lambda x, bar = bar: x[:,bar,:])(\nmelody _over_time[track]\ng = layers.Lambda(lambda x, track = track: x[:,track,:])(\n\ngroove_input\n\nz_input = layers.Concatenate(\naxis = 1, name = \u2018total_input_bar_{} track_{}'.format(bar, track)\n)([e[bar],s,m,g])\n\ntrack_output[track] = barGen[track](z_input)\nbars_output[bar] = layers.Concatenate(axis = -1)(track_output)\n\ngenerator output = layers.Concatenate(axis = 1, name = \u2018concat_bars') (\nbars_output\n\n)\u00ae@\nreturn models.Model(\n{chords input, style input, melody input, groove_input], generator_output\n\n_)\n\ngenerator = Generator()\n\n@ Define the inputs to the generator.\n\n@ Pass the chords input through the temporal network.\n\n\u00ae Pass the melody input through the temporal network.\n\n\u00a9 Create an independent bar generator network for every track.\n\n\u00a9 Loop over the tracks and bars, creating a generated bar for each combination.\n\u00ae Concatenate everything together to form a single output tensor.\n\n@ The MuseGAN model takes four distinct noise tensors as input and outputs a generated multitrack,\nmultibar score.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.19,
                        "section_name": "The MuseGAN Critic",
                        "section_path": "./screenshots-images-2/chapter_11/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_19/71ee4e69-3aed-4f3f-a8a9-2588cf3b2547.png",
                            "./screenshots-images-2/chapter_11/section_19/e0bc8374-2e80-463a-91e6-40b87ab2d4f4.png",
                            "./screenshots-images-2/chapter_11/section_19/602d8b78-406b-48ae-ba47-e154353c5f39.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The MuseGAN Critic\n\nIn comparison to the generator, the critic architecture is much more straightforward (as is often the case\nwith GANs).\n\nThe critic tries to distinguish full multitrack, multibar scores created by the generator from real excerpts\nfrom the Bach chorales. It is a convolutional neural network, consisting mostly of Conv3D layers that col-\nlapse the score into a single output prediction.\n\nCONV3D LAYERS\nSo far in this book, we have only worked with Conv2pD layers, applicable to three-dimensional input images (width,\nheight, channels). Here we have to use Conv3D layers, which are analogous to Conv2D layers but accept four-di-\n\nmensional input tensors (n_bars , n_steps_per_bar, n_pitches, n_tracks).\n\nWe do not use batch normalization layers in the critic as we will be using the WGAN-GP framework for\ntraining the GAN, which forbids this.\n\nThe Keras code to build the critic is given in Example 11-9.\n\nExample 11-9. Building the MuseGAN critic\n\ndef conv(x, f, k, s, p):\n\n0\ni\n\nx = layers.Conv3D(filters\n\n0\nx\n\n, kernel_size\n, padding = p\n, Strides =s\n, kernel_initializer = initializer\n)(x)\n\nx = layers.LeakyReLU() (x)\n\nreturn x\n\ndef Critic():\ncritic_input = layers.Input(\nshape=(N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS),\nname='critic_input'\n\n) 9\n\nx = critic_input\nx = conv(x, \u00a3=128, k = (2,1,1), 8 = (1,1,1), p = \u2018valid') @\nx = conv(x, f=128, k = (N_BARS - 1,1,1), s = (1,1,1), p = \u2018valid')\n\nx = conv(x, f=128, (1,1,12), s = (1,1,12), p = 'same') \u00a9\n\nx = conv(x, f=128, (1,1,7), s = (1,1,7), p = \u2018'same')\nx = conv(x, f=128, k = (1,2,1), s = (1,2,1), p = \u2018same') @\nx = conv(x, f=128, k = (1,2,1), s = (1,2,1), p = \u2018same')\nxX = conv(x, \u00a3=256, k = (1,4,1), s = (1,2,1), p = \u2018same')\nx = conv(x, \u00a3=512, k = (1,3,1), s = (1,2,1), p = \u2018same')\n\nx = layers.Flatten() (x)\n\nx = layers.Dense(1024, kernel_initializer = initializer) (x)\nx = layers.LeakyReLU() (x)\n\ncritic_output = layers.Dense(\n1, activation=None, kernel_initializer = initializer\n)(x) \u00a9\nreturn models.Model(critic_input, critic_output)\ncritic = Critic()\n\n@ The input to the critic is an array of multitrack, multibar scores, each of shape [N_BARS,\nN_STEPS_PER_BAR, N_PITCHES, N_TRACKS] .\n\n\u00ae@ First, we collapse the tensor along the bar axis. We apply Conv3D layers throughout the critic as\nwe are working with 4D tensors.\n\nFirst, we collapse the tensor along the bar axis. We apply Conv3D layers throughout the critic as\nwe are working with 4D tensors.\n\nNext, we collapse the tensor along the pitch axis.\n\nFinally, we collapse the tensor along the timesteps axis.\n\nThe output is a Dense layer with a single unit and no activation function, as required by the\nWGAN-GP framework.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.2,
                        "section_name": "Analysis of the MuseGAN",
                        "section_path": "./screenshots-images-2/chapter_11/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_20/42c9c9a6-77e3-48f7-9e51-55a222c26027.png",
                            "./screenshots-images-2/chapter_11/section_20/9a533684-b3d4-4c96-b2cc-1867487cbe7d.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Analysis of the MuseGAN\n\nWe can perform some experiments with our MuseGAN by generating a score, then tweaking some of the\ninput noise parameters to see the effect on the output.\n\nThe output from the generator is an array of values in the range [-1, 1] (due to the tanh activation func-\ntion of the final layer). To convert this to a single note for each track, we choose the note with the maxi-\nmum value over all 84 pitches for each timestep. In the original MuseGAN paper the authors use a thresh-\nold of 0, as each track can contain multiple notes; however, in this setting we can simply take the maxi-\nmum to guarantee exactly one note per timestep per track, as is the case for the Bach chorales.\n\nFigure 11-16 shows a score that has been generated by the model from random normally distributed\nnoise vectors (top left). We can find the closest score in the dataset (by Euclidean distance) and check that\nour generated score isn\u2019t a copy of a piece of music that already exists in the dataset\u2014the closest score is\nshown just below it, and we can see that it does not resemble our generated score.\n\nGenerated score Changing the style noise\noe a\nSe a a\n= \u2014 ss\n= SS SS\nChanging the melody noise\nClosest real score for the top line only\n\nae\n\nChanging the groove noise\nChanging the chord noise for the bottom line only\nrare) se \u201c| \u2014\u2014S \u2014\u2014\u2014\u2014\u2014 |\nde ee ee ee ee eer? \u2014\u2014 :\u2014\nSe eee \u2014\u2014 \u2014 ==\n2: = = = === nt fa:\n\nFigure 11-16. Example of a MuseGAN predicted score, showing the closest real score in the training data and how the generated score is affected\nby changing the input noise\n\nLet\u2019s now play around with the input noise to tweak our generated score. First, we can try changing the\nchord noise vector\u2014the bottom-left score in Figure 11-16 shows the result. We can see that every track\nhas changed, as expected, and also that the two bars exhibit different properties. In the second bar, the\nbaseline is more dynamic and the top line is higher in pitch than in the first bar. This is because the latent\nvectors that affect the two bars are different, as the input chord vector was passed through a temporal\nnetwork.\n\nWhen we change the style vector (top right), both bars change in a similar way. The whole passage has\nchanged style from the original generated score, in a consistent way (i.e., the same latent vector is being\nused to adjust all tracks and bars).\n\nWe can also alter tracks individually, through the melody and groove inputs. In the center-right score in\nFigure 11-16 we can see the effect of changing just the melody noise input for the top line of music. All\nother parts remain unaffected, but the top-line notes change significantly. Also, we can see a rhythmic\nchange between the two bars in the top line: the second bar is more dynamic, containing faster notes\nthan the first bar.\n\nLastly, the bottom-right score in the diagram shows the predicted score when we alter the groove input\nparameter for only the baseline. Again, all other parts remain unaffected, but the baseline is different.\nMoreover, the overall pattern of the baseline remains similar between bars, as we would expect.\n\nThis shows how each of the input parameters can be used to directly influence high-level features of the\ngenerated musical sequence, in much the same way as we were able to adjust the latent vectors of VAEs\nand GANs in previous chapters to alter the appearance of a generated image. One drawback to the model\nis that the number of bars to generate must be specified up front. To tackle this, the authors show an ex-\ntension to the model that allows previous bars to be fed in as input, allowing the model to generate long-\nform scores by continually feeding the most recent predicted bars back in as additional input.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 11.21,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_11/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_11/section_21/55e000d0-2829-4376-87e2-4ad9115aaee8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter we have explored two different kinds of models for music generation: a Transformer and\na MuseGAN.\n\nThe Transformer is similar in design to the networks we saw in Chapter 9 for text generation. Music and\ntext generation share a lot of features in common, and often similar techniques can be used for both. We\nextended the Transformer architecture by incorporating two input and output streams, for note and du-\nration. We saw how the model was able to learn about concepts such as keys and scales, simply by learn-\ning to accurately generate the music of Bach.\n\nWe also explored how we can adapt the tokenization process to handle polyphonic (multitrack) music\ngeneration. Grid tokenization serializes a piano roll representation of the score, allowing us to train a\nTransformer on a single stream of tokens that describe which note is present in each voice, at discrete,\nequally spaced timestep intervals. Event-based tokenization produces a recipe that describes how to cre-\nate the multiple lines of music in a sequential fashion, through a single stream of instructions. Both meth-\nods have advantages and disadvantages\u2014the success or failure of a Transformer-based approach to mu-\nsic generation is often heavily dependent on the choice of tokenization method.\n\nWe also saw that generating music does not always require a sequential approach\u2014MuseGAN uses con-\nvolutions to generate polyphonic musical scores with multiple tracks, by treating the score as an image\nwhere the tracks are individual channels of the image. The novelty of MuseGAN lies in the way the four\ninput noise vectors (chords, style, melody, and groove) are organized so that it is possible to maintain full\ncontrol over high-level features of the music. While the underlying harmonization is still not as perfect or\nvaried as Bach\u2019s, it is a good attempt at what is an extremely difficult problem to master and highlights\nthe power of GANs to tackle a wide variety of problems.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 12,
                "chapter_name": "Chapter 12. World Models",
                "chapter_path": "./screenshots-images-2/chapter_12",
                "sections": [
                    {
                        "section_id": 12.1,
                        "section_name": "Chapter 12. World Models",
                        "section_path": "./screenshots-images-2/chapter_12/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_1/0983acd7-f90d-4cf8-90db-527513c27656.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 12. World Models\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\ne Walk through the basics of reinforcement learning (RL).\n\n\u00a9 Understand how generative modeling can be used within a world model approach to RL.\n\n\u00a2 See how to train a variational autoencoder (VAE) to capture environment observations in a low-di-\nmensional latent space.\n\n\u00a2 Walk through the training process of a mixture density network-recurrent neural network (MDN-\nRNN) that predicts the latent variable.\n\n\u00a2 Use the covariance matrix adaptation evolution strategy (CMA-ES) to train a controller that can take\nintelligent actions in the environment.\n\n\u00a9 Understand how the trained MDN-RNN can itself be used as an environment, allowing the agent to\ntrain the controller within its own hallucinated dreams, rather than the real environment.\n\nThis chapter introduces one of the most interesting applications of generative models in recent years,\nnamely their use within so-called world models.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_12/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_2/79292aa5-dc99-49f3-afc9-33a41ad97918.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nIn March 2018, David Ha and Jiirgen Schmidhuber published their \u201cWorld Models\u201d paper.\u2019 The paper\nshowed how it is possible to train a model that can learn how to perform a particular task through exper-\nimentation within its own generated dream environment, rather than inside the real environment. It is\nan excellent example of how generative modeling can be used to solve practical problems, when applied\nalongside other machine learning techniques such as reinforcement learning.\n\nA key component of the architecture is a generative model that can construct a probability distribution\nfor the next possible state, given the current state and action. Having built up an understanding of the\nunderlying physics of the environment through random movements, the model is then able to train itself\nfrom scratch on a new task, entirely within its own internal representation of the environment. This ap-\nproach led to world-best scores for both of the tasks on which it was tested.\n\nIn this chapter we will explore the model from the paper in detail, with particular focus on a task that re-\nquires the agent to learn how to drive a car around a virtual racetrack as fast as possible. While we will\nbe using a 2D computer simulation as our environment, the same technique could also be applied to real-\nworld scenarios where testing strategies in the live environment is expensive or infeasible.\n\nTIP\nIn this chapter we will reference the excellent TensorFlow implementation of the \u201cWorld Models\u201d paper available\npublicly on GitHub, which I encourage you to clone and run yourself!\n\nBefore we start exploring the model, we need to take a closer look at the concept of reinforcement\nlearning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.3,
                        "section_name": "Reinforcement Learning",
                        "section_path": "./screenshots-images-2/chapter_12/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_3/81538d39-de44-48d2-bc7d-240d6bde18fc.png",
                            "./screenshots-images-2/chapter_12/section_3/73780f60-3788-43e5-9a51-a745da0c8716.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Reinforcement Learning\n\nReinforcement learning can be defined as follows:\n\nReinforcement learning (RL) is a field of machine learning that aims to train an agent to perform opti-\nmally within a given environment, with respect to a particular goal.\n\nWhile both discriminative modeling and generative modeling aim to minimize a loss function over a\ndataset of observations, reinforcement learning aims to maximize the long-term reward of an agent ina\ngiven environment. It is often described as one of the three major branches of machine learning, along-\nside supervised learning (predicting using labeled data) and unsupervised learning (learning structure\nfrom unlabeled data).\n\nLet\u2019s first introduce some key terminology related to reinforcement learning:\n\nEnvironment\n\nThe world in which the agent operates. It defines the set of rules that govern the game state update\nprocess and reward allocation, given the agent\u2019s previous action and current game state. For exam-\nple, if we were teaching a reinforcement learning algorithm to play chess, the environment would\nconsist of the rules that govern how a given action (e.g., the pawn move e2e4 ) affects the next\ngame state (the new positions of the pieces on the board) and would also specify how to assess if a\ngiven position is checkmate and allocate the winning player a reward of 1 after the winning move.\n\nAgent\n\nThe entity that takes actions in the environment.\n\nGame state\n\nThe data that represents a particular situation that the agent may encounter (also just called a\nstate). For example, a particular chessboard configuration with accompanying game information\n\nsuch as which player will make the next move.\n\nAction\n\nA feasible move that an agent can make.\n\nReward\n\nThe value given back to the agent by the environment after an action has been taken. The agent\naims to maximize the long-term sum of its rewards. For example, in a game of chess, checkmating\nthe opponent\u2019s king has a reward of 1 and every other move has a reward of 0. Other games have\nrewards constantly awarded throughout the episode (e.g., points in a game of Space Invaders).\n\nEpisode\n\nOne run of an agent in the environment; this is also called a rollout.\n\nTimestep\n\nFor a discrete event environment, all states, actions, and rewards are subscripted to show their val-\n\nue at timestep t.\n\nThe relationship between these concepts is shown in Figure 12-1.\n\naction\n\nq,\n\nEnvironment\n\nFigure 12-1. Reinforcement learning diagram\n\nThe environment is first initialized with a current game state, $9. At timestep #, the agent receives the\n\ncurrent game state s; and uses this to decide on its next best action a;, which it then performs. Given this\naction, the environment then calculates the next state s;,; and reward r;,; and passes these back to the\nagent, for the cycle to begin again. The cycle continues until the end criterion of the episode is met (e.g., a\n\ngiven number of timesteps elapse or the agent wins/loses).\n\nHow can we design an agent to maximize the sum of rewards in a given environment? We could build an\nagent that contains a set of rules for how to respond to any given game state. However, this quickly be-\ncomes infeasible as the environment becomes more complex and doesn\u2019t ever allow us to build an agent\nthat has superhuman ability in a particular task, as we are hardcoding the rules. Reinforcement learning\ninvolves creating an agent that can learn optimal strategies by itself in complex environments through\n\nrepeated play.\n\nLet\u2019s now take a look at the CarRacing environment that simulates a car driving around a track.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.4,
                        "section_name": "The CarRacing Environment",
                        "section_path": "./screenshots-images-2/chapter_12/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_4/b7c3da8a-7420-4195-bffc-e9342923beea.png",
                            "./screenshots-images-2/chapter_12/section_4/6b7f0a42-17ff-4bf2-8508-01a9f0532946.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The CarRacing Environment\n\nCarRacing is an environment that is available through the Gymnasium package. Gymnasium is a\nPython library for developing reinforcement learning algorithms that contains several classic reinforce-\nment learning environments, such as CartPole and Pong, as well as environments that present more\ncomplex challenges, such as training an agent to walk on uneven terrain or win an Atari game.\n\nGYMNASIUM\nGymnasium is a maintained fork of OpenAl\u2019s Gym library\u2014since 2021, further development of Gym has shifted to\nGymnasium. In this book, we therefore refer to Gymnasium environments as Gym environments.\n\nAll of the environments provide a step method through which you can submit a given action; the envi-\nronment will return the next state and the reward. By repeatedly calling the step method with the actions\nchosen by the agent, you can play out an episode in the environment. There is also a reset method for re-\nturning the environment to its initial state and a render method that allows you to watch your agent per-\nform in a given environment. This is useful for debugging and finding areas where your agent could\nimprove.\n\nLet\u2019s see how the game state, action, reward, and episode are defined for the CarRacing environment:\n\nGame state\n\nA 64 x 64-pixel RGB image depicting an overhead view of the track and car.\n\nAction\nA set of three values: the steering direction (-1 to 1), acceleration (0 to 1), and braking (0 to 1). The\nagent must set all three values at each timestep.\n\nReward\nA negative penalty of -0.1 for each timestep taken and a positive reward of 1,000/N if a new track\ntile is visited, where N is the total number of tiles that make up the track.\n\nEpisode\nThe episode ends when the car completes the track or drives off the edge of the environment, or\n\nwhen 3,000 timesteps have elapsed.\n\nThese concepts are shown on a graphical representation of a game state in Figure 12-2.\n\nThe track is divided into N\ninvisible \u201ctiles.\u201d The agent\nreceives a reward of 1,000/N\nevery time it discovers a new\n\ntrack tile and also a penalty of\n-0.levery timestep.\n\nThe agent must\n\ndecide the value to\nset for three actions.\n\nAcceleration\n\nTurn direction\n\nBraking\n\nFigure 12-2. A graphical representation of one game state in the CarRacing environment\n\nPERSPECTIVE\nWe should imagine the agent floating above the track and controlling the car from a bird\u2019s-eye view, rather than\n\nviewing the track from the driver\u2019s perspective.\n\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.5,
                        "section_name": "World Model Overview",
                        "section_path": "./screenshots-images-2/chapter_12/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_5/66a0b996-a0d6-436c-abfb-7604a153ba10.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "World Model Overview\n\nWe'll now cover a high-level overview of the entire world model architecture and training process, be-\nfore diving into each component in more detail.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.6,
                        "section_name": "Architecture",
                        "section_path": "./screenshots-images-2/chapter_12/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_6/6595fd9c-a94f-4530-8217-6bdf1c408de6.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Architecture\n\nThe solution consists of three distinct parts, as shown in Figure 12-3, that are trained separately:\n\nVv\n\nA variational autoencoder (VAE)\n\nM\n\nA recurrent neural network with a mixture density network (MDN-RNN)\n\nA controller\n\nWorld model\nVAE (V)\n\nObservation\n\nController (C)\n\nFigure 12-3. World model architecture diagram\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.7,
                        "section_name": "The VAE",
                        "section_path": "./screenshots-images-2/chapter_12/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_7/3e4f3ece-6370-4074-9005-0d6fe2a5862a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The VAE\n\nWhen you make decisions while driving, you don\u2019t actively analyze every single pixel in your view\u2014in-\nstead, you condense the visual information into a smaller number of latent entities, such as the straight-\nness of the road, upcoming bends, and your position relative to the road, to inform your next action.\n\nWe saw in Chapter 3 how a VAE can take a high-dimensional input image and condense it into a latent\nrandom variable that approximately follows a standard Gaussian distribution, through minimization of\nthe reconstruction error and KL divergence. This ensures that the latent space is continuous and that we\nare able to easily sample from it to generate meaningful new observations.\n\nIn the car racing example, the VAE condenses the 64 x 64 x 3 (RGB) input image into a 32-dimensional\nnormally distributed random variable, parameterized by two variables, mu and logvar . Here, logvar\nis the logarithm of the variance of the distribution. We can sample from this distribution to produce a la-\ntent vector z that represents the current state. This is passed on to the next part of the network, the\nMDN-RNN.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.8,
                        "section_name": "The MDN-RNN",
                        "section_path": "./screenshots-images-2/chapter_12/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_8/22511c41-3a4f-46a7-8d97-1a090eac2867.png",
                            "./screenshots-images-2/chapter_12/section_8/42360d20-e259-4502-9091-21f331020c4a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The MDN-RNN\n\nAs you drive, each subsequent observation isn\u2019t a complete surprise to you. If the current observation\nsuggests a left turn in the road ahead and you turn the wheel to the left, you expect the next observation\nto show that you are still in line with the road.\n\nIf you didn\u2019t have this ability, your car would probably snake all over the road as you wouldn\u2019t be able to\nsee that a slight deviation from the center is going to be worse in the next timestep unless you do some-\nthing about it now.\n\nThis forward thinking is the job of the MDN-RNN, a network that tries to predict the distribution of the\nnext latent state based on the previous latent state and the previous action.\n\nSpecifically, the MDN-RNN is an LSTM layer with 256 hidden units followed by a mixture density network\n(MDN) output layer that allows for the fact that the next latent state could actually be drawn from any\none of several normal distributions.\n\nThe same technique was applied by one of the authors of the \u201cWorld Models\u201d paper, David Ha, to a hand-\nwriting generation task, as shown in Figure 12-4, to describe the fact that the next pen point could land in\nany one of the distinct red areas.\n\ne\n=,\n\nFigure 12-4. MDN for handwriting generation\n\nIn the car racing example, we allow for each element of the next observed latent state to be drawn from\nany one of five normal distributions.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.9,
                        "section_name": "The controller",
                        "section_path": "./screenshots-images-2/chapter_12/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_9/c7a66d71-2576-4513-a48e-5f43c8083acc.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The controller\n\nUntil this point, we haven\u2019t mentioned anything about choosing an action. That responsibility lies with\nthe controller. The controller is a densely connected neural network, where the input is a concatenation\nof z (the current latent state sampled from the distribution encoded by the VAE) and the hidden state of\nthe RNN. The three output neurons correspond to the three actions (turn, accelerate, brake) and are\nscaled to fall in the appropriate ranges.\n\nThe controller is trained using reinforcement learning as there is no training dataset that will tell us that\na certain action is good and another is bad. Instead, the agent discovers this for itself through repeated\nexperimentation.\n\nAs we shall see later in the chapter, the crux of the \u201cWorld Models\u201d paper is that it demonstrates how this\nreinforcement learning can take place within the agent\u2019s own generative model of the environment,\nrather than the Gym environment. In other words, it takes place in the agent\u2019s hallucinated version of\nhow the environment behaves, rather than the real thing.\n\nTo understand the different roles of the three components and how they work together, we can imagine a\ndialogue between them:\n\nVAE (looking at latest 64 x 64 x 3 observation): This looks like a straight road, with a slight left bend\napproaching, with the car facing in the direction of the road ( z).\n\nRNN: Based on that description ( z ) and the fact that the controller chose to accelerate hard at the\nlast timestep ( action ), I will update my hidden state ( h) so that the next observation is predicted to\nstill be a straight road, but with slightly more left turn in view.\n\nController: Based on the description from the VAE ( z) and the current hidden state from the RNN\n(h), my neural network outputs [0.34, 0.8, 0] as the next action.\n\nThe action from the controller is then passed to the environment, which returns an updated observation,\nand the cycle begins again.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.1,
                        "section_name": "Training",
                        "section_path": "./screenshots-images-2/chapter_12/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_10/6de9201a-f951-4bb6-9d5c-71be86074eaa.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training\n\nThe training process consists of five steps, run in sequence, which are outlined here:\n\n1.\n\nwon\n\nGa\n\nCollect random rollout data. Here, the agent does not care about the given task, but instead simply\nexplores the environment using random actions. Multiple episodes are simulated and the observed\nstates, actions, and rewards at each timestep are stored. The idea is to build up a dataset of how the\nphysics of the environment works, which the VAE can then learn from to capture the states efficient-\nly as latent vectors. The MDN-RNN can then subsequently learn how the latent vectors evolve over\n\ntime.\n\n. Train the VAE. Using the randomly collected data, we train a VAE on the observation images.\n. Collect data to train the MDN-RNN. Once we have a trained VAE, we use it to encode each of the col-\n\nlected observations into mu and logvar vectors, which are saved alongside the current action and\nreward.\n\nTrain the MDN-RNN. We take batches of episodes and load the corresponding mu, logvar ,\naction, and reward variables at each timestep that were generated in step 3. We then sample a z\nvector from the mu and logvar vectors. Given the current z vector, action, and reward, the\nMDN-RNN is then trained to predict the subsequent z vector and reward.\n\nTrain the controller. With a trained VAE and RNN, we can now train the controller to output an ac-\ntion given the current z and hidden state, h, of the RNN. The controller uses an evolutionary algo-\nrithm, CMA-ES, as its optimizer. The algorithm rewards matrix weightings that generate actions that\nlead to overall high scores on the task, so that future generations are also likely to inherit this de-\n\nsired behavior.\n\nLet\u2019s now take look at each of these steps in more detail.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.11,
                        "section_name": "Collecting Random Rollout Data",
                        "section_path": "./screenshots-images-2/chapter_12/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_11/bbfcf000-d8ef-43a7-a494-34c7d844823a.png",
                            "./screenshots-images-2/chapter_12/section_11/4370c6ab-c6f9-457d-b0aa-239312bb7199.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Collecting Random Rollout Data\n\nThe first step is to collect rollout data from the environment, using an agent taking random actions. This\nmay seem strange, given we ultimately want our agent to learn how to take intelligent actions, but this\nstep will provide the data that the agent will use to learn how the world operates and how its actions (al-\nbeit random at first) influence subsequent observations.\n\nWe can capture multiple episodes in parallel by spinning up multiple Python processes, each running a\nseparate instance of the environment. Each process will run on a separate core, so if your machine has\nlots of cores you can collect data much faster than if you only have a few cores.\n\nThe hyperparameters used by this step are as follows:\n\nparallel_processes\n\nThe number of parallel processes to run (e.g., 8 if your machine has 28 cores)\n\nmax_trials\n\nHow many episodes each process should run in total (e.g., 125 , so 8 processes would create 1,000\nepisodes overall)\n\nmax_frames\nThe maximum number of timesteps per episode (e.g., 300 )\nFigure 12-5 shows an excerpt from frames 40 to 59 of one episode, as the car approaches a corner, along-\n\nside the randomly chosen action and reward. Note how the reward changes to 3.22 as the car rolls over\nnew track tiles but is otherwise -0.1.\n\nes es\n\na=[0.270.0.] [0.270.0.]\nr=-01 r=3.22\na= 10.27 0.0] a= 10420. 0)\nr=-0, r=-0.\na=[0420.0.] a=[0420.0.]\nr=-01 r=-01\n\na=[0.270.0.]\nr=-01\n\nA\n\na= 042 8. 0]\n\nAu\n\na=[0.0.43.0.]\nr=-01\n\na=[0.270.0.]\nr=-01\n\nA\n\na= (0.420. 0)\nT=-0.\n\n1\n\na=[0.0.43.0.]\nr=3.22\n\na=[-0.25. 0.0] a=[-0.25.0.0.]\nr=3.22 r=-01\n\na=[-0.25.0.0.]\nr=-01\n\nFigure 12-5. Frames 40 to 59 of one episode\n\na=[-0.25. 0.0]\nr=-01\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.12,
                        "section_name": "Training the VAE",
                        "section_path": "./screenshots-images-2/chapter_12/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_12/45f93a6f-a5d0-4130-9539-bf43115cf2d9.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the VAE\n\nWe now build a generative model (a VAE) on this collected data. Remember, the aim of the VAE is to allow\nus to collapse one 64 x 64 x 3 image into a normally distributed random variable z, whose distribution is\nparameterized by two vectors, mu and logvar . Each of these vectors is of length 32. The hyperparame-\nters of this step are as follows:\n\nvae_batch_size\n\nThe batch size to use when training the VAE (how many observations per batch) (e.g., 100 )\nz_size\nThe length of latent z vector (and therefore mu and logvar variables) (e.g., 32 )\n\nvae_num_epoch\n\nThe number of training epochs (e.g., 10 )\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.13,
                        "section_name": "The VAE Architecture",
                        "section_path": "./screenshots-images-2/chapter_12/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_13/7940c822-d5cb-4d38-a158-7bc02a86f0fa.png",
                            "./screenshots-images-2/chapter_12/section_13/0b459d8e-2d30-4b90-accc-9f088869980e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The VAE Architecture\n\nAs we have seen previously, Keras allows us to not only define the VAE model that will be trained end-to-\nend, but also additional submodels that define the encoder and decoder of the trained network separate-\nly. These will be useful when we want to encode a specific image or decode a given z vector, for exam-\nple. We'll define the VAE model and three submodels, as follows:\n\nvae\n\nThis is the end-to-end VAE that is trained. It accepts a 64 x 64 x 3 image as input and outputs a re-\nconstructed 64 x 64 x 3 image.\n\nencode_mu_logvar\nThis accepts a 64 x 64 x 3 image as input and outputs the mu and logvar vectors corresponding\nto this input. Running the same input image through this model multiple times will produce the\nsame mu and logvar vectors each time.\n\nencode\n\nThis accepts a 64 x 64 x 3 image as input and outputs a sampled z vector. Running the same input\nimage through this model multiple times will produce a different z vector each time, using the cal-\nculated mu and logvar values to define the sampling distribution.\n\ndecode\n\nThis accepts a z vector as input and returns the reconstructed 64 x 64 x 3 image.\n\nA diagram of the model and submodels is shown in Figure 12-6.\n\ndecode\n\nencode_mu_logvar\n\nFigure 12-6. The VAE architecture from the \u201cWorld Models\u201d paper\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.14,
                        "section_name": "Exploring the VAE",
                        "section_path": "./screenshots-images-2/chapter_12/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_14/4b3db4b1-7280-4ee2-8f42-ecba871d55b8.png",
                            "./screenshots-images-2/chapter_12/section_14/d9b07b03-1325-4bf3-9dca-6d624c014de9.png",
                            "./screenshots-images-2/chapter_12/section_14/54d13360-de38-4338-8f9b-7e8abe1d5a9a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Exploring the VAE\n\nWe'll now take a look at the output from the VAE and each submodel and then see how the VAE can be\nused to generate completely new track observations.\n\nThe VAE model\n\nIf we feed the VAE with an observation, it is able to accurately reconstruct the original image, as shown in\nFigure 12-7. This is useful to visually check that the VAE is working correctly.\n\nInput Output\n\nFigure 12-7. The input and output from the VAE model\n\nThe encoder models\n\nIf we feed the encode_mu_logvar model with an observation, the output is the generated mu and\nlogvar vectors describing a multivariate normal distribution. The encode model goes one step further\nby sampling a particular z vector from this distribution. The diagram showing the output from the two\nencoder models is shown in Figure 12-8.\n\nencode_mu_logvar\n\nFigure 12-8. The output from the encoder models\n\nThe latent variable z is sampled from the Gaussian defined by mu and logvar by sampling froma\nstandard Gaussian and then scaling and shifting the sampled vector (Example 12-1).\n\nExample 12-1. Sampling z from the multivariate normal distribution defined by mu and logvar\n\neps = tf.random_normal(shape=tf.shape(mu) )\nsigma = tf.exp(logvar * 0.5)\nz = mu + eps * sigma\n\nThe decoder model\n\nThe decode model accepts a z vector as input and reconstructs the original image. In Figure 12-9 we\nlinearly interpolate two of the dimensions of z to show how each dimension appears to encode a partic-\nular aspect of the track\u2014in this example z[4] controls the immediate left/right direction of the track\nnearest the car and z[7] controls the sharpness of the approaching left turn.\n\nThis shows that the latent space that the VAE has learned is continuous and can be used to generate new\ntrack segments that have never before been observed by the agent.\n\n2[4] -2.0 2[4]-2.0\n\nz{7]=-1.0 z{7] = 0.0\n\n2[4] -2.0\nz{7]=-2.0\n\n7[4] -2.0\n\n2[4] -2.0\nz[7]=2.0\n\n&\n=}\nr\no\n\n2[4}-1.0\nZI 0\n\na)\ncS)\npm\ni)\n\n4] -1.0\na 2.0\n\nmy\n\u2014\nv\n\nafl\n\n2[4} 0.0 2[4] 0.0 24] 0.0 2[4] 0.0 2[4] 0.0\n27] =-2.0 27) =-10 27]=0.0 27] =1.0 27] =2.0\n4)1.0 4] 1.0 4] 1.0 [4] 1.0 4] 1.0\ndh ] -20 dn : -10 ij ae 0 an 10 an: 2.0\n2[4]2.0 214] 2.0 2[4]2.0 2[4]2.0 2[4]2.0\n27) =-2.0 27) =-10 2[7]=0.0 27) =1.0 27] =2.0\n\nFigure 12-9. A linear interpolation of two dimensions of z\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.15,
                        "section_name": "Collecting Data to Train the MDN-RNN",
                        "section_path": "./screenshots-images-2/chapter_12/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_15/fd9ae447-76ad-4297-81e2-17ceecccd441.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Collecting Data to Train the MDN-RNN\n\nNow that we have a trained VAE, we can use this to generate training data for our MDN-RNN.\n\nIn this step, we pass all of the random rollout observations through the encode_mu_logvar model and\nstore the mu and logvar vectors corresponding to each observation. This encoded data, along with the\nalready collected action, reward,and done variables, will be used to train the MDN-RNN. This\nprocess is shown in Figure 12-10.\n\nRollout dataset RNN training dataset\n\nencode_mu_logvar\n\naction = [0.42, 0, 0]\nreward = -0.1\ndone = 0\n\naction = [0.42, 0, 0]\nreward = -0.1\ndone = 0\n\nFigure 12-10. Creating the MDN-RNN training dataset\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.16,
                        "section_name": "Training the MDN-RNN",
                        "section_path": "./screenshots-images-2/chapter_12/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_16/6a5f2fd3-45e6-480c-8b65-3023c45bbfd5.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the MDN-RNN\n\nWe can now train the MDN-RNN to predict the distribution of the next z vector and reward one timestep\nahead into the future, given the current z vector, current action, and previous reward. We can then use\nthe internal hidden state of the RNN (which can be thought of as the model\u2019s current understanding of\nthe environment dynamics) as part of the input into the controller, which will ultimately decide on the\nbest next action to take.\n\nThe hyperparameters of this step of the process are as follows:\n\nxrnn_batch_size\n\nThe batch size to use when training the MDN-RNN (how many sequences per batch) (e.g., 100 )\n\nrnn_num_steps\n\nThe total number of training iterations (e.g., 4000 )\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.17,
                        "section_name": "The MDN-RNN Architecture",
                        "section_path": "./screenshots-images-2/chapter_12/section_17",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_17/38c5ea6e-8a1b-43ed-be0f-2f9cb676e657.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The MDN-RNN Architecture\n\nThe architecture of the MDN-RNN is shown in Figure 12-11.\n\n\u2014\u2014\u2122\n\n| output: | [(None, None, 256), (None, 256), (None, 256)]\n\nFp [ere None 255\n\ndense.:Dense (None, None, 481)\n\nFigure 12-11. The MDN-RNN architecture\n\nThe MDN-RNN consists of an LSTM layer (the RNN), followed by a densely connected layer (the MDN) that\ntransforms the hidden state of the LSTM into the parameters of a mixture distribution. Let\u2019s walk through\nthe network step by step.\n\nThe input to the LSTM layer is a vector of length 36\u2014a concatenation of the encoded z vector (length 32)\nfrom the VAE, the current action (length 3), and the previous reward (length 1).\n\nThe output from the LSTM layer is a vector of length 256\u2014one value for each LSTM cell in the layer. This\nis passed to the MDN, which is just a densely connected layer that transforms the vector of length 256 into\na vector of length 481.\n\nWhy 481? Figure 12-12 explains the composition of the output from the MDN-RNN. The aim of a mixture\ndensity network is to model the fact that our next z could be drawn from one of several possible distrib-\nutions with a certain probability. In the car racing example, we choose five normal distributions. How\nmany parameters do we need to define these distributions? For each of the 5 mixtures, we need a mu and\na logvar (to define the distribution) and a log-probability of this mixture being chosen ( logpi ), for\neach of the 32 dimensions of z. This makes 5 x 3 x 32 = 480 parameters. The one extra parameter is for\nthe reward prediction.\n\nMixture number\nVector of length 32\n_\n1 2 3 4 5 1 2 3 4 5 1 +2 3 4 ~\u00b05\n[| | | | es | es | es | es | [es es ss os |\na ee |\nlogpi mu logvar\n\nReward\n\nFigure 12-12. The output from the mixture density network\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.18,
                        "section_name": "Sampling from the MDN-RNN",
                        "section_path": "./screenshots-images-2/chapter_12/section_18",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_18/c78c088f-985a-44dc-978e-42363ba2fd8e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Sampling from the MDN-RNN\n\nWe can sample from the MDN output to generate a prediction for the next z and reward at the following\n\ntimestep, through the following process:\n\na\n\ngS\n\ngo\n\nSplit the 481-dimensional output vector into the 3 variables (logpi, mu, logvar ) and the reward\nvalue.\n\nExponentiate and scale logpi so that it can be interpreted as 32 probability distributions over the 5\nmixture indices.\n\nFor each of the 32 dimensions of z, sample from the distributions created from logpi (i.e., choose\nwhich of the 5 distributions should be used for each dimension of z ).\n\nFetch the corresponding values of mu and logvar for this distribution.\n\n5. Sample a value for each dimension of z from the normal distribution parameterized by the chosen\n\nparameters of mu and logvar for this dimension.\n\nThe loss function for the MDN-RNN is the sum of the z vector reconstruction loss and the reward loss.\n\nThe z vector reconstruction loss is the negative log-likelihood of the distribution predicted by the MDN-\n\nRNN, given the true value of z , and the reward loss is the mean squared error between the predicted re-\n\nward and the true reward.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.19,
                        "section_name": "Training the Controller",
                        "section_path": "./screenshots-images-2/chapter_12/section_19",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_19/8de27e70-f95a-4506-927c-bac6c6ae7e4e.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Training the Controller\n\nThe final step is to train the controller (the network that outputs the chosen action) using an evolutionary\nalgorithm called the covariance matrix adaptation evolution strategy (CMA-ES).\n\nThe hyperparameters of this step of the process are as follows:\n\ncontroller _num_worker\n\nThe number of workers that will test solutions in parallel\n\ncontroller _num_worker_trial\n\nThe number of solutions that each worker will be given to test at each generation\n\ncontroller _num_episode\n\nThe number of episodes that each solution will be tested against to calculate the average reward\n\ncontroller _eval_steps\n\nThe number of generations between evaluations of the current best parameter set\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.2,
                        "section_name": "The Controller Architecture",
                        "section_path": "./screenshots-images-2/chapter_12/section_20",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_20/90601d31-3744-4d24-a2e0-f51c92e10d7a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Controller Architecture\n\nThe architecture of the controller is very simple. It is a densely connected neural network with no hidden\nlayers. It connects the input vector directly to the action vector.\n\nThe input vector is a concatenation of the current z vector (length 32) and the current hidden state of\nthe LSTM (length 256), giving a vector of length 288. Since we are connecting each input unit directly to\nthe 3 output action units, the total number of weights to tune is 288 x 3 = 864, plus 3 bias weights, giving\n867 in total.\n\nHow should we train this network? Notice that this is not a supervised learning problem\u2014we are not try-\ning to predict the correct action. There is no training set of correct actions, as we do not know what the\noptimal action is for a given state of the environment. This is what distinguishes this as a reinforcement\nlearning problem. We need the agent to discover the optimal values for the weights itself by experiment-\ning within the environment and updating its weights based on received feedback.\n\nEvolutionary strategies are a popular choice for solving reinforcement learning problems, due to their\nsimplicity, efficiency, and scalability. We shall use one particular strategy, known as CMA-ES.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.21,
                        "section_name": "CMA-ES",
                        "section_path": "./screenshots-images-2/chapter_12/section_21",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_21/8952c246-6d5c-444d-8be6-1dcb7907e62b.png",
                            "./screenshots-images-2/chapter_12/section_21/62eebb94-11f1-4622-92e6-86bf406e8cec.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "CMA-ES\nEvolutionary strategies generally adhere to the following process:\n\n1. Create a population of agents and randomly initialize the parameters to be optimized for each agent.\n2. Loop over the following:\na. Evaluate each agent in the environment, returning the average reward over multiple episodes.\nb. Breed the agents with the best scores to create new members of the population.\nc. Add randomness to the parameters of the new members.\nd. Update the population pool by adding the newly created agents and removing poorly perform-\n\ning agents.\n\nThis is similar to the process through which animals evolve in nature\u2014hence the name evolutionary\nstrategies. \u201cBreeding\u201d in this context simply means combining the existing best-scoring agents such that\nthe next generation are more likely to produce high-quality results, similar to their parents. As with all\nreinforcement learning solutions, there is a balance to be found between greedily searching for locally\noptimal solutions and exploring unknown areas of the parameter space for potentially better solutions.\nThis is why it is important to add randomness to the population, to ensure we are not too narrow in our\n\nsearch field.\n\nCMA-ES is just one form of evolutionary strategy. In short, it works by maintaining a normal distribution\nfrom which it can sample the parameters of new agents. At each generation, it updates the mean of the\ndistribution to maximize the likelihood of sampling the high-scoring agents from the previous timestep.\nAt the same time, it updates the covariance matrix of the distribution to maximize the likelihood of sam-\npling the high-scoring agents, given the previous mean. It can be thought of as a form of naturally arising\ngradient descent, but with the added benefit that it is derivative-free, meaning that we do not need to cal-\n\nculate or estimate costly gradients.\n\nOne generation of the algorithm demonstrated on a toy example is shown in Figure 12-13. Here we are\ntrying to find the minimum point of a highly nonlinear function in two dimensions\u2014the value of the\nfunction in the red/black areas of the image is greater than the value of the function in the white/yellow\n\nparts of the image.\n\nStep1 Step 2 Step3 Step4\n\nFigure 12-13. One update step from the CMA-ES algorithm (source: Ha, 2017)\"\n\nThe steps are as follows:\n\n1. We start with a randomly generated 2D normal distribution and sample a population of candidates,\nshown in blue in Figure 12-13.\n\n2. We then calculate the value of the function for each candidate and isolate the best 25%, shown in\npurple in Figure 12-13\u2014we'll call this set of points P.\n\n3. We set the mean of the new normal distribution to be the mean of the points in P. This can be\nthought of as the breeding stage, wherein we only use the best candidates to generate a new mean\nfor the distribution. We also set the covariance matrix of the new normal distribution to be the co-\nvariance matrix of the points in P, but use the existing mean in the covariance calculation rather\nthan the current mean of the points in P. The larger the difference between the existing mean and\nthe mean of the points in P, the wider the variance of the next normal distribution. This has the ef-\nfect of naturally creating momentum in the search for the optimal parameters.\n\n4. We can then sample a new population of candidates from our new normal distribution with an up-\ndated mean and covariance matrix.\n\nFigure 12-14 shows several generations of the process. See how the covariance widens as the mean moves\nin large steps toward the minimum, but narrows as the mean settles into the true minimum.\n\nGeneration 1 Generation 2 Generation 3\nDies\n\nGeneration 4 Generation 5 Generation 6\n\nae\n-_\n\nFigure 12-14. CMA-ES (source: Wikipedia\n\nFor the car racing task, we do not have a well-defined function to maximize, but instead an environment\nwhere the 867 parameters to be optimized determine how well the agent scores. Initially, some sets of pa-\nrameters will, by random chance, generate scores that are higher than others and the algorithm will\ngradually move the normal distribution in the direction of those parameters that score highest in the\nenvironment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.22,
                        "section_name": "Parallelizing CMA-ES",
                        "section_path": "./screenshots-images-2/chapter_12/section_22",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_22/5981d1cd-97d3-4948-8428-32666f0ed669.png",
                            "./screenshots-images-2/chapter_12/section_22/0170f7e0-85db-4e38-a9f4-3cc9c91fb2f1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Parallelizing CMA-ES\n\nOne of the great benefits of CMA-ES is that it can be easily parallelized. The most time-consuming part of\nthe algorithm is calculating the score for a given set of parameters, since it needs to simulate an agent\nwith these parameters in the environment. However, this process can be parallelized, since there are no\ndependencies between individual simulations. There is a orchestrator process that sends out parameter\nsets to be tested to many node processes in parallel. The nodes return the results to the orchestrator,\nwhich accumulates the results and then passes the overall result of the generation to the CMA-ES object.\nThis object updates the mean and covariance matrix of the normal distribution as per Figure 12-13 and\nprovides the orchestrator with a new population to test. The loop then starts again. Figure 12-15 explains\n\nthis in a diagram.\n\nKey\n@ A candidate solution\n\nes.tell(reward List) [EI]\n577, 823, 754, 723, 582, 688, 668, 723\n\nBB es.asko\n00006000\n\norchestrator\n\nworker worker worker worker\n579 478 674 832 743 687 456 592 698 800 591 613\nav. 577 av. 754 av. 582 av. 668\n\nFigure 12-15. Parallelizing CMA-ES\u2014here there is a population size of eight and four nodes (so t = 2, the number of trials that each node is re-\nsponsible for)\n\n@\u00ae The orchestrator asks the CMA-ES object (es ) for a set of parameters to trial.\n\n\u00ae The orchestrator divides the parameters into the number of nodes available. Here, each of the four\nnode processes gets two parameter sets to trial.\n\n\u00a9 The nodes run a worker process that loops over each set of parameters and runs several episodes\nfor each. Here we run three episodes for each set of parameters.\n\n\u00a9 The rewards from each episode are averaged to give a single score for each set of parameters.\n\n\u00a9 Each node returns its list of scores to the orchestrator.\n\n@ The orchestrator groups all the scores together and sends this list to the es object.\n\n@ The es object uses this list of rewards to calculate the new normal distribution as per Figure 12-\n13.\n\nAfter around 200 generations, the training process achieves an average reward score of around 840 for\nthe car racing task, as shown in Figure 12-16.\n\nCarRacing-vO\n\n1000\n& 800\n=\n2\nS 600\n8\n3\n2, 400\nfi\nao\nZz 200\n\u2014\u2014 Eager\n\u2014 D.Ha original\n0 23 50 75 100 125 150 175 200\nGeneration\n\nFigure 12-16. Average episode reward of the controller training process, by generation (source: Zac Wellmer, \u201cWorld Models\u201d;\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.23,
                        "section_name": "In-Dream Training",
                        "section_path": "./screenshots-images-2/chapter_12/section_23",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_23/738e594d-d040-4425-a4ea-626e4167301b.png",
                            "./screenshots-images-2/chapter_12/section_23/71041b19-2658-413f-b110-58a3c856c4be.png",
                            "./screenshots-images-2/chapter_12/section_23/26f90674-4467-4d0b-a0e0-3e64183909f5.png",
                            "./screenshots-images-2/chapter_12/section_23/53b7391d-faa9-455e-bbf4-6328c6462dd4.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In-Dream Training\n\nSo far, the controller training has been conducted using the Gym CarRacing environment to implement\nthe step method that moves the simulation from one state to the next. This function calculates the next\n\nstate and reward, given the current state of the environment and chosen action.\n\nNotice how the step method performs a very similar function to the MDN-RNN in our model. Sampling\nfrom the MDN-RNN outputs a prediction for the next z and reward, given the current z and chosen\n\naction.\n\nIn fact, the MDN-RNN can be thought of as an environment in its own right, but operating in z -space\nrather than in the original image space. Incredibly, this means that we can actually substitute the real en-\nvironment with a copy of the MDN-RNN and train the controller entirely within an MDN-RNN-inspired\ndream of how the environment should behave.\n\nIn other words, the MDN-RNN has learned enough about the general physics of the real environment\nfrom the original random movement dataset that it can be used as a proxy for the real environment\nwhen training the controller. This is quite remarkable\u2014it means that the agent can train itself to learn a\nnew task by thinking about how it can maximize reward in its dream environment, without ever having\nto test out strategies in the real world. It can then perform well at the task the first time, having never at-\n\ntempted the task in reality.\n\nA comparison of the architectures for training in the real environment and the dream environment fol-\nlows: the real-world architecture is shown in Figure 12-17 and the in-dream training setup is illustrated\n\nin Figure 12-18.\n\nobservation\n\nDecoder\n\nController (C) | action\n\nFigure 12-17. Training the controller in the Gym environment\n\nNotice how in the dream architecture, the training of the controller is performed entirely in z-space\nwithout the need to ever decode the z vectors back into recognizable track images. We can of course do\nso, in order to visually inspect the performance of the agent, but it is not required for training.\n\ndream environment\n\nVAE (V)\ndecoder\n\nFigure 12-18. Training the controller in the MDN-RNN dream environment\n\nOne of the challenges of training agents entirely within the MDN-RNN dream environment is overfitting.\nThis occurs when the agent finds a strategy that is rewarding in the dream environment but does not\ngeneralize well to the real environment, due to the MDN-RNN not fully capturing how the true environ-\nment behaves under certain conditions.\n\nThe authors of the original paper highlight this challenge and show how including a temperature para-\nmeter to control model uncertainty can help alleviate the problem. Increasing this parameter magnifies\nthe variance when sampling z through the MDN-RNN, leading to more volatile rollouts when training in\nthe dream environment. The controller receives higher rewards for safer strategies that encounter well-\nunderstood states and therefore tend to generalize better to the real environment. Increased tempera-\nture, however, needs to be balanced against not making the environment so volatile that the controller\ncannot learn any strategy, as there is not enough consistency in how the dream environment evolves\nover time.\n\nIn the original paper, the authors show this technique successfully applied to a different environment:\nDoomTakeCover , based around the computer game Doom. Figure 12-19 shows how changing the\ntemperature parameter affects both the virtual (dream) score and the actual score in the real\nenvironment.\n\nTEMPERATURE T VIRTUAL SCORE ACTUAL SCORE\n\n0.10 2086 + 140 193+ 58\n0.50 2060 + 277 196 + 50\n1.00 1145 + 690 868 + 511\n1.15 918 + 546 1092 + 556\n1.30 732 + 269 753 + 139\nRANDOM PoLicy N/A 210 + 108\nGYM LEADER N/A 820 + 58\n\nFigure 12-19. Using temperature to control dream environment volatility (source: Ha and Schmidhuber, 2018,\n\nThe optimal temperature setting of 1.15 achieves a score of 1,092 in the real environment, surpassing the\ncurrent Gym leader at the time of publication. This is an amazing achievement\u2014remember, the con-\ntroller has never attempted the task in the real environment. It has only ever taken random steps in the\nreal environment (to train the VAE and MDN-RNN dream model) and then used the dream environment\nto train the controller.\n\nA key benefit of using generative world models as an approach to reinforcement learning is that each\ngeneration of training in the dream environment is much faster than training in the real environment.\nThis is because the z and reward prediction by the MDN-RNN is faster than the z and reward calcula-\ntion by the Gym environment.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 12.24,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_12/section_24",
                        "images": [
                            "./screenshots-images-2/chapter_12/section_24/0b44bf1e-6ab3-4e76-afd9-4a3a18cc5f12.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter we have seen how a generative model (a VAE) can be utilized within a reinforcement\nlearning setting to enable an agent to learn an effective strategy by testing policies within its own gener-\nated dreams, rather than within the real environment.\n\nThe VAE is trained to learn a latent representation of the environment, which is then used as input to a\nrecurrent neural network that forecasts future trajectories within the latent space. Amazingly, the agent\ncan then use this generative model as a pseudo-environment to iteratively test policies, using an evolu-\ntionary methodology, that generalize well to the real environment.\n\nFor further information on the model, there is an excellent interactive explanation available online, writ-\n\nten by the authors of the original paper.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 13,
                "chapter_name": "Chapter 13. Multimodal Models",
                "chapter_path": "./screenshots-images-2/chapter_13",
                "sections": [
                    {
                        "section_id": 13.1,
                        "section_name": "Chapter 13. Multimodal Models",
                        "section_path": "./screenshots-images-2/chapter_13/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_1/f4a82c89-027f-431a-878d-979cbe6df46f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Chapter 13. Multimodal Models\n\nCHAPTER GOALS\n\nIn this chapter you will:\n\n\u00a2 Learn what is meant by a multimodal model.\n\nExplore the inner workings of DALL.E 2, a large-scale text-to-image model from OpenAl.\n\nUnderstand how CLIP and diffusion models such as GLIDE play an integral role in the overall\nDALL.E 2 architecture.\n\nAnalyze the limitations of DALL.E 2, as highlighted by the authors of the paper.\n\nExplore the architecture of Imagen, a large-scale text-to-image model from Google Brain.\n\n\u00a2 Learn about the latent diffusion process used by Stable Diffusion, an open source text-to-image\nmodel.\n\nUnderstand the similarities and differences between DALL.E 2, Imagen, and Stable Diffusion.\n\nInvestigate DrawBench, a benchmarking suite for evaluating text-to-image models.\n\u00a2 Learn the architectural design of Flamingo, a novel visual language model from DeepMind.\n\nUnpick the different components of Flamingo and learn how they each contribute to the model as a\nwhole.\n\u00a2 Explore some of the capabilities of Flamingo, including conversational prompting.\n\nSo far, we have analyzed generative learning problems that focus solely on one modality of data: either\ntext, images, or music. We have seen how GANs and diffusion models can generate state-of-the-art images\nand how Transformers are pioneering the way for both text and image generation. However, as humans,\nwe have no difficulties crossing modalities\u2014for example, writing a description of what is happening ina\ngiven photograph, creating digital art to depict a fictional fantasy world in a book, or matching a film\nscore to the emotions of a given scene. Can we train machines to do the same?\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_13/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_2/21748332-94a6-4679-b212-50cd3b92eb37.png",
                            "./screenshots-images-2/chapter_13/section_2/9ba130b9-dc98-46fa-82d6-c514d65c126f.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Introduction\n\nMultimodal learning involves training generative models to convert between two or more different kinds\nof data. Some of the most impressive generative models introduced in the last two years have been multi-\nmodal in nature. In this chapter we will explore how they work in detail and consider how the future of\ngenerative modeling will be shaped by large multimodal models.\n\nWe'll explore four different vision-language models: DALL.E 2 from OpenAlI; Imagen from Google Brain;\nStable Diffusion from Stability AI, CompVis, and Runway; and Flamingo from DeepMind.\n\nTIP\n\nThe aim of this chapter is to concisely explain how each model works, without going into the fine detail of every de-\nsign decision. For more information, refer to the individual papers for each model, which explain all of the design\nchoices and architecture decisions in detail.\n\nText-to-image generation focuses on producing state-of-the-art images from a given text prompt. For ex-\nample, given the input \u201cA head of broccoli made out of modeling clay, smiling in the sun,\u201d we would like\nthe model to be able to output a image that accurately matches the text prompt, as shown in Figure 13-1.\n\nThis is clearly a highly challenging problem. Text understanding and image generation are difficult to\nsolve in their own right, as we have seen in previous chapters of this book. Multimodal modeling such as\nthis presents an additional challenge, because the model must also learn how to cross the bridge between\nthe two domains and learn a shared representation that allows it to accurately convert from a block of\ntext to a high-fidelity image without loss of information.\n\nAhead of broccoli made out of\nmodeling clay, smiling in the sun\n\nFigure 13-1. An example of text-to-image generation by DALL.E 2\n\nMoreover, in order to be successful the model must be able to combine concepts and styles that it may\nnever have seen before. For example, there are no Michelangelo frescos containing people wearing virtu-\nal reality headsets, but we would like our model to be able to create such an image if we ask it to. Equally,\nit would be desirable for the model to accurately infer how objects in the generated image relate to each\nother, based on the text prompt. For example, a picture of \u201can astronaut riding a doughnut through\nspace\u201d should look very different from one of \u201can astronaut eating a doughnut in a crowded space.\u201d The\nmodel must learn how words are given meaning through context and how to convert explicit textual re-\nlationships between entities to images that imply the same meaning.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.3,
                        "section_name": "DALL.E 2",
                        "section_path": "./screenshots-images-2/chapter_13/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_3/c740bcb8-f198-4007-bf84-47b2d9f0b913.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "DALL.E 2\n\nThe first model we shall explore is DALL.E 2, a model designed by OpenAI for text-to-image generation.\nThe first version of this model, DALLE; was released in February 2021 and sparked a new wave of inter-\nest in generative multimodal models. In this section, we shall investigate the workings of the second itera-\ntion of the model, DALL.E 22 released just over a year later in April 2022.\n\nDALL.E 2 is an extremely impressive model that has furthered our understanding of Al\u2019s ability to solve\nthese types of multimodal problems. It not only has ramifications academically, but also forces us to ask\nbig questions relating to the role of AI in creative processes that previously were thought to be unique to\nhumans. We will start by exploring how DALL.E 2 works, building on key foundational ideas that we have\nalready explored earlier in this book.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.4,
                        "section_name": "Architecture",
                        "section_path": "./screenshots-images-2/chapter_13/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_4/9f6c1093-f125-4948-bd5f-f700e0743cc8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Architecture\n\nTo understand how DALL.E 2 works, we must first survey its overall architecture, as shown in Figure 13-\n2.\n\n\u201cLeonardo da Vinci Text\nearly sketches (a\n\nd Decoder\nof a cyborg\u201d encoder\n\n| =~\nGenerated image\n\nText embedding Image embedding\n\nFigure 13-2. The DALL.E 2 architecture\n\nThere are three distinct parts to consider: the text encoder, the prior, and the decoder. Text is first passed\nthrough the text encoder to produce a text embedding vector. This vector is then transformed by the pri-\nor to produce an image embedding vector. Finally, this is passed through the decoder, along with the orig-\ninal text, to produce the generated image. We will step through each component in turn, to get a complete\npicture of how DALL.E 2 works in practice.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.5,
                        "section_name": "The Text Encoder",
                        "section_path": "./screenshots-images-2/chapter_13/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_5/ac96ae28-17b8-4136-ac5c-8eba9aa1c3eb.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Text Encoder\n\nThe aim of the text encoder is to convert the text prompt into an embedding vector that represents the\nconceptual meaning of the text prompt within a latent space. As we have seen in previous chapters, con-\nverting discrete text to a continuous latent space vector is essential for all downstream tasks, because we\ncan continue to manipulate the vector further depending on our particular goal.\n\nIn DALL.E 2, the authors do not train the text encoder from scratch, but instead make use of an existing\nmodel called Contrastive Language-Image Pre-training (CLIP), also produced by OpenAl. Therefore, to un-\nderstand the text encoder, we must first understand how CLIP works.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.6,
                        "section_name": "CLIP",
                        "section_path": "./screenshots-images-2/chapter_13/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_6/4c053207-0397-4311-941f-3b9814622713.png",
                            "./screenshots-images-2/chapter_13/section_6/b99646f2-82cc-4622-a84e-19a1cdb67e5d.png",
                            "./screenshots-images-2/chapter_13/section_6/f21f70b8-2919-4efd-9007-3ee57bac5680.png",
                            "./screenshots-images-2/chapter_13/section_6/fdfd05d5-e575-4887-9b85-6534ba737e7b.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "CLIP\n\n3\nCLIP\u2019 was unveiled in a paper published by OpenAl in February 2021 (just a few days after the first\nDALL.E paper) that described it as \u201ca neural network that efficiently learns visual concepts from natural\nlanguage supervision.\u201d\n\nIt uses a technique called contrastive learning to match images with text descriptions. The model is\ntrained on a dataset of 400 million text-image pairs scraped from the internet\u2014some example pairs are\nshown in Figure 13-3. For comparison, there are 14 million hand-annotated images in ImageNet. Given an.\nimage and a list of possible text descriptions, its task is to find the one that actually matches the image.\n\na cute banana\nmade from\nmodeling clay\n\na television\nstudio about\nto broadcast\n\na satellite\nphoto of crops\nand fields\n\na pup called\nRex siting on\nthe grass\n\na wildlife park\nin the sun\n\nFigure 13-3. Examples of text-image pairs\n\nThe key idea behind contrastive learning is simple. We train two neural networks: a text encoder that con-\nverts text to a text embedding and an image encoder that converts an image to an image embedding.\nThen, given a batch of text-image pairs, we compare all text and image embedding combinations using\ncosine similarity and train the networks to maximize the score between matching text-image pairs and\nminimize the score between incorrect text-image pairs. This process is shown in Figure 13-4.\n\nCLIP IS NOT GENERATIVE\n\nNote that CLIP is not itself a generative model\u2014it cannot produce images or text. Is it closer to a discriminative mod-\nel, because the final output is a prediction about which text description from a given set most closely matches a given\nimage (or the other way around, which image most closely matches a given text description).\n\na cute banana\nmade from\nmodeling clay\n\nText\nencoder\n\nTransformers are used\nto convert the text and\nimages into embedding\nvectors\n\nCLIP is trained to maximize\nthe cosine similarity of the\nNreal pairs (the diagonal)\nand minimize the cosine\nsimilarity of the N?-N\nincorrect pairs\n\nTransformer\n\nFigure 13-4. The CLIP training process\n\nBoth the text encoder and the image encoder are Transformers\u2014the image encoder is a Vision\nTransformer (ViT), introduced in \u201cViT VQ-GAN\u201d, which applies the same concept of attention to images.\nThe authors tested other model architectures, but found this combination to produce the best results.\n\nWhat makes CLIP especially interesting is the way it can be used for zero-shot prediction on tasks that it\n\nhas never been exposed to. For example, suppose we want to use CLIP to predict the label of a given im-\n\nage in the ImageNet dataset. We can first convert the ImageNet labels into sentences by using a template\n(e.g., \u201ca photo of a <label>\u201d), as shown in Figure 13-5.\n\nCLIP then encodes the\n\nsentences using the\na photo of Text trained text encoder\n2 aise encoder\n2\n= a photo of Transformer\n2\nSs\n= a photo of\nEs\nHy\ng a photo of Any set of labels can be\nFA converted to a sentence\nc using a template\n=<\n\n5 Hee OF (eg., \"a photo of a {object}\")\n\na ship\n\nFigure 13-5. Converting labels in a new dataset to captions, in order to produce CLIP text embeddings\n\nTo predict the label of a given image, we can pass it through the CLIP image encoder and calculate the co-\nsine similarity between the image embedding and all possible text embeddings in order to find the label\nwith the maximum score, as shown in Figure 13-6.\n\nVision transformer\n\nAnew image is encoded\nusing the image encoder\n\nAnew dataset with unseen labels\n\na photo of\na lemon\n\nCLIP compares the image encoding to all\n\npossible text encodings and chooses the\nlabel with the highest cosine similarity\n\nFigure 13-6. Using CLIP to predict the content of an image\n\nNotice that we do not need to retrain either of the CLIP neural networks for it to be readily applicable to\nnew tasks. It uses language as the common domain through which any set of labels can be expressed.\n\nUsing this approach, it is possible to show that CLIP performs well across a wide range of image dataset\nlabeling challenges (Figure 13-7). Other models that have been trained on a specific dataset to predict a\ngiven set of labels often fail when applied to different datasets with the same labels because they are\nhighly optimized to the individual datasets on which they were trained. CLIP is much more robust, as it\nhas learned a deep conceptual understanding of full text descriptions and images, rather than just ex-\ncelling at the narrow task of assigning a single label to a given image in a dataset.\n\nImageNet\n\nResNet-101\nImageNet\n. .\nVaca\nee B Cian\n\nImageNetV2\nwha \u00a2 \u2014\noe DD) [Ss 377%\nImageNet Rendition\n\u2018\n\u00a7 = \u2014_\nhen\u2019 7 32.6%\nObjectNet\n5 j y) \u2014\nImageNet Sketch\na Cre.\n<4 as 27%\nImageNet Adversarial\n\nClip ViT-L\n\n76.2%\n\n\u2014_\n70.1%\n\n88.9%\n\n22.3%\n\n60.2%\n\nTIN%\n\nFigure 13-7. CLIP performs well on a wide range of image labeling datasets (source: Radford et al, 2021\n\nAs mentioned, CLIP is measured on its discriminative ability, so how does it help us to build generative\n\nmodels such as DALL.E 2?\n\nThe answer is that we can take the trained text encoder and use it as one part of a larger model such as\n\nDALL.E 2, with frozen weights. The trained encoder is simply a generalized model for converting text to a\n\ntext embedding, which should be useful for downstream tasks such as generating images. The text en-\n\ncoder is able to capture a rich conceptual understanding of the text, as it has been trained to be as similar\n\nas possible to its matching image embedding counterpart, which is produced only from the paired image.\n\nIt is therefore the first part of the bridge that we need to be able to cross over from the text domain to the\n\nimage domain.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.7,
                        "section_name": "The Prior",
                        "section_path": "./screenshots-images-2/chapter_13/section_7",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_7/fefda2aa-a4d2-49e6-9991-7567f460183a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Prior\n\nThe next stage of the process involves converting the text embedding into a CLIP image embedding. The\nDALL.E 2 authors tried two different methods for training the prior model:\n\n\u00ab An autoregressive model\ne A diffusion model\n\nThey found that the diffusion approach outperformed the autoregressive model and was more computa-\ntionally efficient. In this section, we\u2019ll look at both and see how they differ.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.8,
                        "section_name": "Autoregressive prior",
                        "section_path": "./screenshots-images-2/chapter_13/section_8",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_8/3230f1cc-17ce-4486-b155-0dbc1c37427a.png",
                            "./screenshots-images-2/chapter_13/section_8/2be51c88-5e45-42f9-8f96-be0bb7f621f8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Autoregressive prior\n\nAn autoregressive model generates output sequentially, by placing an ordering on the output tokens (e.g.,\nwords, pixels) and conditioning the next token on previous tokens. We have seen in previous chapters\nhow this is used in recurrent neural networks (e.g., LSTMs), Transformers, and PixelCNN.\n\nThe autoregressive prior of DALL.E 2 is an encoder-decoder Transformer. It is trained to reproduce the\nCLIP image embedding given a CLIP text embedding, as shown in Figure 13-8. Note that there are some\nadditional components to the autoregressive model mentioned in the original paper that we omit here\nfor conciseness.\n\nText embedding\n\n=| | a=\no>\nY (center)\nSE - eo\nY (enter)\n> ee\n\nEe Ee\n\nPredicted image embedding\n\nFigure 13-8. A simplified diagram of the autoregressive prior of DALL.E 2\n\nThe model is trained on the CLIP text-image pair dataset. You can think of it as the second part of the\nbridge that we need in order to jump from the text domain to the image domain: we are converting a vec-\ntor from the text embedding latent space to the image embedding latent space.\n\nThe input text embedding is processed by the encoder of the Transformer to produce another representa-\ntion that is fed to the decoder, alongside the current generated output image embedding. The output is\ngenerated one element at a time, using teacher forcing to compare the predicted next element to the actu-\nal CLIP image embedding.\n\nThe sequential nature of the generation means that the autoregressive model is less computationally effi-\ncient than the other method tried by the authors, which we\u2019ll look at next.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.9,
                        "section_name": "Diffusion prior",
                        "section_path": "./screenshots-images-2/chapter_13/section_9",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_9/6e6a8286-53a6-4776-8419-2e7fe0b19eff.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Diffusion prior\n\nAs we saw in Chapter 8, diffusion models are fast becoming the go-to choice for generative modeling\npractitioners, alongside Transformers. In DALL.E 2 a decoder-only Transformer is used as the prior,\ntrained using a diffusion process.\n\nThe training and generation process is shown in Figure 13-9. Again, this is a simplified version; the origi-\nnal paper contains full details of how the diffusion model is structured.\n\nTraining Generation\nText embedding CLIPimage embedding Textembedding Predicted image embedding\n\naacnoise y 4 CED) 4 Gia)\nnadnase ys CEB) 4 Gio)\nAdd noise v4 Cror) 4Ciod)\nAdd noise v4 Cro) Xai)\n\nRandom noise Random noise\n\nFigure 13-9. A simplified diagram of the diffusion prior training and generation process of DALL.E 2\n\nDuring training, each CLIP text and image embedding pair are first concatenated into a single vector.\nThen, the image embedding is noised over 1,000 timesteps until it is indistinguishable from random\nnoise. The diffusion prior is then trained to predict the denoised image embedding at the previous\ntimestep. The prior has access to the text embedding throughout, so it is able to condition its predictions\non this information, gradually transforming the random noise into a predicted CLIP image embedding.\nThe loss function is the average mean-squared error across denoising steps.\n\nTo generate new image embeddings, we sample a random vector, prepend the relevant text embedding,\nand pass it through the trained diffusion prior multiple times.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.1,
                        "section_name": "The Decoder",
                        "section_path": "./screenshots-images-2/chapter_13/section_10",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_10/9ee68945-6941-43c6-bb67-646e00f75897.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "The Decoder\n\nThe final part of DALL.E 2 is the decoder. This is the part of the model that generates the final image con-\nditioned on the text prompt and the predicted image embedding output by the prior.\n\nThe architecture and training process of the decoder borrows from an earlier OpenAI paper, published in\nDecember 2021, which presented a generative model called Guided Language to Image Diffusion for\n\n4\nGeneration and Editing (GLIDE).\u201d\n\nGLIDE is able to generate realistic images from text prompts, in much the same way that DALL.E 2 can.\nThe difference is that GLIDE does not make use of CLIP embeddings, but instead works directly with the\n\nraw text prompt, training the entire model from scratch, as shown in Figure 13-10.\n\n\"Leonardo da Vinci early\nsketches of a cyborg\"\n\nDALL.E 2\n\nImage embedding\n\nGLIDE\n\n\"a crayon drawing of\n\nText prompt\n\nGenerated image\n\nFigure 13-10. A comparison between DALL.E 2 and GLIDE\u2014GLIDE trains the entire generative model from scratch, whereas DALL.E 2 makes\nuse of CLIP embeddings to carry information forward from the initial text prompt\n\nLet\u2019s see how GLIDE works first.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.11,
                        "section_name": "GLIDE",
                        "section_path": "./screenshots-images-2/chapter_13/section_11",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_11/fe3f7c23-c0c9-420d-809b-cf542099d822.png",
                            "./screenshots-images-2/chapter_13/section_11/1d130fb3-cca2-4b66-92ed-1d233cc7ff47.png",
                            "./screenshots-images-2/chapter_13/section_11/8c0764d1-cfe1-4f01-8863-3d40ccef4e08.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "GLIDE\n\nGLIDE is trained as a diffusion model, with U-Net architecture for the denoiser and Transformer architec-\nture for the text encoder. It learns to undo the noise added to an image, guided by the text prompt.\nFinally, an Upsampler is trained to scale the generated image to 1,024 x 1,024 pixels.\n\nGLIDE trains the 3.5 billion (B) parameter model from scratch\u20142.3B parameters for the visual part of the\nmodel (U-Net and Upsampler) and 1.2B for the Transformer. It is trained on 250 million text-image pairs.\n\nThe diffusion process is shown in Figure 13-11. A Transformer is used to create an embedding of the in-\nput text prompt, which is then used to guide the U-Net throughout the denoising process. We explored the\nU-Net architecture in Chapter 8; it\u2019s a perfect model choice when the overall size of the image should stay\n\nthe same (e.g., for style transfer, denoising, etc.).\n\n\"a crayon drawing of\na space elevator\"\n\nEmbedding\n\nAsee\n\nomel+ CELE\n\nrec CD< So\n\nRandom noise\n\nFigure 13-11. The GLIDE diffusion process\n\nThe DALL.E 2 decoder still uses the U-Net denoiser and Transformer text encoder architectures, but addi-\ntionally has the predicted CLIP image embeddings to condition on. This is the key difference between\nGLIDE and DALL.E 2, as shown in Figure 13-12.\n\n\"Leonardo da Vinci early\nsketches of a cyborg\"\n\nEmbedding Image embedding\n\nRandom noise\n\nFigure 13-12. The DALL.E 2 decoder additionally conditions on the image embedding produced by the prior\n\nAs with all diffusion models, to generate a new image, we simply sample some random noise and run this\nthrough the U-Net denoiser multiple times, conditioned on the Transformer text encoding and image em-\n\nbedding. The output is a 64 x 64-pixel image.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.12,
                        "section_name": "Upsampler",
                        "section_path": "./screenshots-images-2/chapter_13/section_12",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_12/6183df72-6663-4a1c-8639-7c5726e07d08.png",
                            "./screenshots-images-2/chapter_13/section_12/edde7bd9-2a93-4cde-8778-858ed69f623a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Upsampler\n\nThe final part of the decoder is the Upsampler (two separate diffusion models). The first diffusion model\ntransforms the image from 64 x 64 to 256 x 256 pixels. The second transforms it again, from 256 x 256 to\n1,024 x 1,024 pixels, as shown in Figure 13-13.\n\nUpsampling is useful because it means we do not have to build large upstream models to handle high-di-\nmensional images. We can work with small images until the final stages of the process, when we apply\nthe Upsamplers. This saves on model parameters and ensures a more efficient upstream training process.\n\nFigure 13-13. The first Upsampler diffusion model converts the image from 64 x 64 pixels to 256 x 256 pixels while the second converts from 256\nx 256 pixels to 1,024 x 1,024 pixels\n\nThis concludes the DALL.E 2 model explanation! In summary, DALL.E 2 makes use of the pre-trained CLIP\nmodel to immediately produce a text embedding of the input prompt. Then it converts this into an image\nembedding using a diffusion model called the prior. Lastly, it implements a GLIDE-style diffusion model\nto generate the output image, conditioned on the predicted image embedding and Transformer-encoded\n\ninput prompt.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.13,
                        "section_name": "Image variations",
                        "section_path": "./screenshots-images-2/chapter_13/section_13",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_13/893ece7e-525f-41b4-ad32-8ea11fd55450.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Image variations\n\nAs discussed previously, to generate images using the DALL.E 2 decoder we sample an image consisting of\npure random noise and then gradually reduce the amount of noise using the denoising diffusion model,\nconditioned on the provided image embedding. Selecting different initial random noise samples will re-\nsult in different images.\n\nIn order to generate variations of a given image, we therefore just need to establish its image embedding\nto feed to the decoder. We can obtain this using the original CLIP image encoder, which is explicitly de-\n\nsigned to convert an image into its CLIP image embedding. This process is shown in Figure 13-14.\n\nOptional text guidance\n\nDecoder wy\n\n| Different random noise\nstarting samples produce\ndifferent images\n\nFigure 13-14. DALL.E 2 can be used for generating variations of a given image\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.14,
                        "section_name": "Importance of the prior",
                        "section_path": "./screenshots-images-2/chapter_13/section_14",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_14/df8ea5a1-c8df-4953-b545-efe7b0ed9c48.png",
                            "./screenshots-images-2/chapter_13/section_14/189ea934-69b5-440e-a759-88c796429ff9.png",
                            "./screenshots-images-2/chapter_13/section_14/4e7a916b-37ad-45bc-9ed6-5fb417bc6f28.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Importance of the prior\n\nAnother avenue explored by the authors is establishing the importance of the prior. The purpose of the\n\nprior is to provide the decoder with a useful representation of the image to be generated, making use of\nthe pre-trained CLIP model. However, it is feasible that this step isn\u2019t necessary\u2014perhaps we could just\n\npass the text embedding directly to the decoder instead of the image embedding, or ignore the CLIP em-\nbeddings completely and condition only on the text prompt. Would this impact the quality of the\n\ngenerations?\nTo test this, the authors tried three different approaches:\n\n1. Feed the decoder only with the text prompt (and a zero vector for the image embedding).\n2. Feed the decoder with the text prompt and the text embedding (as if it were an image embedding).\n3. Feed the decoder with the text prompt and the image embedding (i.e., the full model).\n\nExample results are shown in Figure 13-15. We can see that when the decoder is starved of image embed-\nding information, it can only produce a rough approximation of the text prompt, missing key information\nsuch as the calculator. Using the text embedding as if it were an image embedding performs slightly bet-\nter, though it is not able to capture the relationship between the hedgehog and the calculator. Only the\nfull model with the prior produces an image that accurately reflects all of the information contained\n\nwithin the prompt.\n\nhedgehog\nusing a Decoder\ncalculator\n\nhedgehog\nusing a Decoder\ncalculator\n\nse\na=\n73\nEA\nBe\nCO]\nFo\n\nhedgehog\nusing a Text\n\nDecoder\ncalculator encoder\n\nText + image\nembedding\n\nFigure 13-15. The prior provides the model with additional context and helps the decoder to produce more accurate generations (source:\nRamesh et al, 2022,\n\nLimitations\n\nIn the DALL.E 2 paper, the authors also highlight several known limitations of the model. Two of these\n(attribute binding and text generation) are shown in Figure 13-16.\n\nFigure 13-16. Two limitations of DALL.E 2\n\nin its ability to bind attributes to objects and reproduce textual information\u2014top prompt: \u201cA red\ncube on top of a blue cube\u201d; bottom prompt: \u201cA sign that says deep learning\u201d (source: Ramesh et al,, 2022)\n\nAttribute binding is the ability of a model to understand the relationship between words in a given text\nprompt, and in particular how attributes relate to objects. For example, the prompt \u201cA red cube on top of\na blue cube\u201d must appear visually distinct from \u201cA blue cube on top of a red cube.\u201d DALL.E struggles\n\nsomewhat with this, compared to earlier models such as GLIDE, though the overall quality of generations\nis better and more diverse.\n\nAlso, DALL.E 2 is not able to accurately reproduce text\u2014this is probably due to the fact that the CLIP em-\nbeddings do not capture spellings, but instead only contain a higher-level representation of the text.\nThese representations can be decoded into text with partial success (e.g., individual letters are mostly cor-\nrect), but not with enough compositional understanding to form full words.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.15,
                        "section_name": "Imagen",
                        "section_path": "./screenshots-images-2/chapter_13/section_15",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_15/98a76185-34e6-4259-87b8-e218cfe0e52c.png",
                            "./screenshots-images-2/chapter_13/section_15/c525c4b2-217c-4214-86ce-b4f1bdadbc45.png",
                            "./screenshots-images-2/chapter_13/section_15/93e47761-c558-4db4-ada4-b8fff4ce5263.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Imagen\n\nJust over a month after OpenAI released DALL.E 2, the Google Brain team released their own text-to-im-\n5\n\nage model called Imagen. Many of the core themes that we have already explored in this chapter are\n\nalso relevant to Imagen: for example, it uses a text encoder and a diffusion model decoder.\n\nIn the next section, we\u2019ll explore the overall architecture of Imagen and compare it with DALL.E 2.\nArchitecture\n\nAn overview of the Imagen architecture is shown in Figure 13-17.\n\n\u201cA golden retriever dog wearing a blue\nText checkered beret and red dotted turtleneck\u201d\n\nText embedding\n\nText-to-image\ndiffusion model\n\n64 x 64 image\nSuper-resolution\ndiffusion model\n256 x 256 image\n\nSuper-resolution\ndiffusion model\n\n1024 x 1024 image\n\nFigure 13-17. The Imagen architecture (source: Saharia et al., 2022)\n\nThe frozen text encoder is the pre-trained T5-XXL model, a large encoder-decoder Transformer. Unlike\nCLIP, this was trained only on text and not images, so it is not a multimodal model. However, the authors\nfound that it still functions extremely well as a text encoder for Imagen and that scaling this model has\nmore impact on overall performance than scaling the diffusion model decoder.\n\nLike DALL.E 2\u2019s, Imagen\u2019s the decoding diffusion model is based on a U-Net architecture, conditioned on\ntext embeddings. There are several architectural improvements made to the standard U-Net architecture,\nto produce what the authors call the Efficient U-Net. This model uses less memory, converges faster, and\nhas better sample quality than previous U-Net models.\n\nThe Upsampler super-resolution models that take the generated image from 64 x 64 to 1,024 x 1,024 pixels\nare also diffusion models that continue to use the text embeddings to guide the upsampling process.\n\nDrawBench\n\nAn additional contribution of the Imagen paper is DrawBench\u2014a suite of 200 text prompts for text-to-im-\nage evaluation. The text prompts cover 11 categories, such as Counting (ability to generate a specified\nnumber of objects), Description (ability to generate complex and long text prompts describing objects),\nand Text (ability to generate quoted text). To compare two models, the DrawBench text prompts are\npassed through each model and the outputs given to a panel of human raters for evaluation across two\nmetrics:\n\nAlignment\n\nWhich image more accurately describes the caption?\n\nFidelity\n\nWhich image is more photorealistic (looks more real)?\n\nThe results from the DrawBench human evaluation are shown in Figure 13-18.\n\nBoth DALL.E 2 and Imagen are remarkable models that have made significant contributions to the field\nof text-to-image generation. Whilst Imagen outperforms DALL.E 2 on many of the DrawBench bench-\nmarks, DALL.E 2 provides additional functionalities that are not present in Imagen. For example, because\nDALL.E 2 utilizes CLIP (a multimodal text-image model), it is able to accept images as input to generate\nimage embeddings. This means DALL.E 2 is able to provide image editing and image variation capabili-\nties. This is not possible with Imagen; the text encoder is a pure text model, so there is no way to input an\nimage.\n\nFigure 13-18. Comparison of Imagen and DALL.E 2 on DrawBench across alignment and image fidelity (source: Saharia et al., 2022\n\nExamples from Imagen\n\nExample Imagen generations are shown in Figure 13-19.\n\nCc) x ~--] \u2014)\nThree spheres made of glass falling Vines in theshape oftext \u201cImagen\u201d \u2014 Astrawberry splashing in the\ninto the ocean. Water is splashing. with flowers and butterflies bursting coffee in a mug under the\nSun is setting. out of an old TV. starry sky.\n\nFigure 13-19. Example Imagen generations (source: Saharia et al,, 2022)\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 13.16,
                        "section_name": "Summary",
                        "section_path": "./screenshots-images-2/chapter_13/section_16",
                        "images": [
                            "./screenshots-images-2/chapter_13/section_16/b15e2e46-777e-449c-9624-025f58decb56.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Summary\n\nIn this chapter we have explored four different state-of-the-art multimodal models: DALL.E 2, Imagen,\nStable Diffusion, and Flamingo.\n\nDALL.E 2 is a large-scale text-to-image model from OpenAI that can generate realistic images across a\nrange of styles given a text prompt. It works by combining pre-trained models (e.g., CLIP) with diffusion\nmodel architectures from previous works (GLIDE). It also has additional capabilities, such as being able\nto edit images through text prompting and provide variations of a given image. While it does have some\nlimitations, such as inconsistent text rendering and attribute binding, DALL.E 2 is an incredibly powerful\nAI model that has helped to propel the field of generative modeling into a new era.\n\nAnother model that has surpassed previous benchmarks is Imagen from Google Brain. This model shares\nmany similarities with DALL.E 2, such as a text encoder and a diffusion model decoder. One of the key\ndifferences between the two models is that the Imagen text encoder is trained on pure text data, whereas\nthe training process for the DALL.E 2 text encoder involves image data (through the contrastive CLIP\nlearning objective). The authors show that this approach leads to state-of-the-art performance across a\nrange of tasks, through their DrawBench evaluation suite.\n\nStable Diffusion is an open source offering from Stability AI, CompVis, and Runway. It is a text-to-image\nmodel whose model weights and code are freely available, so you can run it on your own hardware.\nStable Diffusion is particularly fast and lightweight due to the use of a latent diffusion model that oper-\nates on the latent space of an autoencoder, rather than the images themselves.\n\nFinally, DeepMind\u2019s Flamingo is a visual language model\u2014that is, it accepts a stream of interleaved text\nand visual data (images and video) and is able to continue the prompt with additional text, in the style of\na decoder Transformer. The key contribution is showing how the visual information can be fed to the\nTransformer via a Visual Encoder and Perceiver Resampler that encode the visual input features into a\nsmall number of visual tokens. The Language Model itself is an extension of DeepMind\u2019s earlier\nChinchilla model, adapted to blend in visual information.\n\nAll four are remarkable examples of the power of multimodal models. In the future, it is highly likely that\ngenerative modeling will become more multimodal and AI models will be able to easily cross modalities\nand tasks through interactive language prompting.\n",
                        "extracted-code": ""
                    }
                ]
            }
        ]
    }
}