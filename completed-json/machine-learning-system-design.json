{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "Chapter 2. Data Engineering\nFundamentals",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "Chapter 2. Data Engineering\nFundamentals",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/933a3361-076b-455e-a9c7-edf06dbdb367.png",
                            "./screenshots-images-2/chapter_1/section_1/5d94f83b-9589-4bda-9884-60c9eb60452c.png",
                            "./screenshots-images-2/chapter_1/section_1/551d8e01-9cd6-456a-8f05-a56c0d9126c0.png",
                            "./screenshots-images-2/chapter_1/section_1/4b090cab-c6f6-43dd-b488-967e0cf33a24.png",
                            "./screenshots-images-2/chapter_1/section_1/c726553f-7e35-452b-b842-56c3e24f4cf0.png",
                            "./screenshots-images-2/chapter_1/section_1/b51987df-679a-4099-ae86-8246d7b7e69f.png",
                            "./screenshots-images-2/chapter_1/section_1/9bcd63af-6450-464b-a27b-47b038781d26.png",
                            "./screenshots-images-2/chapter_1/section_1/8788b65a-4b8c-43ee-9969-41c8323c31cf.png",
                            "./screenshots-images-2/chapter_1/section_1/8ce94882-5cdf-4800-97e8-1591dd188d84.png",
                            "./screenshots-images-2/chapter_1/section_1/655c49ed-0caa-4f9d-b7be-ad8874b62fad.png",
                            "./screenshots-images-2/chapter_1/section_1/4cfaa3b7-9c2b-4266-8552-110c96637a44.png",
                            "./screenshots-images-2/chapter_1/section_1/bd587690-42cd-4e33-adfe-d1e89edb657b.png",
                            "./screenshots-images-2/chapter_1/section_1/c741838d-1402-4d78-b32d-baa1a1c85d13.png",
                            "./screenshots-images-2/chapter_1/section_1/e11420eb-5fc7-414e-b42a-6a3156a05487.png",
                            "./screenshots-images-2/chapter_1/section_1/46444ce2-37c9-4dd9-abf2-e75c5bd6ebfc.png",
                            "./screenshots-images-2/chapter_1/section_1/ac1f0292-23d3-4c29-84e6-e28b82fdebf1.png",
                            "./screenshots-images-2/chapter_1/section_1/c7131f99-0e7d-42fd-aa86-21f6c08c1f55.png",
                            "./screenshots-images-2/chapter_1/section_1/0523a63c-c14f-4542-b0b4-a60367dbf576.png",
                            "./screenshots-images-2/chapter_1/section_1/957409ac-324f-48c7-a818-db77a86be246.png",
                            "./screenshots-images-2/chapter_1/section_1/54bb414e-fad2-4a68-9025-a423f32c2acc.png",
                            "./screenshots-images-2/chapter_1/section_1/6985f4ca-9277-4564-b820-3dfae58f201e.png",
                            "./screenshots-images-2/chapter_1/section_1/fa47ea0f-a24a-4aff-afef-1a50b96ab6b6.png",
                            "./screenshots-images-2/chapter_1/section_1/d36d7d4b-c1e0-4ba4-a156-c7e3e506c3a5.png",
                            "./screenshots-images-2/chapter_1/section_1/d462bc9b-c529-4af4-9f11-eaf7560d6f4a.png",
                            "./screenshots-images-2/chapter_1/section_1/ecf05c06-a101-4d5e-83a3-9bc9689ef278.png",
                            "./screenshots-images-2/chapter_1/section_1/b59e1860-afbe-4a1b-a620-d2cd9ff61d50.png",
                            "./screenshots-images-2/chapter_1/section_1/991915b3-15e0-46f3-91c1-f6828563e859.png",
                            "./screenshots-images-2/chapter_1/section_1/8c416946-41a0-46bb-be97-013978780f07.png",
                            "./screenshots-images-2/chapter_1/section_1/deb2263b-09eb-44e0-a13d-3949b54b0086.png",
                            "./screenshots-images-2/chapter_1/section_1/90a0a0b6-4afc-4aa1-9e93-390e19fb1456.png",
                            "./screenshots-images-2/chapter_1/section_1/39d193e6-8ef5-4c63-80d3-19d223c09e16.png",
                            "./screenshots-images-2/chapter_1/section_1/48966b89-72d7-45ea-83e8-035eb7156c22.png",
                            "./screenshots-images-2/chapter_1/section_1/c750a776-f5fe-47cb-84e7-3c0a75e6c934.png",
                            "./screenshots-images-2/chapter_1/section_1/9bf0e8d2-84af-458c-a853-0bb5cac71b14.png",
                            "./screenshots-images-2/chapter_1/section_1/bffa362d-57ce-4187-81a3-5dfceb2db2e9.png",
                            "./screenshots-images-2/chapter_1/section_1/ad774b9d-7fe1-4b3a-8e64-155a69479d12.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A NOTE FOR EARLY RELEASE READERS\n\nWith Early Release ebooks, you get books in their earliest form\u2014the author\u2019s raw and\nunedited content as they write\u2014so you can take advantage of these technologies long\nbefore the official release of these titles.\n\nThis will be the 2nd chapter of the final book. Please note that the GitHub repo will be\nmade active later on.\n\nIf you have comments about how we might improve the content and/or examples in this\nbook, or if you notice missing material within this chapter, please reach out to the author at\nchip@huyenchip.com.\n\nThe rise of machine learning in recent years is tightly coupled with the rise of big data. Big data\nsystems, even without machine learning, are complex. If you haven\u2019t spent years and years\nworking with them, it\u2019s easy to get lost in acronyms. There are many challenges and possible\nsolutions that these systems generate. Industry standards, if there are any, evolve quickly as\nnew tools come out and the needs of the industry expand, creating a dynamic and ever-\nchanging environment. If you look into the data stack for different tech companies, it might\nseem like each is doing its own thing.\n\nIn this chapter, we'll cover the basics of data engineering that will, hopefully, give you a steady\npiece of land to stand on as you explore the landscape for your own needs. We'll start with\ndifferent sources of data that you might work with in a typical ML project. We'll continue to\ndiscuss the formats in which data can be stored. Storing data is only interesting if you intend on\nretrieving that data later. To retrieve stored data, it\u2019s important to know not only how it\u2019s\nformatted but also how it\u2019s structured. Data models define how the data stored in a particular\ndata format is structured.\n\nIf data models describe the data in the real-world, databases specify how the data should be\nstored on machines. We\u2019ll continue to discuss data storage engines, also known as databases,\nfor the two major types of processing: transactional and analytical.\n\nWhen working with data in production, you usually work with data across multiple processes\nand services. For example, you might have a feature engineering service that computes features\nfrom raw data, and a prediction service to generate predictions based on computed features.\nThis means that you\u2019ll have to pass computed features from the feature engineering service to\nthe prediction service. In the following section of the chapter, we'll discuss different modes of\ndata passing across processes.\n\nDuring the discussion of different modes of data passing, we\u2019ll learn about two distinct types of\ndata: historical data in data storage engines, and streaming data in real-time transports. These\ntwo different types of data require different processing paradigms, which we\u2019ll discuss in the\nBatch Processing vs. Stream Processing section.\n\nKnowing how to collect, process, store, retrieve, and process an increasingly growing amount\nof data is essential to people who want to build ML systems in production. If you\u2019re already\nfamiliar with data systems, you might want to move directly to Chapter 3 to learn more about\nhow to sample and generate labels to create training data. If you want to learn more about data\nengineering from a systems perspective, I recommend Martin Kleppman\u2019s excellent book\nDesigning Data Intensive Applications (O\u2019 Reilly, 2017).\n\nData Sources\n\nAn ML system can work with data from many different sources. They have different\ncharacteristics, can be used for different purposes, and require different processing methods.\nUnderstanding the sources your data comes from can help you use your data more efficiently.\nThis section aims to give a quick overview of different data sources to those unfamiliar with\ndata in production. If you\u2019ve already worked with ML in production for a while, feel free to\nskip this section.\n\nOne source is user input data, data explicitly input by users, which is often the input on which\nML models can make predictions. User input can be texts, images, videos, uploaded files, etc.\nIf there is a wrong way for humans to input data, humans are going to do it, and as a result, user\ninput data can be easily mal-formatted. If user input is supposed to be texts, they might be too\nlong or too short. If it\u2019s supposed to be numerical values, users might accidentally enter texts. If\nyou expect users to upload files, they might upload files in the wrong formats. User input data\nrequires more heavy-duty checking and processing. Users also have little patience. In most\ncases, when we input data, we expect to get results back immediately. Therefore, user input\ndata tends to require fast processing.\n\nAnother source is system-generated data. This is the data generated by different components\nof your systems, which include various types of logs and system outputs such as model\npredictions.\n\nLogs can record the state of the system and significant events in the system, such as memory\nusage, number of instances, services called, packages used, etc. They can record the results of\ndifferent jobs, including large batch jobs for data processing and model training. These types of\nlogs provide visibility into how the system is doing, and the main purpose of this visibility is\nfor debugging and possibly improving the application. Most of the time, you don\u2019t have to look\nat these types of logs, but they are essential when something is on fire.\n\nBecause logs are system generated, they are much less likely to be malformatted the way user\ninput data is. Overall, logs don\u2019t need to be processed as soon as they arrive, the way you\nwould want to process user input data. For many use cases, it\u2019s acceptable to process logs\n\nperiodically, such as hourly or even daily. However, you might still want to process your logs\nfast to be able to detect and be notified whenever something interesting happens\u2019.\n\nBecause debugging ML systems is hard, it\u2019s a common practice to log everything you can. This\nmeans that your volume of logs can grow very, very quickly. This leads to two problems. The\nfirst is that it can be hard to know where to look because signals are lost in the noise. There\nhave been many services that process and analyze logs, such as Logstash, DataDog, Logz, etc.\nMany of them use ML models to help you process and make sense of your massive amount of\nlogs.\n\nThe second problem is how to store a rapidly growing amount of logs. Luckily, in most cases,\nyou only have to store logs for as long as they are useful, and can discard them when they are\nno longer relevant for you to debug your current system. If you don\u2019t have to access your logs\nfrequently, they can also be stored in low-access storage that costs much less than higher-\nfrequency-access storage\u201d.\n\nSystem also generates data to record users\u2019 behaviors, such as clicking, choosing a suggestion,\nscrolling, zooming, ignoring a popup, or spending an unusual amount of time on certain pages.\nEven though this is system-generated data, it\u2019s still considered part of user data? and might be\nsubject to privacy regulations. This kind of data can also be used for ML systems to make\npredictions and to train their future versions.\n\nThere are also internal databases, generated by various services and enterprise applications in\nacompany. These databases manage their assets such as inventory, customer relationship,\nusers, and more. This kind of data can be used by ML models directly or by various\ncomponents of an ML system. For example, when users enter a search query on Amazon, one\nor more ML models will process that query to detect the intention of that query \u2014 what\nproducts users are actually looking for? \u2014 then Amazon will need to check their internal\ndatabases for the availability of these products before ranking them and showing them to users.\n\nThen there\u2019s the wonderfully weird word of third-party data that, to many, is riddled with\nprivacy concerns. First-party data is the data that your company already collects about your\nusers or customers. Second-party data is the data collected by another company on their own\ncustomers that they make available to you, though you'll probably have to pay for it. Third-\nparty data companies collect data on the public who aren\u2019t their customers.\n\nThe rise of the Internet and smartphones has made it much easier for all types of data to be\ncollected. It used to be especially easy with smartphones since each phone used to have a\nunique advertiser ID \u2014 iPhones with their Apple\u2019s Identifier for Advertisers (IDFA) and\nAndroid phones with their Android Advertising ID (AAID) \u2014 which acts as a unique ID to\naggregate all activities on a phone. Data from apps, websites, check-in services, etc. are\ncollected and (hopefully) anonymized to generate activity history for each person.\n\nYou can buy all types of data such as social media activities, purchase history, web browsing\nhabits, car rentals, and political leaning for different demographic groups getting as granular as\nmen, age 25-34, working in tech, living in the Bay Area. From this data, you can infer\ninformation such as people who like brand A also like brand B. This data can be especially\n\nhelpful for systems such as recommendation systems to generate results relevant to users\u2019\ninterests. Third-party data is usually sold as structured data after being cleaned and processed\nby vendors.\n\nHowever, as users demand more privacy to their data, companies have been taking steps to curb\nthe usage of advertiser IDs. In early 2021, Apple made their IDFA opt-in. This change has\nreduced significantly the amount of third-party data available on iPhones, forcing many\ncompanies to focus more on first-party data\u2018. To fight back this change, advertisers have been\ninvesting in workarounds. For example, China Advertising Association, a state-supported trade\nassociation for China\u2019s advertising industry, invested in a device fingerprinting system called\nCAID that allowed apps like TikTok and Tencent to keep tracking iPhone users\u00b0.\n\nData Formats\n\nOnce you have data, you might want to store it (or \u201cpersist\u201d it, in technical terms). Since your\ndata comes from multiple sources with different access patterns\u00ae, storing your data isn\u2019t always\nstraightforward and, for some cases, can be costly. It\u2019s important to think about how the data\nwill be used in the future so that the format you use will make sense. Here are some of the\nquestions you might want to consider. How do I store multimodal data? When each sample\nmight contain both images and texts? Where to store your data so that it\u2019s cheap and still fast to\naccess? How to store complex models so that they can be loaded and run correctly on different\nhardware?\n\nThe process of converting a data structure or object state into a format that can be stored or\ntransmitted and reconstructed later is data serialization. There are many, many data\nserialization formats. When considering a format to work with, you might want to consider\ndifferent characteristics such as human readability, access patterns, and whether it\u2019s based on\ntext or binary, which influences the size of its files. Table 2-1 consists of just a few of the\ncommon formats that you might encounter in your work. For a more comprehensive list, check\nout the wonderful Wikipedia page Comparison of data-serialization formats.\n\nFormat Binary/Text Human-readable? ~~ Example use cases\n\nJSON Text Yes Everywhere\n\ncsv Text Yes Everywhere\n\nParquet Binary No Hadoop, Amazon Redshift\n\nAvro Binary primary No Hadoop\n\nProtobuf Binary primary No Google, TensorFlow (TFRecord)\nPickle Binary No Python, PyTorch serialization\n\nWe'll go over a few of these formats, starting with JSON.\n\nJSON\n\nJSON, JavaScript Object Notation, is everywhere. Even though it was derived from JavaScript,\nit\u2019s language-independent \u2014 most modern programming languages can generate and parse\nJSON. It\u2019s human-readable. Its key-value pair paradigm is simple but powerful, capable of\nhandling data of different levels of structuredness. For example, your data can be stored in a\nstructured format like the following.\n\n{\n\n\u201cfirstName\": \"Boatie\",\n\n\u201clastName\u2019 \u201cMcBoatFace\",\n\u201cisVibing\": true,\n\n\"age\": 12,\n\n\"address\": (\n\n\"streetAddress\": \"12 Ocean Drive\",\n\"city\": \"Port Royal\",\n\n\u201cpostalCode\": \"10021-3100\"\n}\n}\n\nThe same data can also be stored in an unstructured blob of text like the following.\n{\n\n\"text\": \u201c\"Boatie McBoatFace, aged 12, is vibing, at 12 Ocean Drive, Port Royal,\n10021-3100\"\n}\n\nBecause JSON is ubiquitous, the pain it causes can also be felt everywhere. Once you\u2019ve\ncommitted the data in your JSON files to a schema, it\u2019s pretty painful to retrospectively go back\nto change the schema. JSON files are text files, which means they take up a lot of space, as\nwe'll see in the section Text vs. Binary Format below.\n\nRow-major vs. Column-major Format\n\nThe two formats that are common and represent two distinct paradigms are CSV and Parquet.\nCSV is row-major, which means consecutive elements in a row are stored next to each other in\nmemory. Parquet is column-major, which means consecutive elements in a column are stored\nnext to each other.\n\nBecause modern computers process sequential data more efficiently than non-sequential data, if\na table is row-major, accessing its rows will be faster than accessing its columns in expectation.\nThis means that for row-major formats, accessing data by rows is expected to be faster than\naccessing data by columns.\n\nImagine we have a dataset of 1000 examples, each example has 10 features. If we consider\neach example as a row and each feature as a column, then the row-major formats like CSV are\nbetter for accessing examples, e.g. accessing all the examples collected today. Column-major\nformats like Parquet are better for accessing features, e.g. accessing the timestamps of all your\nexamples. See Figure 2-1.\n\nColumn.mjor\ndatas stored and retrieved clumn-by-column\n\u00a9 ood fr accessing features\n\n{\nColumnt} Column2 Column 3\n\nRor ame! ww |\n\u00a2 catais stored and retrieved\n\nrove 0 Eramgle2\nood fr accessing samples\nample}.\n\nLS\n\nFigure 2-1. Row-major vs. column-major formats\n\nI use CSV as an example of the row-major format because it\u2019s popular and generally\nrecognizable by everyone I\u2019ve talked to in tech. However, some of the early reviewers of\nthis book got upset by the mention of CSV because they believe CSV is a horrible data\nformat. It serializes non-text characters poorly. For example, when you write float values to\na CSV file, some precision might be lost \u2014 0.12345678901232323 could be arbitrarily\nrounded up as \u201c0.12345678901\u201d \u2014 as complained about here and here. People on Hacker\nNews have passionately argued against using CSV.\n\nColumn-major formats allow flexible column-based reads, especially if your data is large\nwith thousands, if not millions, of features. Consider if you have data about ride-sharing\ntransactions that has 1000 features but you only want 4 features: time, location, distance, price.\nWith column-major formats, you can read the 4 columns corresponding to these 4 features\ndirectly. However, with row-major formats, if you don\u2019t know the sizes of the rows, you will\nhave to read in all columns then filter down to these 4 columns. Even if you know the sizes of\nthe rows, it can still be slow as you'll have to jump around the memory, unable to take\nadvantage of caching.\n\nRow-major formats allow faster data writes. Consider the situation when you have to keep\nadding new individual examples to your data. For each individual example, it\u2019d be much faster\nto write it to a file that your data is already in a row-major format.\n\nOverall, row-major formats are better when you have to do a lot of writes, whereas column-\nmajor ones are better when you have to do a lot of column-based reads.\n\nNUMPY VS. PANDAS\n\nOne subtle point that a lot of people don\u2019t pay attention to, which leads to misuses of\nPandas, is that this library is built around the columnar format.\n\nPandas is built around DataFrame, a concept inspired by R\u2019s Data Frame, which is column-\nmajor. A DataFrame is a two-dimensional table with rows and columns.\n\nIn NumPy, the major order can be specified. When an ndarray is created, it\u2019s row-major by\ndefault if you don\u2019t specify the order. People coming to pandas from NumPy tend to treat\nDataFrame the way they would ndarray, e.g. trying to access data by rows, and find\nDataFrame slow.\n\nIn Figure 2-3a, you can see that accessing a DataFrame by row is so much slower than\naccessing the same DataFrame by column. If you convert this same DataFrame to a NumPy\nndarray, accessing a row becomes much faster, as you can see in Figure 2-3b.\u201d\n\n# Iterating pandas DataFrame by column\nstart = time.time()\nfor col in df.columns:\n\nfor item in df[col]:\n\npass\nprint(time.time() - start, \u201cseconds\")\n\n0.06656503677368164 seconds <=\n\n# Iterating pandas DataFrame by row\nn_rows = len(df)\nstart = time.time()\nfor i in range(n_rows):\nfor item in df.iloc[i]:\npass\nprint(time.time() - start, \u201cseconds\")\n\n2.4123919010162354 seconds a\n\nFigure 2-2. a: Iterating a pandas DataFrame by column takes 0.07 seconds but iterating the same DataFrame by row\ntakes 2.41 seconds.\n\ndf_np = df.to_numpy()\nn_rows, n_cols = df_np.shape\n\n# Iterating NumPy ndarray by column\nstart = time.time()\nfor j in range(n_cols):\nfor item in df_np[:, j]:\npass\nprint(time.time() - start, \u201cseconds\")\n\n0.005830049514770508 seconds| <\u00a2\u2014\u2014\u2014\n\n# Iterating NumPy ndarray by row\nstart = time.time()\nfor i in range(n_rows):\nfor item in df_np[i]:\npass\nprint(time.time() - start, \u201cseconds\")\n\n0.019572019577026367 seconds| <}\u2014\u2014\u2014\n\nFigure 2-3. b: When you convert the same DataFrame into a NumPy ndarray, accessing its rows becomes much faster.\n\nText vs. Binary Format\n\nCSV and JSON are text files whereas Parquet files are binary files. Text files are files that are\nin plain texts, which usually mean they are human-readable. Binary files, as the name suggests,\nare files that contain 0\u2019s and 1\u2019s, and meant to be read or used by programs that know how to\ninterpret the raw bytes. A program has to know exactly how the data inside the binary file is\nlaid out to make use of the file. If you open text files in your text editors (e.g. VSCode,\nNotepad), you\u2019ll be able to read the texts in them. If you open a binary file in your text editors,\nyou'll see blocks of numbers, likely in hexadecimal values, for corresponding bytes of the file.\n\nBinary files are more compact. Here\u2019s a simple example to show how binary files can save\nspace compared to text files. Consider you want to store the number 1000000. If you store it in\na text file, it'll require 7 characters, and if each character is 1 byte, it\u2019ll require 7 bytes. If you\nstore it in a binary file as int32, it'll take only 32 bits or 4 bytes.\n\nAs an illustration, I use interviews.csv, which is a CSV file (text format) of 17,654 rows and 10\ncolumns. When I converted it to a binary format (Parquet), the file size went from 14MB to\n6MB, as shown in Figure 2-4.\n\nIn [2]: df = pd.read_csv(\"data/interviews.csv\")\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 17654 entries, 0 to 17653\n\nData columns (total 10 columns):\n\n#\n\nOrIDUSWNHRO\n\n9\n\nColumn\n\nDate\nUpvotes\nOffer\nExperience\nDifficulty\nReview\n\nNon-Null Count\n\n16376\n17654\n\nnon-null\nnon-null\nnon-null\nnon-null\nnon-null\nnon-null\nnon-null\nnon-null\nnon-null\nnon-null\n\nobject\n\ndtypes: float64(1), int64(1), object(8)\nmemory usage: 1.3+ MB\n\nIn [3]: Path(\"data/interviews.csv\").stat().st_size\n\nOut[3]: }14200063 =<.\n\nIn [4]: df\u00a3.to_parquet(\"data/interviews.parquet\")\nPath(\"data/interviews.parquet\").stat().st_size\n\nOut[4]: |6211862 =<\n\nFigure 2-4. When stored in CSV format, my interview file is 14MB. But when stored in Parquet, the same file is 6MB.\n\nAWS recommends using the Parquet format because \u201cthe Parquet format is up to 2x faster to\nunload and consumes up to 6x less storage in Amazon $3, compared to text formats.\u201d\u00ae\n\nData Models\n\nData models describe how data is represented. Consider cars in the real world. In a database, a\ncar can be described using its maker, its model year, its color, and its price. These attributes\nmake up a data model for cars. Alternatively, you can also describe a car using its owner, its\nlicense plate, and its history of registered addresses. This is another data model for cars.\n\nHow you choose to represent data not only affects the way your systems are built, but also the\nproblems your systems can solve. For example, the way you represent cars in the first data\nmodel makes it easier for people looking to buy cars, whereas the second data model makes it\neasier for police officers to track down criminals.\n\nIn this section, we\u2019ll study two types of models that seem opposite to each other but are\nactually converging: relational models and NoSQL models. We'll go over examples to show\nthe types of problems each model is suited for.\n\nRelational Model\n\nRelational models are among the most persistent ideas in computer science. Invented by Edgar\nF. Codd in 1970%, the relational model is still going strong today, even getting more popular.\nThe idea is simple but powerful. In this model, data is organized into relations, each relation is\na set of tuples. A table is an accepted visual representation of a relation, and each row of a table\nmakes up a tuple\u2019\u00ae, as shown in Figure 2-5. Relations are unordered. You can shuffle the order\nof the rows or the order of the columns in a relation and it\u2019s still the same relation. Data\nfollowing the relational model is usually stored in file formats like CSV, Parquet.\n\nTupe (ow\nunordered\n\nColumn f Column 2 Column 3 rm\n\nColumn:\nunordered\n\nFigure 2-5. In a relation, the order of neither the rows nor the columns matter.\n\nIt\u2019s often desirable for relations to be normalized. Data normalization can follow normal forms\nsuch as the first normal form (1NF), second normal form (2NF), etc., and readers interested can\nread more about it on Wikipedia. In this book, we'll go through an example to show how\nnormalization works and how it can reduce data redundancy and improve data integrity.\n\nConsider the relation Book shown in Table 2-2. There are a lot of duplicates in this data. For\nexample, row | and 2 are nearly identical, except for format and price. If the publisher\ninformation changes, for example, its name changes from \u201cBanana Press\u201d to \u201cPineapple Press\u201d\nor its country changes, we\u2019ll have to update columns 1, 2, and 4. If we separate publisher\ninformation into its own table, as shown in Table 2-3a and Table 2-3b, when a publisher\u2019s\ninformation changes, we only have to update the Publisher relation'\u2019. This practice allows us to\nstandardize spelling of the same value across different columns. It also makes it easier to make\n\nchanges to these values, either because when these values change or when you want to translate\nthem into different languages.\n\nOne major downside of normalization is that your data is now spread across multiple relations.\nYou can join the data from different relations back together, but joining can be expensive for\nlarge tables.\n\nDatabases built round the relational data model are relational databases. Once you\u2019 ve put data\nin your databases, you\u2019 ll want a way to retrieve it. The language that you can use to specify the\ndata that you want from a database is called a query language. The most popular query\nlanguage for relational databases today is SQL. Even though inspired by the relational model,\nthe data model behind SQL has deviated from the original relational model. For example, SQL\ntables can contain row duplicates, whereas true relations can\u2019t contain duplicates. However, this\nsubtle difference has been safely ignored by most people.\n\nThe most important thing to note about SQL is that it\u2019s a declarative language, as opposed to\nPython which is an imperative language. In the imperative paradigm, you specify the steps\nneeded for an action and the computer executes these steps to return the outputs. In the\ndeclarative paradigm, you specify the outputs you want, and the computer figures out the steps\nneeded to get you the queried outputs.\n\nWith an SQL database, you specify the pattern of data you want \u2014 the tables you want the data\nfrom, the conditions the results must meet, the basic data transformations such as join, sort,\ngroup, aggregate, etc. \u2014 but not how to retrieve the data. It is up to the database system to\ndecide how to break the query into different parts, what methods to use to execute each part of\nthe query, and the order in which different parts of the query should be executed.\n\nWith certain added features, SQL can be Turing-complete, which means that in theory, SQL\ncan be used to solve any computation problem (without making any guarantee about the time or\nmemory required). However, in practice, it\u2019s not always easy to write a query to solve a specific\ntask, and it\u2019s not always feasible or tractable to execute a query. Anyone working with SQL\ndatabases might have nightmarish memories of painfully long SQL queries that are impossible\nto understand and nobody dares to touch for fear that things might break'2.\n\nFiguring out how to execute an arbitrary query is the hard part, which is the job of query\noptimizers. A query optimizer examines all possible ways to execute a query and finds the\nfastest way to do so'?. It\u2019s possible to use ML to improve query optimizers based on learning\nfrom incoming queries'*. Query optimization is one of the most challenging problems in\ndatabase systems, and normalization means that data is spread out on multiple relations, which\nmakes joining it together even harder. Even though developing a query optimizer is hard, the\ngood news is that you generally only need one query optimizer and all your applications can\nleverage it.\n\nFROM DECLARATIVE DATA SYSTEMS TO DECLARATIVE ML SYSTEMS\n\nPossibly inspired by the success of declarative data systems, many people have looked\nforward to declarative ML'\u00ae. With a declarative ML system, users only need to declare the\nfeatures\u2019 schema and the task, and the system will figure out the best model to perform that\ntask with the given features. Users won\u2019t have to write code to construct, train, and tune\nmodels. Popular frameworks for declarative ML are Ludwig, developed at Uber, and H20\nAutoML. In Ludwig, users can specify the model structure \u2014 such as the number of fully\nconnected layers and the number of hidden units \u2014 on top of the features\u2019 schema and\noutput. In H2O AutoML, you don\u2019t need to specify the model structure or hyperparameters.\nIt experiments with multiple model architectures and picks out the best model given the\nfeatures and the task.\n\nHere is an example to show how H20 AutoML works. You give the system your data\n(inputs and outputs and specify the number of models you want to experiment. It\u2019Il\nexperiment with that number of models and show you the best performing model.\n\n# y predictors and response\nx = train.columns\n\ny = \"response\"\nx\n\n. remove (y)\n\n# For binary fication, response should be a factor\ntrain[y] = train[y].asfactor()\n\ntest[y] = test[y].asfactor()\n\n# Run AutoML for 20 base models\naml = H20AutoML (max_models=20, seed=1)\naml.train(x=x, y=y, training frame=train)\n\n# Show the best performing models on the AutoML Leaderboard\n\nlb = aml. leaderboard\n\n# Get the best performing model\naml. leader\n\nWhile declarative ML can be useful in many cases, it leaves unanswered the biggest\nchallenges with ML in production. Declarative ML systems today abstract away the model\ndevelopment part, and as we\u2019ll cover in the next six chapters, with models being\nincreasingly commoditized, model development is often the easier part. The hard part lies\nin feature engineering, data processing, model evaluation, data shift detection, continual\nlearning, etc.\n\nNoSQL\n\nThe relational data model has been able to generalize to a lot of use cases, from ecommerce to\nfinance to social networks. However, for certain use cases, this model can be restrictive. For\nexample, it demands that your data follows a strict schema and schema management is painful.\n\nIn a survey by Couchbase in 2014, frustration with schema management was the #1 reason for\nthe adoption of their nonrelational database. It can also be difficult to write and execute SQL\nqueries for specialized applications.\n\nThe latest movement against the relational data model is NoSQL. Originally started as a\nhashtag for a meetup to discuss nonrelational databases, NoSQL has been retroactively\nreinterpreted as Not Only SQL\"\u00ae as many NoSQL data systems also support relational models.\nTwo major types of nonrelational models are the document model and the graph model. The\ndocument model targets use cases where data comes in self-contained documents and\nrelationships between one document and another are rare. The graph model goes in the opposite\ndirection, targeting use cases where relationships between data items are common and\nimportant. We\u2019ll examine each of these two models, starting with the document model.\n\nDocument Model\n\nThe document model is built around the concept of \u201cdocument\u201d. A document is often a single\ncontinuous string, encoded as JSON, XML, or a binary format like BSON. All documents in a\ndocument database are assumed to be encoded in the same format. Each document has a unique\nkey that represents that document, which can be used to retrieve that document.\n\nA collection of documents could be considered analogous to a table in a relational database,\nand a document analogous to a row. In fact, you can convert a relation into a collection of\ndocuments that way. For example, you can convert the book data in Table 2-3a and Table 2-3b\ninto three JSON documents as shown in Figure 2-6. However, a collection of documents is\nmuch more flexible than a table. All rows in a table must follow the same schema (e.g. have the\nsame sequence of columns), while documents in the same collection can have completely\ndifferent schemas.\n\n# Document 1: harry _potter.json\n{\n\"Title\": \"Harry Potter\",\n\"Author\": \"J.K. Rowling\",\n\"Publisher\": \"Banana Press\",\n\"Country\": \"UK\",\n\"Sold as\": [\n{\"Format\": \"Paperback\", \"Price\": \"$20\"},\n{\"Format\": \"E-book\", \"Price\": \"$10\"}\n\n}\n\n# Document 2: sherlock_holmes.json\n{\n\"Title\": \"Sherlock Holmes\",\n\"Author\": \"Conan Doyle\",\n\"Publisher\": \"Guava Press\",\n\"Country\": \"US\",\n\"Sold as\": [\n{\"Format\": \"Paperback\", \"Price\": \"$30\"},\n{\"Format\": \"E-book\", \"Price\": \"$15\"}\n\n}\n\n# Document 3: the_hobbit.json\n{\n\"Title\": \"The Hobbit\",\n\"Author\": \"J.R.R. Tolkien\",\n\"Publisher\": \"Banana Press\",\n\"Country\": \"UK\",\n\"Sold as\": [\n{\"Format\": \"Paperback\", \"Price\": \"$30\"},\n\nFigure 2-6. Representing the book data in Table 2-3a and Table 2-3b\nin the document data model.\n\nBecause the document model doesn\u2019t enforce a schema, it\u2019s often referred to as schemaless.\nThis is misleading because, as discussed previously, data stored in documents will be read later.\nThe application that reads the documents usually assumes some kind of structure of the\ndocuments. Document databases just shift the responsibility of assuming structures from the\napplication that writes the data to the application that reads the data.\n\nThe document model has better locality than the relational model. Consider the book data\nexample in Table 2-3a and Table 2-3b where the information about a book is spread across both\nthe Book table and the Publisher table (and potentially also the Format table). To retrieve\ninformation about a book, you\u2019ll have to query multiple tables. In the document model, all\ninformation about a book can be stored in a document, making it much easier to retrieve.\n\nHowever, compared to the relational model, it\u2019s harder and less efficient to execute joins across\ndocuments compared to across tables. For example, if you want to find all books whose prices\nare below $25, you\u2019ll have to read all documents, extract the prices, compare them to $25, and\nreturn all the documents containing the books with prices below $25.\n\nBecause of the different strengths of the document and relational data models, it\u2019s common to\nuse both models for different tasks in the same database systems. More and more database\nsystems, such as PostgreSQL and MySQL, support them both.\n\nGraph Model\n\nThe graph model is built around the concept of a \u201cgraph\u201d. A graph consists of nodes and edges,\nwhere the edges represent the relationships between the nodes. A database that uses graph\nstructures to store its data is called a graph database. If in document databases, the content of\neach document is the priority, then in graph databases, the relationships between data items are\nthe priority.\n\nBecause the relationships are modeled explicitly in graph models, it\u2019s faster to retrieve data\nbased on relationships. Consider an example of a graph database in Figure 2-7. The data from\nthis example could potentially come from a simple social network. In this graph, nodes can be\nof different data types: person, city, country, company, etc.\n\ntype: country\n\nname: USA\nwithin\ntype: country type: state\nname: France name: California\nwithin within\ntype: cy type cy type cy\nname: Pais name: Palo Alto born.in fame: Stanford\nves in bom_in (ves.in\ntype: person coworker type: person type: person\nname: Max Halford name: Zhenzhong Xu name: Chloe He\n\nFigure 2-7. An example of a simple graph database.\nThis data could potentially come from a social network.\n\nImagine you want to find everyone who was born in the USA. Given this graph, you can start\nfrom the node USA and traverse the graph following the edges \u201cwithin\u201d and \u201cborn_in\u201d to find\nall the nodes of the type \u201cperson\u201d. Now, imagine that instead of using the graph model to\nrepresent this data, we use the relational model. There\u2019d be no easy way to write a SQL query\nto find everyone who was born in the USA, especially given that there are unknown number of\n\nhops between country and person \u2014 there are 3 hops between Zhenzhong Xu and USA while\nthere are only 2 hops between Chloe He and USA. Similarly, there\u2019d be no easy way for this\ntype of queries with a document database.\n\nMany queries are easy to do in one data model but harder to do in another. Picking the right\ndata model for your application can make your life so much easier.\n\nStructured vs. Unstructured Data\n\nStructured data is data that follows a predefined data model, also known as a data schema. For\nexample, the data model might specify that each data item consists of two values: the first\nvalue, \u201cname\u201d, is a string at most 50 characters, and the second value, \u201cage\u201d, is an 8-bit integer\nin the range between 0 and 200. The predefined structure makes your data easier to analyze. If\nyou want to know the average age of people in the database, all you have to do is to extract all\nthe age values and average them out.\n\nThe disadvantage of structured data is that you have to commit your data to a predefined\nschema. If your schema changes, you'll have to retrospectively update all your data, often\ncausing mysterious bugs in the process. For example, you\u2019ve never kept your users\u2019 email\naddresses before but now you do, so you have to retrospectively update email information to all\nprevious users. One of the strangest bugs one of my colleagues encountered was when they\ncould no longer use users\u2019 ages with their transactions, and their data schema replaced all the\nnull ages with 0, and their ML model thought the transactions were made by people of 0 years\nold'\u201d.\n\nBecause business requirements change over time, committing to a predefined data schema can\nbecome too restricting. Or you might have data from multiple data sources which are beyond\nyour control, and it\u2019s impossible to make them follow the same schema. This is where\nunstructured data becomes appealing. Unstructured data is data that doesn\u2019t adhere to a\npredefined data schema. It\u2019s usually text but can also be numbers, dates, etc. For example, a\ntext file of logs generated by your ML model is unstructured data.\n\nEven though unstructured data doesn\u2019t adhere to a schema, it might still contain intrinsic\npatterns that help you extract structures. For example, the following text is unstructured, but\nyou can notice the pattern that each line contains two values separated by a comma, the first\nvalue is textual and the second value is numerical. However, there is no guarantee that all lines\nmust follow this format. You can add a new line to that text even if that line doesn\u2019t follow this\nformat.\n\n\u201cLisa, 43\n\nJack, 23\nNguyen, 59\u201d\n\nUnstructured data also allows for more flexible storage options. For example, if your storage\nfollows a schema, you can only store data following that schema. But if your storage doesn\u2019t\nfollow a schema, you can store any type of data. You can convert all your data, regardless of\ntypes and formats into bytestrings and store them together.\n\nA repository for storing structured data is called a data warehouse. A repository for storing\nunstructured data is called a data lake. Data lakes are usually used to store raw data before\nprocessing. Data warehouses are used to store data that have been processed into formats ready\nto be used. Table 2-2 shows a summary of the key differences between structured and\nunstructured data.\n\nData formats and data models specify the interface for how users can store and retrieve data.\nStorage engines, also known as databases, are the implementation of how data is stored and\nretrieved on machines. It\u2019s useful to understand different types of databases as your team or\nyour adjacent team might need to select a database appropriate for your application.\n\nTypically, there are two types of workloads that databases are optimized for: transactional\nprocessing and analytical processing, and there\u2019s a big difference between them. In this section,\nwe will discuss the difference between transaction processing and analytical processing. We\nwill then cover the basics of the ETL (Extract, Transform, Load) process that you will\ninevitably encounter when building an ML system in production.\n\n[TODO] Readers tuned into data engineering trends might wonder why batch processing versus\nstream processing is missing from this chapter. We\u2019ll cover this topic in Chapter 6:\nDeployment since I believe it\u2019s more related to other deployment concepts.\n\nTransactional and Analytical Processing\n\nTraditionally, a transaction refers to the action of buying or selling something. In the digital\nworld, a transaction refers to any kind of actions that happen online: tweeting, ordering a ride\nthrough a ridesharing service, uploading a new model, watching a YouTube video, etc. Even\nthough these different transactions involve different types of data, the way they\u2019re processed is\nsimilar across applications. The transactions are inserted as they are generated, and\noccasionally updated when something changes, or deleted when they are no longer needed*\u00ae.\nThis type of processing is known as OnLine Transaction Processing (OLTP).\n\nBecause these transactions often involve users, they need to be processed fast (low latency) so\nthat they don\u2019t keep users waiting. The processing method needs to have high availability \u2014\ne.g. the processing system needs to be available any time a user wants to make a transaction. If\nyour system can\u2019t process a transaction, that transaction won't go through.\n\nTransactional databases are designed to process online transactions and satisfy the low latency,\nhigh availability requirements. When people hear transactional databases, they usually think of\nACID (Atomicity, Consistency, Isolation, Durability). Here are their quick definitions of ACID\nfor those needing a quick reminder.\n\n\u00a2 Atomicity: to guarantee that all the steps in a transaction are completed successfully as\na group. If any step between the transaction fails, all other steps must fail also. For\nexample, if a user\u2019s payment fails, you don\u2019t want to still assign a driver to that user.\n\n+ Consistency: to guarantee that all the transactions coming through must follow\npredefined rules. For example, a transaction must be made by a valid user.\n\n\u00a2 Isolation: to guarantee that two transactions happen at the same time as if they were\nisolated. Two users accessing the same data won\u2019t change it at the same time. For\nexample, you don\u2019t want two users to book the same driver at the same time.\n\n* Durability: to guarantee that once a transaction has been committed, it will remain\ncommitted even in the case of a system failure. For example, after you\u2019ve ordered a\nride and your phone dies, you still want your ride to come.\n\nHowever, transactional databases don\u2019t necessarily need to be ACID, and some developers find\nACID to be too restrictive. According to Martin Klepmann, systems that do not meet the ACID\ncriteria are sometimes called BASE, which stands for Basically Available, Soft state, and\nEventual consistency. This is even more vague than the definition of. ACID\".\n\nBecause each transaction is often processed as a unit separately from other transactions,\ntransactional databases are often row-major. This also means that transactional databases might\nnot be efficient for questions such as \u201cWhat's the average price for all the rides in September in\nSan Francisco?\u201d. This kind of analytical question requires aggregating data in columns across\nmultiple rows of data. Analytical databases are designed for this purpose. They are efficient\nwith queries that allow you to look at data from different viewpoints. We call this type of\nprocessing OnLine Analytical Processing (OLAP).\n\nHowever, both the terms OLTP and OLAP have become outdated, as shown in Figure 2-8, for\nthree reasons. First, the separation of transactional and analytical databases was due to\nlimitations of technology \u2014 it was hard to have databases that could handle both transactional\nand analytical queries efficiently. However, this separation is being closed. Today, we have\ntransactional databases that can handle analytical queries, such as CockroachDB. We also have\nanalytical databases that can handle transactional queries, such as Apache Iceberg and\nDuckDB.\n\n@ OLAP \u00a9 OUP\n\nSearch term Search term\n\n+ Add comparison\n\nWorldwide + \u20182004- present \u00a5 Alicetegories \u00a5 Web Search *\n\nle\n\u201c\n\nInterest over time\n\nL\n\nFigure 2-8. OLAP and OLTP are outdated terms, as of 2021, according to Google Trends\n\nSecond, in the traditional OLTP or OLAP paradigms, storage and processing are tightly\ncoupled \u2014 how data is stored is also how data is processed. This may result in the same data\nbeing stored in multiple databases and use different processing engines to solve different types\nof queries. An interesting paradigm in the last decade has been to decouple storage from\nprocessing (also known as compute), as adopted by many data vendors including Google\u2019s\nBigQuery, Snowflake, IBM, and Teradata. In this paradigm, the data can be stored in the same\nplace, with a processing layer on top that can be optimized for different types of queries.\n\nThird, \u201conline\u201d has become an overloaded term that can mean many different things. Online\nused to just mean \u201cconnected to the Internet\u201d. Then, it grew to also mean \u201cin production\u201d \u2014 we\nsay a feature is online after that feature has been deployed in production.\n\nIn the data world today, \u201conline\u201d might refer to the speed at which your data is processed and\nmade available: online, nearline, or offline. According to Wikipedia, online processing means\ndata is immediately available for input/output. Nearline, which is short for near-online, means\ndata is not immediately available, but can be made online quickly without human intervention.\nOffline means data is not immediately available, and requires some human intervention to\nbecome online.\n\nETL: Extract, Transform, and Load\n\nIn the early days of the relational data model, data was mostly structured. When data is\nextracted from different sources, it\u2019s first transformed into the desirable format before being\nloaded into the target destination such as a database or a data warehouse. This process is called\nETL, which stands for Extract, Transform, and Load.\n\nEven before ML, ETL (extract, transform, load) was all the rage in the data world, and it\u2019s still\nrelevant today for ML applications. ETL refers to the general purpose processing and\naggregating data into the shape and the format that you want.\n\nExtract is extracting the data you want from data sources. Your data will likely come from\nmultiple sources in different formats. Some of them will be corrupted or malformatted. In the\nextracting phase, you need to validate your data and reject the data that doesn\u2019t meet your\nrequirements. For rejected data, you might have to notify the sources. Since this is the first step\nof the process, doing it correctly can save you a lot of time downstream.\n\nTransform is the meaty part of the process, where most of the data processing is done. You\nmight want to join data from multiple sources and clean it. You might want to standardize the\nvalue ranges (e.g. one data source might use \u201cMale\u201d and \u201cFemale\u201d for genders, but another\nuses \u201cM\u201d and \u201cF\u201d or \u201c1\u201d and \u201c2\u201d). You can apply operations such as transposing,\ndeduplicating, sorting, aggregating, deriving new features, more data validating, etc.\n\nLoad is deciding how and how often to load your transformed data into the target destination,\nwhich can be a file, a database, or a data warehouse.\n\nThe idea of ETL sounds simple but powerful, and it\u2019s the underlying structure of the data layer\nat many organizations. An overview of the ETL process is shown in Figure 2-9.\n\nIMAGE TO COME\n\nFigure 2-9. An overview of the ETL process\n\nWhen the Internet first became ubiquitous and hardware had just become so much more\npowerful, collecting data suddenly became so much easier. The amount of data grew rapidly.\nNot only that, but the nature of data also changed. The number of data sources expanded, and\ndata schemas evolved.\n\nFinding it difficult to keep data structured, some companies had this idea: \u201cWhy not just store\nall data in a data lake so we don\u2019t have to deal with schema changes? Whichever application\nneeds data can just pull out raw data from there and process it.\u201d This process of loading data\ninto storage first then processing it later is sometimes called ELT (Extract, Load, Transform).\n\nThis paradigm allows for the fast arrival of data since there\u2019s little processing needed before\ndata is stored.\n\nHowever, as data keeps on growing, this idea becomes less attractive. It\u2019s inefficient to search\nthrough a massive amount of raw data for the piece of data that you want2\u00b0, At the same time,\nas companies switch to running applications on the cloud and infrastructures become\nstandardized, data structures also become standardized. Committing data to a predefined\nschema becomes more feasible.\n\nAs companies weigh the pros and cons of storing structured data vs. storing unstructured data,\nvendors evolve to offer hybrid solutions that combine the flexibility of data lakes and the data\nmanagement aspect of data warehouses. For example, Databricks and Snowflake both provide\ndata lakehouse solutions.\n\nModes of Dataflow\n\nIn this chapter, we\u2019ve been discussing data formats, data models, data storage and processing\nfor data used within the context of a single process. Most of the time, in production, you don\u2019t\nhave a single process but multiple. A question arises: how do we pass data between different\nprocesses that don\u2019t share memory? When a data is passed from one process to another, we say\nthat the data flows from one process to another, which gives us a dataflow.\n\nThere are three main modes of dataflow:\n* Data passing through databases.\n\n+ Data passing through services using requests such as the requests provided by REST\nand RPC APIs (e.g. POST / GET requests).\n\n\u00a2 Data passing through a real-time transport like Apache Kafka and Amazon Kinesis.\n\nWe\u2019ll go over each of them in this section.\n\nData Passing Through Databases\n\nThe easiest way to pass data between two processes is through databases, which we\u2019ve\ndiscussed in the Data Storage Engines and Processing section. For example, to pass data\nfrom process A and process B, process A can write that data into a database, and process B\nsimply reads from that database.\n\nThis mode, however, doesn\u2019t always work because of two reasons. First, it requires that both\nprocesses must be able to access the same database. This might be infeasible especially if the\ntwo processes are run by two different companies.\n\nSecond, it requires both processes to access data from databases, and read/write from databases\ncan be slow, making it unsuitable for applications with strict latency requirements \u2014 e.g.\nalmost all consumer-facing applications.\n\nData Passing Through Services\n\nOne way to pass data between two processes is to send data directly through a network that\nconnects these two processes. To pass data from process B to process A, process A first sends a\nrequest to process B that specifies that data A needs, and B returns the requested data through\nthe same network. Because processes communicate through requests, we say that this is\nrequest-driven.\n\nThis mode of data passing is tightly coupled with the service-oriented architecture. A service is\na process that can be accessed remotely, e.g. through a network. In this example, B is exposed\nto A asa service that A can send requests to. For B to be able to request data from A, A will\nalso need to be exposed to B as a service.\n\nTwo services in communication with each other can be run by different companies in different\napplications. For example, a service might be run by a stock exchange that keeps track of the\ncurrent stock prices. Another service might be run by an investment firm who requests the\ncurrent stock prices from the other service and use them to predict future stock prices.\n\nTwo services in communication with each other can also be parts of the same application.\nStructuring different components of your application as separate services allows each\ncomponent to be developed, tested, and maintained independently of each other. Structuring an\napplication as separate services gives you a microservice architecture.\n\nTo put the microservice architecture in the context of ML systems, imagine you\u2019re an ML\nengineer working on the price optimization problem for a company that owns a ride-sharing\napplication like Lyft. In reality, Lyft has hundreds of services in its microservice architecture,\nbut for the sake of simplicity, let\u2019s consider only three services:\n\n* The driver management service that predicts how many drivers will be available in\nthe next minute in a given area.\n\n* The ride management service that predicts how many rides will be requested in the\nnext minute in a given area.\n\n* The price optimization service predicts the optimal price for each ride. The price for\na ride should be low enough for riders to be willing to pay, yet high enough for drivers\nto be willing to drive and for the company to make a healthy profit.\n\nBecause the price depends on supply (the available drivers) and demand (the requested rides),\nthe price optimization service needs data from both the driver management and ride\nmanagement services. Each time a user requests a ride, the price optimization service requests\nthe predicted number of rides and predicted number of drivers to predict the optimal price for\nthis ride?\"\n\nThe most popular styles of requests used for passing data through networks are REST\n(Representational State Transfer) and RPC (Remote Procedure Call). Their detailed analysis is\nbeyond the scope of this book, but one major difference is that REST was designed for requests\nover networks whereas RPC \u201ctries to make a request to a remote network service look the same\n\nas calling a function or method in your programming language\u201d. Because of this, \u201cREST seems\nto be the predominant style for public APIs. The main focus of RPC frameworks is on requests\nbetween services owned by the same organization, typically within the same datacenter.\u201d72\n\nImplementations of a REST architecture are said to be RESTful. Even though many people\nthink of REST as HTTP, REST doesn\u2019t exactly mean HTTP because HTTP is just an\nimplementation of REST.\n\nData Passing Through Real-time Transport\n\nTo understand the motivation for real-time transports, let\u2019s go back to the above example of the\nride-sharing app with three simple services: driver management, ride management, and price\noptimization. In the last section, we discussed how the price optimization service needs data\nfrom the ride and driver management services to predict the optimal price for each ride.\n\nNow, imagine that the driver management service also needs to know the number of rides from\nthe ride management service to know how many drivers to mobilize. It also wants to know the\npredicted prices from the price optimization service to use them as incentives for potential\ndrivers (e.g. if you get on the road now you can get a 2x surge charge). Similarly, the ride\nmanagement service might also want data from the driver management and price optimization\nservices. If we pass data through services as discussed in the previous section, each of these\nservices need to send requests to the other two services, as shown in Figure 2-10.\n\nRide Driver\nmanagement management\n\noptimization\n\nFigure 2-10. In the request-driven architecture, each service needs to send requests to two other services.\n\nWith only 3 services, data passing is already getting complicated. Imagine having hundreds, if\nnot thousands of services like what major Internet companies have. Inter-service data passing\ncan blow up and become a bottleneck, slowing down the entire system.\n\nRequest-driven data passing is synchronous: the target service has to listen to the request for\nthe request to go through. If the price optimization service requests data from the driver\nmanagement service and the driver management service is down, the price optimization service\nwill keep resending the request until it times out. And if the price optimization service is down\nbefore it receives a response, the response will be lost. A service that is down can cause all\nservices that require data from it to be down.\n\nWhat if there\u2019s a broker that coordinates data passing among services? Instead of having\nservices request data directly from each other and creating a web of complex inter-service data\npassing, each service only has to communicate with the broker, as shown in Figure 2-11. For\nexample, instead of having other services request the driver management services for the\npredicted number of drivers for the next minute, what if whenever the driver management\nservice makes a prediction, this prediction is broadcasted to a broker? Whichever service wants\ndata from the driver management service can check that broker for the most recent predicted\n\nnumber of drivers. Similarly, whenever the price optimization service makes a prediction about\nthe surge charge for the next minute, this prediction is broadcasted to the broker.\n\nRide Driver\n\nmanagement management\n\nBroker |\n\nPrice\n\noptimization\n\nFigure 2-11. With a broker, a service only has to communicate with the broker instead of with other services.\n\nTechnically, a database can be a broker \u2014 each service can write data to a database and other\nservices that need the data can read from that database. However, as mentioned in the Data\nPassing Through Databases session, reading and writing from databases are too slow for\napplications with strict latency requirements. Instead of using databases to broker data, we use\nin-memory storage to broker data? Real-time transports can be thought of as in-memory storage\nfor data passing among services.\n\nA piece of data broadcasted to a real-time transport is called an event. This architecture is,\ntherefore, also called event-driven. A real-time transport is sometimes called an event bus.\n\nRequest-driven architecture works well for systems that rely more on logic than on data. Event-\ndriven architecture works better for systems that are data-heavy.\n\nThe two most common types of real-time transports are pubsub, which is short for publish-\nsubscribe, and message queue. In the pubsub model, any service can publish to different topics\nin a real-time transport and any service that subscribes to a topic can read all the events in that\ntopic. The services that produce data don\u2019t care about what services consume their data. Pubsub\nsolutions often have a retention policy \u2014 data will be retained in the real-time transport for a\ncertain period of time (e.g. 7 days) before being deleted or moved to a permanent storage (like\na database or $3). See Figure 2-12.\n\nPermwet: satue (8) | seeesennene\n\n\\ DA Pes eaten 106 - Cues ti\nEves) Fas US Boks ogy cats ay \u2018ning eves\n\n/ 104 + ls ee ard\n\nDiscord | Seeeceessseseesseaseent\n\nyng\nSSeS\n\nTine\n\nFigure 2-12. Incoming events are stored in in-memory storage before being discarded or moved to more permanent storage.\n\nIn a message queue model, an event often has intended consumers (an event with intended\nconsumers is called a message), and the message queue is responsible for getting the message\nto the right consumers.\n\n12M comgen es reported y use Malka ther tec stacks ic\n\nShai, an Spotty.\n\nda BOS uD EME\n\nem, it\n\nanid QR OP-2Eu-\n\nFigure 2-13. Companies that use Apache Kafka and Amazon Kinesis. Screenshotted on Stackshare\n\nExamples of pubsub solutions are Apache Kafka\u201d? and AWS Kinesis. Examples of message\nqueues are Apache RocketMQ and RabbitMQ. Both paradigms have gained a lot of traction in\nthe last few years. Figure 2-13 shows some of the companies that use Apache Kafka and\nRabbitMQ.\n\nBatch Processing vs. Stream Processing\n\nOnce your data arrives in data storage engines like databases, data lakes, or data warehouses, it\nbecomes historical data. This is opposed to streaming data (data that is still streaming in).\nHistorical data is often processed in batch jobs \u2014 jobs that are kicked off periodically. For\nexample, once a day, you might want to kick off a batch job to compute the average surge\ncharge for all the rides in the last day.\n\nWhen data is processed in batch jobs, we refer to it as batch processing. Batch processing has\nbeen a research subject for many decades, and companies have come up with distributed\nsystems like MapReduce and Spark to process batch data efficiently.\n\nWhen you have data in real-time transports like Apache Kafka and AWS Kinesis, we say that\nyou have streaming data. Stream processing refers to doing computation on streaming data.\nComputation on streaming data can also be kicked off periodically, but the periods are usually\nmuch shorter than the periods for batch jobs (e.g. every 5 minutes instead of every day).\nComputation on streaming data can also be kicked off whenever the need arises. For example,\nwhenever a user requests a ride, you process your data stream to see what drivers are currently\navailable.\n\nStream processing, when done right, can give low latency because you can process data as soon\nas data is generated, without having to first write it into databases. Many people believe that\nstreaming processing is less efficient than batch processing because you can\u2019t leverage tools\nlike MapReduce or Spark. This is not always the case for two reasons. First, streaming\ntechnologies like Apache Flink are proven to be highly scalable and fully distributed, which\nmeans they can do computation in parallel. Second, the strength of stream processing is in\nstateful computation. Consider the case where you want to process user engagement during a\n30 day trial. If you kick off this batch job every day, you\u2019ll have to do computation over the last\n30 days every day. With stream processing, it\u2019s possible to continue computing only the new\ndata each day and joining the new data computation with the older data computation,\npreventing redundancy.\n\nBecause batch processing happens much less frequently than stream processing, in ML, batch\nprocessing is usually used to compute features that change less often, such as when a driver\njoined the ridesharing app, what their rating is (if a driver has had hundreds of rides, their rating\nis less likely to change significantly from one day to the next). Batch features \u2014 features\nextracted through batch processing \u2014 are also known as static features.\n\nStream processing is used to compute features that change quickly, such as how many drivers\nare available right now, how many rides are being requested right now, how many rides will be\nfinished in the next 2 minutes, what\u2019s the average price of the last 10 rides in this area, etc.\nFeatures about the current state of the system like these are important to make the optimal price\npredictions. Streaming features \u2014 features extracted through stream processing \u2014 are also\nknown as dynamic features.\n\nFor many problems, you need not only historical features or streaming features, but both. You\nneed infrastructure that allows you to process streaming data as well as batch data and join\nthem together to feed into your ML models.\n\nTo do computation on data streams, you need a stream computation engine (the way Spark and\nMapReduce are batch computation engines). For simple streaming computation, you might be\nable to get away with built-in stream computation capacity of real-time transports like Apache\nKafka, but Kafka stream processing is limited in its ability to deal with various data sources.\n\nFor ML systems that leverage streaming features, the streaming computation is rarely simple.\nThe number of stream features used in an application such as fraud detection and credit scoring\ncan be in the hundreds, if not thousands. The stream feature extraction logic can require\ncomplex queries with join and aggregation along different dimensions. To extract these features\n\nrequires efficient stream processing engines. For this purpose, you might want to look into tools\nlike Apache Flink, KSQL, and Spark Streaming. Of these three engines, Apache Flink and\nKSQL are more recognized in the industry and provide a nice SQL abstraction for data\nscientists.\n\nStream processing is more difficult because the data amount is unbounded and the data comes\nin at variable rates and speeds. It\u2019s easier to make a stream processor do batch processing than\nmaking a batch processor do stream processing. Apache Flink\u2019s core maintainers have been\narguing for years that batch processing is a special case of stream processing.\n\nSummary\n\nThis chapter is built on the foundations established in Chapter | around the importance of data\nin developing ML systems. In this chapter, we learned it\u2019s important to choose the right format\nto store our data to make it easier to use the data in the future. We discussed different data\nformats, the pros and cons of row-major vs. column-major formats as well as text vs. binary\nformats.\n\nWe continued to cover three major data models: relational, document, and graph. Even though\nthe relational model is the most well-known given the popularity of SQL, all three models are\nwidely used today, and each is good for a certain set of tasks. One model can be emulated in\nterms of another model, and we went through an example to show how a relational database\ncan be represented using the document model. However, the wrong data model can make it\ndifficult to do our job.\n\nWhen talking about the relational model compared to the document model, many people think\nof the former as structured and the latter as unstructured. Even though the discussion of\nstructured vs. unstructured data can get quite heated, the division is quite fluid. Some people\neven argue that there\u2019s no such thing as unstructured or structured data. The question is who\nhas to shoulder the responsibility of assuming the structure of data. Structured data means that\nthe code that writes the data has to assume the structure. Unstructured data means that the code\nthat reads the data has to assume the structure.\n\nWe continued the chapter with data storage engines and processing. We studied databases\noptimized for two distinct types of data processing: transactional processing and analytical\nprocessing. We also studied a process that is ubiquitous in data science and analytics workloads\ntoday: ETL. These fundamentals will hopefully help readers become better prepared when\nfacing seemingly overwhelming data in production. We put data storage engines and processing\ntogether in the same section because traditionally, storage is coupled with processing:\ntransactional databases for transactional processing and analytical databases for analytical\nprocessing. However, in recent years, many vendors have worked on decoupling storage and\nprocessing. Today, we have transactional databases that can handle analytical queries and\nanalytical databases that can handle transactional queries.\n\nWhen discussing data formats, data models, data storage engines and processing, data is\nassumed to be within a process. However, while working in production, you'll likely work with\nmultiple processes, and you'll likely need to transfer data between them. We discussed three\nmodes of data passing. The simplest mode is passing through databases. The most popular\nmode of data passing for processes is data passing through services. In this mode, a process is\nexposed as a service that another process can send requests for data. This mode of data passing\nis tightly coupled with microservice architectures, where each component of an application is\nset up as a service.\n\nA mode of data passing that has become increasingly popular over the last decade is data\npassing through a real-time transport like Apache Kafka and RabbitMQ. This mode of data\npassing is somewhere between passing through databases and passing through services: it\nallows for asynchronous data passing with reasonable low latency.\n\nAs data in real-time transports have different properties from data in databases, they require\ndifferent processing techniques as discussed in the Batch Processing vs. Stream Processing\nsection. Data in databases is often processed in batch jobs and produces static features, whereas\ndata in real-time transports is often processed using stream computation engines and produces\ndynamic features. Some people argue that batch processing is a special case of stream\nprocessing, and stream computation engines can be used to unify both processing pipelines.\n\nOnce we\u2019ve had our data systems figured out, we can collect data and create training data,\nwhich will be the focus of the next chapter.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 2,
                "chapter_name": "Chapter 3. Training Data",
                "chapter_path": "./screenshots-images-2/chapter_2",
                "sections": [
                    {
                        "section_id": 2.1,
                        "section_name": "Chapter 3. Training Data",
                        "section_path": "./screenshots-images-2/chapter_2/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_2/section_1/63a156e5-883e-4f91-8d60-f3444e87a2cb.png",
                            "./screenshots-images-2/chapter_2/section_1/19dbbe6b-f203-45ea-973d-b257c7bbcbbd.png",
                            "./screenshots-images-2/chapter_2/section_1/95276e62-995e-4dc7-a223-b21533e14281.png",
                            "./screenshots-images-2/chapter_2/section_1/831c1355-8aea-479f-a670-b3d4a6edfb08.png",
                            "./screenshots-images-2/chapter_2/section_1/d62cdb40-473b-48b8-a292-34c7f4b04a4c.png",
                            "./screenshots-images-2/chapter_2/section_1/85976f82-1e77-4fd4-be50-b913b37a7556.png",
                            "./screenshots-images-2/chapter_2/section_1/ead553cf-908f-461a-b0a1-c66ce8858b3f.png",
                            "./screenshots-images-2/chapter_2/section_1/dba150f5-c6e6-4e39-8535-c7532c086b5f.png",
                            "./screenshots-images-2/chapter_2/section_1/79775540-f058-42e7-b277-8760aba85f45.png",
                            "./screenshots-images-2/chapter_2/section_1/33150c45-b3d7-48b6-a7d5-129326f98faf.png",
                            "./screenshots-images-2/chapter_2/section_1/612518b4-0b34-402d-ad66-8e52901e19b8.png",
                            "./screenshots-images-2/chapter_2/section_1/f8715f83-69e8-4936-b7c1-2d768e6b5d17.png",
                            "./screenshots-images-2/chapter_2/section_1/a54e13b9-44f1-46ad-9cdd-ecc31add8030.png",
                            "./screenshots-images-2/chapter_2/section_1/ea832a8b-140b-4b6c-acac-e490b991a740.png",
                            "./screenshots-images-2/chapter_2/section_1/b857b3b7-6bda-4e6e-811f-17a0f486b5bd.png",
                            "./screenshots-images-2/chapter_2/section_1/d18cf6cc-f81d-4434-853b-30a1a7aeb1d2.png",
                            "./screenshots-images-2/chapter_2/section_1/7a87f5d2-0acb-4534-a25f-735a8132a99e.png",
                            "./screenshots-images-2/chapter_2/section_1/c9515d1e-187c-47f7-9262-6a08ff7fa083.png",
                            "./screenshots-images-2/chapter_2/section_1/2d3f051f-0387-4f1e-bb5e-b1b4a153a5a8.png",
                            "./screenshots-images-2/chapter_2/section_1/3c74022d-f2dc-499a-ae1c-874edf95854c.png",
                            "./screenshots-images-2/chapter_2/section_1/d9ec07c6-4c36-4846-9a49-9359b5faef78.png",
                            "./screenshots-images-2/chapter_2/section_1/eea8afba-ce87-4c31-903e-07a52de4e5b2.png",
                            "./screenshots-images-2/chapter_2/section_1/b938f84f-711e-4d1e-9ffc-77cd88b2dc3c.png",
                            "./screenshots-images-2/chapter_2/section_1/27132325-3476-4468-af78-4884122edd32.png",
                            "./screenshots-images-2/chapter_2/section_1/e3850447-be6c-4d6a-822c-01b25ebd63fd.png",
                            "./screenshots-images-2/chapter_2/section_1/16272416-9435-438a-99e2-7f72638a60ef.png",
                            "./screenshots-images-2/chapter_2/section_1/5b9b5e4e-841a-41d1-8022-6b9562b3e8e5.png",
                            "./screenshots-images-2/chapter_2/section_1/c889ac19-1329-4419-bde8-db7bb7d2be1b.png",
                            "./screenshots-images-2/chapter_2/section_1/18f8ef7d-c82c-4452-960e-e791bf6826ae.png",
                            "./screenshots-images-2/chapter_2/section_1/f9d989e5-f2a3-4c6a-9cc0-eb246e5cb9dc.png",
                            "./screenshots-images-2/chapter_2/section_1/7c4b7e3f-694c-4921-adfb-8762e719ba2f.png",
                            "./screenshots-images-2/chapter_2/section_1/e8aad866-e99c-45c1-bc11-672155af4197.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A NOTE FOR EARLY RELEASE READERS\n\nWith Early Release ebooks, you get books in their earliest form\u2014the author's raw and unedited content as\nthey write\u2014so you can take advantage of these technologies long before the official release of these titles.\n\nThis will be the 3rd chapter of the final book. Please note that the GitHub repo will be made active later on.\n\nIf you have comments about how we might improve the content and/or examples in this book, or if you\nnotice missing material within this chapter, please reach out to the author at chip@huyenchip.com.\n\nIn chapter 2, we covered how to handle data from the systems perspective. In this chapter, we'll go over how to\nhandle data from the data science perspective. Despite the importance of training data in developing and\nimproving ML models, ML curricula are heavily skewed towards modeling, which is considered by many\nresearchers and engineers as the \u201cfun\u201d part of the process. Building a state-of-the-art model is interesting.\nSpending days wrangling with a massive amount of malformatted data that doesn\u2019t even fit into your machine\u2019s\nmemory is frustrating.\n\nData is messy, complex, unpredictable, and potentially treacherous. If in school, training data is a cute little\npuppy then, in production, it\u2019s a Kraken that, if not tamed, can easily sink your entire ML operation. But this is\nprecisely the reason why ML engineers should learn how to handle data well, saving us time and headache\ndown the road.\n\nIn this chapter, we will go over techniques to obtain or create good training data. Training data, in this chapter,\nencompasses all the data used in the developing phase of ML models, including the different splits used for\ntraining, validation, and testing (the train, validation, test splits). This chapter starts with different sampling\ntechniques to select data for training. We'll then address common challenges in creating training data including\nthe label multiplicity problem, the lack of labels problem, the class imbalance problem, and techniques in data\naugmentation to address the lack of data problem.\n\nWe use the term \u201ctraining data\u201d instead of \u201ctraining dataset\u201d, because \u201cdataset\u201d denotes a set that is finite and\nstationary. Data in production is neither finite nor stationary, a phenomenon that we will cover in Chapter 7.\nLike other steps in building ML systems, creating training data is an iterative process. As your model evolves\nthrough a project lifecycle, your training data will likely also evolve.\n\nBefore we move forward, I just want to echo a word of caution that has been said many times yet is still not\nenough. Data is full of potential biases. These biases have many possible causes. There are biases caused during\ncollecting, sampling, or labeling. Historical data might be embedded with human biases and ML models,\ntrained on this data, can perpetuate them. Use data but don\u2019t trust it too much!\n\nSampling\n\nSampling is an integral part of the ML workflow that is, unfortunately, often overlooked in typical ML\ncoursework. Sampling happens in many steps of an ML project lifecycle, such as sampling from all possible\nreal-world data to create training data, sampling from a given dataset to create splits for training, validation, and\ntesting, or sampling from all possible events that happen within your ML system for monitoring purposes. In\nthis section, we'll focus on sampling methods for creating training data, but these sampling methods can also be\nused for other steps in an ML project lifecycle.\n\nIn many cases, sampling is necessary. One example is when you don\u2019t have access to all possible data in the\nreal world, the data that you use to train a model are subsets of real-world data, created by one sampling\nmethod or another. Another example is when it\u2019s infeasible to process all the data that you have access to \u2014\nbecause it requires either too much time or too much compute power or too much money \u2014 you have to sample\nthat data to create a subset that you can process. In many other cases, sampling is helpful as it allows you to\naccomplish a task faster and cheaper. For example, when considering a new model, you might want to do a\nquick experiment with a small subset of your data to see if it's promising first before running this new model on\nall the data you have\u2019.\n\nUnderstanding different sampling methods and how they are being used in our workflow can, first, help us\navoid potential sampling biases, and second, help us choose the methods that improve the efficiency of the data\nwe sample.\n\nThere are two families of sampling: non-probability sampling and random sampling. We will start with non-\nprobability sampling methods, followed by several common random methods. We'll analyze the pros and cons\nof each method.\n\nNon-Probability Sampling\nNon-probability sampling is when the selection of data isn\u2019t based on any probability criteria. Here are some of\nthe criteria for non-probability sampling.\n\n+ Convenience sampling: samples of data are selected based on their availability. This sampling method\nis popular because, well, it's convenient.\n\nSnowball sampling: future samples are selected based on existing samples. For example, to scrape\nlegitimate Twitter accounts without having access to Twitter databases, you start with a small number\nof accounts then you scrape all the accounts in their following, and so on.\n\n+ Judgment sampling: experts decide what samples to include.\n\n* Quota sampling: you select samples based on quotas for certain slices of data without any\nrandomization. For example, when doing a survey, you might want 100 responses from each of the age\ngroups: under 30 years old, between 30 and 60 years old, above 50 years old, regardless of the actual\nage distribution in the real world.\n\nThe samples selected by non-probability criteria are not representative of the real-world data, and therefore, are\nriddled with selection biases. Because of these biases, you might think that it\u2019s a bad idea to select data to train\nML models using this family of sampling methods. You're right. Unfortunately, in many cases, the selection of\ndata for ML models is still driven by convenience.\n\nOne example of these cases is language modeling. Language models are often trained not with data that is\nrepresentative of all possible texts but with data that can be easily collected \u2014 Wikipedia, CommonCrawl,\nReddit.\n\nAnother example is data for sentiment analysis of general text. Much of this data is collected from sources with\nnatural labels (ratings) such as IMDB reviews and Amazon reviews. These datasets are then used for other\nsentiment analysis tasks. IMDB reviews and Amazon reviews are biased towards users who are willing to leave\nreviews online, and not necessarily representative of people who don\u2019t have access to the Internet and aren\u2019t\nwilling to put reviews online.\n\nThe third example is data for training self-driving cars. Initially, data collected for self-driving cars came\nlargely from two areas: Phoenix in Arizona (because of its lax regulations) and the Bay Area in California\n(because many companies that build self-driving cars are located here). Both areas have generally sunny\n\nweather. In 2016, Waymo expanded its operations to Kirkland, WA specially for Kirkland\u2019s rainy weather, but\nthere\u2019s still a lot more self-driving car data for sunny weather than for rainy or snowy weather.\n\nNon-probability sampling can be a quick and easy way to gather your initial data to get your project off the\nground. However, for reliable models, you might want to use probability-based sampling, which we will cover\nnext.\n\nSimple Random Sampling\n\nIn the simplest form of random sampling, you give all samples in the population equal probabilities of being\nselected. For example, you randomly select 10% of all samples, giving all samples an equal 10% chance of\nbeing selected.\n\nThe advantage of this method is that it\u2019s easy to implement. The drawback is that rare categories of data might\nnot appear in your selection. Consider the case where a class appears only in 0.01% of your data population. If\nyou randomly select 1% of your data, samples of this rare class will unlikely be selected. Models trained on this\nselection might think that this rare class doesn\u2019t exist.\n\nStratified Sampling\n\nTo avoid the drawback of simple random sampling listed above, you can first divide your population into the\ngroups that you care about and sample from each group separately. For example, to sample 1% of data that has\ntwo classes A and B, you can sample 1% of class A and 1% of class B. This way, no matter how rare class A or\nB is, you'll ensure that samples from it will be included in the selection. Each group is called a strata, and this\nmethod is called stratified sampling.\n\nOne drawback of this sampling method is that it isn't always possible, such as when it\u2019s impossible to divide all\nsamples into groups. This is especially challenging when one sample might belong to multiple groups as in the\ncase of multilabel tasks\u00ae. For instance, a sample can be both class A and class B.\n\nWeighted Sampling\n\nIn weighted sampling, each sample is given a weight, which determines the probability of it being selected. For\nexample, if you have three samples A, B, C and want them to be selected with the probabilities of 50%, 30%,\n20% respectively, you can give them the weights 0.5, 0.3, 0.2.\n\nThis method allows you to leverage domain expertise. For example, if you know that a certain subpopulation of\ndata, such as more recent data, is more valuable to your model and want it to have a higher chance of being\nselected, you can give it a higher weight.\n\nThis also helps with the case when the data you have comes from a different distribution compared to the true\ndata. For example, if in your data, red samples account for 25% and blue samples account for 75%, but you\nknow that in the real world, red and blue have equal probability to happen, you can give red samples the\nweights three times higher than blue samples.\n\nIn Python, you can do weighted sampling with random.choices as follows:\n\n# Choose two items from the list such that 1, 2, 3, 4 each has\n# 20% chance of being selected, while 100 and 1000 each have only 10% chance.\nrandom.choices(population=[1, 2, 3, 4, 100, 1000],\n\nweights=[0.2, 0.2, 0.2, 0.2, 0.1, 0.1],\n\nke2)\n\n# This is equivalent to the following\n\nes (population=[1, 1, 2, 2, 3, 3, 4, 4, 100, 1000],\nke2)\n\nrandom. cho.\n\nA common concept in ML that is closely related to weighted sampling is sample weights. Weighted sampling is\nused to select samples to train your model with, whereas sample weights are used to assign \u201cweights\u201d or\n\n\u201cimportance\u201d to training samples. Samples with higher weights affect the loss function more. Changing sample\nweights can change your model\u2019s decision boundaries significantly, as shown in Figure 3-1.\n\nModified weights\n\nFigure 3-1. How sample weights can affect the decision boundary. On the left is when all samples are given equal weights. On the right is when\nsamples are given different weights. Source: SVM: Weighted samples (sklearn).\n\nImportance Sampling\n\nImportance sampling is one of the most important sampling methods, not just in ML. It allows us to sample\nfrom a distribution when we only have access to another distribution.\n\nImagine you have to sample z from a distribution P(x), but P(x) is really expensive, slow, or infeasible to\nsample from. However, you have a distribution Q(z) that is a lot easier to sample from. So you sample z from\nQ(z) instead and weight this sample by Br az) is called the proposal distribution or the importance\ndistribution. (Q(z) can be any distribution as long as Q(a) > 0 whenever P(x) # 0. The equation below shows\n\nthat in expectation, x sampled from P(X) is equal to z sampled from Q(z) weighted by 52).\n\nEp [2] =D, P(2)2 = 0. Q (2)2. Get = Bar [eget |\n\nOne example where importance sampling is used in ML is policy-based reinforcement learning. Consider the\ncase when you want to update your policy. You want to estimate the value functions of the new policy, but\ncalculating the total rewards of taking an action can be costly because it requires considering all possible\noutcomes until the end of the time horizon after that action. However, if the new policy is relatively close to the\nold policy, you can calculate the total rewards based on the old policy instead and reweight them according to\nthe new policy. The rewards from the old policy make up the proposal distribution.\n\nReservoir Sampling\n\nReservoir sampling is a fascinating algorithm that is especially useful when you have to deal with continually\nincoming data, which is usually what you have in production.\n\nImagine you have an incoming stream of tweets and you want to sample a certain number, k, of tweets to do\nanalysis or train a model on. You don\u2019t know how many tweets there are but you know you can\u2019t fit them all in\nmemory, which means you don\u2019t know the probability at which a tweet should be selected. You want to ensure\nthat:\n\n1. Every tweet has an equal probability of being selected and,\n2. You can stop the algorithm at any time and the tweets are sampled with the correct probability.\n\nOne solution for this problem is reservoir sampling. The algorithm involves a reservoir, which can be an array,\nand consists of three steps:\n\n1. Put the first k elements into the reservoir.\n2. For each incoming nth element, generate a random number i such that 1 <i <n.\n3. If 1 <i<k: replace the ith element in the reservoir with the nth element. Else, do nothing.\n\nThis means that each incoming i** element has \u201c probability of being in the reservoir. You can also prove that\neach element in the reservoir has + probability of being there. This means that all samples have an equal\nchance of being selected. If we stop the algorithm at any time, all samples in the reservoir have been sampled\nwith the correct probability. Figure 3-2 shows an illustrative example of how reservoir sampling works.\n\nIncoming Sanales\n\ni m0 Oo e.88.8\n\n| \u2018 \\\n\n]\n\n]\n\n]\n\n| selected = 4/5 Reservoir\n| | aq\n\n|\n\n| olselected) = 4/6\n\n|\nolselecte) = 4/n\n\nFigure 3-2. A visualization of how reservoir sampling works.\n\nLabeling\n\nDespite the promise of unsupervised ML, most ML models in production today are supervised, which means\nthat they need labels to learn. The performance of an ML model still depends heavily on the quality and\nquantity of the labeled data it\u2019s trained on.\n\nThere are tasks where data has natural labels or it\u2019s possible to collect natural labels on the fly. For example, for\npredicting the click-through rate on an ad, labels are whether users click on an ad or not. Similarly, for\nrecommendation systems, labels are whether users click on a recommended item or not. However, for most\ntasks, natural labels are not available or not accessible, and you will need to obtain labels by other means.\n\nIna talk to my students, Andrej Karpathy, Director of AI at Tesla, shared an anecdote about when he decided to\nhave an in-house labeling team, his recruiter asked how long he'd need this team for. He responded: \u201cHow long\n\ndo we need an engineering team for?\u201d Data labeling has gone from being an auxiliary task to being a core\nfunction of many ML teams in production.\n\nIn this section, we will discuss the challenges of obtaining labels for your data including the label multiplicity\nproblem and what to do when you lack hand labeled data.\n\nHand Labels\n\nAnyone who has ever had to work with data in production has probably felt this at a visceral level: acquiring\nhand labels for your data is difficult for many, many reasons. First, hand-labeling data can be expensive,\nespecially if subject matter expertise is required. To classify whether a comment is spam, you might be able\nto find 200 annotators on a crowdsourcing platform and train them in 15 minutes to label your data. However, if\nyou want to label chest X-rays, you'd need to find board-certified radiologists, whose time is limited and\nexpensive.\n\nSecond, hand labeling poses a threat to data privacy. Hand labeling means that someone has to look at your\ndata, which isn\u2019t always possible if your data has strict privacy requirements. For example, you can\u2019t just ship\nyour patient's medical records or your company\u2019s confidential financial information to a third-party service for\nlabeling. In many cases, your data might not even be allowed to leave your organization, and you might have to\nhire or contract annotators to label your data on-premise.\n\nThird, hand labeling is slow. For example, accurate transcription of speech utterance at phonetic level can take\n400 times longer than the utterance duration. So if you want to annotate | hour of speech, it'll take 400 hours or\nalmost 3 working months to do so. In a study to use ML to help classify lung cancers from X-rays, my\ncolleagues had to wait almost a year to obtain sufficient labels.\n\nSlow labeling leads to slow iteration speed and makes your model less adaptive to changing environments and\nrequirements. If the task changes or data changes, you'll have to wait for your data to be relabeled before\nupdating your model. Imagine the scenario when you have a sentiment analysis model to analyze the sentiment\nof every tweet that mentions your brand. It has only two classes: NEGATIVE and POSITIVE. However, after\ndeployment, your PR team realizes that the most damage comes from angry tweets and they want to attend to\nangry messages faster. So you have to update your sentiment analysis model to have three classes:\nNEGATIVE, POSITIVE, and ANGRY. To do so, you will need to look at your data again to see which\nexisting training examples should be relabeled ANGRY. If you don\u2019t have enough ANGRY examples, you will\nhave to collect more data. The longer the process takes, the more your existing model performance will\ndegrade.\n\nLabel Multiplicity\n\nOften, to obtain enough labeled data, companies have to use data from multiple sources and rely on multiple\nannotators who have different levels of expertise. These different data sources and annotators also have\ndifferent levels of accuracy. This leads to the problem of label ambiguity or label multiplicity: what to do when\nthere are multiple possible labels for a data instance.\n\nConsider this simple task of entity recognition. You give three annotators the following sample and ask them to\nannotate all entities they can find.\n\nDarth Sidious, known simply as the Emperor , was a Dark Lord of the Sith who reigned over the galaxy\nas Galactic Emperor of the First Galactic Empire.\n\nYou receive back three different solutions, as shown in Table 3-1. Three annotators have identified different\n\nentities. Which one should your model train on? A model trained on data labeled mostly by annotator | will\nperform very differently from a model trained on data labeled mostly by annotator 2.\n\nAnnotator # entities Annotation\n\n1 3 (Darth Sidious), known simply as the Emperor, was 2 [Dark Lord of the Sith] who reigned over the\ngalaxy as [Galactic Emperor of the First Galactic Empire]\n\n2 6 [Darth Sidious), known simply as the [Emperor], was a [Dark Lord] of the [Sith] who reigned over the\ngalaxy as [Galactic Emperor] of the (First Galactic Empire).\n\n3 4 [Darth Sidious], known simply as the [Emperor], was a [Dark Lord of the Sith] who reigned over the\ngalaxy as [Galactic Emperor of the First Galactic Empire).\n\nDisagreements among annotators are extremely common. The higher level of domain expertise required, the\nhigher the potential for annotating disagreement\u2019. If one human-expert thinks the label should be A while\nanother believes it should be B, how do we resolve this conflict to obtain one single ground truth? If human\nexperts can\u2019t agree on a label, what does human-level performance even mean?\n\nTo minimize the disagreement among annotators, it\u2019s important to, first, have a clear problem definition. For\nexample, in the entity recognition task above, some disagreements could have been eliminated if we clarify that\nin case of multiple possible entities, pick the entity that comprises the longest substring. This means Galactic\n\nEmperor of the First Galactic Empire instead of Galactic Emperor and First Galactic Empire. Second,\nyou need to incorporate that definition into training to make sure that all annotators understand the rules.\n\nData Lineage\n\nIndiscriminately using data from multiple sources, generated with different annotators, without examining their\nquality can cause your model to fail mysteriously. Consider a case when you've trained a moderately good\nmodel with 100K data samples. Your ML engineers are confident that more data will improve the model\nperformance, so you spend a lot of money to hire annotators to label another million data samples.\n\nHowever, the model performance actually decreases after being trained on the new data. The reason is that the\nnew million samples were crowdsourced to annotators who labeled data with much less accuracy than the\noriginal data. It can be especially difficult to remedy this if you've already mixed your data and can\u2019t\ndifferentiate new data from old data.\n\nOn top of that, it\u2019s good practice to keep track of the origin of each of our data samples as well as its labels, a\ntechnique known as data lineage. Data lineage helps us both flag potential biases in our data as well as debug\nour models. For example, if our model fails mostly on the recently acquired data samples, you might want to\nlook into how the new data was acquired. On more than one occasion, we've discovered that the problem\nwasn\u2019t with our model, but because of the unusually high number of wrong labels in the data that we'd acquired\nrecently.\n\nHandling the Lack of Hand Labels\n\nBecause of the challenges in acquiring sufficient high-quality labels, many techniques have been developed to\naddress the problems that result. In this section, we will cover four of them: weak supervision, semi-\nsupervision, transfer learning, and active learning.\n\nMethod How Ground truths required?\n\nWeak supervision Leverages (often noisy) heuristics to generate No, but a small number of labels are recommended to guide the development\nlabels of heuristics\n\nSemi- Leverages structural assumptions to generate labels Yes. A small number of initial labels as seeds to generate more labels\nsupervision\n\n\u2018Transfer learning Leverages models pretrained on _No for zero-shot learning\nanother task for your new task Yes for fine-tuning, though the number of ground truths required is often much smaller\nthan what would be needed if you train the model fom scratch.\n\nActive leaning Labels data samples that are most useful to your model Yes\n\nWeak supervision\n\nIf hand labeling is so problematic, what if we don\u2019t use hand labels altogether? One approach that has gained\npopularity is weak supervision. One of the most popular open-source tools for weak supervision is Snorkel,\ndeveloped at the Stanford AI Lab\u00ae. The insight behind weak supervision is that people rely on heuristics, which\ncan be developed with subject matter expertise, to label data. For example, a doctor might use the following\nheuristics to decide whether a patient's case should be prioritized as emergent.\n\nIf the nurse's note mentions a serious condition like pneumonia, the patient's case should be given priority\nconsideration.\n\nLibraries like Snorkel are built around the concept of a labeling function (LF): a function that encodes\nheuristics. The above heuristics can be expressed by the following function.\ndef labeling function (note):\n\nLFs can encode many different types of heuristics. Here are some of the heuristics.\n* Keyword heuristic, such as the example above.\n+ Regular expressions, such as if the note matches or fails to match a certain regular expression.\n* Database lookup, such as if the note contains the disease listed in the dangerous disease list.\n+ The outputs of other models, such as if an existing system classifies this as EMERGENT.\nAfter you've written LFs, you can apply them to the samples you want to label.\n\nBecause LFs encode heuristics, and heuristics are noisy, LFs are noisy. Multiple labeling functions might apply\nto the same data examples, and they might give conflicting labels. One function might think a note is\nEMERGENT but another function might think it\u2019s not. One heuristic might be much more accurate than\nanother heuristic, which you might not know because you don\u2019t have ground truth labels to compare them to.\nIt\u2019s important to combine, denoise, and reweight all LFs to get a set of most likely-to-be-correct labels.\n\n[Intuition]\nLook at agreements & disagreements\n\n\u2014\u2014 ip. B\u00ae.\n\ngv (Np = Ip! +22\"\n- (1) Provably consistent matrix completion-style\nalgorithm over inverse covariance\n\nFigure 3-3. A high level overview of how labeling functions are combined. Image by Ratner et al.\n\nIn theory, you don\u2019t need any hand labels for weak supervision. However, to get a sense of how accurate your\nLFs are, a small amount of hand labels is recommended. These hand labels can help you discover patterns in\nyour data to write better LFs.\n\nWeak supervision can be especially useful when your data has strict privacy requirements. You only need to see\na small, cleared subset of data to write LFs, which can be applied to the rest of your data without anyone\nlooking at it.\n\nWith LFs, subject matter expertise can be versioned, reused, and shared. Expertise owned by one team can be\nencoded and used by another team. If your data changes or your requirements change, you can just reapply LFs\n\nto your data samples. The approach of using labeling functions to generate labels for your data is also known as\nprogrammatic labeling. Table 3-3 shows some of the advantages of programmatic labeling over hand labeling.\n\nHand labeling Programmatic labeling\nExpensive: Especially when subject matter expertise required Cost saving: Expertise can be versioned, shared, and reused across an organization\n\n\u2018Non-private: Need to ship data tohuman Privacy: Create LFs using a cleared dats subsample then apply LPs to other data without looking at\nannotators individual samples.\n\nSlow: Time required scales linearly with # labels needed Fast: Easily scale from 1K to 1M samples\n\nNon-adaptive: Every change requires re-labeling the data Adaptive: When changes happen, just reapply LFs!\n\nHere is a case study to show how well weak supervision works in practice. In a study with Stanford Medicine\u00ae,\nmodels trained with weakly-supervised labels obtained by a single radiologist after 8 hours of writing labeling\nfunctions had comparable performance with models trained on data obtained through almost a year of hand\nlabeling. There are two interesting facts about the results of the experiment. First, the models continued\nimproving with more unlabeled data even without more labeling functions. Second, labeling functions were\nbeing reused across tasks. The researchers were able to reuse 6 labeling functions between the CXR (Chest X-\nRays) task and EXR (Extremity X-Rays) task.\n\n10\n\n=>\nis\n\n\u2014\nco\n\nMean ROC-AUC\n\u2014\na\n\nMean ROC-AUC\n=>\na\n\n\u2014 \u2014 0\n\nCAR, 20LFs = FS EXR 8LFs 9\n\n0) 0.4\n2000 10000 50000 3000 10000 30000\n\nNumber of Datapoints Number of Datapoints\n\nFigure 3-4. Comparison of the performance of a model trained on fully supervised labels (FS) and a model trained with programmatic labels (DP)\non CXR and EXR tasks. Image by Dunamon et al\n\n\u2014)\nwo\n\nMy students often ask that if heuristics work so well to label data, why do we need machine learning models?\nOne reason is that your labeling functions might not cover all your data samples, so you need to train ML\nmodels to generalize to samples that aren't covered by any labeling function.\n\nWeak supervision is a simple but powerful paradigm. However, it\u2019s not perfect. In some cases, the labels\nobtained by weak supervision might be too noisy to be useful. But it\u2019s often a good method to get you started\nwhen you want to explore the effectiveness of ML without wanting to invest too much in hand labeling upfront.\n\nSemi-supervision\n\nIf weak supervision leverages heuristics to obtain noisy labels, semi-supervision leverages structural\nassumptions to generate new labels based on a small set of initial labels. Unlike weak supervision, semi-\nsupervision requires an initial set of labels.\n\nSemi-supervised learning is a technique that was used back in the 90s\u2019, and since then, many semi-supervision\nmethods have been developed. A comprehensive review of semi-supervised learning is out of the scope of this\nbook. We'll go over a small subset of these methods to give readers a sense of how they are used. For a\ncomprehensive review, I recommend Semi-Supervised Learning Literature Survey (Xiaojin Zhu, 2008) and A\nsurvey on semi-supervised learning (Engelen and Hoos, 2018).\n\nA classic semi-supervision method is self-training. You start by training a model on your existing set of\nlabeled data, and use this model to make predictions for unlabeled samples. Assuming that predictions with\nhigh raw probability scores are correct, you add the labels predicted with high probability to your training set,\nand train a new model on this expanded training set. This goes on until you're happy with your model\nperformance.\n\nAnother semi-supervision method assumes that data samples that share similar characteristics share the same\nlabels. The similarity might be obvious, such as in the task of classifying the topic of Twitter hashtags as\nfollows. You can start by labeling the hashtag \u201c#AI\u201d as Computer Science. Assuming that hashtags that appear\nin the same tweet or profile are likely about the same topic, given the profile of MIT CSAIL below, you can\nalso label the hashtags \u201c#ML\u201d and \u201c#BigData\u201d as Computer Science.\n\nMIT CSAIL @\n\nMIT's largest research lab, the Computer Science & Artificial\nIntelligence Lab.\n\nFigure 3-5, Because #ML and #BigData appears in the same Twitter profile as #Al, we can assume that they belong to the same topic\n\nIn most cases, the similarity can only be discovered by more complex methods. For example, you might need to\nuse a clustering method or a K-nearest neighbor method to discover samples that belong to the same cluster.\n\nA semi-supervision method that has gained popularity in recent years is the perturbation-based method. It\u2019s\nbased on the assumption that small perturbations to a sample shouldn't change its label. So you apply small\nperturbations to your training samples to obtain new training samples. The perturbations might be applied\ndirectly to the samples (e.g. adding white noise to images) or to their representations (e.g. adding small values\nto embeddings of words). The perturbed samples have the same labels as the unperturbed samples. We'll\ndiscuss more about this in the section Perturbation later in this chapter under Augmentation.\n\nIn some cases, semi-supervision approaches have reached the performance of purely supervised learning, even\nwhen a substantial portion of the labels in a given dataset has been discarded\u00ae. Semi-supervision is the most\nuseful when the number of training labels is limited. One thing to consider when doing semi-supervision is how\nmuch of this limited amount should be used for evaluation. If you evaluate multiple model candidates on the\nsame test set and choose the one that performs best on the test set, you might have chosen a model that overfits\nthe most on the test set. On the other hand, if you choose models based on a validation set, the value gained by\nhaving a validation set might be less than the value gained by adding the validation set to the limited training\nset.\n\nTransfer learning\n\nTransfer learning refers to the family of methods where a model developed for a task is reused as the starting\npoint for a model on a second task. First, the base model is trained for a base task such as language modeling.\nThe base task is usually a task that has cheap and abundant training data. Language modeling is a great\ncandidate because it doesn\u2019t require labeled data. You can collect any body of text \u2014 books, Wikipedia articles,\nchat histories \u2014 and the task is: given a sequence of tokens, predict the next token. When given a sequence \u201cI\nbought NVIDIA shares because I believe in the importance of\u201d, a language model might output \u201chardware\u201d as\nthe next token.\n\nYou then use this pretrained base model on the task that you\u2019re interested in, such as sentiment analysis, intent\ndetection, question answering, etc. This task is called a downstream task. In some cases, such as in zero-shot\nlearning scenarios, you might be able to use the base model on a downstream task directly. In many cases, you\nmight need to fine-tune the base model. Fine-tuning means making small changes to the base model, which can\n\nbe continuing training the entire base model or a subset of the base model on data from a given downstream\ntask\",\n\nSometimes, you might need to modify the inputs using a template that can prompt the base model to generate\nthe outputs that you want\", For example, to use a language model as the base model for a question answering\ntask, you might want to use the following prompt.\n\nQ: When was the United States founded?\n\nA: July 4, 1776.\n\nQ: Who wrote the Declaration of Independence?\nA: Thomas Jefferson.\n\nQ: What year was Alexander Hamilton born?\nAs\n\nWhen you input this prompt into a language model such as GPT-3, it might output the year Alexander Hamilton\nwas born.\n\nTransfer learning is especially appealing for tasks that don\u2019t have a lot of labeled data. Even for tasks that have\na lot of labeled data, using a pretrained model as the starting point can often boost the performance significantly\ncompared to training from scratch.\n\nTransfer learning has gained a lot of interest in recent years for the right reasons. It has enabled many\napplications that were previously impossible due to the lack of training samples. A non-trivial portion of ML\nmodels in production today are the results of transfer learning, including object detection models that leverage\nmodels pretrained on ImageNet and text classification models that leverage pretrained language models such as\nBERT\" or GPT-3\"\u00b0. It also lowers the entry barriers into ML, as it helps reduce the upfront cost needed for\nlabeling data to build ML applications.\n\nA trend that has emerged in the last five years is that usually, the larger the pretrained base model, the better its\nperformance on downstream tasks. Large models are expensive to train. Based on the configuration of GPT-3,\nit's estimated that the cost of training this model is in the tens of millions USD. Many have hypothesized that in\nthe future, only a handful of companies can afford to train large pretrained models. The rest of the industry will\nuse these pretrained models directly or finetune them for their specific needs.\n\nActive learning\n\nActive learning is a method for improving the efficiency of data labels. The hope here is that ML models can\nachieve greater accuracy with fewer training labels if they can choose which data samples to learn from. Active\nlearning is sometimes called query learning \u2014 though this term is getting increasingly unpopular \u2014 because a\nmodel (active learner) sends back queries in the form of unlabeled samples to be labeled by annotators (usually\nhumans).\n\nInstead of randomly labeling data samples, you label the samples that are most helpful to your models\naccording to some heuristics. The most straightforward heuristic is uncertainty measurement \u2014 label the\nexamples that your model is the least certain about hoping that they will help your model learn the decision\nboundary better. For example, in the case of classification problems where your model outputs raw probabilities\nfor different classes, it might choose the data examples with the lowest probabilities for the predicted class.\nFigure 3-6 illustrates how well this method works on a toy example.\n\n(i) ) )\n\nFigure 3-6. How uncertainty-based active learning works. (a) A toy data set of 400 instances, evenly sampled from two class Gaussians. (b) A model\ntrained on 30 examples randomly labeled gives an accuracy of 70%. (c} A model trained on 30 examples chosen by active learning gives an\naccuracy of 90%. Image by Burr Setties.\n\nAnother common heuristic is based on disagreement among multiple candidate models. This method is called\nquery-by-committee. You need a committee of several candidate models, which are usually the same model\ntrained with different sets of hyperparameters. Each model can make one vote for which examples to label next,\nwhich it might vote based on how uncertain it is about the prediction. You then label the examples that the\ncommittee disagrees on the most.\n\nThere are other heuristics such as choosing examples that, if trained on them, will give the highest gradient\nupdates, or will reduce the loss the most. For a comprehensive review of active learning methods, check out\nActive Learning Literature Survey (Burr Settles, 2010).\n\nThe examples to be labeled can come from different data regimes. They can be synthesized where your model\ngenerates examples in the region of the input space that it\u2019s most uncertain about'*. They can come from a\nstationary distribution where you've already collected a lot of unlabeled data and your model chooses examples\nfrom this pool to label. They can come from the real-world distribution where you have a stream of data\ncoming in, as in production, and your model chooses examples from this stream of data to label.\n\nI'm the most excited about active learning when a system works with real-time data. Data changes all the time,\na phenomenon we briefly touched on in Chapter | and will go more in detail in Chapter 7. Active learning in\nthis data regime will allow your model to learn more effectively in real-time and adapt faster to changing\nenvironments.\n\nClass Imbalance\n\nClass imbalance typically refers to a problem in classification tasks where there is a substantial difference in the\nnumber of samples in each class of the training data. For example, in a training dataset for the task of detecting\n\nlung cancer from X-Ray images, 99.99% of the X-Rays might be of normal lungs, and only 0.01% might\ncontain cancerous cells.\n\nClass imbalance can also happen with regression tasks where the labels are continuous. Consider the task of\nestimating healthcare bills\"\u00ae. Healthcare cost is very high skewed \u2014 the median bill is low, but the 95th\npercentile bill is astronomical. When predicting hospital bills, it might be more important to predict accurately\nthe bills at the 95th percentile than the median bills. A 100% difference in a $250 bill is acceptable (actual\n$500, predicted $250), but a 100% difference on a $10k bill is not (actual $20k, predicted $10k). Therefore, we\nmight have to train the model to be better at predicting 95th percentile bills, even if it reduces the overall\nmetrics.\n\nChallenges of Class Imbalance\n\nML works well in situations when the data distribution is more balanced, and not so well when the classes are\nheavily imbalanced, as illustrated in Figure 3-7. Class imbalance can make learning difficult for the three\nfollowing reasons.\n\nSmall data and rare occurrences\n\nML works well when Not so well when it\nthe data distribution is this:\nis this:\n\n1000 1500\nwon\nbl\n\u00b0 e\ns s\ni E\nos 8\n8 \u00b0\nj j\n3 3\n2 2\nz z\nPi\n\u2018 i\nCat Dog Chair Bike Person ffs Atectass ass CosidetonHeria\n\nFigure 3-7. ML works well in situations where the classes are balanced, Image by Andrew Ng.\n\nThe first reason is that class imbalance often means that there\u2019s insufficient signal for your model to learn to\ndetect the minority classes. In the case where there is a small number of instances in the minority class, the\nproblem becomes a few-shot learning problem where your model only gets to see the minority class a few times\nbefore having to make a decision on it. In the case where there is no instance of the rare classes in your training\nset, your model might assume that these rare classes don\u2019t exist.\n\nThe second reason is that class imbalance makes it easier for your model to get stuck in a non-optimal\nsolution by learning a simple heuristic instead of learning anything useful about the underlying structure of the\ndata. Consider the lung cancer detection example above. If your model learns to always output the majority\nclass, its accuracy is already 99.99%. This heuristic can be very hard for gradient-descent algorithms to beat\nbecause a small amount of randomness added to this heuristic might lead to worse accuracy.\n\nThe third reason is that class imbalance leads to asymmetric costs of error \u2014 the cost of a wrong prediction\non an example of the rare class might be much higher than a wrong prediction on an example of the majority\n\nclass.\n\nFor example, misclassification on an X-Ray with cancerous cells is much more dangerous than\nmisclassification on an X-Ray of a normal lung. If your loss function isn\u2019t configured to address this\nasymmetry, your model will treat all examples the same way. As a result, you might obtain a model that\nperforms equally well on both majority and minority classes, while you much prefer a model that performs less\nwell on the majority class but much better on the minority one.\n\nWhen I was in school, most datasets I was given had more or less balanced classes *\u00ae. It was a shock for me to\nstart working and realize that class imbalance is the norm. In real-world settings, rare events are often more\ninteresting (or more dangerous) than regular events, and many tasks focus on detecting those rare events.\n\nThe classical example of tasks with class imbalance is fraud detection. Most credit card transactions are not\nfraudulent. As of 2018, 6.8\u00a2 for every $100 in cardholder spending is fraudulent. Another is churn prediction.\nThe majority of your customers are not planning on cancelling their subscription. If they are, your business has\nmore to worry about than churn prediction algorithms. Other examples include disease screening \u2014 most\npeople, fortunately, don\u2019t have terminal illness, and resume screening \u2014 98% of job seekers are eliminated at\nthe initial resume screening. A less obvious example of a task with class imbalance is object detection. Object\ndetection algorithms currently work by generating a large number of bounding boxes over an image then\npredicting which boxes are most likely to have objects in them. Most bounding boxes do not contain a relevant\nobject.\n\nOutside the cases where class imbalance is inherent in the problem, class imbalance can also be caused by\nbiases during the sampling process. Consider the case when you want to create training data to detect whether\nan email is spam or not. You decide to use all the anonymized emails from your company\u2019s email database.\nAccording to Talos Intelligence, as of May 2021, nearly 85% of all emails are spam. But most spam emails\nwere filtered out before they reached your company\u2019s database, so in your dataset, only a small percentage is\nspam.\n\nAnother cause for class imbalance, though less common, is due to labeling errors. Your annotators might have\nread the instructions wrong or followed the wrong instructions (thinking there are only two classes POSITIVE\nand NEGATIVE while there are actually three), or simply made errors. Whenever faced with the problem of\nclass imbalance, it\u2019s important to examine your data to understand the causes of it.\n\nHandling Class Imbalance\n\nBecause of its prevalence in real-world applications, class imbalance has been thoroughly studied over the last\ntwo decades'\u201d. Class imbalance affects tasks differently based on the level of imbalance. Some tasks are more\nsensitive to class imbalance than others. Japkowicz showed that sensitivity to imbalance increases with the\ncomplexity of the problem, and that non-complex, linearly separable problems are unaffected by all levels of\nclass imbalance'\u00ae. Class imbalance in binary classification problems is a much easier problem than class\nimbalance in multiclass classification problems. Ding et al. showed that very-deep neural networks \u2014 with\n\u201cvery deep\u201d meaning over 10 layers back in 2017 \u2014 performed much better on imbalanced data than shallower\nneural networks\"?\n\nThere have been many techniques suggested to mitigate the effect of class imbalance. However, as neural\nnetworks have grown to be much larger and much deeper, with more learning capacity, some might argue that\nyou shouldn't try to \u201cfix\u201d class imbalance if that\u2019s how the data looks in the real world. A good model should\nlearn to model that class imbalance. However, developing a model good enough for that can be challenging, so\nwe still have to rely on special training techniques.\n\nIn this section, we will cover three approaches to handle class imbalance: choosing the right metrics for your\nproblem, data-level methods, which means changing the data distribution to make it less imbalanced, and\n\nalgorithm-level methods, which means changing your learning method to make it more robust to class\nimbalance.\n\nThese techniques might be necessary but not sufficient. For a comprehensive survey, I recommend Survey on\ndeep learning with class imbalance (Johnson and Khoshgoftaar, Journal of Big Data 2019).\n\nUsing the right evaluation metrics\n\nThe most important thing to do when facing a task with class imbalance is to choose the appropriate evaluation\nmetrics. Wrong metrics will give you the wrong ideas of how your models are doing, and subsequently, won't\nbe able to help you develop or choose models good enough for your task.\n\nThe overall accuracy and error rate are the most frequently used metrics to report the performance of ML\nmodels. However, they are insufficient metrics for tasks with class imbalance because they treat all classes\nequally, which means the performance of your model on the majority class will dominate the accuracy. This is\nespecially bad when the majority class isn\u2019t what you care about.\n\nConsider a task with two labels: CANCER (positive) and NORMAL, where 90% of the labeled data is\nNORMAL. Consider two models A and B with the following confusion matrices.\n\nModel B Actual CANCER \u2014 Actual NORMAL\n\nPredicted CANCER 90 90\n\nPredicted NORMAL 10 810\n\nIf you're like most people, you'd probably prefer model B to make predictions for you since it has a better\nchance of telling you if you actually have cancer. However, they both have the same accuracy of 0.9.\n\nMetrics that help you understand your model\u2019s performance with respect to specific classes would be better\nchoices. Accuracy can still be a good metric if you use it for each class individually. The accuracy of Model A\non the CANCER is 10% and the accuracy of model B on the CANCER class is 90%.\n\nF1 and recall are metrics that measure your model\u2019s performance with respect to the positive class in binary\nclassification problems, as they rely on true positive \u2014 an outcome where the model correctly predicts the\npositive class\u00b0, F1 and recall are asymmetric metrics, which means that their values change depending on\nwhich class is considered the positive class. In our case, if we consider CANCER the positive class, model A\u2019s\nF1 is 0.17. However, if we consider NORMAL the positive class, model A\u2019s F1 is 0.95.\n\nIn multiclass classification problems, you can calculate F1 for each individual class.\n\nMany classification problems can be modeled as regression problems. Your model can output a value, and\nbased on that value, you classify the example. For example, if the value is greater than 0.5, it\u2019s a positive label,\nand if it\u2019s less than or equal to 0.5, it\u2019s a negative label. This means that you can tune the threshold to increase\nthe true positive rate (also known as recall) while decreasing the false positive rate (also known as the\nprobability of false alarm), and vice versa. We can plot the true positive rate against the false positive rate for\ndifferent thresholds. This plot is known as the ROC curve (Receiver Operating Characteristics). When your\nmodel is perfect, the recall is 1.0, and the curve is just a line at the top. This curve shows you how your model's\nperformance changes depending on the threshold, and helps you choose the threshold that works best for you.\nThe closer to the perfect line the better your model's performance.\n\nThe area under the curve (AUC) measures the area under the ROC curve. Since the closer to the perfect line the\nbetter, the larger this area the better.\n\nRecall\n\nPerfect\n\n1.0\n\n0.0\n\na\n\n0.0 10 False\n\nalann\nFigure 3-8. ROC curve\n\nLike F1 and recall, the ROC curve focuses only on the positive class and doesn't show how well your model\ndoes on the negative class. Davis and Goadrich suggested that we should plot precision against recall instead, in\n\nwhat they termed the Precision-Recall Curve. They argued that this curve s give a more informative picture of\nan algorithm's performance on tasks with heavy class imbalance2\".\n\nData-level methods: Resampling\n\nData-level methods modify the distribution of the training data to reduce the level of imbalance to make it\neasier for the model to learn. A common family of techniques is resampling. Resampling includes\noversampling, adding more examples from the minority classes and undersampling, removing examples of the\nmajority classes. The simplest way to undersample is to randomly remove instances from the majority class,\nwhile the simplest way to oversample is to randomly make copies of the minority class until you have a ratio\nthat you're happy with.\n\nUndersampling Oversampling\n\nCopies of tha\nmine class:\nSamples of\nmajority class\n= | ail\n\nOriginal dataset\n\nFigure 3-9. Hlustrations of how undersampling and oversampling works. Image by Rafael Alencar\n\nA popular method of undersampling low-dimensional data that was developed back in 1976 is Tomek links??,\nWith this technique, you find pairs of samples from opposite classes that are close in proximity, and remove the\nsample of the majority class in each pair.\n\nWhile this makes the decision boundary more clear and arguably helps models learn the boundary better, it may\nmake the model less robust by removing some of the subtleties of the true decision boundary.\n\nA popular method of oversampling low-dimensional data is SMOTE. It synthesizes novel samples of the\nminority class through sampling convex? combinations of existing data points within the minority class.\n\nBoth SMOTE and Tomek Links have only been proven effective in low-dimensional data. Many of the\nsophisticated resampling techniques, such as Near-Miss\u201d* and one-sided selection\u201d\u00b0, require calculating the\ndistance between instances or between instances and the decision boundaries, which can be expensive or\n\ninfeasible for high-dimensional data or in high-dimensional feature space, such as the case with large neural\n\nnetworks.\n\nWhen you resample your training data, never evaluate your model on resampled data, since it'll cause your\nmodel to overfit to that resampled distribution.\n\nUndersampling runs the risk of losing important data from removing data. Oversampling runs the risk of\noverfitting on training data, especially if the added copies of the minority class are replicas of existing data.\nMany sophisticated sampling techniques have been developed to mitigate these risks.\n\nOne such technique is two-phase learning\u201d\u00ae. You first train your model on the resampled data. This resampled\ndata can be achieved by randomly undersampling large classes until each class has only N instances. You then\nfinetune your model on the original data.\n\nAnother technique is dynamic sampling: oversample the low performing classes and undersample the high\nperforming classes during the training process. Introduced by Pouyanfar et al.27, the method aims to show the\nmodel less of what it has already learned and more of what it has not.\n\nAlgorithm-level methods\n\nIf data-level methods mitigate the challenge of class imbalance by altering the distribution of your training data,\nalgorithm-level methods keep the training data distribution intact but alter the algorithm to make it more robust\nto class imbalance.\n\nBecause the loss function (or the cost function) guides the learning process, many algorithm-level methods\ninvolve adjustment to the loss function. The key idea is that if there are two instances 2, and 2, and the loss\nresulting from making the wrong prediction on 2: higher than :r\u00bb, the model will prioritize making the correct\nprediction on z; over making the correct prediction on x2. By giving the training instances we care about higher\nweight, we can make the model focus more on learning these instances.\n\nLet 1,{z; 6) be the loss caused by the instance x for the model with the parameter set 6. The model's loss is\noften defined as the average loss caused by all instances.\n\nL(X;0) = SAL (2:8)\n\nThis loss function values the loss caused by all instances equally, even though wrong predictions on some\ninstances might be much costlier than wrong predictions on other instances. There are many ways to modify\nthis cost function. In this section, we will focus on three of them, starting with cost-sensitive learning.\n\nCost-sensitive learning\n\nBack in 2001, based on the insight that misclassification of different classes incur different cost, Elkan\nproposed cost-sensitive learning where the individual loss function is modified to take into account this varying\ncost\u201d\u00ae, The method started by using a cost matrix to specify C;,: the cost if class 7 is classified as class j. If\n\ni = j, it\u2019s a correct classification, and the cost is usually 0. If not, it\u2019s a misclassification. If classifying\nPOSITIVE examples as NEGATIVE is twice as costly as the other way around, you can make C}q twice as\nhigh as Ci).\n\nFor example, if you have two classes: POSITIVE and NEGATIVE, the cost matrix can look like this.\n\nActual NEGATIVE Actual POSITIVE\nPredicted NEGATIVE \u00a9 (0,0) = Cyo CUD) = Co\n\nPredicted POSITIVE \u00a9 (0,1) = Chy CUNY = Cy\n\nThe loss caused by instance x of class i will become the weighted average of all possible classifications of\ninstance x.\n\nL (x50) = Xj CP (as 8)\n\nThe problem with this loss function is that you have to manually define the cost matrix, which is different for\ndifferent tasks at different scales.\n\nClass-balanced loss\nWhat might happen with a model trained on an imbalanced dataset is that it\u2019ll bias toward majority classes and\nmake wrong predictions on minority classes. What if we punish the model for making wrong predictions on\nminority classes to correct this bias?\nIn its vanilla form, we can make the weight of each class inversely proportional to the number of samples in\nthat class, so that the rarer classes have higher weights. In the following equation, N denotes the total number of\ntraining samples.\n\nN\n\ny.\nnumber of samples of classi\n\nThe loss caused by instance x of class i will become as follows, with Loss(x, j) being the loss when x is\nclassified as class j. It can be cross entropy or any other loss function.\n\nL (2; 6) = W, )> P (jx; 6) Loss (2, 9)\ni\n\nA more sophisticated version of this loss can take in account the overlap among existing samples, such as\nClass-Balanced Loss Based on Effective Number of Samples (Cui et al., CVPR 2019).\n\nFocal loss\n\nIn our data, some examples are easier to classify than others, and our model might learn to classify them\nquickly. We want to incentivize our model to focus on learning the samples they still have difficulty classifying.\nWhat if we adjust the loss so that if a sample has a lower probability of being right, it'll have a higher weight?\nThis is exactly what Focal Loss does\u201d\u00ae.\n\nll I\n\nI! |\n\n\u2014_~ =\u2014\nSs\n\n7 &\n\n3s =\n\n=\n\n~\n\nSs\n\noa\n\n=\n\nwell-classified\nexamples\n\n0 0.2 0.4 06 08 1\nprobability of ground truth class\n\nFigure 3-10. The model trained with focal loss (FL) shows reduced loss values compared to the model trained with cross entropy loss (CE). Image by\nLin et al\n\nIn practice, ensembles have shown to help with the class imbalance problem\u00ae?. However, we don\u2019t include\n\nensembling in this section because class imbalance isn\u2019t usually why ensembles are used. Ensemble techniques\n\nwill be covered in Chapter 5: Model Development and Evaluation.\n\nData Augmentation\n\nData augmentation is a family of techniques that are used to increase the amount of training data. Traditionally,\nthese techniques are used for tasks that have limited training data, such as in medical imaging projects.\nHowever, in the last few years, they have shown to be useful even when we have a lot of data because\naugmented data can make our models more robust to noise and even adversarial attacks.\n\nData augmentation has become a standard step in many computer vision tasks and is finding its way into\nnatural language processing (NLP) tasks. The techniques depend heavily on the data format, as image\n\nmanipulation is different from text manipulation. In this section, we will cover three main types of data\naugmentation: simple label-preserving transformations, perturbation, which is a term for \u201cadding noises\u201d, and\ndata synthesis. In each type, we'll go over examples for both computer vision and NLP.\n\nSimple Label-Preserving Transformations\n\nIn computer vision, the simplest data augmentation technique is to randomly modify an image while preserving\nits label. You can modify the image by cropping, flipping, rotating, inverting (horizontally or vertically),\nerasing part of the image, and more. This makes sense because a rotated image of a dog is still a dog. Common\nML frameworks like PyTorch and Keras both have support for image augmentation. According to Krizhevsky\net al., in their legendary AlexNet paper, \u201cthe transformed images are generated in Python code on the CPU\nwhile the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect,\ncomputationally free\u00b0\"\u201d\n\nIn NLP, you can randomly replace a word with a similar word, assuming that this replacement wouldn't change\nthe meaning or the sentiment of the sentence. Similar words can be found either with a dictionary of\nsynonymous words, or by finding words whose embeddings are close to each other in a word embedding space.\n\nOriginal sentences I\u2019m so happy to see you.\n\nGenerated sentences I'm so glad to see you.\nI'm so happy to see y'all.\nI'm very happy to see you.\n\nThis type of data augmentation is a quick way to double, even triple your training data.\n\nPerturbation\n\nPerturbation is also a label-preserving operation, but because sometimes, it\u2019s used to trick models into making\nwrong predictions, I thought it deserves its own section.\n\nNeural networks, in general, are sensitive to noise. In the case of computer vision, this means that by adding a\nsmall amount of noise to an image can cause a neural network to misclassify it. Su et al. showed that 67.97% of\nthe natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet test images can be\nmisclassified by changing just one pixel\u00ae2 (See Figure 3-11).\n\nUsing deceptive data to trick a neural network into making wrong predictions is called adversarial attacks.\nAdding noise to samples to create adversarial samples is a common technique for adversarial attacks. The\nsuccess of adversarial attacks is especially exaggerated as the resolution of images increases.\n\nAdding noisy samples to our training data can help our models recognize the weak spots in their learned\ndecision boundary and improve their performance**,**. Noisy samples can be created by either adding random\nnoise or by a search strategy. Moosavi-Dezfooli et al. proposed an algorithm, called DeepFool, that finds the\nminimum possible noise injection needed to cause a misclassification with high confidence\u00ae\u00ae. This type of\naugmentation is called adversarial augmentation\u00ae\u00ae.\n\nAdversarial augmentation is less common in NLP (an image of a bear with randomly added pixels still looks\nlike a bear, but adding random characters to a random sentence will render it gibberish), but perturbation has\nbeen used to make models more robust. One of the most notable examples is BERT, where the model chooses\n15% of all tokens in each sequence at random, and chooses to replace 10% of the chosen tokens with random\nwords. For example, given the sentence \u201cmy dog is hairy\u201d and the model randomly replaces \u201chairy\u201d with\n\u201capple\u201d, the sentence becomes \u201cmy dog is apple\u201d. So 1.5% of all tokens might result in nonsensical meaning.\nTheir ablation studies show that a small fraction of random replacement gives their model a small performance\nboost\u00ae\u201d.\n\nIn chapter 5, we'll go over how to use perturbation not just as a way to improve your model's performance, but\nalso a way to evaluate its performance.\n\nVGG\n\nSHIP HORSE DEER\nCAR(99.7%) FROG(99.9%) AIRPLANE(85.3%)\n\nHORSE BIRD\nDOG(70.7%) CAT(75.5%) FROG(86.5%)\ncS\n\nCAR DEER CAT\nAIRPLANE(82.4%) DOG(86.4%) BIRD(66.2%)\n\nDEER BIRD SHIP\n\nAIRPLANE(49.8%) FROG(88.8%) AIRPLANE(88.2%)\n\nHORSE SHIP cAT\n\nDOG(88.0%) AIRPLANE(62.7%) DOG(78.2%)\n\nFigure 3-11. Changing one pixel can cause a neural network to make wrong predictions. Three models used are AllConv, NiN, and VGG. The\noriginal labels made by those models are in black, and the labels made after one pixel was changed are below: Image by Su et al.\n\nData Synthesis\n\nSince collecting data is expensive and slow with many potential privacy concems, it\u2019d be a dream if we could\nsidestep it altogether and train our models with synthesized data. Even though we're still far from being able to\nsynthesize all training data, it\u2019s possible to synthesize some training data to boost a model's performance.\n\nIn NLP, templates can be a cheap way to bootstrap your model. One of the teams I worked with used templates\nto bootstrap training data for their conversational Al (chatbot). A template might look like: \u201cFind me a\n[CUISINE] restaurant within [NUMBER] miles of [LOCATION].\u201d With lists of all possible cuisines,\nreasonable numbers (you would probably never want to search for restaurants beyond 1000 miles), and\nlocations (home, office, landmarks, exact addresses) for each city, you can generate thousands of training\nqueries from a template.\n\nGenerated queries Find me a Vietnamese restaurant within 2 miles of my office.\nPind me a Thai restaurant within 5 miles of my home.\nFind me a Mexican restaurant within 3 miles of Google headquarters.\n\nIn computer vision, a straightforward way to synthesize new data is to combine existing examples with discrete\nlabels to generate continuous labels. Consider a task of classifying images with two possible labels: DOG\n(encoded as 0) and CAT (encoded as 1). From example x; of label DOG and example x of label CAT, you can\ngenerate x\u2019 such as:\n\nX= yx, +(1 ye\nThe label of x\u2019 is a combination of the labels of x, and x: y * 0 + (1 \u2014 y) * 1. This method is called mixup. The\nauthors showed that mixup improves models\u2019 generalization, reduces their memorization of corrupt labels,\n\nincreases their robustness to adversarial examples, and stabilizes the training of generative adversarial\nnetworks.\u00b0\u00b0\n\nUsing neural networks to synthesize training data is an exciting approach that is actively being researched but\nnot yet popular in production. Sandfort et al. showed that by adding images generated using a CycleGAN to\ntheir original training data, they were able to improve their model's performance significantly on CT\nsegmentation tasks.\u00b0\u00b0\n\nIf you're interested in learning more about data augmentation for computer vision, A survey on Image Data\nAugmentation for Deep Learning (Connor Shorten & Taghi M. Khoshgoftaar, 2019) is a comprehensive review.\n\nSummary\n\nTraining data still forms the foundation of modern ML algorithms. No matter how clever your algorithms might\nbe, if your training data is bad, your algorithms won't be able to perform well. It\u2019s worth it to invest time and\neffort to curate and create training data that will enable your algorithms to learn something meaningful.\n\nIn this chapter, we've discussed the multiple steps to create training data. We first covered different sampling\nmethods, both non-probability sampling and random sampling, that can help us sample the right data for our\nproblem.\n\nMost ML algorithms in use today are supervised ML algorithms, so obtaining labels is an integral part of\ncreating training data. Many companies rely on human annotators to annotate their data. However, hand\nlabeling comes with many drawbacks. For example, hand labels can be expensive and slow. To combat the lack\nof hand labels, we discussed alternatives including weak supervision, semi-supervision, transfer learning, and\nactive learning.\n\nML algorithms work well in situations when the data distribution is more balanced, and not so well when the\nclasses are heavily imbalanced. Unfortunately, problems with class imbalance are the norm in the real-world. In\nthe following section, we discussed why class imbalance made it hard for ML algorithms to learn. We also\ndiscussed different techniques to handle class imbalance, from choosing the right metrics to resampling data to\nmodifying the loss function to encourage the model to pay attention to certain samples.\n\nWe ended the chapter with a discussion on data augmentation techniques that can be used to improve a model\u2019s\nperformance and generalization for both computer vision and NLP tasks.\n\nOnce you have your training data, you will want to extract features from it to train your ML models, which we\nwill cover in the next chapter.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 3,
                "chapter_name": "Chapter 4. Feature Engineering",
                "chapter_path": "./screenshots-images-2/chapter_3",
                "sections": [
                    {
                        "section_id": 3.1,
                        "section_name": "Chapter 4. Feature Engineering",
                        "section_path": "./screenshots-images-2/chapter_3/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_3/section_1/115e1bdc-3bb7-4720-9d97-fb4b7acdcacd.png",
                            "./screenshots-images-2/chapter_3/section_1/dd5b7d0b-c2b0-48a6-a418-416b6abc49bf.png",
                            "./screenshots-images-2/chapter_3/section_1/3745b8d5-b6d3-4b8f-9918-c3c6a33f2a2d.png",
                            "./screenshots-images-2/chapter_3/section_1/fdb1f62b-3978-43e3-b794-bf21d6e8b654.png",
                            "./screenshots-images-2/chapter_3/section_1/fafae14d-1245-46ad-90b6-13bdb9ab6e7d.png",
                            "./screenshots-images-2/chapter_3/section_1/ca322231-e849-4770-a1c7-5e91fe0b1f59.png",
                            "./screenshots-images-2/chapter_3/section_1/3ba66ea3-c3a1-48e5-973b-536aaaf9b5df.png",
                            "./screenshots-images-2/chapter_3/section_1/5041904c-9a52-48c5-8e60-cb6dac3f3ec7.png",
                            "./screenshots-images-2/chapter_3/section_1/b76548d1-80d6-4e02-a5ac-33e298fab293.png",
                            "./screenshots-images-2/chapter_3/section_1/0a1e8558-639e-4248-bd03-6709f97ad92a.png",
                            "./screenshots-images-2/chapter_3/section_1/3223d18c-ade5-43b4-9d58-99a08d8aac69.png",
                            "./screenshots-images-2/chapter_3/section_1/1c4c8c0b-272c-48a9-9f19-47b9250b4850.png",
                            "./screenshots-images-2/chapter_3/section_1/3b77f98c-9491-4d9a-9f5b-2f13089bc860.png",
                            "./screenshots-images-2/chapter_3/section_1/c5e23e6a-cb8f-4bb6-9a48-0f73f3d44738.png",
                            "./screenshots-images-2/chapter_3/section_1/cfe5420b-31e9-489a-99e5-981a49002cb5.png",
                            "./screenshots-images-2/chapter_3/section_1/a27295f7-73b7-4c00-ae0c-7548409032ed.png",
                            "./screenshots-images-2/chapter_3/section_1/cff31016-d137-4a84-a23e-916027dd7fce.png",
                            "./screenshots-images-2/chapter_3/section_1/cf9e51ab-ef68-4445-ad93-ae942dc7b435.png",
                            "./screenshots-images-2/chapter_3/section_1/c8e990a6-b52a-479e-8590-03bd4ffce4c9.png",
                            "./screenshots-images-2/chapter_3/section_1/759cd193-7e0a-479d-9688-fd47da717173.png",
                            "./screenshots-images-2/chapter_3/section_1/b8df0c6d-c3e9-47f8-ad5d-b408617bb12e.png",
                            "./screenshots-images-2/chapter_3/section_1/41ed3f0e-1a52-4777-ae1d-9d84a16c6118.png",
                            "./screenshots-images-2/chapter_3/section_1/0002c1d2-f22e-4224-94c6-d662de62d07e.png",
                            "./screenshots-images-2/chapter_3/section_1/32d3d019-112e-4730-a341-9a7a79061b68.png",
                            "./screenshots-images-2/chapter_3/section_1/c64cb067-6b56-47c0-a05f-3c001cd5a2c8.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "A NOTE FOR EARLY RELEASE READERS\n\nWith Early Release ebooks, you get books in their earliest form\u2014the author\u2019s raw and unedited content as they\nwrite\u2014so you can take advantage of these technologies long before the official release of these titles.\n\nThis will be the 4th chapter of the final book. Please note that the GitHub repo will be made active later on.\n\nIf you have comments about how we might improve the content and/or examples in this book, or if you notice\nmissing material within this chapter, please reach out to the author at chip@huyenchip.com.\n\nIn 2014, the paper Practical Lessons from Predicting Clicks on Ads at Facebook claimed that having the right\nfeatures is the most important thing in developing their ML models. Since then, many of the companies that I've\nworked with have discovered time and time again that once they have a workable model, having the right features\ntends to give them the biggest performance boost compared to clever algorithmic techniques such as\nhyperparameter tuning. State-of-the-art model architectures can still perform poorly if they don\u2019t use a good set of\nfeatures.\n\nDue to its importance, a large part of many ML engineering and data science jobs is to come up with new useful\nfeatures. In this chapter, we will go over common techniques and important considerations with respect to feature\nengineering. We will dedicate a section to go into detail about a subtle yet disastrous problem that has derailed\nmany ML systems in production: data leakage and how to detect and avoid it.\n\nWe will end the chapter discussing how to engineer good features, taking into account both the feature importance\nand feature generalization.\n\nLearned Features vs. Engineered Features\n\nWhen I cover this topic in class, my students frequently ask: \u201cWhy do we have to worry about feature\nengineering? Doesn't deep learning promise us that we no longer have to engineer features?\u201d\n\nThey are right. The promise of deep learning is that we won't have to handcraft features. For this reason, deep\nlearning is sometimes called feature learning\u2019. Many features can be automatically learned and extracted by\nalgorithms. However, we\u2019re still far from the point where all features can be automated. This is not to mention\nthat, as of writing, the majority of ML applications in production aren't deep learning. Let's go over an example to\nunderstand what features can be automatically extracted and what features still need to be handcrafted.\n\nImagine that you want to build a sentiment analysis classifier to classify whether a comment is spam or not. Before\ndeep learning, when given a piece of text, you would have to manually apply classical text processing techniques\nsuch as lemmatization, expanding contraction, removing punctuation, and lowercasing everything. After that, you\nmight want to split your text into n-grams with n values of your choice.\n\nAs a refresher, an n-gram is a contiguous sequence of n items from a given sample of text. The items can be\nphonemes, syllables, letters, or words. For example, given the post \u201cI like food\u201d, its word-level 1-grams are [\u201cI\u201d,\n\u201clike\u201d, \u201cfood\u201d] and its word-level 2-grams are [\u201cI like\u201d, \u201clike food\u201d). This sentence\u2019s set of n-gram features, if we\nwant n to be | and 2, is: [\u201cI\", \u201clike\u201d, \u201cfood\u201d, \u201cI like\u201d, \u201clike food\u201d).\n\nFigure 4-1 shows an example of classical text processing techniques you can use to handcraft n-gram features for\nyour text.\n\nOriginal text\n\nStopword removal\n\nLemmatization\n\nContraction\n\nPunctuation\n\nLowercase\n\nTokenization\n\nN-gram\n\n| have a dog. He\u2019s sleeping.\nv\n\n| have dog. He\u2019s sleeping.\n\ny\n\nI have dog. He's sleep.\n\n| have dog. He is sleep.\n\ny\n\n| have dog He is sleep\n\ny\n\ni have dog he is sleep\n\ny\n\ni, have, dog, he, is, sleep\n\ny\n\n{i, have, dog, he, is, sleep, i have,\nhave dog, dog he, he is, is sleep]\n\nFigure 4-1. An example of techniques that you can use to handcraft n-gram features for your text\n\nOnce you've generated n-grams for your training data, you can create a vocabulary that matches each n-gram to an\nindex. Then you can convert each post into a vector based on its n-grams\u2019 indices. For example, if we have a\nvocabulary of 7 n-grams as shown in Table 4-1, each post can be a vector of 7 elements. Each element corresponds\nto the number of times the n-gram at that index appears in the post. \u201cI like food\u201d will be encoded as the vector [1,\n1, 0, 1, 1, 0, 1]. This vector can then be inputted into an ML model such as logistic regression.\n\nFeature engineering requires knowledge of domain-specific techniques \u2014 in this case, the domain is natural\nlanguage processing (NLP). It tends to be an iterative process which can be brittle. When | followed this method\nfor one of my early NLP projects, | kept having to restart my process either because I had forgotten to apply one\ntechnique or because one technique I used turned out to be working poorly and | had to undo it.\n\nHowever, much of this pain has been alleviated since the rise of deep learning. Instead of having to worry about\nlemmatization, punctuation, or stopword removal, you can just split your raw text into words, create a vocabulary\nout of those words, and convert each of your words into one-hot vectors using this vocabulary. Your model will\nhopefully learn to extract useful features from this. In this new method, much of feature engineering for text has\nbeen automated. Similar progress has been made for images too. Instead of having to manually extract features\nfrom raw images and input those features into your ML models, you can just input raw images directly into your\ndeep learning models.\n\nHowever, an ML system will likely need data beyond just text and images. For example, when detecting whether a\ncomment is spam or not, on top of the text in the comment itself, you might want to use other information about:\n\n* the comment: such as who posted this comment, how many upvotes/downvotes it has.\n\n* the user who posted this comment: such as when this account was created, how often they post, how many\nupvotes/downvotes they have.\n\nthe thread in which the comment was posted: such as how many views it has, because popular threads\ntend to attract more spam.\n\nComment Time User Tet. \u00ab=f. #y Link \u00a9 fig Thread Replyto replies,\n\n93880839 20204020 girekt Youromse 1 60 0 6 3322332 nftabot |\nTAS UTC nice lady,\n\nUserID Created User Subs fa ty? Karma | # Verified Awards,\nreplies threads email\n\n4402903 | 206-0557T  giekt inl, 5 0 8 4 | 6 No\n309 8T fimemes.\nrisocilis!]\n\nThreadID Time = User_\u2014Text fa #y Link \u2014 #img | #replies views \u2014 Awards\n\n93383208 20204030 doge = Humanis = 20.501 6 2 205 1\nT245 PST tenporry,\nAGlis \u2018orever\n\nFigure 4-2, Some of the possible features about a comment, a thread, or @ user to be included in your model\n\nThere are so many possible features to use in your model, some of them are shown in Figure 4-2. The process of\nchoosing what to use and extracting the information you want to use is feature engineering. For important tasks\nsuch as recommending videos for users to watch next on Tiktok, the number of features used can go up to millions.\nFor domain-specific tasks such as predicting whether a transaction is fraudulent, you might need subject matter\nexpertise with banking and frauds to be able to extract useful features.\n\nCommon Feature Engineering Operations\n\nBecause of the importance and the ubiquity of feature engineering in ML projects, there have been many\ntechniques developed to streamline the process. In this section, we will discuss several of the most important\noperations that you should consider, if you haven't already, while engineering features from your data. They\ninclude handling missing values, scaling, discretization, encoding categorical features, generating the old-school\nbut still very effective cross features as well as the newer and exciting positional features. This list is nowhere near\nbeing comprehensive, but it does comprise some of the most common and useful operations to give you a good\nstarting point. Let's dive in!\n\nHandling Missing Values\n\nOne of the first things you might notice when dealing with data in production is that some values are missing.\nHowever, one thing that many ML engineers I've interviewed don\u2019t know is that not all types of missing values are\nequal\u00ae. To illustrate this point, consider the task of predicting whether someone is going to buy a house in the next\n12 months. A portion of the data we have is in Table 4-2.\n\n2 n B 50,000 \u2018Teacher No\n\n3 A 100,000 Married 2 Yes\n4 40 B 2 Engineer Yes\ns 38 B Single Ct) Doctor Yes\n6 A 50,000 0 \u2018Teacher No\n7 B B 60,000 Single \u2018Teacher No\n8 \u00bb B 10,000 Student No\n\nThere are three types of missing values. The official names for these types are a little bit confusing so we'll go into\ndetailed examples to mitigate the confusion.\n\n1, Missing not at random (MNAR): when the reason a value is missing is because of the value itself. In this\nexample, we might notice that respondents of gender \u201cB\u201d with higher income tend not to disclose their\nincome. The income values are missing for reasons related to the values themselves.\n\n2. Missing at random (MAR): when the reason a value is missing is not due to the value itself, but due to\nanother observed variable. In this example, we might notice that age values are often missing for\nrespondents of the gender \u201cA\u201d, which might be because the people of gender A in this survey don\u2019t like\ndisclosing their age.\n\n3. Missing completely at random (MCAR): when there\u2019s no pattern in when the value is missing. In this\nexample, we might think that the missing values for the column \u201cJob\u201d might be completely random, not\nbecause of the job itself and not because of another variable. People just forget to fill in that value\nsometimes for no particular reason. However, this type of missing is very rare. There are usually reasons\nwhy certain values are missing, and you should investigate.\n\nWhen encountering missing values, you can either fill in the missing values with certain values, (imputation), or\nremove the missing values (deletion). We'll go over both.\n\nDeletion\n\nWhen I ask candidates about how to handle missing values during interviews, many tend to prefer deletion, not\nbecause it\u2019s a better method, but because it\u2019s easier to do.\n\nOne way to delete is column deletion: if a variable has too many missing values, just remove that variable. For\nexample, in the example above, over 50% of the values for the variable \u201cMarital status\u201d are missing, so you might\nbe tempted to remove this variable from your model. The drawback of this approach is that you might remove\nimportant information and reduce the accuracy of your model. Marital status might be highly correlated to buying\nhouses, as married couples are much more likely to be homeowners than single people*.\n\nAnother way to delete is row deletion: if an example has missing value(s), just remove that example from the data.\nThis method can work when the missing values are completely at random (MCAR) and the number of examples\nwith missing values is small, such as less than 0.1%. You don\u2019t want to do row deletion if that means 10% of your\ndata examples are removed.\n\nHowever, removing rows of data can also remove important information that your model needs to make\npredictions, especially if the missing values are not at random (MNAR). For example, you don\u2019t want to remove\nexamples of gender B respondents with missing income because whether income is missing is information itself\n(missing income might mean higher income, and thus, more correlated to buying a house) and can be used to make\npredictions.\n\nOn top of that, removing rows of data can create biases in your model, especially if the missing values are at\nrandom (MAR). For example, if you remove all examples missing age values in the data in Table 4-2, you will\nremove all respondents with gender A from your data, and your model won't be able to make predictions for\nrespondents with gender A.\n\nimputation\n\nEven though deletion is tempting because it\u2019s easy to do, deleting data can lead to losing important information or\ncause your model to be biased. If you don\u2019t want to delete missing values, you will have to impute them, which\nmeans \u201cfill them with certain values.\u201d Deciding which \u201ccertain values\u201d to use is the hard part.\n\nOne common practice is to fill in missing values with their defaults. For example, if the job is missing, you might\nfill it with an empty string \u201c\u201d. Another common practice is to fill in missing values with the mean, median, or\nmode (the most common value). For example, if the temperature value is missing for a data example whose month\nvalue is July, it\u2019s not a bad idea to fill it with the median temperature of July.\n\nBoth practices work well in many cases, but sometimes, they can cause hair-splitting bugs. One time, in one of the\nprojects I was helping with, we discovered that the model was spitting out garbage because the app\u2019s front-end no\nlonger asked users to enter their age, so age values were missing, and the model filled them with 0. But the model\nnever saw the age value of 0 during training, so it couldn't make reasonable predictions.\n\nIn general, you might not want to fill missing values with possible values, such as filling the missing number of\nchildren with 0 \u2014 0 is a possible value for the number of children. It makes it hard to distinguish between people\nfor whom you don\u2019t have children information and people who don\u2019t have children.\n\nMultiple techniques might be used at the same time or in sequence to handle missing values for a particular set of\ndata. Regardless of what techniques you use, one thing is certain: there is no perfect way to handle missing values.\nWith deletion, you risk losing important information or accentuating biases. With imputation, you risk adding\nnoise to your data, or worse, data leakage. If you don\u2019t know what data leakage is, don\u2019t panic, we'll cover it in the\nData Leakage section of this chapter.\n\nScaling\n\nConsider the task of predicting whether someone will buy a house in the next 12 months, and the data is shown in\nTable 4-2. The values of the variable Age in our data go between 20 and 40, whereas the values of the variable\nAnnual Income go between 10,000 and 150,000. When we input these two variables into an ML model, it won't\nunderstand that 150,000 and 40 represent different things. It will just see them both as numbers, and because the\nnumber 150,000 is much bigger than 40, it might give it more importance, regardless of which variable is actually\nmore useful for the predicting task.\n\nDuring data processing, it\u2019s important to scale your features so that they're in similar ranges. This process is called\nfeature scaling. This is one of the simplest things you can do that often result in a performance boost for your\nmodel. Neglecting to do so can cause your model to make gibberish predictions, especially with classical\nalgorithms like gradient-boosted trees and logistic regression\u00ae.\n\nAn intuitive way to scale your features is to get each feature to be in the range [0, 1]. Given a variable x, its values\ncan be rescaled to be in this range using the following formula.\nx~ min (2)\n\nad x(a) min (2)\n\nYou can validate that if x is the maximum value, the scaled value x\u2019 will be 1. If x is the minimum value, the\nscaled value x\u2019 will be 0.\n\nIf you want your feature to be in an arbitrary range [a, b] \u2014 empirically, I find the range [-1, 1] to work better than\nthe range [0, 1] \u2014 you can use the following formula.\n\n(e min (z})(b a)\n\n2d\u2122 at\nmax (\u00ab)\u2014 min (2)\n\nScaling to an arbitrary range works well when you don\u2019t want to make any assumptions about your variables. If\nyou think that your variables might follow a normal distribution, it might be helpful to normalize them so that they\nhave zero-mean and unit variance. This process is called standardization.\n\n\u00a3\n\nray\no\n\nwith x being the mean of variable x, and o being its standard deviation.\n\nIn practice, ML models tend to struggle with features that follow a skewed distribution. To help mitigate the\nskewness, a technique commonly used is log transformation: apply the log function to your feature. An example of\nhow the log transformation can make your data less skewed is shown in Figure 4-3. While this technique can yield\nperformance gain in many cases, it doesn\u2019t work for all cases and you should be wary of the analysis performed on\nlog-transformed data instead of the original data\u00ae.\n\nAfr trasfrmatn\n\nos # 8 mM os Be ons ow\n\n4 4 4 '\n\nFigure 4-3. In many cases, the log transformation can help reduce the skewness of your data\n\nThere are two important things to note about scaling. One is that it\u2019s a common source of data leakage, (this will be\ncovered in greater detail in the Data Leakage section). Another is that it often requires global statistics \u2014 you have\nto look at the entire or a subset of training data to calculate its min, max, or mean. During inference, you reuse the\nstatistics you had obtained during training to scale new data. If the new data has changed significantly compared to\nthe training, these statistics won\u2019t be very useful. Therefore, it\u2019s important to retrain your model often to account\nfor these changes. We'll discuss more on how to handle changing data in production in the section on continual\nlearning in Chapter 8.\n\nDiscretization\n\nImagine that we've built a model with the data in Table 4-2. During training, our model has seen the annual\nincome values of 150000, 50000, 100000, 50000, 60000, and 10000. During inference, our model encounters an\nexample with an annual income of 9000.50.\n\nIntuitively, we know that $9000.50 a year isn\u2019t much different from $10,000/year, and we want our model to treat\nboth of them the same way. But the model doesn\u2019t know that. Our model only knows that 9000.50 is different from\n10000, and will treat them differently.\n\nDiscretization is the process of turning a continuous feature into a discrete feature. This process is also known as\nquantization. This is done by creating buckets for the given values. For annual income, you might want to group\n\nthem into three buckets as follows.\n* Lower income: less than $35,000/year\n* Middle income: between $35,000 and $100,000/year\n+ Upper income: more than $100,000/year\n\nNow, instead of having to learn an infinite number of possible incomes, our model can focus on learning only three\ncategories, which is a much easier task to lear.\n\nEven though by definition, it\u2019s used for continuous features, the same technique can be used for discrete features\ntoo. The age variable is discrete, it might still be useful to group them into buckets such as follows.\n\n+ Less than 18\n\n\u00ab Between 18 and 22\n* Between 22 and 30\n+ Between 30-40\n\n+ Between 40 - 65\n\n* Over 65\n\nA question with this technique is how to best choose the boundaries of categories. You can try to plot the\nhistograms of the values and choose the boundaries that make sense. In general, common sense, basic quantiles,\nand sometimes subject matter expertise can get you a long way.\n\nEncoding Categorical Features\n\nWe've talked about how to turn continuous features into categorical features. In this section, we'll discuss how to\nbest handle categorical features.\n\nPeople who haven\u2019t worked with data in production tend to assume that categories are static, which means the\ncategories don\u2019t change over time. This is true for many categories. For example, age brackets and income\nbrackets are unlikely to change and you know exactly how many categories there are in advance. Handling these\ncategories is straightforward. You can just give each category a number and you're done.\n\nHowever, in production, categories change. Imagine you're building a recommendation system to predict what\nproducts users might want to buy for Amazon. One of the features you want to use is the product brand. When\nlooking at Amazon\u2019s historical data, you realize that there are a lot of brands. Even back in 2019, there were\nalready over 2 million brands on Amazon\u201d!\n\nThe number of brands is overwhelming but you think: \u201cI can still handle this.\u201d You encode each brand a number,\nso now you have 2 million numbers from 0 to 1,999,999 corresponding to 2 million brands. Your model does\nspectacularly on the historical test set, and you get approval to test it on 1% of today\u2019s traffic.\n\nIn production, your model crashes because it encounters a brand it hasn't seen before and therefore can\u2019t encode.\nNew brands join Amazon all the time. You create a category \u201cUNKNOWN\u201d with the value of 2,000,000 to catch\nall the brands your model hasn't seen during training.\n\nYour model doesn\u2019t crash anymore but your sellers complain that their new brands are not getting any traffic. It\u2019s\nbecause your model didn\u2019t see the category UNKNOWN in the train set, so it just doesn\u2019t recommend any product\nof the UNKNOWN brand. Then you fix this by encoding only the top 99% most popular brands and encode the\nbottom 1% brand as UNKNOWN. This way, at least your model knows how to deal with UNKNOWN brands.\n\nYour model seems to work fine for about | hour, then the click rate on recommended products plummets. Over the\nlast hour, 20 new brands joined your site, some of them are new luxury brands, some of them are sketchy knockoff\n\nbrands, some of them are established brands. However, your model treats them all the same way it treats unpopular\nbrands in the training set.\n\nThis isn\u2019t an extreme example that only happens if you work at Amazon on this task. This problem happens quite a\nlot. For example, if you want to predict whether a comment is spam, you might want to use the account that posted\nthis comment as a feature, and new accounts are being created all the time. The same goes for new product types,\nnew website domains, new restaurants, new companies, new IP addresses, and so on. If you work with any of\nthem, you'll have to deal with this problem.\n\nFinding a way to solve this problem turns out to be surprisingly difficult. You don\u2019t want to put them into a set of\nbuckets because it can be really hard\u2014how would you even go about putting new user accounts into different\ngroups?\n\nOne solution to this problem is the hashing trick, popularized by the package Vowpal Wabbit developed at\nMicrosoft\u00ae. The gist of this trick is that you use a hash function to generate a hashed value of each category. The\nhashed value will become the index of that category. Because you can specify the hash space, you can fix the\nnumber of encoded values for a feature in advance, without having to know how many categories there will be. For\nexample, if you choose a hash space of 18 bits, which corresponds to 2'* = 262,144 possible hashed values, all the\ncategories, even the ones that your model has never seen before, will be encoded by an index between 0 and 262,\n143.\n\nOne problem with hashed functions is collision: two categories being assigned the same index. However, with\nmany hash functions, the collisions are random, new brands can share index with any of the old brands instead of\nalways sharing index with unpopular brands, which is what happens when we use the UNKNOWN category\nabove. The impact of colliding hashed features is, fortunately, not that bad. In a research done by Booking.com,\neven for 50% colliding features, the performance loss is less than half percent, as shown in Figure 4-4.\n\n22 0.2785\n2\u00bb\n0.2780\nos\n=e\nEn\n:\nF 02775 Fy\nae s\n=\n_\n0.2770\n-\nos\n2\u201c 02765\n00 02 O4 06 OB 10\n% Colksions:\n\nFigure 4-4. 50% collision rate only causes the log loss to increase less than half a percent, Image by Lucas Bernardi.\n\nYou can choose a hash space large enough to reduce the collision. You can also choose a hash function with\nproperties that you want, such as a locality-sensitive hashing function where similar categories (such as websites\nwith similar names) are hashed into values close to each other.\n\nBecause it\u2019s a trick, it\u2019s often considered hacky by academics and excluded from ML curricula. But its wide\nadoption in the industry is a testimonial to how effective the trick is. It\u2019s essential to Vowpal Wabbit and it\u2019s part of\nthe frameworks scikit-learn, TensorFlow, and gensim. It can be especially useful in continual learning settings\nwhere your model learns from incoming examples in production. We'll cover continual learning in Chapter 8.\n\nFeature Crossing\n\nFeature crossing is the technique to combine two or more features to generate new features. This technique is\nuseful to model the non-linear relationships between features. For example, for the task of predicting whether\nsomeone will want to buy a house in the next 12 months, you suspect that there might be a non-linear relationship\nbetween marital status and number of children, so you combine them to create a new feature \u201cmarriage and\nchildren\u201d as in Table 4-3.\n\nMarriage Single Married Single Single Married\n\nMarriage & children Single, 0 Married, 2 Single, 1 Single, 0 Married, 1\n\nBecause feature crossing helps model non-linear relationships between variables, it\u2019s essential for models that\ncan\u2019t learn or are bad at learning non-linear relationships, such as linear regression, logistic regression, and tree-\nbased models. It\u2019s less important in neural networks, but can still be useful because explicit feature crossing\noccasionally helps neural networks learn non-linear relationships faster. DeepFM and xDeepFM are the family of\nmodels that have successfully leverage explicit feature interactions for recommendation systems and click-\nthrough-rate prediction tasks.\n\nA caveat of feature crossing is that it can make your feature space blow up. Imagine feature A has 100 possible\nvalues and feature B has 100 possible features, crossing these two features will result in a feature with 100 x 100 =\n10,000 possible values. You will need a lot more data for models to learn all these possible values. Another caveat\nis that because feature crossing increases the number of features models use, it can make models overfit to the\ntraining data.\n\nDiscrete and Continuous Positional Embeddings\n\nFirst introduced to the deep learning community in the paper Attention Is All You Need (Vaswani et al., 2017),\npositional embedding has become a standard data engineering technique for many applications in both computer\nvision and natural language processing. We'll walk through an example to show why positional embedding is\nnecessary and how to do it.\n\nConsider the task of language modeling where you want to predict the next token based on the previous sequence\nof tokens. In practice, a token can be a word, a character, or a subword, and a sequence length can be up to 512 if\nnot larger. However, for simplicity, let\u2019s use words as our tokens and use the sequence length of 8. Given an\narbitrary sequence of 8 words, such as, \u201cSometimes all I really want to do is\u201d, we want to predict the next word.\nIf we use a recurrent neural network, it will process words in sequential order, which means the order of words is\nimplicitly inputted. However, if we use a model like a transformer, words are processed in parallel, so words\u2019\n\npositions need to be explicitly inputted so that our model knows which word follows which word (\u201ca dog bites a\nchild\u201d is very different from \u201ca child bites a dog\u201d). We don\u2019t want to input the absolute positions: 0, 1, 2, ..., 7 into\nour model because empirically, neural networks don\u2019t work well with inputs that aren\u2019t unit-variance (that\u2019s why\nwe scale our features, as discussed previously in the section Scaling).\n\nIf we rescale the positions to between 0 and 1, so 0, 1, 2, ..., 7 become 0, 0.143, 0.286, ..., 1, the differences\nbetween the two positions will be too small for neural networks to learn to differentiate.\n\nA way to handle position embeddings is to treat it the way we'd treat word embedding. With word embedding, we\nuse an embedding matrix with the vocabulary size as its number of columns, and each column is the embedding\nfor the word at the index of that column. With position embedding, the number of columns is the number of\npositions. In our case, since we only work with the previous sequence size of 8, the positions go from 0 to 7 (see\nFigure 4-5).\n\nThe embedding size for positions is usually the same as the embedding size for words so that they can be summed.\nFor example, the embedding for the word \u201cfood\u201d at position 0 is the sum of the embedding vector for the word\n\u201cfood\u201d and the embedding vector for position 0. This is the way position embeddings are implemented in\nHuggingFace\u2019s BERT as of August 2021. Because the embeddings change as the model weights get updated, we\nsay that the position embeddings are learned.\n\nWord embedding matrix Position embedding matrix\n\nemo for word \u201cfood\u201d (index 0} emb for position 0\n\nOO | 0023 NS | NOE, QO | 00023 ome | d008\na. 0. &. 0. a. 0. a a.\n\nwod postion)\n\nembedding |) embedding\n\nsae 4 a 4 4 . size 4. ood 4.\nfood i you jae.\n\nVocab\n0 1 2 3\n\nFigure 4-5. One way to embed positions is to treat them the way you'd treat word embeddings\nPosition embeddings can also be fixed. The embedding for each position is still a vector with S elements (S is the\n\nposition embedding size), but each element is predefined using a function, usually sine and cosine. In the original\nTransformer paper, if the element is at an even index, use sine. Else, use cosine. See Figure 4-6.\n\nemb for position p\n\n;\n\n(\n\u00b0 sin(p/10000\u00b0\")\n1 cos(p/10000\"\")\nposition\nembedding { 2 sin(p/100007\")\nsize\n\n3 cos(p/10000*\")\n\n\\\n\nFigure 4-6. Example of fixed position embedding. H is the dimension of the outputs produced by the model.\nFixed positional embedding is a special case of what is known as Fourier features. If positions in positional\nembeddings are discrete, Fourier features can also be continuous. Consider the task involving representations of\n3D objects, such as a teapot. Each position on the surface of the teapot is represented by a 3-dimensional\ncoordinate, which is continuous. When positions are continuous, it'd be very hard to build an embedding matrix\nwith continuous column indices, but fixed position embeddings using sine and cosine functions still work.\nThis is the generalized format for the embedding vector at coordinate v, also called the Fourier features of\ncoordinate v. Fourier features have been shown to improve models\u2019 performance for tasks that take in coordinates\n(or positions) as inputs.?\n\n\u2018y(v) = [ar cos (2mb,7e), a; sin (26:70), d!, am cos (2b 70), yy sin (2xb,.7u) |\n\nData Leakage\n\nOne pattern of failures that I\u2019ve encountered often in production is when models perform beautifully on the test\nset, but fail mysteriously in production. The cause, after a long and painful investigation, is data leakage.\n\nData leakage refers to the phenomenon when a form of the label \u201cleaks\u201d into the set of features used for making\npredictions, and this same information is not available during inference.\n\nData leakage is subtle because often, the leakage is non-obvious. It\u2019s dangerous because it can cause your models\nto fail in an unexpected and spectacular way, even after extensive evaluation and testing. Let\u2019s go over an example\nto demonstrate what data leakage is.\n\nSuppose you want to build an ML model to predict whether a CT scan of a lung shows signs of cancer. You\nobtained the data from hospital A, removed the doctors\u2019 diagnosis from the data, and trained your model. It did\nreally well on the test data from hospital A, but poorly on the data from hospital B.\n\nAfter extensive investigation, you learned that at hospital A, when doctors think that a patient has lung cancer, they\nsend that patient to a more advanced scan machine, which outputs slightly different CT scan images. Your model\nlearned to rely on the information on the scan machine used to make predictions on whether a scan image shows\nsigns of lung cancer. Hospital B sends the patients to different CT scan machines at random, so your model has no\ninformation to rely on. We say that labels are leaked into the features during training.\n\nData leakage can happen not only with newcomers to the field, but has also happened to several experienced\nresearchers whose work I admire, and in one of my own projects. Despite its prevalence, data leakage is rarely\ncovered in ML curricula.\n\nCAUTIONARY TALE: DATA LEAKAGE WITH KAGGLE COMPETITION\n\nIn 2020, University of Liverpool launched an Ion Switching competition on Kaggle. The task was to identify\nthe number of ion channels open at each time point. They synthesized test data from train data, and some\npeople were able to reverse engineer and obtain test labels from the leak\"\u00ae, The two winning teams in this\ncompetition are the two teams that were able to exploit the leak, though they might have still been able to win\nwithout exploiting the leak\".\n\nCommon Causes for Data Leakage\n\nIn this section, we'll go over some common causes for data leakage and how to avoid them.\n\n1. Splitting time-correlated data randomly instead of by time\n\nWhen I learned ML in college, I was taught to randomly split my data into train, validation, and test splits.\nThis is also how data is often split in many ML research papers. However, this is also one common cause\nfor data leakage.\n\nIn many cases, data is time-correlated, which means that the time the data is generated affects how it\nshould be labeled. Sometimes, the correlation is obvious, as in the case of stock prices. To oversimplify it,\nthe prices of many stocks tend to go up and down together. If 90% of the stocks go down today, it\u2019s very\nlikely the other 10% of the stocks go down too. When building models to predict the future stock prices,\nyou want to split your training data by time, such as training your model on data from the first 6 days and\nevaluating it on data from the 7th day. If you randomly split your data, prices from the 7th day will be\nincluded in your train split and leak into your model the condition of the market on that day. We say that\nthe information from the future is leaked into the training process.\n\nHowever, in many cases, the correlation is non-obvious but it\u2019s still there. Consider the task of predicting\nwhether someone will click on a song commendation. Whether someone will listen to a song depends not\nonly on their taste in music but also on the general music trend that day. If an artist passes away one day,\n\npeople will be much more likely to listen to that artist. By including examples from that day into the train\n\nsplit, information about the music trend that day will be passed into your model, making it easier for it to\nmake predictions on other examples on that day.\n\nTo prevent future information from leaking into the training process and allowing models to cheat during\nevaluation on the test split, split your training data by time whenever possible, instead of random splitting.\nFor example, if you have data from 5 weeks, use the first 4 weeks for the train split, then randomly split\nweek 5 into validation and test splits as shown in Figure 4-7.\n\nTrain split\n\nWeek1 Week2 =\u00bb Week3 = Week 4 Week\n\nXt X2I 31 X4l x6\n\nVal spl\nx w x\n\nX83 XB 3 YB | \u00a583\nTestsplt\na 7 XX\n\nFigure 4-7. Split data by time to prevent future information from leaking into the training process\n2. 2. Scaling before splitting\nAs discussed in the Scaling section in this chapter, it\u2019s important to scale your features. Scaling requires\nglobal statistics about your data, such as the mean of your data. One common mistake is to use the entire\ntraining data to generate global statistics before splitting it into different splits, leaking the mean of the\ntest examples into the training process, allowing a model to adjust to the mean of the future examples.\nThis information isn\u2019t available in production so the model\u2019s performance will likely degrade.\n\nTo avoid this type of leakage, always split your data first before scaling, then use the statistics from the\ntrain split to scale all the splits. Some even suggest that we split our data before any exploratory data\nanalysis and data processing, so that we don\u2019t accidentally gain information about the test split.\n\n3. 3. Filling in missing data with statistics from the test split\n\nOne common way to handle the missing values of a feature is to fill them with the mean or median of all\nvalues present. Leakage might occur if the mean or median is calculated using entire data instead of just\n\nthe train split. This type of leakage is similar to the type of leakage above, and can be prevented by using\nonly statistics from the train split to fill in missing values in all the splits.\n\n+\n\n. 4. Poor handling of data duplication before splitting\n\nData duplication is quite common. It can happen because the data handed to you has duplication in it\u2019,\nwhich likely results from data collection or merging of different data sources. It can also happen because\nof data processing \u2014 for example, oversampling might result in duplicating certain examples. If you fail\nto remove duplicates before splitting your data into different splits, the same examples might appear in\nboth the train and the validation/test splits. To avoid this, first, always check for duplicates before splitting\nand also after splitting just to make sure. If you oversample your data, do it only after splitting.\n\n. 5. Group leakage\n\nw\n\nA group of examples have strongly correlated labels but are divided into different splits. For example, a\npatient might have two lung CT scans that are a week apart, which likely have the same labels on whether\nthey contain signs of lung cancer, but one of them is in the train split and the third is in the test split. To\navoid this type of data leakage, an understanding of your data is essential.\n\na\n\n. 6. Leakage from data collection process\n\nThe example above about how information on whether a CT scan shows signs of lung cancer is leaked via\nthe scan machine is an example of this type of leakage. Detecting this type of data leakage requires a deep\nunderstanding of the way data is collected and occasionally, subject matter expertise in the task. For\nexample, it would be very hard to figure out that the model\u2019s poor performance in hospital B is due to its\ndifferent scan machine procedure if you don\u2019t know about different scan machines or that the procedures\nat the two hospitals are different.\n\nThere's no foolproof way to avoid this type of leakage, but you can mitigate the risk by keeping track of the\nsources of your data, and understanding how it is collected and processed. Normalize your data so that data from\ndifferent sources can have the same means and variances. If different CT scan machines output images with\ndifferent resolutions, normalizing all the images to have the same resolution would make it harder for models to\nknow which image is from which scan machine.\n\nDetecting Data Leakage\n\nData leakage can happen during many steps, from generating, collecting, sampling, splitting, processing data to\nfeature engineering. It\u2019s important to monitor for data leakage during the entire lifecycle of an ML project.\nMeasure how each feature or a set of features are correlated to the target variable (label). If a feature has unusually\nhigh correlation, investigate how this feature is generated and whether the correlation makes sense. It\u2019s possible\nthat two features independently don\u2019t contain leakage, but two features together can contain leakage. For example,\nwhen building a model to predict how long an employee will stay at a company, the starting date and the end date\nseparately doesn\u2019t tell us much about their tenure, but both together can give us that information.\n\nDo ablation studies to measure how important a feature or a set of features is to your model. If removing a feature\ncauses the model's performance to deteriorate significantly, investigate why that feature is so important. If you\nhave a massive amount of features, say a thousand features, it might be infeasible to do ablation studies on every\npossible combination of them, but it can still be useful to occasionally do ablation studies with a subset of features\nthat you suspect the most. This is another example why subject matter expertise can come in handy in feature\n\nengineering. Ablation studies can be run offline at your own schedule, so you can leverage your machines during\ndown time for this purpose.\n\nKeep an eye out for new features added to your model. If adding a new feature significantly improves your\nmodel\u2019s performance, either that feature is really good or that feature just contains leaked information about labels.\n\nBe very careful every time you look at the test split. If you use the test split in any way other than to report a\nmodel\u2019s final performance, whether to come up with ideas for new features or to tune hyperparameters, you risk\nleaking information from the future into your training process.\n\nEngineering Good Features\n\nGenerally, adding more features leads to better model performance. In my experience, the list of features used for a\nmodel in production only grows over time. However, more features doesn\u2019t always mean better model\nperformance. Having too many features can be bad both during training and serving your model for the following\nreasons.\n\n1. The more features you have, the more opportunities there are for data leakage.\n2. Too many features can cause overfitting.\n\n3. Too many features can increase memory required to serve a model, which, in turn, might require you to\nuse a more expensive machine/instance to serve your model.\n\n4. Too many features can increase inference latency when doing online prediction, especially if you need to\nextract these features from raw data for predictions online. We'll go deeper into online prediction in\nChapter 6.\n\n5. Useless features become technical debts. Whenever your data pipeline changes, all the affected features\nneed to be adjusted accordingly. For example, if one day your application decides to no longer take in\ninformation about users\u2019 age, all features that use users\u2019 age need to be updated.\n\nIn theory, if a feature doesn\u2019t contribute to help a model make good predictions, regularization techniques like L1\nregularization should reduce that feature\u2019s weight to 0. However, in practice, it might help models learn faster if\nthe features that are no longer useful (and even possibly harmful) are removed, prioritizing good features.\n\nYou can store removed features to add them back later. You can also just store general feature definitions to reuse\nand share across teams in an organization. When talking about feature definition management, some people might\nthink of feature stores as the solution. However, not all feature stores manage feature definitions.\n\nThere are two factors you might want to consider when evaluating whether a feature is good for a model:\nimportance to the model and generalization to unseen data.\n\nFeature Importance\n\nThere are many different methods for measuring a feature\u2019s importance. If you use a classical ML algorithm like\nboosted gradient trees, the easiest way to measure the importance of your features is to use built-in feature\nimportance functions implemented by XGBoost'?. For more general models, you might want to look into SHAP\n(SHapley Additive exPlanations)\"*. InterpretML is a great open-source package that leverages feature importance\nto help you understand how your model makes predictions.\n\nThe exact algorithm for feature importance measurement is complex, but intuitively, a feature\u2019s importance to a\nmodel is measured by how much that model\u2019s performance deteriorates if that feature or a set of features\ncontaining that feature is removed from the model. SHAP is great because it not only measures a feature\u2019s\nimportance to an entire model, it also measures each feature\u2019s contribution to a model\u2019s specific prediction.\nFigure 4-9a and 4-8b show how SHAP can help you understand the contribution of each feature to a model\u2019s\npredictions.\n\n296 = TAX -0.47 g\n006 = CRIM 0.43 |\n4.09 = DIS 041 a\n\n15.3 = PTRATIO } 40.26\n55.2 = AGE } +0.19\n4 other features -0.04 {\n\n19 20 21 22 23 24\nELAN] = 2\n\nFigure 4-8. a: How much each feature contributes to a model's single prediction, measured by SHAP. The value LSTAT=4 contributes the most to this\nspecific prediction. Image by Scott Lundberg from the GitHub repasitory github com/slundbeng/shap\n\nHigh\n\nLSTAT\n\nDIS\n\nAGE\nCRIM\nNOX\nPTRATIO.\nTAX\n\nB\n\nFeature value\n\nSum of 4 other features\n\nLow\n\n-10 0 5 0 5 10 15 20\nSHAP value (impact on model output)\n\nFigure 4-9. b: How much each feature contributes to a model, measured by SHAP. The feature LSTAT has the highest importance. Image by Scott\nLundberg from the GitHub repository github. com/slundberg/skap.\nOften, a smal! number of features accounts for a large portion of your model\u2019s feature importance. When\nmeasuring feature importance for a click-through rate prediction model, the ads team at Facebook found out that\nthe top 10 features are responsible for about half of the model\u2019s total feature importance, while the last 300\nfeatures contribute less than 1% feature importance, as shown in Figure 4-10'\u00b0.\n\n0.9\n0.1 08\n0.01 o7\n0.6\n0.001 05\n0.4\n0.0001 03\n0.00001 0.2\n0.1\n\n0.000001 0\n\n1 51 101 1 201 231 301 351 401\n\n=\u2014=I|mportance ==Cumulative Importance\n\nFigure 4-10. Boosting Feature Importance. X-axis corresponds to the number of features. Feature importance is in log scale. Image by He et al.\n\nNot only good for choosing the right features, feature importance techniques are also great for interpretability as\nthey help you understand how your models work under the hood. We will discuss more about Interpretability in\nChapter [TODO].\n\nFeature Generalization\n\nSince the goal of an ML model is to make correct predictions on unseen data, features used for the model should\nbe generalizable to unseen data. Not all features generalize equally. For example, for the task of predicting whether\na comment is spam, the identifier of each comment is not generalizable at all and shouldn't be used as a feature for\nthe model. However, the identifier of the user who posts the comment, such as username, might still be useful for a\nmodel to make predictions.\n\nMeasuring feature generalization is a lot less scientific than measuring feature importance, and requires more\n\nintuition and subject matter expertise than statistical knowledge. Overall, there are two aspects you might want to\nconsider with regards to generalization: feature coverage and distribution of feature values.\n\nCoverage is the percentage of the examples that has values for this feature in the data. A rough rule of thumb is\nthat if this feature appears in a very small percentage of your data, it\u2019s not going to be very useful. For example, if\nyou want to build a model to predict whether someone will buy a house in the next 12 months and you think that\nthe number of children someone has will be a good feature, but you can only get this information for 1% of your\ndata, this feature might not be very useful.\n\nThis rule of thumb is rough because some features can still be useful even if they are missing in most of your data.\nThis is especially useful if the missing values are not at random, which means having the feature or not might be a\nstrong indication of its value). For example, if a feature appears only in 1% of your data, but 99% of the examples\nwith this feature have POSITIVE labels, this feature is very useful and you should use it.\n\nCoverage of a feature can differ wildly between different slices of data and in the same slice of data over time. If\nthe coverage of a feature differs a lot between the train and test split (such as it appears in 90% of the examples in\nthe train split but only in 20% of the examples in the test split), this is an indication that your train and test splits\ndon\u2019t come from the same distribution. You might want to investigate whether the way you split your data makes\nsense and whether this feature is a cause for data leakage.\n\nFor the feature values that are present, you might want to look into their distribution. If the set of values that\nappears in the seen data (such as the train split) has no overlap with the set of values that appears in the unseen\ndata (such as the test split), this feature might even hurt your model\u2019s performance.\n\nAs a concrete example, imagine you want to build a model to estimate the time it will take for a given taxi ride.\nYou retrain this model every week, and you want to use the data from the last 6 days to predict the ETAs\"\u00ae for\ntoday. One of the features is DAY_OF_THE_WEEK, which you think is useful because the traffic on weekdays is\nusually worse than on the weekend. This feature coverage is 100%, because it\u2019s present in every feature. However,\nin the train split, the values for this feature are Monday to Saturday, while in the test split, the value for this feature\nis Sunday. If you include this feature in your model, it won't generalize to the test split, and might harm your\nmodel\u2019s performance.\n\nOn the other hand, HOUR_OF_THE_DAY is a great feature, because the time in the day affects the traffic too, and\nthe range of values for this feature in the train split overlaps with the test split 100%.\n\nWhen considering a feature\u2019s generalization, there\u2019s a tradeoff between generalization and specificity. You might\nrealize that the traffic during an hour only changes depending on whether that hour is the rush hour. So you\ngenerate the feature IS_ RUSH_HOUR and set it to 1 if the hour is between 7am and 9am or between 4pm and\n6pm. IS_RUSH_HOUR is more generalizable but less specific than HOUR_OF_THE_DAY. Using\nIS_RUSH_HOUR without HOUR_OF_THE_DAY might cause models to lose important information about the\nhour.\n\nSummary\n\nBecause the success of today\u2019s ML systems still depend on their features, it\u2019s important for organizations interested\nin using ML in production to invest time and effort into feature engineering.\n\nHow to engineer good features is a complex question with no fool-proof answers. The best way to learn is through\nexperience: trying out different features and observing how they affect your models\u2019 performance. It\u2019s also\npossible to learn from experts. I find it extremely useful to read about how the winning teams of Kaggle\ncompetitions engineer their features to learn more about their techniques and the considerations they went through.\nFeature engineering often involves subject matter expertise, and subject matter experts might not always be\nengineers, so it\u2019s important to design your workflow in a way that allows non-engineers to contribute to the\nprocess.\n\nHere is a summary of best practices for feature engineering.\n\n+ Split data by time into train/valid/test spi\n\ninstead of doing it randomly.\n+ Ifyou oversample your data, do it after splitting.\n* Scale and normalize your data after splitting to avoid data leakage.\n\n+ Use statistics from only the train split, instead of the entire data, to scale your features and handle missing\nvalues.\n\n* Understand how your data is generated, collected and processed. Involve domain experts if necessary.\n\u00bb Keep track of its lineage.\n\n+ Understand feature importance to your model.\n\n* Use features that generalize well.\n\n* Remove no longer useful features from your models.\n\nWith a set of good features, we'll move to the next part of the workflow: training ML models. Before we move on,\nI just want to reiterate that moving to modeling doesn\u2019t mean we're done with handling data or feature\nengineering. We are never done with data and features. In most real-world ML projects, the process of collecting\ndata and feature engineering goes on as long as your models are in production. We need to use new, incoming data\nto continually improve models. We'll cover continual learning in Chapter 8.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 4,
                "chapter_name": "Chapter 5. Model Development",
                "chapter_path": "./screenshots-images-2/chapter_4",
                "sections": [
                    {
                        "section_id": 4.1,
                        "section_name": "Chapter 5. Model Development",
                        "section_path": "./screenshots-images-2/chapter_4/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_4/section_1/f9aef847-11bc-498f-a8d4-3761a1adcab2.png",
                            "./screenshots-images-2/chapter_4/section_1/1f17f29a-9b2a-4366-a9d5-750c90d7b5fd.png",
                            "./screenshots-images-2/chapter_4/section_1/e8e0ee5e-883f-41b8-8cef-4cac3be784fd.png",
                            "./screenshots-images-2/chapter_4/section_1/a8940482-64bf-4f3b-bef0-51cc97881fcf.png",
                            "./screenshots-images-2/chapter_4/section_1/1f3bd20f-cf0a-4bc7-a54f-020a13704961.png",
                            "./screenshots-images-2/chapter_4/section_1/80c0c297-5098-4505-85bb-d11f8cd8f1c9.png",
                            "./screenshots-images-2/chapter_4/section_1/4f0ed5b3-393b-4651-bc69-e7444174ed8f.png",
                            "./screenshots-images-2/chapter_4/section_1/12cbdcdb-1f0c-4a94-83ca-0c2be294b994.png",
                            "./screenshots-images-2/chapter_4/section_1/40fdc69d-e465-4bfb-912d-3cef00bce81e.png",
                            "./screenshots-images-2/chapter_4/section_1/9bf12a9f-16eb-4ee7-b2f0-36d99990830e.png",
                            "./screenshots-images-2/chapter_4/section_1/813f8c21-37aa-42ff-b148-dd6f8da87e81.png",
                            "./screenshots-images-2/chapter_4/section_1/7ccb06f7-d5fc-4256-b747-9ca6107b9eee.png",
                            "./screenshots-images-2/chapter_4/section_1/1aaa6a2e-6fef-44ec-8acd-5045e1368d1b.png",
                            "./screenshots-images-2/chapter_4/section_1/7b8fbf25-69a3-41ff-9923-00bbcb4cbfdf.png",
                            "./screenshots-images-2/chapter_4/section_1/cd423725-93bc-4471-8147-c5b1034a7a4f.png",
                            "./screenshots-images-2/chapter_4/section_1/4bd4c639-fe97-495a-a40c-8ef2987116cb.png",
                            "./screenshots-images-2/chapter_4/section_1/17f1dc8f-711b-4b39-91b7-e00f226c0e7f.png",
                            "./screenshots-images-2/chapter_4/section_1/24ee3ec8-5338-4fbc-a411-bbafd3ca5aa6.png",
                            "./screenshots-images-2/chapter_4/section_1/1b50c1de-5242-4493-b4a1-5448a71aa5c2.png",
                            "./screenshots-images-2/chapter_4/section_1/bad9bae6-eac1-489d-ae59-51261f45db9f.png",
                            "./screenshots-images-2/chapter_4/section_1/9316c213-6ad6-44b7-8096-0f712d2c5e2d.png",
                            "./screenshots-images-2/chapter_4/section_1/2008725f-4b75-4bad-8ea7-304af5667fb3.png",
                            "./screenshots-images-2/chapter_4/section_1/3b2e9561-fad1-4b0c-9182-cb866503f863.png",
                            "./screenshots-images-2/chapter_4/section_1/7d230c9b-71cc-44d1-bdd2-63a4cc61c28d.png",
                            "./screenshots-images-2/chapter_4/section_1/4e657a4c-182f-401f-b714-ccd2e3da1cb8.png",
                            "./screenshots-images-2/chapter_4/section_1/1d42840c-ff7b-402b-9d75-e63584c06705.png",
                            "./screenshots-images-2/chapter_4/section_1/8203cc1c-25fe-4be4-bbbf-046d757071e3.png",
                            "./screenshots-images-2/chapter_4/section_1/adfe1c21-2000-4a85-802f-8125c1ad4530.png",
                            "./screenshots-images-2/chapter_4/section_1/b130ee9f-97ab-400d-9a90-144086aee746.png",
                            "./screenshots-images-2/chapter_4/section_1/f8e97cd6-a281-41e4-b577-4b7c5e2a3c8a.png",
                            "./screenshots-images-2/chapter_4/section_1/f5d43338-0c48-452a-882b-2b8478a9efe9.png",
                            "./screenshots-images-2/chapter_4/section_1/89fd6ea9-c1ab-4557-8c35-4d182ecaa638.png",
                            "./screenshots-images-2/chapter_4/section_1/652a1304-c4a0-40f6-a918-f5326adf6932.png",
                            "./screenshots-images-2/chapter_4/section_1/ff663d3a-6cd1-409c-80dd-9e1010ab2690.png",
                            "./screenshots-images-2/chapter_4/section_1/fd6da393-cdd4-492a-ba82-8ed15af3cb5b.png",
                            "./screenshots-images-2/chapter_4/section_1/24288b16-54fd-4dbc-9bfe-81fc12e462d7.png",
                            "./screenshots-images-2/chapter_4/section_1/77040ee7-e2a3-4d3d-b7b6-4a2f0bdd1df8.png",
                            "./screenshots-images-2/chapter_4/section_1/818ad098-b833-49ed-b0c5-9b4f1d868183.png",
                            "./screenshots-images-2/chapter_4/section_1/1238c617-8e92-495e-b697-3e4f5215cec9.png",
                            "./screenshots-images-2/chapter_4/section_1/a642cc48-13c6-4741-9b58-fb3cacf53bf7.png",
                            "./screenshots-images-2/chapter_4/section_1/50b895f8-90f5-4b40-8042-6c9c5306733c.png",
                            "./screenshots-images-2/chapter_4/section_1/c5295255-d6e1-4cd7-8e0a-f3f0192b6229.png",
                            "./screenshots-images-2/chapter_4/section_1/6d3e0827-fc01-47e9-84cb-469c6dc87280.png",
                            "./screenshots-images-2/chapter_4/section_1/1f4eba37-03f4-4a8e-83e4-cfbc056baca8.png",
                            "./screenshots-images-2/chapter_4/section_1/c06aa0cd-ea06-4d98-82db-ed31a1a62c70.png",
                            "./screenshots-images-2/chapter_4/section_1/e1d144b9-ade1-466c-acc8-8b0d8228b0a6.png",
                            "./screenshots-images-2/chapter_4/section_1/0154e2f5-c376-4790-851b-3512fbe349dc.png",
                            "./screenshots-images-2/chapter_4/section_1/3e3f58b3-f328-4305-879b-a88d5b85af6f.png",
                            "./screenshots-images-2/chapter_4/section_1/46ad1318-bcc0-4b3b-9368-8d30aede31fd.png",
                            "./screenshots-images-2/chapter_4/section_1/cd0fae87-7855-4a7a-8a92-b7c44b389aa9.png",
                            "./screenshots-images-2/chapter_4/section_1/732179c5-a6f8-49b0-85d4-bc2cdedd00a9.png",
                            "./screenshots-images-2/chapter_4/section_1/48122da2-34c6-42da-bbaa-deba6f28f366.png",
                            "./screenshots-images-2/chapter_4/section_1/5c00fd54-0af6-4eb3-b41f-e248bbf1496e.png",
                            "./screenshots-images-2/chapter_4/section_1/aa369f0f-5f0d-458b-93ba-2ecd9c2613bf.png",
                            "./screenshots-images-2/chapter_4/section_1/b39cadfe-c3ce-4c19-bd91-66acce4b3ffc.png",
                            "./screenshots-images-2/chapter_4/section_1/79210b12-b106-4b4e-b415-dcf522ad4791.png",
                            "./screenshots-images-2/chapter_4/section_1/1f551f0d-68fa-44cc-a68b-556d4a6b57b4.png",
                            "./screenshots-images-2/chapter_4/section_1/4d5dd92c-aff9-47f3-ac3c-91431b9893a9.png",
                            "./screenshots-images-2/chapter_4/section_1/406a5d56-b0a2-4205-bb92-a188c62a9d23.png",
                            "./screenshots-images-2/chapter_4/section_1/ca6a1243-60f3-4c46-81aa-31cf466fb185.png",
                            "./screenshots-images-2/chapter_4/section_1/302fd4f4-af4f-411a-aad8-25bb6a05cc06.png",
                            "./screenshots-images-2/chapter_4/section_1/ceeb72be-a8f0-4e54-b9d4-77adfa0c664e.png",
                            "./screenshots-images-2/chapter_4/section_1/7ac44655-d1c9-4daa-ab43-94479aca2f73.png",
                            "./screenshots-images-2/chapter_4/section_1/c52a0d26-7ed8-44ec-af25-e77e9da2d3f1.png",
                            "./screenshots-images-2/chapter_4/section_1/67f9997e-c57b-4403-b2f6-8d4ba0c840b9.png",
                            "./screenshots-images-2/chapter_4/section_1/e2c679a5-824c-497e-abc8-c44043bf1166.png",
                            "./screenshots-images-2/chapter_4/section_1/09e1bc6b-b61e-42cb-a092-83e45a73972a.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In Chapter 3, we discussed how to create training data for your model and\nin Chapter 4, we discussed how to engineer features from that training data.\nWith the initial set of features, we can move to the ML algorithm part of\nML systems, which is covered in this chapter. Many ML engineers consider\nthe fun part. This is when you can see your data being transformed into\nintelligent systems that generate predictions that you can play around with.\n\nI expect that most readers already have an understanding of common ML\nalgorithms such as decision trees, K-nearest neighbors, and different types\nof neural networks. This chapter will discuss techniques surrounding\nalgorithms but won\u2019t explain these algorithms. Because this chapter deals\nwith ML algorithms, it requires a lot more ML knowledge than other\nchapters. If you\u2019re not familiar with them, I recommend taking an online\ncourse or reading a book on ML algorithms before reading this chapter.\nReaders wanting a quick refresh on basic ML concepts might find the\nsection Basic ML Reviews in Appendix helpful.\n\nBefore using ML algorithms to solve your problem, you first need to frame\nyour problem into a task that ML can solve. We\u2019ll start this chapter with\nhow to frame your ML problems. Once you\u2019ve had an ML problem, you\nmight want to develop an ML model to tackle it. The section that follows\ndiscusses different aspects of model development, including model\nselection, debugging, experiment tracking and versioning. It\u2019l| also cover\nmore advanced techniques including ensembling, distributed training, and\nAutoML.\n\nModel development is an iterative process. After each iteration, you'll want\nto compare your model\u2019s performance against the performance of the\nprevious iterations and evaluate how suitable this iteration is for production.\nThe last section of this chapter is dedicated to how to evaluate your model\noffline before deploying it to production.\n\nFraming ML Problems\n\nImagine you\u2019re an ML engineering tech lead at a bank that targets\nmillennial users. One day, your boss hears about a rival bank that uses ML\nto speed up their customer service support that supposedly helps the rival\nbank process their customer requests 2x faster. He demands that your team\nlook into using ML to speed up your customer service support too.\n\nSlow customer support is a problem, but it\u2019s not an ML problem. An ML\nproblem is defined by inputs, outputs, and the objective function that guides\nthe learning process \u2014 neither of these three components are obvious from\nyour boss\u2019s request. It\u2019s your job, as a seasoned ML engineer, to use your\nknowledge of what problems ML can solve to frame this request into an\nML problem.\n\nYou discover that the bottleneck in responding to customer support lies in\nrouting customer requests to the right department \u2014 say there are 4\ndepartments that handle customer requests: accounting, inventory, HR, and\nIT. You can alleviate this bottleneck by developing an ML model to predict\nwhich of these 4 departments a request should go to. This makes it a\nclassification problem. The input is the customer request. The output is the\ndepartment the request should go to. The objective function is to minimize\nthe difference between the predicted department and the actual department.\n\nWe\u2019ve discussed intensively how to extract features from raw data to input\ninto your ML model in Chapter 4. In this section, we\u2019ll focus on two\naspects: the output of your model and the objective function that guides the\nlearning process.\n\nTypes of ML Tasks\n\nThe output of your model dictates the task type of your ML problem. The\nmost general types of ML tasks are classification and regression. Within\nclassification, there are more subtypes, as shown in Figure 5-1. We\u2019ll go\nover each of these task types in this section.\n\nTask type\n\n/\\\n\nRegression Classification\n\n|\n\nBinary Multiclass  Muttilabel\n\nLow cardinality High cardinality\n\nFigure 5-1. Common task types in ML.\n\nClassification vs. Regression\n\nClassification models classify inputs into different categories. For example,\nyou want to classify each email to be either spam or not spam. Regression\nmodels output a continuous value. An example is a house prediction model\nthat outputs the price of a given house.\n\nA regression model can easily be framed as a classification model and vice\nversa. For example, house prediction can become a classification task if we\nquantize the house prices into buckets such as under $100,000, $100,000 -\n200,000, $200,000 - 500,000, ... and predict the bucket the house should be\nin. The email classification model can become a regression model if we\nmake it output values between 0 and 1, and use a threshold to determine\nwhich values should be SPAM (for example, if the value is above 0.5, the\nemail is spam).\n\nBinary vs. Multiclass Classification\n\nWithin classification problems, the fewer classes there are to classify, the\nsimpler the problem is. The simplest is binary classification where there\nare only two possible classes. Examples of binary classification include\nclassifying whether a comment is toxic or not toxic, whether a lung scan\nshows signs of cancer or not, whether a transaction is fraudulent or not. It\u2019s\nunclear whether this type of problem is common because they are common\nin nature or simply because ML practitioners are most comfortable handling\nthem.\n\nWhen there are more than two classes, the problem becomes multiclass\nclassification. Dealing with binary classification problems is much easier\nthan dealing with multiclass problems. For example, calculating F1 and\nvisualizing confusion matrices are a lot more intuitive and easier to\nunderstand when there are only two classes than when there are more than\ntwo classes.\n\nWhen the number of classes is high, such as disease diagnosis where the\nnumber of diseases can go up to thousands or product classifications where\nthe number of products can go up to tens of thousands, we say the\nclassification task has high cardinality. High cardinality problems can be\nvery challenging. The first challenge is in data collection. In my experience,\nML models typically need at least 100 examples for each class to learn to\nclassify that class. So if you have 1000 classes, you already need at least\n100,000 examples. The data collection can be especially difficult when\nsome of the classes are rare. When you have thousands of classes, it\u2019s likely\nthat some of them are rare.\n\nWhen the number of classes is large, hierarchical classification might be\nuseful. In hierarchical classification, you have a classifier to first classify\neach example into one of the large groups. Then you have another classifier\nto classify this example into one of the subgroups. For example, for product\nclassification, you can first classify each product into one of the four main\ncategories: electronics, home & kitchen, fashion, and pet supplies. After a\nproduct has been classified into a category, say fashion, you can use another\nclassifier to put this product into one of the subgroups: shoes, shirt, jeans,\naccessories.\n\nMulticlass vs. Multilabel Classification\n\nIn both binary and multiclass classification, each example belongs to\nexactly one class. When an example can belong to multiple classes, we\nhave a multilabel classification problem. For example, when building a\nmodel to classify articles into four topics: [tech, entertainment, finance,\npolitics], an article can be in both tech and finance.\n\nThere are two major approaches to multilabel classification problems. The\nfirst is to treat it as you would a multiclass classification. In multiclass\nclassification, if there are four possible classes [tech, entertainment, finance,\npolitics] and the label for an example is entertainment, you represent this\nlabel with the vector [0, 1, 0, 0]. In multilabel classification, if an example\nhas both labels entertainment and finance, its label will be represented as [0,\n1, 1, 0).\n\nThe second approach is to turn it into multiple binary classification\nproblems. For the article classification problem above, you can have four\nmodels corresponding to four topics, where each model outputs whether an\narticle is in that topic or not.\n\nOut of all task types, multilabel classification is usually the task type that\nI\u2019ve seen companies having the most problems with. Multilabel means that\nthe number of classes an example can have varies from example to\nexample. First, this makes it difficult for label annotation since it increases\nthe label multiplicity problem that we discussed in Chapter 3. For example,\nan annotator might believe an example belongs to 2 classes while another\nannotator might believe the same example to belong in only 1 class, and it\nmight be difficult resolving their disagreements.\n\nSecond, this varying number of classes makes it hard to extract predictions\nfrom raw probability. Consider the same task of classifying articles into\nfour topics above. Imagine that given an article, your model outputs this\nraw probability distribution: [0.45, 0.2, 0.02, 0.33]. In the multiclass setting,\nyou know that an example can belong to only one category, you simply pick\nthe category with the highest probability, which is 0.45 in this case. In the\nmultilabel setting, because you don\u2019t know how many categories an\nexample can belong to, you might pick the two highest probability\ncategories (corresponding to 0.45 and 0.33) or three highest probability\ncategories (corresponding to 0.45, 0.2, and 0.33).\n\nMultiple Ways to Frame a Problem\n\nChanging the way you frame your problem might make your problem\nsignificantly harder or easier. Consider the task of predicting what app a\nphone user wants to use next. A naive setup would be to frame this as a\nmulticlass classification task \u2014 use the user\u2019s and environment\u2019s features\n(user demographic information, time, location, previous apps used) as input\nand output a probability distribution for every single app on the user\u2019s\nphone. Let N be the number of apps you want to consider recommending to\na user. In this framing, for a given user at a given time, there is only\nprediction to make, and the prediction is a vector of the size N.\n\nProblem: predict the app users will most lkely open next\n\nClassification\n\nINPUT 9072 015.067 | us\n\n~~ \u2014_\nUser's features Environment\ntime, location, ee,\n\nQUIPUT = gq\n\nFigure 5-2. Given the problem of predicting the app a user will most likely open next, you can frame\nit as a classification problem. The input is the users features and environment\u2019 features. The output\nis a distribution over all apps on the phone.\n\nThis is a bad approach because whenever a new app is added, you might\nhave to retrain from scratch your model, or at least all the components of\nyour model whose number of parameters depends on N. A better approach\nis to frame this as a regression task. The input is the user\u2019s, the\nenvironment\u2019s, and the app\u2019s features. The output is a single value between\n0 and 1, the higher the value, the more likely the user will open the app\ngiven the context. In this framing, for a given user at a given time, there are\nN predictions to make, one for each app, but each prediction is just a\nnumber.\n\nProblem rei heap ser wl most ely open net\n\nReaesin\n\nwer) a. (al\nUses Eonmett Ag eyes\ntie can at.\nINPUTS CO OT Oe (ls Ap!\nWAT. WO. O Is ip.\n\nFigure 5-3. Given the problem of predicting the app a user will most likely open next, you can frame\nit as a regression problem. The input is the users features, environments features, and an apps\n\nfeatures. The output is a single value between 0 and I denoting how likely the user will open the app\ngiven the context.\n\nIn this new framing, whenever there\u2019s a new app you want to consider\nrecommending to a user, you simply need to use new inputs with this new\napp\u2019s feature instead of having to retrain your model or part of your model\nfrom scratch.\n\nObjective Functions\n\nTo learn, an ML model needs an objective function to guide the learning\nprocess. An objective function is also called a loss function, because the\nobjective of the learning process is usually to minimize (or optimize) the\nloss caused by wrong predictions made by the model. For supervised ML,\nthis loss can be computed by comparing the model\u2019s outputs with the\nground truth labels using a measurement like Root Mean Squared Error\n(RMSE) or Cross Entropy.\n\nTo illustrate this point, let\u2019s again go back to the above task of classifying\narticles into four topics [tech, entertainment, finance, politics]. Consider an\narticle that belongs to the politics class, e.g. its ground truth label is [0, 0, 0,\n1]. Imagine that given this article, your model outputs this raw probability\ndistribution: [0.45, 0.2, 0.02, 0.33]. The cross entropy loss of this model,\ngiven this example, is the cross entropy of [0.45, 0.2, 0.02, 0.33] relative to\n[0, 0, 0, 1]. In Python, you can calculate cross entropy with the following\ncode.\n\nimport numpy as np\n\ndef cross_entropy(p, q):\n\nreturn -sum([p[i] * np.log(q[i]) for i in range(len(p))])\np= [0, 0, 0, 1)\n\naq = (0.45, 0.2, 0.02, 0.33]\n\ncross_entropy(p, q)\n\nChoosing an objective function is usually straightforward. It isn\u2019t because\nobjective functions are easy. Coming up with meaningful objective\nfunctions requires algebra knowledge, so most ML engineers just use\ncommon loss functions like RMSE or MAE (Mean Absolute Error) for\n\nregression, Logistic Loss (also Log Loss) for binary classification, and\nCross Entropy for multiclass classification.\nDecoupling Objectives\n\nFraming ML problems can be tricky when you want to minimize multiple\nobjective functions. Imagine you\u2019re building a system to rank items on\nusers\u2019 newsfeed. Your original goal is to maximize users\u2019 engagement. You\nwant to achieve this goal through the following three objectives.\n\n1. Filter out spam\n2. Filter out NSFW content\n3. Rank posts by engagement: how likely users will click on it\n\nHowever, you quickly learned that optimizing for users\u2019 engagement alone\ncan lead to questionable ethical concerns. Because extreme posts tend to get\nmore engagements, your algorithm learned to prioritize extreme content'.\nYou want to create a more wholesome newsfeed. So you have a new goal:\nmaximize users\u2019 engagement while minimizing the spread of extreme\nviews and misinformation. To obtain this goal, you add two new\nobjectives to your original plan.\n\n1. Filter out spam\n\n2. Filter out NSFW content\n\n3. Filter out misinformation\n\n4, Rank posts by quality\n\n5. Rank posts by engagement: how likely users will click on it\n\nNow, objectives 4 and 5 are in conflict with each other. If a post is very\nengaging but it\u2019s of questionable quality, should that post rank high or low?\n\nAn objective is represented by an objective function. To rank posts by\nquality, you first need to predict posts\u2019 quality and you want posts\u2019\npredicted quality to be as close to their actual quality as possible.\n\nEssentially, you want to minimize quality_loss: the difference between\neach post\u2019s predicted quality and its true quality\u00b0.\n\nSimilarly, to rank posts by engagement, you first need to predict the number\nof clicks each post will get. You want to minimize engagement_loss: the\ndifference between each post\u2019s predicted clicks and its actual number of\nclicks.\n\nOne approach is to combine these two losses into one loss and train one\nmodel to minimize that loss.\n\nloss = a quality_loss + B engagement_loss\n\nYou can randomly test out different values of a and f to find the values that\nwork best. If you want to be more systematic about tuning these values, you\ncan check out Pareto optimization, \u201can area of multiple criteria decision\nmaking that is concerned with mathematical optimization problems\ninvolving more than one objective function to be optimized\nsimultaneously\u201d.\n\nA problem with this approach is that each time you tune a and 6 \u2014 for\nexample, if your users\u2019 newsfeed\u2019s quality goes up but users\u2019 engagement\ngoes down, you might want to decrease a and increase # \u2014 you\u2019ll have to\nretrain your model.\n\nAnother approach is to train two different models, each optimizing one loss.\nSo you have two models:\n\n* quality_model minimizes quality_loss and outputs the predicted\nquality of each post.\n\n* engagement_model minimizes engagement_loss and outputs the\npredicted number of clicks of each post.\n\nYou can combine the outputs of these two models and rank posts by their\ncombined scores:\n\na quality_score + 8 engagement_score\n\nNow you can tweak @ and \u00a3 without retraining your models!\n\nIn general, when there are multiple objectives, it\u2019s a good idea to decouple\nthem first because it makes model development and maintenance easier.\nFirst, it\u2019s easier to tweak your system without retraining models, as\nexplained above. Second, it\u2019s easier for maintenance since different\nobjectives might need different maintenance schedules. Spamming\ntechniques evolve much faster than the way post quality is perceived, so\nspam filtering systems need updates at a much higher frequency than\nquality ranking systems.\n\nModel Development and Training\n\nIn this section, we\u2019ll discuss necessary aspects to help you develop and train\nyour model, including how to evaluate different ML models for your\nproblem, creating ensembles of models to solve your problem, experiment\ntracking and versioning, and distributed training, which is necessary for the\nscale at which models today are usually trained at. We\u2019ll end this section\nwith the more advanced topic of AutoML \u2014 using ML to automatically\nchoose a model best for your problem.\n\nEvaluating ML Models\n\nDeveloping a model starts with selecting the right model for your problem.\nThere are many possible solutions to any given problem, both ML solutions\nand non-ML solutions. Should you start with the good old logistic\nregression? You've heard of a fancy new model that is supposed to be the\nnew state-of-the-art for your problem, should you spend two weeks learning\nthat process then three months implementing it? Should you try an\nensemble of various decision trees?\n\nIf you had unlimited time and compute power, the rational thing to do\nwould be to try all possible solutions and see what is best for you. However,\ntime and compute power are limited resources, and you have to be strategic\nabout what models we select.\n\nWhen thinking about ML algorithms, many people think of classical ML\nalgorithms versus neural networks. There are a lot of interests in neural\nnetworks, especially in deep learning, which is understandable given that\nmost of the AI progress in the last decade is due to neural networks getting\nbigger and deeper.\n\nMany newcomers to the field that I\u2019ve talked to think that deep learning is\nreplacing classical ML algorithms. However, even though deep learning is\nfinding more use cases in production, classical ML algorithms are not going\naway. Many recommendation systems still rely on collaborative filtering\nand matrix factorization. Tree-based algorithms, including gradient-boosted\ntrees, still power many classification tasks with strict latency requirements.\n\nEven in applications where neural networks are deployed, classic ML\nalgorithms are still being used in tandem, either in an ensemble or to help\nextract features to feed into neural networks.\n\nWhen selecting a model for your problem, you don\u2019t choose from every\npossible model out there, but usually focus on a set of models suitable for\nyour problem. For example, if your boss tells you to build a system to\ndetect toxic tweets, you know that this is a text classification problem \u2014\ngiven a piece of text, classify whether it\u2019s toxic or not \u2014 and common\nmodels for text classification include Naive Bayes, Logistic Regression,\nrecurrent neural networks, Transformer-based models such as BERT, GPT,\nand their variants.\n\nIf your client wants you to build a system to detect fraudulent transactions,\nyou know that this is the classic abnormality detection problem \u2014\nfraudulent transactions are abnormalities that you want to detect \u2014 and\ncommon algorithms for this problem are many, including k-nearest\nneighbors, isolation forest, clustering, and neural networks.\n\nKnowledge of common ML tasks and the typical approaches to solve them\nis essential in this process.\n\nDifferent types of algorithms require different amounts of labels as well as\ndifferent amounts of compute power. Some take longer to train than others,\nwhile some take longer to make predictions. Non-neural network\n\nNR\n\nthat these models would be the best solutions for their problems \u2014\nwhy try an old solution if you believe that a newer and superior\nsolution exists? Many business leaders also want to use SOTA\nmodels because they want to use them to make their businesses\nappear cutting-edge. Developers might also be more excited\ngetting their hands on new models than getting stuck into the same\nold things over and over again.\n\nResearchers often only evaluate models in academic settings,\nwhich means that a model being SOTA often only means that it\nperforms better than existing models on some static datasets. It\ndoesn\u2019t mean that this model will be fast enough or cheap enough\nfor you to implement in your case. It doesn\u2019t even mean that this\nmodel will perform better than other models on your data.\n\nWhile it\u2019s essential to stay up-to-date to new technologies and\nbeneficial to evaluate them for your businesses, the most important\nthing to do when solving a problem is finding solutions that can\nsolve that problem. If there\u2019s a solution that can solve your\nproblem that is much cheaper and simpler than SOTA models, use\nthe simpler solution.\n\n. Start with the simplest models\n\nZen of Python states that \u201csimple is better than complex\u201d, and this\nprinciple is applicable to ML as well. Simplicity serves three\npurposes. First, simpler models are easier to deploy, and deploying\nyour model early allows you to validate that your prediction\npipeline is consistent with your training pile. Second, starting with\nsomething simple and adding more complex components step-by-\nstep makes it easier to understand your model and debug it. Third,\nthe simplest model serves as a baseline to which you can compare\nyour more complex models.\n\nSimplest models are not always the same as models with the least\neffort. For example, pretrained BERT models are complex, but\nthey require little effort to get started with, especially if you use a\n\nhad\n\nready-made implementation like the one in HuggingFace\u2019s\nTransformer. In this case, it\u2019s not a bad idea to use the complex\nsolution, given that the community around this solution is well-\ndeveloped enough to help you get through any problems you might\nencounter. However, you might still want to experiment with\nsimpler solutions, if you haven\u2019t already, to make sure that\npretrained BERT is indeed better than those simpler solutions for\nyour problem.\n\nAvoid human biases in selecting models\n\nImagine an engineer on your team is assigned the task of\nevaluating which model is better for your problem: a gradient\nboosted tree or a pretrained BERT model. After two weeks, this\nengineer announced that the best BERT model outperforms the\nbest gradient boosted tree by 5%. Your team decides to go with the\npretrained BERT model.\n\nA few months later, however, a seasoned engineer joins your team.\nShe decides to look into gradient boosted trees again and finds out\nthat this time, the best gradient boosted tree outperforms the\npretrained BERT model you currently have in production. What\nhappened?\n\nThere are a lot of human biases in evaluating models. Part of the\nprocess of evaluating an ML architecture is to experiment with\ndifferent features and different sets of hyperparameters to find the\nbest model of that architecture. If an engineer is more excited\nabout an architecture, she will likely spend a lot more time\nexperimenting with it, which might result in better performing\nmodels for that architecture.\n\nWhen comparing different architectures, it\u2019s important to compare\nthem under comparable setups. If you run 100 experiments for an\narchitecture, it\u2019s not fair to only run a couple of experiments for the\narchitecture you\u2019re evaluating it against. You also want to run 100\nexperiments for the other architectures too.\n\nhad\n\nready-made implementation like the one in HuggingFace\u2019s\nTransformer. In this case, it\u2019s not a bad idea to use the complex\nsolution, given that the community around this solution is well-\ndeveloped enough to help you get through any problems you might\nencounter. However, you might still want to experiment with\nsimpler solutions, if you haven\u2019t already, to make sure that\npretrained BERT is indeed better than those simpler solutions for\nyour problem.\n\nAvoid human biases in selecting models\n\nImagine an engineer on your team is assigned the task of\nevaluating which model is better for your problem: a gradient\nboosted tree or a pretrained BERT model. After two weeks, this\nengineer announced that the best BERT model outperforms the\nbest gradient boosted tree by 5%. Your team decides to go with the\npretrained BERT model.\n\nA few months later, however, a seasoned engineer joins your team.\nShe decides to look into gradient boosted trees again and finds out\nthat this time, the best gradient boosted tree outperforms the\npretrained BERT model you currently have in production. What\nhappened?\n\nThere are a lot of human biases in evaluating models. Part of the\nprocess of evaluating an ML architecture is to experiment with\ndifferent features and different sets of hyperparameters to find the\nbest model of that architecture. If an engineer is more excited\nabout an architecture, she will likely spend a lot more time\nexperimenting with it, which might result in better performing\nmodels for that architecture.\n\nWhen comparing different architectures, it\u2019s important to compare\nthem under comparable setups. If you run 100 experiments for an\narchitecture, it\u2019s not fair to only run a couple of experiments for the\narchitecture you\u2019re evaluating it against. You also want to run 100\nexperiments for the other architectures too.\n\nBecause the performance of a model architecture depends heavily\non the context it\u2019s evaluated in \u2014 e.g. the task, the training data,\nthe test data, the hyperparameters, etc. \u2014 it\u2019s extremely difficult to\nmake claims that a model architecture is better than another\narchitecture. The claim might be true in a context, but unlikely true\nfor all possible contexts.\n\n. Evaluate good performance now vs. good performance later\n\nThe best model now doesn\u2019t always mean the best model two\nmonths from now on. For example, a tree-based model might work\nbetter now because you don\u2019t have a ton of data yet, but two\nmonths from now, you might be able to double your amount of\ntraining data, and your neural network might perform much\nbetter5.\n\nA simple way to estimate how your model\u2019s performance might\nchange with more data is to use learning curves. A learning curve\nof a model is a plot of its performance \u2014 e.g. training loss, training\naccuracy, validation accuracy \u2014 against the number of training\nsamples it uses, as shown in Figure 5-4. The learning curve won\u2019t\nhelp you estimate exactly how much performance gain you can get\nfrom having more training data, but it can give you a sense of\nwhether you can expect any performance gain at all from more\ntraining data.\n\n{eaming Curves (Nave Bayes) Leaming Curves (SV, RBF kee, = 0,00)\n> Crosalaton score\n\n04 0%\n\nOH;\n\n154 Oy\n\n+ Teng se\n+ (iosvldation soe\n\nu-+\u2014\u2014\u2014:\u00abSON\n20 400 600 800 1000 L200 1400 200 40 600 800 1000 LO 1400\nTearing eranples Tring exangles\n\nFigure 5-4. The learning curves of a Naive Bayes model and an SVM model. Example\nfrom scikit-learn.\n\nwn\n\nA situation that I\u2019ve encountered is when a team evaluates a simple\nneural network against a collaborative filtering model for making\nrecommendations. When evaluating both models offline, the\ncollaborative filtering model outperformed. However, the simple\nneural network can update itself with each incoming example\nwhereas the collaborative filtering has to look at all the data to\nupdate its underlying matrix. The team decided to deploy both the\ncollaborative filtering model and the simple neural network. They\nused the collaborative filtering model to make predictions for\nusers, and continually trained the simple neural network in\nproduction with new, incoming data. After two weeks, the simple\nneural network was able to outperform the collaborative filtering\nmodel\u00ae.\n\nWhile evaluating models, you might want to take into account their\npotential for improvements in the near future, and how\neasy/difficult it is to achieve those improvements.\n\n. Evaluate trade-offs\n\nThere are many tradeoffs you have to make when selecting models.\nUnderstanding what\u2019s more important in the performance of your\nML system will help you choose the most suitable model.\n\nOne classic example of tradeoff is the false positives and false\nnegatives tradeoff. Reducing the number of false positives might\nincrease the number of false negatives, and vice versa. In a task\nwhere false positives are more dangerous than false negatives, such\nas fingerprint unlocking (unauthorized people shouldn\u2019t be\nclassified as authorized and given access), you might prefer a\nmodel that makes less false positives. Similarly, in a task where\nfalse negatives are more dangerous than false positives, such as\ncovid screening (patients with covid shouldn\u2019t be classified as no\ncovid), you might prefer a model that makes less false negatives.\n\nAnother example of tradeoff is compute requirement and accuracy\n\u2014a more complex model might deliver higher accuracy but might\n\nrequire a more powerful machine, such as a GPU instead of a CPU,\nto generate predictions with acceptable inference latency. Many\npeople also care about the interpretability and performance\ntradeoff. A more complex model can give a better performance but\nits results are less interpretable.\n\n. Understand your model\u2019s assumptions\n\nThe statistician George Box said in 1976 that \u201call models are\nwrong, but some are useful\u201d. The real world is intractably\ncomplex, and models can only approximate using assumptions.\nEvery single model comes with its own assumptions.\nUnderstanding what assumptions a model makes and whether our\ndata satisfies those assumptions can help you evaluate which\nmodel works best for your use case.\n\nBelow are some of the common assumptions. It\u2019s not meant to be\nan exhaustive list, but just a demonstration.\n\n* Prediction assumption: every model that aims to predict\nan output Y from an input X makes the assumption that\nit\u2019s possible to predict Y based on X.\n\nIID: Neural networks assume that the examples are\nindependent and identically distributed, which means that\nall the examples are independently drawn from the same\njoint distribution.\n\nSmoothness: Every supervised machine learning method\nassumes that there\u2019s a set of functions that can transform\ninputs into outputs such that similar inputs are\ntransformed into similar outputs. If an input X produces\nan output Y, then an input close to X would produce an\noutput proportionally close to Y.\n\nTractability: Let X be the input and Z be the latent\nrepresentation of X. Every generative model makes the\n\nassumption that it\u2019s tractable to compute the probability\nP(Z)|X).\n\u00a2 Boundaries: A linear classifier assumes that decision\n\nboundaries are linear.\n\n* Conditional independence: A Naive Bayes classifier\nassumes that the attribute values are independent of each\nother given the class.\n\nNormally distributed: many statistical methods assume\nthat data is normally distributed.\n\nEnsembles\n\nWhen considering an ML solution to your problem, you might want to start\nwith a system that contains just one model, and the process of selecting one\nmodel for your problem is discussed above. After you\u2019ve deployed your\nsystem, you might think about how to continue improving its performance.\nOne method that has consistently given your system a performance boost is\nto use an ensemble of multiple models instead of just an individual model to\nmake predictions. Each model in the ensemble is called a base learner. For\nexample, for the task of predicting whether an email is SPAM or NOT\nSPAM, you might have 3 different models. The final prediction for each\nemail is the majority vote of all these three models. So if at least two base\nlearners output SPAM, the email will be classified as SPAM.\n\n20 out of 22 winning solutions on Kaggle competitions in 2021, as of\nAugust 2021, use ensembles\u2019. An example of an ensemble used for a\nKaggle competition is shown in Figure 5-5. As of January 2022, 20 top\nsolutions on SQUAD 2.0, the Stanford Question Answering Dataset, are\nensembles, as shown in Figure 5-6.\n\nResNext60 32x4d\nPretrained Imagenet +\nFinetuning\nva Stage {Models =\nviT\nPretrained Imagenet +\nFinetuning\nfn ;\nFinetuning gargnat)\nMobilenetV3\nPretrained CropNet\n\nFigure 5-5. The ensemble used in the top solution for the Cassava Leaf Disease Classification\ncompetition on Kaggle. Image by Jannis Hanke.\n\nEnsembling methods are less favored in production because ensembles are\nmore complex to deploy and harder to maintain. However, they are still\ncommon for tasks where a small performance boost can lead to a huge\nfinancial gain such as predicting click-through rate (CTR) for ads.\n\nRank Model\n\nHuman Performance\nStanford University\n(Rajpurkar & Jia et al. '18)\n\nIE-Net (ensemble)\nRICOH_SRCB_DML\n\nFPNet (ensemble)\nAnt Service Intelligence Team\n\nIE-NetV/2 (ensemble)\nRICOH_SRCB_DML\n\nSA-Net on Albert (ensemble)\nQIANXIN\n\nSA-Net-V2 (ensemble)\nQIANXIN\n\nRetro-Reader (ensemble)\n\nShanghai Jiao Tong University\nhttp://arxiv.org/abs/2001.09694\n5 FPNet (ensemble)\nYuYang\n6 TransNets + SFVerifier + SFEnsembler\nlense\n\nSenseforth Al Research\n\nEM\n\n86.831\n\n90.939\n\n90.871\n\n90.860\n\n90.724\n\n90.679\n\n90.578\n\n90.600\n\n90.487\n\nFi\n\n89.452\n\n93.183\n\n93.100\n\n93.011\n\n92.948\n\n92.978\n\n92.899\n\n92.894\n\nFigure 5-6. As of January 2022, top 20 solutions on SQuAD 2.0 are all ensembles.\n\nWe'll go over an example to give you the intuition of why ensembling\nworks. Imagine you have three email spam classifiers, each with an\naccuracy of 70%. Assuming that each classifier has an equal probability of\nmaking a correct prediction for each email, and that these three classifiers\nare not correlated, we\u2019ll show that by taking the majority vote of these three\nclassifiers, we can get an accuracy of 78.4%.\n\nFor each email, each classifier has a 70% chance of being correct. The\nensemble will be correct if at least 2 classifiers are correct. Table 5-1 shows\nthe probabilities of different possible outcomes of the ensemble given an\nemail. This ensemble will have an accuracy of 0.343 + 0.441 = 0.784, or\n78.4%.\n\nOnly 1 is correct (0.3 * 0.3 * 0.7) *3=0.189 Wrong\n\nNone is correct 0.3 * 0.3 * 0.3 =0.027 Wrong\n\nThis calculation only holds if the classifiers in an ensemble are\nuncorrelated. If all classifiers are perfectly correlated \u2014 all three of them\nmake the same prediction for every email \u2014 the ensemble will have the\nsame accuracy as each individual classifier. When creating an ensemble, the\nless correlation there is among base learners, the better the ensemble will\nbe. Therefore, it\u2019s common to choose very different types of models for an\nensemble. For example, you might create an ensemble that consists of one\ntransformer model, one recurrent neural network, and one gradient boosted\ntree.\n\nThere are three ways to create an ensemble: bagging to reduce variance,\nboosting to reduce bias, and stacking to help with generalization. Other than\nto help boost performance, according to several survey papers, ensemble\nmethods such as boosting and bagging, together with resampling, have\nshown to help with imbalanced datasets\u00ae, 9. We'll go over each of these\nthree methods, starting with bagging.\n\nBagging\n\nBagging, shortened from bootstrap aggregating, is designed to improve\nboth the training stability\u2019? and accuracy of ML algorithms. It reduces\nvariance and helps to avoid overfitting.\n\nGiven a dataset, instead of training one classifier on the entire dataset, you\nsample with replacement to create different datasets, called bootstraps, and\ntrain a classification or regression model on each of these bootstraps.\nSampling with replacement ensures that each bootstrap is independent from\nits peers. Figure 5-7 shows an illustration of bagging.\n\nIf the problem is classification, the final prediction is decided by the\nmajority vote of all models. For example, if 10 classifiers vote SPAM and 6\nmodels vote NOT SPAM, the final prediction is SPAM.\n\nIf the problem is regression, the final prediction is the average of all\nmodels\u2019 predictions.\n\nBagging generally improves unstable methods, such as neural networks,\nclassification and regression trees, and subset selection in linear regression.\nHowever, it can mildly degrade the performance of stable methods such as\nk-nearest neighbors\".\n\nOriginal Data\n\nBootstrapping\n\nClassifier Classifie e Aggregating\n\nEnsemble classifier Bagging\n\nFigure 5-7. Bagging illustration by Sirakorn\n\nA random forest is an example of bagging. A random forest is a collection\nof decision trees constructed by both bagging and feature randomness,\n\nwhere each tree can pick only from a random subset of features to use.\n\nBoosting\n\nBoosting is a family of iterative ensemble algorithms that convert weak\nlearners to strong ones. Each learner in this ensemble is trained on the same\nset of samples but the samples are weighted differently among iterations. As\na result, future weak learners focus more on the examples that previous\nweak learners misclassified. Figure 5-8 shows an illustration of boosting.\n\nN\n\nwe\n\nwn\n\nna\n\n. You start by training the first weak classifier on the original\n\ndataset.\n\n. Samples are reweighted based on how well the first classifier\n\nclassifies them, e.g. misclassified samples are given higher weight.\n\n. Train the second classifier on this reweighted dataset. Your\n\nensemble now consists of the first and the second classifiers.\n\n. Samples are weighted based on how well the ensemble classifies\n\nthem.\n\n. Train the third classifier on this reweighted dataset. Add the third\n\nclassifier to the ensemble.\nRepeat for as many iterations as needed.\n\nForm the final strong classifier as a weighted combination of the\nexisting classifiers \u2014 classifiers with smaller training errors have\nhigher weights.\n\n} hy 00g.\ntM \u201c \u2019 '\n\nvt Ne\n\nOrginal Data | Weighted data Weighted data\n\nonl\n\nv ; .\n00) 000 00\n\u201ci\n\n\nFigure 5-8. Boosting illustration by Sirakorn\n\nAn example of a boosting algorithm is Gradient Boosting Machine which\nproduces a prediction model typically from weak decision trees. It builds\nthe model in a stage-wise fashion like other boosting methods do, and it\ngeneralizes them by allowing optimization of an arbitrary differentiable loss\nfunction.\n\nXGBoost, a variant of GBM, used to be the algorithm of choice for many\nwinning teams of machine learning competitions. It\u2019s been used in a wide\nrange of tasks from classification, ranking, to the discovery of the Higgs\nBoson'?. However, many teams have been opting for LightGBM, a\ndistributed gradient boosting framework that allows parallel learning which\ngenerally allows faster training on large datasets.\n\nStacking\n\nStacking means that you train base learners from the training data then\ncreate a meta-learner that combines the outputs of the base learners to\noutput final predictions, as shown in Figure 5-9. The meta-learner can be as\nsimple as a heuristic: you take the majority vote (for classification tasks) or\nthe average vote (for regression tasks) from all base learners. It can be\nanother model, such as a logistic regression model or a linear regression\nmodel.\n\nFigure 5-9. A visualization of a stacked ensemble from 3 base learners\n\nFor more great advice on how to create an ensemble, refer to this awesome\nensemble guide by one of Kaggle\u2019s legendary team MLWave.\n\nExperiment Tracking and Versioning\n\nDuring the model development process, you often have to experiment with\nmany architectures and many different models to choose the best one for\nyour problem. Some models might seem similar to each other and differ in\nonly one hyperparameter \u2014 such as one model uses the learning rate of\n0.003 while the other model uses the learning rate of 0.002 \u2014 and yet their\nperformances are dramatically different. It\u2019s important to keep track of all\nthe definitions needed to recreate an experiment and its relevant artifacts.\nAn artifact is a file generated during an experiment \u2014 examples of artifacts\ncan be files that show the loss curve, evaluation loss graph, logs, or\nintermediate results of a model throughout a training process. This enables\nyou to compare different experiments and choose the one best suited for\nyour needs. Comparing different experiments can also help you understand\nhow small changes affect your model\u2019s performance, which, in turn, gives\nyou more visibility into how your model works.\n\nThe process of tracking the progress and results of an experiment is called\nexperiment tracking. The process of logging all the details of an experiment\nfor the purpose of possibly recreating it later or comparing it with other\nexperiments is called versioning. These two go hand-in-hand with each\nother. Many tools originally set out to be experiment tracking tools, such as\nWeights & Biases, have grown to incorporate versioning. Many tools\noriginally set out to be versioning tools, such as DVC, have also\nincorporated experiment tracking.\n\nExperiment tracking\n\nA large part of training an ML model is babysitting the learning processes.\nMany problems can arise during the training process, including loss not\ndecreasing, overfitting, underfitting, fluctuating weight values, dead\nneurons, and running out of memory. It\u2019s important to track what\u2019s going on\nduring training not only to detect and address these issues but also to\nevaluate whether your model is learning anything useful.\n\nWhen I just started getting into ML, all I was told to track was loss and\nspeed. Fast forward several years, people are tracking so many things that\ntheir experiment tracking boards look both beautiful and terrifying at the\nsame time. Below is just a short list of things you might want to consider\ntracking for each experiment during its training process.\n\nThe loss curve corresponding to the train split and each of the eval\nsplits.\n\nThe model performance metrics that you care about on all non-\ntest splits, such as accuracy, F1, perplexity.\n\nThe speed of your model, evaluated by the number of steps per\nsecond or, if your data is text, the number of tokens processed per\nsecond.\n\nSystem performance metrics such as memory usage and\nCPU/GPU utilization. They\u2019re important to identify bottlenecks\nand avoid wasting system resources.\n\n* The values over time of any parameter and hyperparameter\nwhose changes can affect your model\u2019s performance, such as the\nlearning rate if you use a learning rate schedule, gradient norms\n(both globally and per layer) if you\u2019re clipping your gradient\nnorms, weight norm especially if you\u2019re doing weight decay.\n\nIn theory, it\u2019s not a bad idea to track everything you can. Most of the time,\nyou probably don\u2019t need to look at most of them. But when something does\nhappen, one or more of them might give you clues to understand and/or\ndebug your model. However, in practice, due to limitations of tooling today,\nit can be overwhelming to track too many things, and tracking less\nimportant things can distract you from tracking really important things.\n\nExperiment tracking enables comparison across experiments. By observing\nhow a certain change in a component affects the model\u2019s performance, you\ngain some understanding into what that component does.\n\nA simple way to track your experiments is to automatically make copies of\nall the code files needed for an experiment and log all outputs with their\ntimestamps\u2018*. However, using third-party experiment tracking tools can\ngive you nice dashboards and allow you to share your experiments with\nyour coworkers.\n\nVersioning\n\nImagine this scenario. You and your team spent the last few weeks tweaking\nyour model and one of the runs finally showed promising results. You\nwanted to use it for more extensive tests so you tried to replicate it using the\nset of hyperparameters you\u2019d noted down somewhere, only to find out that\nthe results weren\u2019t quite the same. You remembered that you\u2019d made some\nchanges to the code between that run and the next, so you tried your best to\nundo the changes from memory because your reckless past self had decided\nthat the change was too minimal to be committed. But you still couldn\u2019t\nreplicate the promising result because there are just too many possible ways\nto make changes.\n\nThis problem could have been avoided if you versioned your ML\nexperiments. ML systems are part code, part data so you need to not only\nversion your code but your data as well. Code versioning has more or less\nbecome a standard in the industry. However, at this point, data versioning is\nlike floss. Everyone agrees it\u2019s a good thing to do but few do it.\n\nThere are a few reasons why data versioning is challenging. One reason is\nthat because data is often much larger than code, we can\u2019t use the same\nstrategy that people usually use to version code to version data.\n\nFor example, code versioning is done by keeping track of all the changes\nmade to a codebase. A change is known as a diff, short for difference. Each\nchange is measured by line-by-line comparison. A line of code is usually\nshort enough for line-by-line comparison to make sense. However, a line of\nyour data, especially if it\u2019s stored in a binary format, can be indefinitely\nlong. Saying that this line of 1,000,000 characters is different from the other\nline of 1,000,000 characters isn\u2019t going to be that helpful.\n\nTo allow users to revert to a previous version of the codebase, code\nversioning tools do that by keeping copies of all the old files. However, a\ndataset used might be so large that duplicating it multiple times might be\nunfeasible.\n\nTo allow for multiple people to work on the same code base at the same\ntime, code versioning tools duplicate the code base on each person\u2019s local\nmachine. However, a dataset might not fit into a local machine.\n\nSecond, there\u2019s still confusion in what exactly constitutes a diff when we\nversion data. Would diffs mean changes in the content of any file in your\ndata repository, only when a file is removed or added, or when the\nchecksum of the whole repository has changed?\n\nAs of 2021, data versioning tools like DVC only register a diff if the\nchecksum of the total directory has changed and if a file is removed or\nadded.\n\nAnother confusion is in how to resolve merge conflicts: if developer 1 uses\ndata version X to train model A and developer 2 uses data version Y to train\n\nmodel B, it doesn\u2019t make sense to merge data versions X and Y to create Z,\nsince there\u2019s no model corresponding with Z.\n\nThird, if you use user data to train your model, regulations like GDPR\n\nmight make versioning this data complicated. For example, regulations\nmight mandate that you delete user data if requested, making it legally\n\nimpossible to recover older versions of your data.\n\nAggressive experiment tracking and versioning helps with reproducibility,\nbut doesn\u2019t ensure reproducibility. The frameworks and hardware you use\nmight introduce non-determinism to your experiment results'*, making it\nimpossible to replicate the result of an experiment without knowing\neverything about the environment your experiment runs in.\n\nThe way we have to run so many experiments right now to find the best\npossible model is the result of us treating ML as a blackbox. Because we\ncan\u2019t predict which configuration will work best, we have to experiment\nwith multiple configurations. However, I hope that as the field progresses,\nwe\u2019ll gain more understanding into different models and can reason about\nwhat model will work best instead of running hundreds or thousands of\nexperiments.\n\nDEBUGGING ML MODELS\n\nDebugging is an inherent part of developing any piece of software. ML\nmodels aren\u2019t an exception. Debugging is never fun, and debugging ML\nmodels can be especially frustrating for the following three reasons.\n\nFirst, ML models fail silently. When traditional software fails, you\nmight get some warnings such as crashes, runtime errors, 404.\nHowever, AI applications can fail silently. The code compiles. The loss\ndecreases as it should. The correct functions are called. The predictions\nare made, but the predictions are wrong. The developers don\u2019t notice\nthe errors. And worse, users don\u2019t either and use the predictions as if\nthe application was functioning as it should.\n\nSecond, even when you think you\u2019ve found the bug, it can be\nfrustratingly slow to validate whether the bug has been fixed. When\ndebugging a traditional software program, you might be able to make\nchanges to the buggy code and see immediately the result. However,\nwhen making changes to an ML model, you might have to retrain the\nmodel and wait until it converges to see whether the bug is fixed, which\ncan take hours. In some cases, you can\u2019t even be sure whether the bugs\nare fixed until the application is deployed to the users.\n\nThird, debugging ML models is hard because of their cross-functional\ncomplexity. There are many components in an ML system: data, labels,\nfeatures, machine learning algorithms, code, infrastructure, ... These\ndifferent components might be owned by different teams. For example,\ndata is managed by the data science team, labels by subject matter\nexperts, ML algorithms by the ML engineers, and infrastructure by the\nDevOps engineers. When an error occurs, it could be because of any of\nthese components or a combination of them, making it hard to know\nwhere to look or who should be looking into it.\n\nHere are some of the things that might cause an ML model to fail.\n\n\u00ab Theoretical constraints: As discussed above, each model\ncomes with its own assumptions about the data and the features\n\nit uses. A model might fail because the data it learns from\ndoesn\u2019t conform to its assumptions. For example, you use a\nlinear model for the data whose decision boundaries aren\u2019t\nlinear.\n\nPoor implementation of model: The model might be a good\nfit for the data, but the bugs are in the implementation of the\nmodel. For example, if you use PyTorch, you might have\nforgotten to stop gradient updates during evaluation when you\nshould. The more components a model has, the more things\nthat can go wrong, and the harder it is to figure out which goes\nwrong. However, with models being increasingly\ncommoditized and more and more companies using off-the-\nshelf models, this is becoming less of a problem.\n\nPoor choice of hyperparameters: With the same model, a set\nof hyperparameters can give you the state-of-the-art result but\nanother set of hyperparameters might cause the model to never\nconverge. The model is a great fit for your data, and its\nimplementation is correct, but a poor set of hyperparameters\nmight render your model useless.\n\nData problems: Many things that could go wrong in data\ncollection and preprocessing that might cause your models to\nperform poorly, such as data samples and labels being\nincorrectly matched, label inaccuracies, features normalized\nusing outdated statistics, and more.\n\nPoor choice of features: There might be many possible\nfeatures for your models to learn from. Too many features\nmight cause your models to overfit to the training data. Too\nfew features might lack predictive power to allow your models\nto make good predictions.\n\nDebugging should be both preventive and curative. You should have\nhealthy practices to minimize the opportunities for bugs to proliferate as\n\nwell as a procedure for detecting, locating, and fixing bugs. Having the\ndiscipline to follow both the best practices and the debugging procedure\nis crucial in developing, implementing, and deploying ML models.\n\nThere is, unfortunately, still no scientific approach to debugging in ML.\nHowever, there have been a number of tried-and-true debugging\ntechniques published by experienced ML engineers and researchers.\nHere are three of them. Readers interested in learning more might want\nto check out Andrej Karpathy\u2019s awesome post A Recipe for Training\nNeural Networks.\n\nil,\n\nN\n\nStart simple and gradually add more components\n\nStart with the simplest model and then slowly add more\ncomponents to see if it helps or hurts the performance. For\nexample, if you want to build a recurrent neural network\n(RNN), start with just one level of RNN cell before stacking\nmultiple together, or adding more regularization. If you want to\nuse a BERT-like model (Devlin et al., 2018) which uses both\nmasked language model (MLM) and next sentence prediction\nloss (NSP), you might want to use only the MLM loss before\nadding NSP loss.\n\nCurrently, many people start out by cloning an open-source\nimplementation of a state-of-the-art model and plugging in\ntheir own data. On the off-chance that it works, it\u2019s great. But\nif it doesn\u2019t, it\u2019s very hard to debug the system because the\nproblem could have been caused by any of the many\ncomponents in the model.\n\n. Overfit a single batch\n\nAfter you have a simple implementation of your model, try to\noverfit a small amount of training data and run evaluation on\nthe same data to make sure that it gets to the smallest possible\nloss. If it\u2019s for image recognition, overfit on 10 images and see\nif you can get the accuracy to be 100%, or if it\u2019s for machine\n\ntranslation, overfit on 100 sentence pairs and see if you can get\nto the BLEU score of near 100. If it can\u2019t overfit a small\namount of data, there\u2019s something wrong with your\nimplementation.\n\nWw\n\n. Set a random seed\n\nThere are so many factors that contribute to the randomness of\nyour model: weight initialization, dropout, data shuffling, etc.\nRandomness makes it hard to compare results across different\nexperiments\u2014you have no idea if the change in performance is\ndue to a change in the model or a different random seed.\nSetting a random seed ensures consistency between different\ntuns. It also allows you to reproduce errors and other people to\nreproduce your results.\n\nDistributed Training\n\nAs models are getting bigger and more resource-intensive, companies care a\nlot more about training at scale\u2019. Expertise in scalability is hard to acquire\nbecause it requires having regular access to massive compute resources.\nScalability is a topic that merits a series of books. This section covers some\nnotable issues to highlight the challenges of doing ML at scale and provide\na scaffold to help you plan the resources for your project accordingly.\n\nIt\u2019s common to train a model using a dataset that doesn\u2019t fit into memory. It\nhappens a lot when dealing with medical data such as CT scans or genome\nsequences. It can also happen with text data if you\u2019re a tech giant with\nenough compute resources to work with a massive dataset (cue OpenAI,\nGoogle, NVIDIA, Facebook).\n\nWhen your data doesn\u2019t fit into memory, you will first need algorithms for\npreprocessing (e.g. zero-centering, normalizing, whitening), shuffling, and\nbatching data out-of-memory and in parallel. When a sample of your data is\ntoo large, e.g. one machine can handle a few samples at a time, you might\n\nonly be able to work with a small batch size which leads to instability for\ngradient descent-based optimization.\n\nIn some cases, a data sample is so large it can\u2019t even fit into memory and\nyou will have to use something like gradient checkpointing, a technique that\nleverages the memory footprint and compute tradeoff to make your system\ndo more computation with less memory. According to the authors of the\nopen-source package gradient-checkpointing, \u201cfor feed-forward model, we\nwere able to fit more than 10x larger models onto our GPU, at only a 20%\nincrease in computation time\u201d'\u00ae, Even when a sample fits into memory,\nusing checkpointing can allow you to fit more samples into a batch, which\nmight allow you to train your model faster.\n\nData Parallelism\n\nIt\u2019s now the norm to train ML models on multiple machines (CPUs, GPUs,\nTPUs). Modern ML frameworks make it easy to do distributed training. The\nmost common parallelization method is data parallelism: you split your data\non multiple machines, train your model on all of them, and accumulate\ngradients. This gives rise to a couple of issues.\n\nA challenging problem is how to accurately and effectively accumulate\ngradients from different machines. As each machine produces its own\ngradient, if your model waits for all of them to finish a ran \u2014 Synchronous\nstochastic gradient descent (SSGD) \u2014 stragglers will cause the entire\nmodel to slow down, wasting time and resources'\u2019. The straggler problem\ngrows with the number of machines, as the more workers, the more likely\nthat at least one worker will run unusually slowly in a given iteration.\nHowever, there have been many algorithms that effectively address this\nproblem 18 19, 20,\n\nIf your model updates the weight using the gradient from each machine\nseparately \u2014 Asynchronous SGD (ASGD) \u2014 gradient staleness might\nbecome a problem because the gradients from one machine have caused the\nweights to change before the gradients from another machine have come\nin21\n\nThe difference between SSGD and ASGD is illustrated in Figure 5-7.\n\nAsynchronous SGD Synchronous SGD\n\n2. update to Newiodel when\u2019\nall 256 Gradients recvd\n\n2.download\nNewtodel\ninmediately\n\n1, send(Gradients) 1. send(Gradients)\n\n3.Next iteration with new Batch and Newtodel\n\nFigure 5-10. ASGD vs. SSGD for data parallelism. Image by Jim Dowling\u201d?\n\nIn theory, ASGD converges but requires more steps than SSGD. However,\nin practice, when gradient updates are sparse, meaning most gradient\nupdates only modify small fractions of the parameters, the model converges\nsimilarly?\u00b0.\n\nAnother problem is that spreading your model on multiple machines can\ncause your batch size to be very big. If a machine processes a batch size of\n1000, then 1000 machines process a batch size of 1M (OpenAI\u2019s GPT-3\n175B uses a batch size of 3.2M in 20204). If training an epoch on a\nmachine takes 1M steps, training on 1000 machines takes 1000 steps. An\nintuitive approach is to scale up the learning rate to account for more\nlearning at each step, but we also can\u2019t make the learning rate too big as it\nwill lead to unstable convergence. In practice, increasing the batch size past\na certain point yields diminishing returns\u201d\u00b0,76,\n\nLast but not least, with the same model setup, the master-worker sometimes\nuses a lot more resources than other workers. If that\u2019s the case, to make the\nmost use out of all machines, you need to figure out a way to balance out\nthe workload among them. The easiest way, but not the most effective way,\nis to use a smaller batch size on the master-worker and a larger batch size\non other workers.\n\nModel Parallelism\n\nMaand My\nmust wat for\nthe1* stagetto\n\ncomplete!\n\n\nFigure 5-11. Data parallelism and model parallelism. Image by Jure Leskovec.2\u201d\n\nWith data parallelism, each worker has its own copy of the model and does\nall the computation necessary for the model. Model parallelism is when\ndifferent components of your model are trained on different machines, as\nshown in Figure 5-11. For example, machine 0 handles the computation for\nthe first two layers while machine 1 handles the next two layers, or some\nmachines can handle the forward pass while several others handle the\nbackward pass.\n\nModel parallelism can be misleading in some cases when parallelism\ndoesn\u2019t mean that different parts of the model in different machines are\nexecuted in parallel. For example, if your model is a massive matrix and the\nmatrix is split into two halves on two machines, then these two halves\nmight be executed in parallel. However, if your model is a neural network\nand you put the first layer on machine | and the second layer on machine 2,\nand layer 2 needs outputs from layer 1 to execute, then machine 2 has to\nwait for machine 1 to finish first to run.\n\nPipeline parallelism is a clever technique to make different components of\na model on different machines run more in parallel. There are multiple\nvariants to this, but the key idea is to break the computation of each\nmachine into multiple parts, and when machine | finishes the first part of its\ncomputation, it passes the result onto machine 2, then continues executing\nthe second part, and so on. Machine 2 now can execute its computation on\npart 1 while machine | executes its computation on part 2.\n\nTo make this concrete, consider you have 4 different machines and the first,\nsecond, third, and fourth layers are on machine 1, 2, 3, and 4 respectively.\nGiven a mini-batch, you break it into 4 micro-batches. Machine 1 computes\nthe first layer on the first micro-batch, then machine 2 computes the second\nlayer on machine 1\u2019s results for the first micro-batch while machine 1\ncomputes the first layer on the second micro-batch, and so on. Figure 5-12\nshows how pipeline parallelism looks like on 4 machines, each machine\nruns both the forward pass and the backward pass for one component of a\nneural network.\n\nFigure 5-12. Pipeline parallelism for a neural network on 4 machines, each machine runs both the\nforward pass (F) and the backward pass (B) for one component of the neural network. Image by\nHuang et al.\nModel parallelism and data parallelism aren\u2019t mutually exclusive. Many\ncompanies use both methods for better utilization of their hardware, even\nthough the setup to use both methods can require significant engineering\neffort.\n\nAutoML\n\nThere\u2019s a joke that a good ML researcher is someone who will automate\nthemselves out of job, designing an AI algorithm intelligent enough to\n\ndesign itself. It was funny until the TensorFlow DevSummit 2018, where\nJeff Dean took the stage and declared that Google intended on replacing\nML expertise with 100 times more computational power, introducing\nAutoML to the excitement and horror of the community. Instead of paying a\ngroup of 100 ML researchers/engineers to fiddle with various models and\neventually select a sub-optimal one, why not use that money on compute to\nsearch for the optimal model? A screenshot from the recording of the event\nis shown in Figure 5-13.\n\nFigure 5-13. Jeff Dean unveiling Google's AutoML at TensorFlow Dev Summit 2018\n\nSoft AutoML: Hyperparameter Tuning\n\nAutoML refers to automating the process of finding ML algorithms to solve\nreal-world problems. One mild form, and the most popular form, of\nAutoML in production is hyperparameter tuning. A hyperparameter is a\nparameter supplied by users whose value is used to control the learning\nprocess, e.g. learning rate, batch size, number of hidden layers, number of\nhidden units, dropout probability, and in Adam optimizer, etc. Even\nquantization level \u2014 e.g. mixed-precision, fixed-point \u2014 can be considered\na hyperparameter to tune.\n\nWith different sets of hyperparameters, the same model can give drastically\ndifferent performances on the same dataset. Melis et al. showed in their\n2018 paper On the State of the Art of Evaluation in Neural Language\nModels that weaker models with well-tuned hyperparameters can\noutperform stronger, fancier models. The goal of hyperparameter tuning is\nto find the optimal set of hyperparameters for a given model within a search\nspace \u2014 the performance of each set evaluated on the validation set.\n\nDespite knowing its importance, many still ignore systematic approaches to\nhyperparameter tuning in favor of a manual, gut-feeling approach. The most\npopular is arguably Graduate Student Descent (GSD), a technique in which\na graduate student fiddles around with the hyperparameters until the model\nworks?8,\n\nHowever, more and more people are adopting hyperparameter tuning as part\nof their standard pipelines. Popular ML frameworks either come with built-\nin utilities or have third-party utilities for hyperparameter tuning, e.g. scikit-\nlearn with auto-sklearn2\u2019, TensorFlow with Keras Tuner, Ray with Tune.\nPopular methods for hyperparameter tuning including random search\u00ae\u00b0,\ngrid search, Bayesian optimization. The book AutoML: Methods, Systems,\nChallenges by the AutoML group at the University of Freiburg dedicates its\nfirst chapter to hyperparameter optimization, which you can read online for\nfree.\n\nWhen tuning hyperparameters, keep in mind that a model\u2019s performance\nmight be more sensitive to the change in one hyperparameter than another,\nand therefore sensitive hyperparameters should be more carefully tuned.\n\nWARNING\n\nOne important thing is to never use your test split to tune hyperparameters. Choose the\nbest set of hyperparameters for a model based on its performance on a validation split,\nthen report the model's final performance on the test split. If you use your test split to\ntune hyperparameters, you risk overfitting your model to the test split.\n\nHard AutoML: Architecture search and learned optimizer\n\nSome teams take hyperparameter tuning to the next level: what if we treat\nother components of a model or the entire model as hyperparameters. The\nsize of a convolution layer or whether or not to have a skip layer can be\nconsidered a hyperparameter. Instead of manually putting a pooling layer\nafter a convolutional layer or ReLu after linear, you give your algorithm\nthese building blocks and let it figure out how to combine them. This area\nof research is known as architectural search, or neural architecture search\n(NAS) for neural networks, as it searches for the optimal model\narchitecture.\n\nANAS setup consists of three components:\n\n* asearch space that defines possible neural networks, e.g. building\nblocks to choose from and constraints on how they can be\ncombined.\n\n* a performance estimation strategy to evaluate the performance of a\ncandidate architecture. Even though the final architecture resulting\nfrom the research might need retraining, estimating a candidate\narchitecture should be done without having to re-construct or re-\ntrain this candidate model from scratch.\n\n* asearch strategy to explore the search space. A simple approach is\nrandom search \u2014 randomly choosing from all possible\n\ntasks to train learned optimizers. Their learned optimizer was able to\ngeneralize to both new datasets and domains as well as new architectures*4.\nAnd the beauty of this approach is that the learned optimizer can then be\nused to train a better-learned optimizer, an algorithm that improves on itself.\n\nWhether it\u2019s architecture search or meta-learning learning rules, the upfront\ntraining cost is expensive enough that only a handful of companies in the\nworld can afford to pursue them. However, it\u2019s important for people\ninterested in ML in production to be aware of the progress in AutoML for\ntwo reasons. First, the resulting architectures and learned optimizers can\nallow ML algorithms to work off-the-shelf on multiple real-world tasks,\nsaving production time and cost, during both training and inferencing. For\nexample, EfficientNets, a family of models produced by Google\u2019s AutoML\nteam, surpass state-of-the-art accuracy with up to 10x better efficiency\u00ae\u00ae.\nSecond, they might be able to solve many real-world tasks previously\nimpossible with existing architectures and optimizers.\n\ntasks to train learned optimizers. Their learned optimizer was able to\ngeneralize to both new datasets and domains as well as new architectures*4.\nAnd the beauty of this approach is that the learned optimizer can then be\nused to train a better-learned optimizer, an algorithm that improves on itself.\n\nWhether it\u2019s architecture search or meta-learning learning rules, the upfront\ntraining cost is expensive enough that only a handful of companies in the\nworld can afford to pursue them. However, it\u2019s important for people\ninterested in ML in production to be aware of the progress in AutoML for\ntwo reasons. First, the resulting architectures and learned optimizers can\nallow ML algorithms to work off-the-shelf on multiple real-world tasks,\nsaving production time and cost, during both training and inferencing. For\nexample, EfficientNets, a family of models produced by Google\u2019s AutoML\nteam, surpass state-of-the-art accuracy with up to 10x better efficiency\u00ae\u00ae.\nSecond, they might be able to solve many real-world tasks previously\nimpossible with existing architectures and optimizers.\n\nFOUR PHASES OF ML MODEL DEVELOPMENT\n\nBefore we transition to model training, let\u2019s take a look at the four\nphases of ML model development. Once you\u2019ve decided to explore\nML, your strategy depends on which phase of ML adoption you are in.\nThere are four phases of adopting ML. The solutions from a phase can\nbe used as baselines to evaluate the solutions from the next phase.\n\n\u00ab Phase 1. Before machine learning\n\nIf this is your first time trying to make this type of prediction\nfrom this type of data, start with non-ML solutions. Your first\nstab at the problem can be the simplest heuristics. For example,\nto predict what letter users are going to type next in English,\nyou can show the top three most common English letters, \u201ce\u201d.\n\u201c| and \u201ca\u201d, which is correct 30% of the time.\n\nFacebook newsfeed was introduced in 2006 without any\nintelligent algorithms \u2014 posts were shown in chronological\norder. It wasn\u2019t until 2011 that Facebook started displaying\nnews updates you were most interested in at the top of the feed,\nas shown in Figure 5-14\u00b0\u00b0.\n\nModel Offline Evaluation\n\nOne common but quite difficult question I often encounter when helping\ncompanies with their AI strategies is: \u201cHow do I know that the ML model is\ngood?\u201d In one case, a company deployed ML to detect intrusions to 100\nsurveillance drones, but they had no way of measuring how many intrusions\ntheir system failed to detect, and couldn\u2019t decide if one ML algorithm was\nbetter than another for their needs.\n\nLacking a clear understanding of how to evaluate your ML systems is not\nnecessarily a reason for your ML project to fail, but it might make it\nimpossible to find the best solution for your need, and make it harder to\nconvince your managers to adopt ML.\n\nIdeally, the evaluation methods should be the same in both the development\nand production environments. But in many cases, the ideal is impossible\nbecause during development, you have ground truths, but in production,\nyou don\u2019t have ground truths.\n\nFor certain tasks, it\u2019s possible to infer or approximate ground truths in\nproduction based on user\u2019s feedback. For example, for the recommendation\ntask, it\u2019s possible to infer if a recommendation is good by whether users\nclick on it. However, there are many biases associated with this. The section\nContinual Learning in Chapter 8 will cover how to leverage users\u2019 feedback\nto improve your systems in production.\n\nFor other tasks, you might not be able to evaluate your model\u2019s\nperformance in production directly, and might have to rely on extensive\nmonitoring to detect changes in your model\u2019s performance in particular and\nto your system in general. We\u2019ll cover the causes of changes in your\nmodel\u2019s performance in Chapter 7 and the tools you can use to detect these\nchanges in the section Monitoring in chapter 8.\n\nBoth monitoring and continual learning can happen once your model has\nbeen deployed. In this section, we'll discuss methods to evaluate your\nmodel\u2019s performance before it\u2019s deployed. We'll start with the baselines\n\nagainst which we will evaluate our models. Then we\u2019ll cover some of the\ncommon methods to evaluate your model beyond overall accuracy metrics.\n\nSomeone once told me that her new generative model achieved the FID*\u00ae\nscore of 10.3 on ImageNet. I had no idea what this number meant or\nwhether her model would be useful for my problem.\n\nAnother time, I helped a company implement a classification model where\nthe positive class appears 90% of the time. An ML engineer on the team\ntold me, all excited, that their initial model achieved an F1 score of 0.90. I\nasked him how it was compared to random. He had no idea. It turned out\nthat if his model randomly outputted the positive class 90% of the time, its\nF1 score would also be around 0.90\u00b09. His model might as well be making\npredictions at random, which meant it probably didn\u2019t learn anything much.\n\nEvaluation metrics, by themselves, mean little. When evaluating your\nmodel, it\u2019s essential to know the baseline you\u2019re evaluating it against. The\nexact baselines should vary from one use case to another, but here are the\nfive baselines that might be useful across use cases.\n\n1. Random baseline\n\nIf our model just predicts at random, what\u2019s the expected\nperformance? At random means both \u201cfollowing a uniform random\ndistribution\u201d or \u201cfollowing the same distribution as the task\u2019s label\ndistribution.\u201d\n\nFor example, consider the task that has two labels, NEGATIVE\nthat appears 90% of the time and POSITIVE that appears 10% of\nthe time. Table 5-2 shows the F1 and accuracy scores of baseline\nmodels making predictions at random. However, as an exercise to\nsee how challenging it is for most people to have an intuition for\nthese values, try to calculate these raw numbers in your head\nbefore looking at the table.\n\nnew model to these existing solutions. Your ML model doesn\u2019t\nalways have to be better than existing solutions to be useful. A\nmodel whose performance is a little bit inferior can still be useful if\nit\u2019s much easier or cheaper to use.\n\nicking up the usefulness thread from the last section, a good system isn\u2019t\necessarily useful. A system meant to replace human experts often has to\nerform at least as well as human experts to be useful. In some cases, even\nit\u2019s better than human experts, people might still not trust it, as in the case\nf self-driving cars. On the contrary, a system that predicts what word a\n\nser will type next on their phone can perform much worse than a native\npeaker and still be useful.\n\n:valuation Methods\n\n1 academic settings, when evaluating ML models, people tend to fixate on\n1eir performance metrics. However, in production, we also want our\n10dels to be robust, fair, calibrated, and overall, make sense. We\u2019ll\nitroduce some evaluation methods that help with measuring the above\nharacteristics of a model.\n\nerturbation Tests\n\n. group of my students wanted to build an app to predict whether someone\nas covid-19 through their cough. Their best model worked great on the\naining data, which consisted of 2-second long cough segments collected\ny hospitals. However, when they deployed it to actual users, this model\u2019s\nredictions were close to random.\n\nine of the reasons is that actual users\u2019 coughs contain a lot of noise\nompared to the coughs collected in hospitals. Users\u2019 recordings might\nontain background music or nearby chatter. The microphones they use are\nf varying quality. They might start recording their coughs as soon as\n-cording is enabled or wait for a fraction of a second.\n\nleally, the inputs used to develop your model should be similar to the\niputs your model will have to work with in production, but it\u2019s not possible\n\nin many cases. This is especially true when data collection is expensive or\ndifficult and you have to rely on the data collected by someone else. As a\nresult, inputs in production are often noisy compared to inputs used in\ndevelopment*\u00ae. The model that performs best on the training data isn\u2019t\nnecessarily the model that performs best on noisy inputs in production.\n\nTo get a sense of how well your model might perform with noisy data, you\ncan make small changes to your test splits to see how these changes affect\nyour model\u2019s performance. For the task of predicting whether someone has\ncovid-19 from their cough, you could randomly add some background noise\nor randomly clip the testing clips to simulate how the recordings might be\nin production. You might want to choose the model that works best on the\nperturbed data instead of the one that works best on the clean data.\n\nThe more sensitive your model is to noise, the harder it will be to maintain\nit since if your users\u2019 behaviors change just slightly, such as they change\ntheir phones and get much higher quality microphones, your model\u2019s\nperformance might degrade. It also makes your model susceptible to\nadversarial attack.\n\nInvariance Tests\n\nA Berkeley study found out that between 2008 and 2015, 1.3 million\ncreditworthy black and Latino applicants had their mortgage applications\nrejected because of their races. When the researchers used the income and\ncredit scores of the rejected applications but deleted the race identifiers, the\napplications were rejected.\n\nCertain changes to the inputs shouldn\u2019t lead to changes in the output. In the\ncase above, changes to race information shouldn\u2019t affect the mortgage\noutcome. Similarly, changes to applicants\u2019 names shouldn\u2019t affect their\nresume screening results nor should someone\u2019s gender affect how much\nthey should be paid. If these happen, there are biases in your model, which\nmight render it unusable no matter how good its performance is.\n\nTo avoid these biases, one solution is to do the same process that helped the\nBerkeley researchers discover the biases: keep the inputs the same but\n\nin many cases. This is especially true when data collection is expensive or\ndifficult and you have to rely on the data collected by someone else. As a\nresult, inputs in production are often noisy compared to inputs used in\ndevelopment*\u00ae. The model that performs best on the training data isn\u2019t\nnecessarily the model that performs best on noisy inputs in production.\n\nTo get a sense of how well your model might perform with noisy data, you\ncan make small changes to your test splits to see how these changes affect\nyour model\u2019s performance. For the task of predicting whether someone has\ncovid-19 from their cough, you could randomly add some background noise\nor randomly clip the testing clips to simulate how the recordings might be\nin production. You might want to choose the model that works best on the\nperturbed data instead of the one that works best on the clean data.\n\nThe more sensitive your model is to noise, the harder it will be to maintain\nit since if your users\u2019 behaviors change just slightly, such as they change\ntheir phones and get much higher quality microphones, your model\u2019s\nperformance might degrade. It also makes your model susceptible to\nadversarial attack.\n\nInvariance Tests\n\nA Berkeley study found out that between 2008 and 2015, 1.3 million\ncreditworthy black and Latino applicants had their mortgage applications\nrejected because of their races. When the researchers used the income and\ncredit scores of the rejected applications but deleted the race identifiers, the\napplications were rejected.\n\nCertain changes to the inputs shouldn\u2019t lead to changes in the output. In the\ncase above, changes to race information shouldn\u2019t affect the mortgage\noutcome. Similarly, changes to applicants\u2019 names shouldn\u2019t affect their\nresume screening results nor should someone\u2019s gender affect how much\nthey should be paid. If these happen, there are biases in your model, which\nmight render it unusable no matter how good its performance is.\n\nTo avoid these biases, one solution is to do the same process that helped the\nBerkeley researchers discover the biases: keep the inputs the same but\n\nchange the sensitive information to see if the outputs change. Better, you\nshould exclude the sensitive information from the features used to train the\nmodel in the first place\u2019.\n\nDirectional Expectation Tests\n\nCertain changes to the inputs should, however, cause predictable changes in\noutputs. For example, when developing a model to predict housing prices,\nkeeping all the features the same but increasing the lot size shouldn\u2019t\ndecrease the predicted price, and decreasing the square footage shouldn\u2019t\nincrease the output. If the outputs change in the opposite expected direction,\nyour model might not be learning the right thing, and you need to\ninvestigate it further before deploying it.\n\nModel Calibration\n\nModel calibration is a subtle but crucial concept to grasp. Imagine that\nsomeone makes a prediction that something will happen with a probability\nof 70%. What this prediction means is that out of the times this prediction is\nmade, this event happens 70% of the time. If a model predicts that team A\nwill beat team B with a 70% probability, and out of the 1000 times these\ntwo teams play together, team A only wins 60% of the time, then we say\nthat this model isn\u2019t calibrated. A calibrated model should predict that team\nA wins with a 60% probability.\n\nModel calibration is often overlooked by ML practitioners, but it\u2019s one of\nthe most important properties of any system that makes predictions. To\nquote Nate Silver in his book The Signal and the Noise, calibration is \u201cone\nof the most important tests of a forecast \u2014 I would argue that it is the single\nmost important one.\u201d\n\nWe'll walk through two examples to show why model calibration is\nimportant. First, consider the task of building a recommender system to\nrecommend what movies users will likely to watch next. Suppose user A\nwatches romance movies 80% of the time and comedy 20% of the time. If\nyou choose your recommendations to consist of only the movies A will\nmost likely to watch, the recommendations will only consist of romance\n\nmovies because A is much more likely to watch romance than comedy\nmovies. You might want a calibrated recommendation system whose\nrecommendations are representative of users\u2019 actual watching habits. In this\ncase, they should consist of 80% romance and 20% comedy*?.\n\nSecond, consider the task of building a model to predict how likely it is that\na user will click on an ad. For the sake of simplicity, imagine that there are\nonly 2 ads, ad A and ad B. Your model predicts that this user will click on\nad A with a 10% probability and on ad B with a 8% probability. You don\u2019t\nneed your model to be calibrated to rank ad A above ad B. However, if you\nwant to predict how many clicks your ads will get, you\u2019ll need your model\nto be calibrated. If your model predicts that a user will click on ad A with a\n10% probability but in reality, the ad is only clicked on 5% of the time, your\nestimated number of clicks will be way off. If you have another model that\ngives the same ranking but is better calibrated than this model, you might\nwant to consider this other model.\n\nTo measure a model\u2019s calibration, a simple method is counting: you count\nthe number of times your model outputs the probability X and the\nfrequency Y of that prediction coming true, and plot X against Y. In scikit-\nlearn, you can plot the calibration curve of a binary classifier with the\nmethod sklearn.calibration.calibration_curve, as shown in Figure 5-15.\n\nCalibration plots (reliability curve)\n\n10\n08\n8 a6]\nF\ni\n&\n6\n\u00a2\nS\nfu\n02\nteens Perfectly calibrated\n\u2014\u00ae Logistic\n\u2014\u00ae Naive Bayes\n\u2014\u00ae Support Vector Classification\n% \u2014\u00ae Random Forest\n00 02 oF 06 08 10\n40000 Logistic [1 Support Vector Classification\n[J Naive Bayes [2] Random Forest\n\n00 02 04 06 08 10\nMean predicted value\n\nFigure 5-15. The calibration curves of different models on a toy task. The Logistic Regression model\nis the best calibrated model because it directly optimizes logistic loss. Image by scikit-learn.\n\nTo calibrate your models, a common method is Platt scaling, which is\nimplemented in scikit-learn with sklearn.calibration.CalibratedClassifierCV.\nAnother good open-source implementation by Geoff Pleiss can be found on\nGitHub. For readers who want to learn more about the importance of model\ncalibration and how to calibrate neural networks, Lee Richardson and\nTaylor Pospisil have an excellent blog post based on their work at Google.\n\nConfidence Measurement\n\nConfidence measurement can be considered a way to think about the\nusefulness threshold for each individual prediction. Indiscriminately\nshowing all model\u2019s predictions to users, even the predictions that the\nmodel is unsure about, can, at best, cause annoyance and make users lose\ntrust in the system, such as an activity detection system on your smartwatch\nthat thinks you\u2019re running even though you\u2019re just walking a bit fast. At\nworst, it can cause catastrophic consequences, such as a predictive policing\nalgorithm that flags an innocent person as a potential criminal).\n\nIf you only want to show the predictions that your model is certain about,\nhow do you measure that certainty? What is the certainty threshold at which\nthe predictions should be shown? What do you want to do with predictions\nbelow that threshold \u2014 discard it, loop in humans, or ask for more\ninformation from users?\n\nWhile most other metrics deal with system-level measuring system\u2019s\nperformance on average, confidence measurement is a metric for each\nindividual instance. System-level measurement is useful to get a sense of\noverall performance, but instance-level metrics are crucial when you care\nabout your system\u2019s performance on every instance, and the model\u2019s failure\nin just one instance can be catastrophic.\n\nSlice-based Evaluation\n\nSlicing means to separate your data into subgroups and look at your\nmodel\u2019s performance on those subgroups separately. A common mistake\n\nthat I\u2019ve seen in many companies is that they are focused only on coarse-\ngrained metrics like overall F1 or accuracy on the entire datasets. This can\nlead to two problems.\n\nOne is that their model performs differently on different slices (subsets) of\ndata when the model should perform the same. For example, if their data\nhas two subgroups, one majority and one minority, and the majority\nsubgroup accounts for 90% of the data. Model A achieves 98% accuracy on\nthe majority subgroup but only 80% on the minority subgroup, which\nmeans its overall accuracy is 96.2%. Model B achieves 95% accuracy on\nthe majority and 95% on the minority, which means its overall accuracy is\n95%. These two models are compared in Table 5-3.\n\nIfa company focuses only on overall metrics, they might go with Model A.\nThey might be very happy with this model\u2019s high accuracy until one day,\ntheir end users discover that this model is biased against the minority\nsubgroup because the minority subgroup happens to correspond to an\nunderrepresented demographic group*?. The focus on overall performance\nis harmful not only because of the potential public\u2019s backlash, but also\nbecause it blinds the company to huge potential model\u2019s improvements. If\nthe company sees the two models\u2019 performance on different subgroups,\nthey can use different strategies regarding these two models\u2019 performance:\nimprove model A\u2019s performance on the minority subgroup while improving\nmodel\u2019s performance overall, and choose one after having weighed the pros\nand cons of both.\n\n\nprediction (predicting when a user will cancel a subscription or a service),\npaid users are more critical than non-paid users. Focusing on a model\u2019s\noverall performance might hurt its performance on these critical slices.\n\nA fascinating and seemingly counterintuitive reason why slice-based\nevaluation is crucial is Simpson\u2019s paradox, a phenomenon in which a trend\nappears in several groups of data but disappears or reverses when the\ngroups are combined. This means that model A can perform better than\nmodel B on all data together but model B performs better than model A on\neach subgroup separately. Consider model A\u2019s and model B\u2019s performance\non group A and group B as shown in Table 5-4. Model A outperforms\nmodel B for both group A and B, but when combined, model B outperforms\nmodel A.\n\nGroup A Group B Overall\n\nModel A 93% (81/87) 73% (192/263) 78% (273/350)\n\nModel B 87% (234/270) 69% (55/80) 83% (289/350)\n\nSimpson\u2019s paradox is more common than you\u2019d think. In 1973, Berkeley\ngraduate statistics showed that the admission rate for men was much higher\nthan for women, which caused people to suspect biases against women.\nHowever, a closer look into individual departments showed that the\nadmission rates for women were actually higher than those for men in 4 out\nof 6 departments, as shown in Figure 5-16.\n\nTotal\n\nApplicants\n12763\n\nAdmitted\n\nApplicants Admitted Applicants Admitted\n\n41%\n\n8442\n\n44%\n\n4321\n\n35%\n\nAopicants Adnitd Applicants Adnited Appian Admit\n\noo OoOolUUDCOCOl CUD\n\nE\nF\n\nAll\n\nWomen\n\n933\n\n64%\n\n825\n\n(2%\n\n108\n\n8%\n\n585\n\n03%\n\n560\n\n63%\n\n5\n\n(8%\n\n18\n792\n\n35%\n%\n\n$25\n417\n\n3%\n33%\n\n598\n35\n\n34%\n5%\n\n584\n\n5%\n\n19\n\n28%\n\n3983\n\n24%\n\n114\n\n6%\n\n373\n\n%\n\n$4\n\nT%\n\nFigure 5-16. The overall graduate admission rate for men and women at Berkeley in 1973 caused\n\npeople to suspect biases against women. However, a closer look into individual departments showed\n\nthat the admission rates for women were actually higher than those for men in 4 out of 6\n\ndepartments. Data from Sex Bias in Graduate Admissions: Data from Berkeley (Bickel et al., 1975).\nScreenshotted on Wikipedia.\n\nWhether this paradox happens in our work or not, the point here is that\naggregation can conceal and contradict actual situations. To make informed\ndecisions regarding what model to choose, we need to take into account its\nperformance not only on the entire data, but also on individual slices. Slice-\nbased evaluation can give you insights to improve your model\u2019s\nperformance both overall and on critical data and help detect potential\nbiases. It might also help reveal non-machine learning problems. Once, our\nteam discovered that our model performed great overall but very poorly on\ntraffic from mobile users. After investigating, we realized that it was\nbecause a button was half hidden on small screens, like phone screens.\n\nEven when you don\u2019t think slices matter, understanding how your model\nperforms in a more fine-grained way can give you confidence in your\nmodel to convince other stakeholders, like your boss or your customers, to\ntrust your ML models.\n\nTo track your model\u2019s performance on critical slices, you'd first need to\nknow what your critical slices are. You might wonder how to discover\ncritical slices in your data. Slicing is, unfortunately, still more of an art than\na science, requiring intensive data exploration and analysis. Here are the\nthree main approaches:\n\n\u00ab Heuristics-based: slice your data using existing knowledge you\nhave of the data and the task at hand. For example, when working\nwith web traffic, you might want to slice your data along\ndimensions like mobile versus desktop, browser type, and\nlocations. Mobile users might behave very differently from desktop\nusers. Similarly, Internet users in different geographic locations\nmight have different expectations on what a website should look\nlike**. This approach might require subject matter expertise.\n\n\u00a2 Error analysis: manually go through misclassified examples and\nfind patterns among them. We discovered our model\u2019s problem\nwith mobile users when we saw that most of the misclassified\nexamples were from mobile users.\n\n* Slice finder: there has been research to systemize the process of\nfinding slices, including Chung et al.\u2019s Slice finder: Automated\ndata slicing for model validation in 2019 and covered in Sumyea\nHelal\u2019s Subgroup Discovery Algorithms: A Survey and Empirical\nEvaluation (2016). The process generally starts with generating\nslice candidates with algorithms such as beam search, clustering, or\ndecision, then prune out clearly bad candidates for slices, and then\nrank the candidates that are left.\n\nSummary\n\nIn this chapter, we\u2019ve covered what many ML practitioners consider to be\nthe most fun part of an ML project cycle: developing, training, and\nevaluating ML models. Not all parts are equally fun, however. Making your\nmodels work on a large distributed system, like the one that runs models\nwith hundreds of millions, if not billions, of parameters, can be challenging\nand require specialized system engineering expertise. Intensive tracking and\nversioning your many experiments are generally agreed to be necessary, but\ndoing it might feel like a chore. Evaluating your models\u2019 fitness for the\nproduction environment while you only have access to training data is\ndifficult. However, these methods are necessary to sanity check your\nmodels before further evaluating your models in a production environment.\n\nOften, no matter how good your offline evaluation of a model is, you still\ncan\u2019t be sure of your model\u2019s performance in production until that model\nhas been deployed. In the next chapter, we'll go over how to deploy a\nmodel. And in the chapter after that, we\u2019ll cover how to continually monitor\nand evaluate your model in production.\n\n1 Facebook Employee Raises Powered by \u2018Really Dangerous\u2019 Algorithm That Favors Angry\nPosts (SFist, 2019)\n\n2 The Making of a YouTube Radical (NYT, 2019)\n\n3 For simplicity, let\u2019s pretend for now that we know to measure a post\u2019s quality.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 5,
                "chapter_name": "Chapter 6. Model Deployment",
                "chapter_path": "./screenshots-images-2/chapter_5",
                "sections": [
                    {
                        "section_id": 5.1,
                        "section_name": "Chapter 6. Model Deployment",
                        "section_path": "./screenshots-images-2/chapter_5/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_5/section_1/52c2914d-7b29-4788-853b-9dcfe0b288b1.png",
                            "./screenshots-images-2/chapter_5/section_1/4bfc7982-9b8c-4ec2-9586-81e6db558cd8.png",
                            "./screenshots-images-2/chapter_5/section_1/a51b7c89-b9f4-4402-b2eb-e5b036783d8f.png",
                            "./screenshots-images-2/chapter_5/section_1/18d2616b-8152-4346-ad3d-53e2206a9e25.png",
                            "./screenshots-images-2/chapter_5/section_1/d2097eff-43da-46e8-bfcc-db9184d80f2b.png",
                            "./screenshots-images-2/chapter_5/section_1/1b5f0c49-0df3-4e7d-bbb4-c4fc8281a8f2.png",
                            "./screenshots-images-2/chapter_5/section_1/c88b7d3b-3c43-4da1-adeb-f0e46b60fa26.png",
                            "./screenshots-images-2/chapter_5/section_1/7148daa5-07c9-4acb-807d-79def29b31a5.png",
                            "./screenshots-images-2/chapter_5/section_1/7cb81b2c-88c8-4cff-bb29-25ebb879c360.png",
                            "./screenshots-images-2/chapter_5/section_1/4e3bab74-2464-400c-b66f-baa648446cd5.png",
                            "./screenshots-images-2/chapter_5/section_1/e5175110-0b7d-4fc4-8daa-3f1e4d248cb9.png",
                            "./screenshots-images-2/chapter_5/section_1/8b9fe9c1-0806-483e-a1b0-344557d4e223.png",
                            "./screenshots-images-2/chapter_5/section_1/fef03f20-b9e7-435f-a157-99d381313228.png",
                            "./screenshots-images-2/chapter_5/section_1/653f6205-9ad5-4a1c-bbcf-f73ef193e191.png",
                            "./screenshots-images-2/chapter_5/section_1/41b66f2a-6980-41e7-a10a-2babfc77cada.png",
                            "./screenshots-images-2/chapter_5/section_1/7a055d6d-232c-449a-aabe-bc68e466315c.png",
                            "./screenshots-images-2/chapter_5/section_1/64a1a44f-b598-476a-bf50-4221209554ad.png",
                            "./screenshots-images-2/chapter_5/section_1/cb928842-e48f-42a7-8e38-defc861fc22b.png",
                            "./screenshots-images-2/chapter_5/section_1/95dcc736-a31a-438a-81a1-b7e4f7033408.png",
                            "./screenshots-images-2/chapter_5/section_1/f944c77c-5323-4ec5-907c-40d76a26a83a.png",
                            "./screenshots-images-2/chapter_5/section_1/944dbd80-fb41-4739-92f3-6dd92bd0153a.png",
                            "./screenshots-images-2/chapter_5/section_1/473a62e3-9062-42ec-81ba-52c8ff7ff1dd.png",
                            "./screenshots-images-2/chapter_5/section_1/eaf9faeb-b5d1-44fc-ab17-7d7ba618ba1f.png",
                            "./screenshots-images-2/chapter_5/section_1/bca28c45-f2b4-4bd2-9de4-d25d1d7931cf.png",
                            "./screenshots-images-2/chapter_5/section_1/cf5f544b-ce08-46e2-8926-4c7d0b434231.png",
                            "./screenshots-images-2/chapter_5/section_1/26321fd1-3143-4e20-8947-2dc8887cd3f0.png",
                            "./screenshots-images-2/chapter_5/section_1/8b5453e6-add0-48b3-811d-50c1b24d61f9.png",
                            "./screenshots-images-2/chapter_5/section_1/4146d41a-c901-40b4-bb15-2188afcfb24d.png",
                            "./screenshots-images-2/chapter_5/section_1/dc8f6d98-6f10-450c-921c-1babcf82ecb3.png",
                            "./screenshots-images-2/chapter_5/section_1/70a6f7b9-8535-4cca-b754-95ad1db68057.png",
                            "./screenshots-images-2/chapter_5/section_1/4386a9c1-ca82-4ba4-b154-5cb9776a165b.png",
                            "./screenshots-images-2/chapter_5/section_1/f06c636b-655a-4194-bde0-ba481aeeb172.png",
                            "./screenshots-images-2/chapter_5/section_1/385dfb88-0f46-496a-a09f-3e806cf30127.png",
                            "./screenshots-images-2/chapter_5/section_1/9bd28900-d6f8-463c-b641-bdb2ac82fe3b.png",
                            "./screenshots-images-2/chapter_5/section_1/7205c055-a91b-4e72-8120-98f0231e7f70.png",
                            "./screenshots-images-2/chapter_5/section_1/cf3745a0-d683-4cb7-af51-1fa55daa4fdc.png",
                            "./screenshots-images-2/chapter_5/section_1/bc4acfb8-b7d9-43ae-9178-8db4547d4b60.png",
                            "./screenshots-images-2/chapter_5/section_1/776db46b-5526-4b16-b60d-e059317cd5e1.png",
                            "./screenshots-images-2/chapter_5/section_1/3b951f0e-bfca-4a8f-8944-dc138dd0e9e6.png",
                            "./screenshots-images-2/chapter_5/section_1/bdb08043-3234-4f51-8409-e19b8434db5d.png",
                            "./screenshots-images-2/chapter_5/section_1/0bee4f6d-ac37-4961-a611-3f6fdc49492f.png",
                            "./screenshots-images-2/chapter_5/section_1/15eebc42-ff94-4ff8-8a90-75e9e74e2073.png",
                            "./screenshots-images-2/chapter_5/section_1/cfe4b124-1687-4552-b0dc-71fa53f303fd.png",
                            "./screenshots-images-2/chapter_5/section_1/65a23dcf-5219-4853-a0cc-ba0da4ebf751.png",
                            "./screenshots-images-2/chapter_5/section_1/044be594-342f-4346-89a5-d397192d0423.png",
                            "./screenshots-images-2/chapter_5/section_1/ba53296b-d78c-451d-95c5-a9fbc8fc79d3.png",
                            "./screenshots-images-2/chapter_5/section_1/214a0a2c-012a-411a-a5ea-4b9d9e348435.png",
                            "./screenshots-images-2/chapter_5/section_1/db9bbea6-943f-43c8-912c-28caa6ea74a2.png",
                            "./screenshots-images-2/chapter_5/section_1/ab36323a-a984-435b-87d5-92eda08d1c55.png",
                            "./screenshots-images-2/chapter_5/section_1/8bcbeb21-4576-42cd-8934-857659e5cfc9.png",
                            "./screenshots-images-2/chapter_5/section_1/c8763598-d3d0-424a-b5cb-49e33e26bacf.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In Chapter 3 through 5, we have discussed the considerations for\ndeveloping an ML model, from creating training data, extracting features,\ndeveloping a model, and crafting metrics to evaluate this model. These\nconsiderations constitute the logic of the model \u2014 instructions on how to\ngo from raw data into as shown in Figure 6-1. Developing this logic\nrequires both ML knowledge and subject matter expertise. In many\ncompanies, this is the part of the process that is done by the ML or data\nscience teams. Models are usually developed in development environments.\n\nML mocle| logic\n\nData engineering\n\nFeature engineering\n\n\nFigure 6-1. Different aspects that make up the ML model logic.\n\nIn this chapter, we\u2019ll discuss another part in the iterative process: deploying\nyour model. Deploy is a loose term that generally means making your\nmodel running and accessible. You can deploy your model on a test\nenvironment for testing. You can also deploy it to a production environment\nto be accessible and usable by your end users. In this chapter, we focus on\ndeploying models to production environments.\n\nHowever, keep in mind that production is a spectrum. For some teams,\nproduction means generating nice plots from notebook results to show to\nthe business team. For other teams, production means keeping your models\nup and running for millions of users a day. If your work is in the first\nscenario, your production environment is similar to the development\nenvironment, and this chapter is less relevant for you. If your work is closer\nto the second scenario, read on.\n\nA wise person on the Internet once said: deploying is easy if you ignore all\nthe hard parts. If you want to deploy a model for your friends to play with,\nall you have to do is to create an endpoint to your prediction function, push\nyour model to AWS, create an app with Streamlit or Dash. If you\u2019re\nfamiliar with these tools, you can have a functional deployment in an hour.\nMy students, after a 10-week course\u2019, were all able to deploy an ML\napplication as their final projects even though few have had deployment\nexperience before.\n\nThe hard parts include making your model available to millions of users\nwith a latency of milliseconds and 99% uptime, setting up the infrastructure\nso that the right person can be immediately notified when something went\nwrong, figuring out what went wrong, and seamlessly deploying the\nupdates to fix what\u2019s wrong.\n\nIn many companies, the responsibility of deploying models falls into the\nhands of the same people who developed those models. In many other\ncompanies, once a model is ready to be deployed, it\u2019ll be exported and\nhanded off from the data scientists or ML engineers to another team to\ndeploy it. However, this separation of responsibilities can cause high\n\noverhead communications across teams and make it slow to update your\nmodel. It also can make it hard to debug should something go wrong. We'll\ndiscuss more on team structures in Chapter 9.\n\nNOTE\n\nExporting a model means converting this model into a format that can be used by\nanother application. Some people call this process serialization2. There are two parts of\na model that you can export: the model definition and the model's parameter values. The\nmodel definition defines the structure of your model, such as how many hidden layers it\nhas, how many units in each layer. The parameter values provide the values for these\nunits and layers. Usually, these two parts are exported together.\n\nIn TensorFlow 2, you might use tf.keras.Model.save() to export your model into\nTensorFlow\u2019s SavedModel format. In PyTorch, you might use torch.onnx.export() to\nexport your model into ONNX format.\n\nRegardless of whether your job involves deploying ML models, being\ncognizant of how your models are used can give you an understanding of\ntheir constraints and help you tailor them to their purposes.\n\nIn this chapter, we\u2019ll start off with some common myths about ML\ndeployment that I\u2019ve often heard from people who haven\u2019t deployed ML\nmodels. We\u2019ll then discuss the two main ways a model generates and serves\nits predictions to users: online prediction and batch prediction. The process\nof generating predictions is called inference.\n\nWe\u2019ll continue with where the computation for generating predictions\nshould be done: on the device (also referred to as edge) and the cloud. How\na model serves and computes the predictions influences how it should be\ndesigned, the infrastructure it requires, and the behaviors that users\nencounter.\n\nIf you come from an academic background, some of the topics discussed in\n\nthis chapter might be outside your comfort zone. I\u2019ll try to take it slow. If an\nunfamiliar term comes up, take a moment to look it up. If a section becomes\ntoo dense, feel free to skip it. This chapter is modular, so skipping a section\n\nshouldn\u2019t affect your understanding of another section.\n\nCAUTION AGAINST CATEGORICAL THINKING\n\nThe way we categorize a system into batch prediction vs. online\nprediction and edge computing vs. cloud computing provides an anchor\nto think about the requirements and limitations of different systems. It\u2019s\nnot a guide. Seemingly different ways of doing things might be\nfundamentally similar, and the choices don\u2019t have to be mutually\nexclusive. For example, you don\u2019t have to do only batch predictions or\nonly online predictions -- you can do both. If you\u2019re doing batch\npredictions, switching to online predictions might be easier than you\nthink.\n\nPutting things in buckets might cause organizations to get stuck in one\nway of doing things without realizing that the other way isn\u2019t that\ndifferent but can provide much more value.\n\nMachine Learning Deployment Myths\n\nAs discussed in the first chapter, deploying an ML model can be very\ndifferent from deploying a traditional software program. This difference\nmight cause people who have never deployed a model before to either\napprehend the process or underestimate how much time and effort it will\ntake. In this section, we\u2019ll debunk some of the common myths about the\ndeployment process, which will, hopefully, put you in a good state of mind\nto begin the process. This section will be most helpful to people with little\nto no deploying experience.\n\nMyth 1. You only deploy one or two ML models at a time\n\nWhen doing academic projects, I was advised to choose a small\nproblem to focus on, which usually led to a single model. Many people\nfrom academic backgrounds I\u2019ve talked to tend to also think of machine\nlearning production in the context of a single model. Subsequently, the\ninfrastructure they have in mind doesn\u2019t work for actual applications,\nbecause it can only support one or two models.\n\nIn reality, companies have many, many ML models. An application\nmight have many different features, and each feature might require its\nown model. Consider a ridesharing app like Uber. It needs a model for\neach of the following elements: to predict rider demand, driver\navailability, estimated time of arrival, optimal price, fraudulent\ntransaction, customer churn, and more. Additionally, if this app operates\nin 20 countries, until you can have models that generalize across\ndifferent user-profiles, cultures, and languages, each country would\nneed its own set of models. So with 20 countries and 10 models for each\ncountry, you already have 200 models. Figure 6-2 shows some of the\ntasks that leverage ML at Netflix.\n\nNETFLI\n\nContent Valuation a\nnel Screenplay Anas Using LP\n\nPredict ualty of Network 2) 22\u00b0\" 70570\" aching Translation\nsail Content Tagging\n\nFraud\nPredict Chur Classify Support Tickets\n\nIncremental impact of Marketing Tet pana\nter Estimate Word- Mouth Effects\nOptimal CDN Caching\n\nFigure 6-2. Different tasks that leverage ML at Netflix\u2019, Image by Ville Tuulos.\n\nIn fact, Uber has thousands of models in production*. At any given\nmoment, Google has thousands of models training concurrently with\nhundreds of billions parameters in size. Booking.com has 150+\nmodels\u00ae, A 2021 study by Algorithmia shows that among organizations\nwith over 25,000 employees, 41% have more than 100 models in\nproduction.\n\nMyth 2. If we dont do anything, model performance remains the same\n\nSoftware doesn\u2019t age like fine wine. It ages poorly. The phenomenon in\nwhich a software program degrades over time even if nothing seems to\nhave changed is known as \u201csoftware rot\u201d or \u201cbit rot\u201d.\n\nML systems aren\u2019t immune to it. On top of that, ML systems suffer\nfrom what\u2019s known as concept drift, whose simplest case is when the\ndistribution of the data your model encounters in production becomes\ndifferent from the distribution of the data it was trained on\u2019. Therefore,\nan ML model tends to perform best right after training, and degrade\nover time.\n\nMyth 3. You won t need to update your models as much\n\nPeople tend to ask me: \u201cHow often SHOULD I update my models?\u201d\nIt\u2019s the wrong question to ask. The right question should be: \u201cHow often\nCAN I update my models?\u201d\n\nSince a model\u2019s performance decays over time, we want to update it as\nfast as possible. This is an area of ML where we should learn from\nexisting DevOps best practices. Even back in 2015, people were already\nconstantly pushing out updates to their systems. Etsy deployed 50\ntimes/day, Netflix 1000s times/day, AWS every 11.7 seconds\u00ae.\n\nWhile many companies still only update their models once a month, or\neven once a quarter, Weibo\u2019s iteration cycle for updating some of their\nML models is 10 minutes?. I\u2019ve heard similar numbers at companies\nlike Alibaba and ByteDance (the company behind TikTok).\n\nIn the words of Josh Wills, a former staff engineer at Google and\nDirector of Data Engineering at Slack, \u201cwe\u2019re always trying to bring\nnew models into production just as fast as humanly possible.'\u00b0\u201d\n\nWe\u2019ll discuss more on the frequency to retrain your models in the\nsection Continual Learning in Chapter 8.\n\nMyth 4. Most ML engineers don t need to worry about scale\n\nWhat \u201cscale\u201d means varies from application to application, but\nexamples include a system that serves hundreds of queries per second or\nmillions of users a month.\n\nYou might argue that if so, only a small number of companies need to\nworry about it. There is only one Google, one Facebook, one Amazon.\nThat\u2019s true, but a small number of large companies employ the majority\nof the software engineering workforce. According to StackOver\nDeveloper Survey 2019, more than half of the respondents worked for a\ncompany of at least 100 employees (see Figure 6-3).\n\nCompany Size\n\nJustme-|ama frelaner, sole propo, etc. 61% (IN)\n2Senpiyees 103% SL\nfOtotSenplyees 94% TS\nDio Wenpoyees 21.24 ST\n10010400 enplyees 17.9%\nSO to 88 employees 64%, NN\n1000104999 enpoyes 105% [I\n500109988 employees 42% (I\n11000 ormoreenploees 141%, [SS\n\n7179 responses\n\nFigure 6-3. The distribution of the size of companies where software engineers work, StackOverflow\nSurvey 2019\"\n\nI couldn\u2019t find a survey for ML specific roles, so I asked on Twitter and\nfound similar results. This means that if you\u2019re looking for an ML-related\njob in the industry, you\u2019ll likely work for a company of at least 100\nemployees, whose ML applications likely need to be scalable. Statistically\nspeaking, an ML engineer should care about scale.\n\nBatch Prediction vs. Online Prediction\n\nOne fundamental decision you'll have to make that will affect both your\nend users and developers working on your system is how it generates and\nserves its predictions to end-users: online or batch.\n\nOnline prediction is when predictions are generated and returned as soon\nas requests for these predictions arrive. For example, you enter an English\nsentence into Google Translate and get back its French translation\nimmediately. Online prediction is also known as synchronous prediction:\npredictions are generated in synchronization with requests, or on-demand\nprediction: predictions are generated after requests for these predictions,\nnot before. Traditionally, when doing online prediction, requests are sent to\nthe prediction service via RESTful APIs (e.g. HTTP requests \u2014 see Data\npassing through services in Chapter 2).\n\nBatch prediction is when predictions are generated periodically or\nwhenever triggered. The predictions are stored somewhere, such as in SQL\ntables or CSV files, and retrieved as needed. For example, Netflix might\ngenerate movie recommendations for all of its users every four hours, and\nthe precomputed recommendations are fetched and shown to users when\nthey log onto Netflix. Batch prediction is also known as asynchronous\nprediction: predictions are generated asynchronously with requests arrive\n\nThe terms online prediction and batch prediction can be confusing. Both\ncan make predictions for multiple samples (in batch) or one sample at a\ntime. To avoid this confusion, people sometimes prefer the terms\nsynchronous prediction and asynchronous prediction.\n\nFigure 6-4 shows a simplified architecture for batch prediction and\nFigure 6-5 shows a simplified version of online prediction using only batch\nfeatures. We\u2019ll go over what it means to use only batch features next.\n\nBatch Prediction\n\noo\n=\n>\n\nRequests Predictions\nData 7\nWarehouse Prediction\nPrecomputed | (sowfe| | Bath { seni (e\nPredicions \u2014* | SQuy3 features\n\nNY\n\nFigure 6-4. A simplified architecture for batch prediction.\n\nOnline Prediction\n\nPredictions\n\n| Prediction\n\n| service\n\nC\n\nBatch\n+ Data\nfeatures\nWarehouse\n\nNL\n\nFigure 6-5. A simplified architecture for online prediction that uses only batch features.\n\nAs discussed in Chapter 2, features computed from historical data, such as\ndata in databases and data warehouses, are batch features. Features\ncomputed of streaming data \u2014 data in real-time transports \u2014 are\nstreaming features. In batch prediction, only batch features are used. In\nonline prediction, however, it\u2019s possible to use both batch features and\nstreaming features. For example, after a user puts in order on Doordash,\nthey might need the following features to estimate the delivery time:\n\nBatch features: the mean preparation time of this restaurant in the past.\n\nStreaming features: at this moment, how many other orders they have, how\nmany delivery people are available.\n\nOnline Prediction {Streaming}\n\nPredictions\n\nReskime Stream\nvenspor features\nBatch\n\u00bb Data\nfeatures\nWarehouse\n\nNY\n\nBsesrve is HOR VR VATR KH YD\n\nBatch prediction (asynchronous) Online prediction (synchronous)\n\nFrequency Periodical, such as every 4 hours As soon as requests come\n\nUseful for Processing accumulated data when you don\u2019t When predictions are needed\nneed immediate results (such as as soon as data sample is\nrecommendation systems) generated\n\n(such as fraud detection)\n\nOptimized for High throughput Low latency\n\nExamples Netflix recommendations Google Assistant speech recognition\n\nMany people believe that online prediction is less efficient, both in terms of\ncost and performance than batch prediction because you might not be able\nto batch inputs together and leverage vectorization or other optimization\ntechniques. This is not necessarily true, as we already discussed in the\nBatch Processing vs. Stream Processing section in Chapter 2.\n\nAlso, with online prediction, you don\u2019t have to generate predictions for\nusers who aren\u2019t visiting your site. Imagine you run an app where only 2%\nof your users log in daily \u2014 e.g. in 2020, GrubHub had 31 million users and\n622,000 daily orders. If you generate predictions for every user each day,\nthe compute used to generate 98% of your predictions will be wasted.\n\nFrom Batch Prediction To Online Prediction\n\nTo people coming to ML from an academic background, the more natural\nway to serve predictions is probably online. You give your model an input\nand it generates a prediction as soon as it receives that input. This is likely\nhow most people interact with their models while prototyping. This is also\nlikely easier to do for most companies when first deploying a model. You\nexport your model, upload the exported model to AWS Sagemaker or\nGoogle App Engine, and get back an endpoint\u2019. Now, if you send a request\nwhich contains an input to that endpoint, it will send back a prediction\nbased on that input.\n\nA problem with online prediction is that your model might take too long to\ngenerate predictions. If your model takes a couple of seconds too long, your\nusers might get bored.\n\nInstead of generating predictions as soon as they arrive, what if you\ncompute predictions in advance and store them in your database, and fetch\nthem when requests arrive? With this approach, you can generate\npredictions for multiple inputs at once, leveraging distributed techniques to\nprocess a high volume of samples efficiently.\n\nBecause the predictions are precomputed, you don\u2019t have to worry about\nhow long it\u2019ll take your models to generate predictions. For this reason,\nbatch prediction can also be seen as a trick to reduce the inference latency\nof more complex models if the time it takes to retrieve a prediction is less\nthan the time it takes to generate it.\n\nBatch prediction is good for when you want to generate a lot of predictions\nand don\u2019t need the results immediately. One of its common use cases is\nrecommendation systems \u2014 generating recommendations for users every\nfew hours and only pulling out the recommendation for each user when\nthey log into the system. You don\u2019t have to use all the predictions\ngenerated. For example, you can make predictions for all customers on how\nlikely they are to buy a new product, and reach out to the top 10%.\n\nHowever, the problem with batch prediction is that it makes your model\nless responsive to users\u2019 change preferences. This limitation can be seen\neven in more technologically progressive companies like Netflix. Say,\n\nyou\u2019ve been watching a lot of horror movies lately, so when you first log\ninto Netflix, horror movies dominate recommendations. But you're feeling\nbright today so you search \u201ccomedy\u201d and start browsing the comedy\ncategory. Netflix should learn and show you more comedy in your list of\ntheir recommendations, right? But it can\u2019t update the list until the next\nbatch of recommendations is generated.\n\nAnother problem with batch prediction is that you need to know what\nrequests to generate predictions for in advance. In the case of\nrecommending movies for users, you know in advance how many users to\ngenerate recommendations for'\u00ae. However, for cases when you have\nunpredictable queries \u2014 if you have a system to translate from English to\nFrench, it might be impossible to anticipate every possible English text to\nbe translated \u2014 you need to use online prediction to generate predictions as\nrequests arrive.\n\nIn the Netflix example, batch prediction causes mild inconvenience (which\nis tightly coupled with user engagement and retention), not catastrophic\nfailures. There are many applications where batch prediction would lead to\ncatastrophic failures or just wouldn\u2019t work. Examples where online\nprediction is crucial include high frequency trading, autonomous vehicles,\nvoice assistants, unlocking your phones using face or fingerprints, fall\ndetection for elderly care, and fraud detection. Being able to detect a\nfraudulent transaction that happened 3 hours ago is still better than not\ndetecting it at all, but being able to detect it in real-time can prevent it from\ngoing through.\n\nBatch prediction is a workaround for when online prediction isn\u2019t cheap\nenough or isn\u2019t fast enough. Why generate 1 million predictions in advance\nand worry about storing and retrieving them if you can generate each\nprediction as needed at the exact same cost and same speed?\n\nAs hardware becomes more customized/powerful and better techniques are\nbeing developed to allow faster, cheaper online predictions, online\nprediction might become the default.\n\nInference\n\nTraining <>\n\nFigure 6-7. Having two different pipelines for training and inference is a common source for bugs for\nML in production\n\nFigure 6-8 shows a more detailed but also more complex feature of the data\n\npipeline for ML systems that do online prediction. The boxed element\n\nlabeled Research is what people are often exposed to in an academic\n\nenvironment.\n\nInference Steaming oroecang Features\ndata\n\nTraining Static presensing\ndata\n\nFigure 6-7. Having two different pipelines for training and inference is a common source for bugs for\nML in production\n\nmodel\n\nFigure 6-8 shows a more detailed but also more complex feature of the data\npipeline for ML systems that do online prediction. The boxed element\nlabeled Research is what people are often exposed to in an academic\nenvironment.\n\nData pipeline for online ML systems\n\nResearch\n\n\nFigure 6-8. A data pipeline for ML systems that do online prediction\n\nBuilding infrastructure to unify stream processing and batch processing has\nbecome a popular topic in recent years for the ML community. Companies\nincluding Uber and Weibo have made major infrastructure overhauls to\nunify their batch and stream processing pipelines with Apache Flink'4\"5.\n\nModel Compression\n\nWe\u2019ve talked about a streaming pipeline that allows an ML system to\nextract streaming features from incoming data and input them into an ML\nmodel in (near) real-time. However, having a near (real-time) pipeline isn\u2019t\nenough for online prediction. In the next section, we\u2019ll discuss techniques\nfor fast inference for ML models.\n\nIf the model you want to deploy takes too long to generate predictions,\nthere are three main approaches to reduce its inference latency: make it do\ninference faster, make the model smaller, or make the hardware it\u2019s\ndeployed on run faster.\n\nThe process of making a model smaller is called model compression, and\nthe process to make it do inference faster is called inference optimization.\nOriginally, model compression was to make models fit on edge devices.\nHowever, making models smaller often makes them run faster.\n\nWe\u2019ll discuss inference optimization in the section Model Optimization,\nand we\u2019 ll discuss the landscape for hardware backends being developed\nspecifically for running ML models faster in the section ML on the Edge\nlater in this chapter. Here, we\u2019ll discuss model compression.\n\nThe number of research papers on model compression is growing. Off-the-\nshelf utilities are proliferating. As of September 2021, Awesome Open\nSource has a list of The Top 108 Model Compression Open Source Projects\nand that list is growing. While there are many new techniques being\ndeveloped, the four types of techniques that you might come across the\nmost often are low-rank optimization, knowledge distillation, pruning, and\n\nquantization. Readers interested in a comprehensive review might want to\ncheck out Cheng et al.\u2019s A survey of model compression and acceleration\nfor deep neural networks which was updated in 2020.\n\nLow-rank Factorization\n\nThe key idea behind low-rank factorization\u2019\u00ae is to replace high-\ndimensional tensors with lower dimensional tensors. One type of low-rank\nfactorization is compact convolutional filters where the over-\nparameterized (having too many parameters) convolution filters are\nreplaced with compact blocks to both reduce the number of parameters and\nincrease speed.\n\nFor example, by using a number of strategies including replacing 3x3\nconvolution with 1x1 convolution, SqueezeNets achieves AlexNet-level\naccuracy on ImageNet with 50 times fewer parameters\u2018\u2019.\n\nSimilarly, MobileNets decomposes the standard convolution of size\nKxKxC into a depthwise convolution (KxKx1) and a pointwise convolution\n(1x1xC) with K being the kernel size and C being the number of channels.\nThis means that each new convolution uses only K? + C instead of KC\nparameters. If K=3, this means a 8x to 9x reduction in the number of\nparameters\u20198 (see Figure 6-9).\n\nIMAGE TO COME\n\nFigure 6-9. Compact convolutional filters in MobileNets. The standard convolutional filters in (a) are\nreplaced by depthwise convolution in (b) and pointwise convolution in (c) to build a depthwise\nseparable filter. Image by Howard et al.\n\nThis method has been used to develop smaller models with significant\nacceleration compared to standard models. However, it tends to be specific\nto certain types of models (for example, compact convolutional filters are\nspecific to convolutional neural networks) and requires a lot of architectural\nknowledge to design, so it\u2019s not widely applicable to many use cases yet.\n\nKnowledge Distillation\n\nKnowledge distillation is a method in which a small model (student) is\ntrained to mimic a larger model or ensemble of models (teacher). The\nsmaller model is what you\u2019ll deploy. Even though the student is often\ntrained after a pre-trained teacher, both may also be trained at the same\ntime\u2019?. One example of a distilled network used in production is\nDistiIBERT, which reduces the size of a BERT model by 40%, while\nretaining 97% of its language understanding capabilities and being 60%\nfaster2\u00b0,\n\nThe advantage of this approach is that it can work regardless of the\narchitectural differences between the teacher and the student networks. For\nexample, you can get a random forest as the student and a transformer as\nthe teacher. The disadvantage of this approach is that it\u2019s highly dependent\non the availability of a teacher network. If you use a pre-trained model as\nthe teacher model, training the student network will require less data and\nwill likely be faster. However, if you don\u2019t have a teacher available, you\u2019 ll\nhave to train a teacher network before training a student network, and\ntraining a teacher network will require a lot more data and take more time\nto train. This method is also sensitive to applications and network\narchitectures, and therefore hasn\u2019t found wide usage in production.\n\nPruning\n\nPruning was a method originally used for decision trees where you remove\nsections of a tree that are uncritical and redundant for classification2\u2019. As\nneural networks gain wider adoption, people started to realize that neural\n\nnetworks are over-parameterized, and think about how to reduce the\nworkload caused by the extra parameters.\n\nPruning, in the context of neural networks, has two meanings. One is to\nremove entire nodes of a neural network, which means changing its\narchitecture and reducing its number of parameters. The more common\nmeaning is to find parameters least useful to predictions and set them to 0.\nIn this case, pruning doesn\u2019t reduce the total number of parameters, only the\nnumber of non-zero parameters. The architecture of the neural network\nremains the same. This helps with reducing the size of a model because\npruning makes a neural network more sparse, and sparse architecture tends\nto require less storage space than dense structure. Experiments show that\npruning techniques can reduce the non-zero parameter counts of trained\nnetworks by over 90%, decreasing storage requirements and improving\ncomputational performance of inference without compromising accuracy\u201d.\n\nWhile it\u2019s generally agreed that pruning works?\u00b0, there have been many\ndiscussions on the actual value of pruning. Liu et al. argued that the main\nvalue of pruning isn\u2019t in the inherited \u201cimportant weights\u201d, but in the\npruned architecture itself. In some cases, pruning can be useful as an\narchitecture search paradigm, and the pruned architecture should be\nretrained from scratch as a dense model. However, Zhu et al. showed that\nthe large sparse model after pruning outperformed the retrained small\ncounterpart.\n\nQuantization\n\nQuantization is the most general and commonly used model compression\nmethod. It\u2019s straightforward to do and generalizes over tasks and\narchitectures.\n\nQuantization reduces a model\u2019s size by using fewer bits to represent its\nparameters. By default, most software packages use 32 bits to represent a\nfloat number (single precision floating point). If a model has 100M\nparameters, each requires 32 bits to store, it\u2019l] take up 400MB. If we use 16\n\nbits to represent a number, we\u2019ll reduce the memory footprint by half.\nUsing 16 bits to represent a float is called half precision.\n\nInstead of using floats, you can have a model entirely in integers, each\ninteger takes only 8 bits to represent. This method is also known as \u201cfixed\npoint\u201d. In the extreme case, some have attempted the 1-bit representation of\neach weight (binary weight neural networks), e.g. BinaryConnect\u201d4 and\nXnor-Net?\u00b0. The authors of the Xnor-Net paper spun off Xnor.ai, a startup\nthat focused on model compression. In early 2020, it was acquired by Apple\nfor a reported $200M.\n\nQuantization not only reduces memory footprint but also improves the\ncomputation speed. First, it allows us to increase our batch size. Second,\nless precision speeds up computation, which further reduces training time\nand inference latency. Consider the addition of two numbers. If we perform\nthe addition bit by bit, each takes x nanosecond, it\u2019ll take 32x nanoseconds\nfor 32-bit numbers but only 16x nanoseconds for 16-bit numbers.\n\nThere are downsides to quantization. Reducing the number of bits to\nrepresent your numbers means that you can represent a smaller range of\nvalues. For values outside that range, you\u2019ll have to round them up and/or\nscale them to be in range. Rounding numbers leads to rounding errors, and\nsmall rounding errors can lead to big performance changes. You also run the\nrisk of rounding/scaling your numbers to under-/over-flow and rendering it\nto 0. Efficient rounding and scaling is non-trivial to implement at a low\nlevel, but luckily, major frameworks have this built in.\n\nQuantization can either happen during training, which is referred to as\nquantization aware training2\u00b0, where models are trained in lower precision,\nor post-training, where models are trained in single-precision floating point,\nthen trained models are quantized for inference. Using quantization during\ntraining means that you can use less memory for each parameter, which\nallows you to train larger models on the same hardware.\n\nIn the last few years, low precision training has become increasingly\npopular, with support from most modern training hardware. NVIDIA\nintroduced Tensor Cores, processing units that support mixed-precision\n\ntraining\u00ae\u2019. Google TPUs also support training with Bfloat] 6 (16-bit Brain\nFloating Point Format), which the company dubbed as \u201cthe secret to high\nperformance on Cloud TPUs\u201d\u00ae. Training in fixed-point is not yet as\npopular, but has had a lot of promising results29\u00b0\u00b0.\n\nFixed-point inference has become a standard in the industry. Some edge\ndevices only support fixed-point inference. Most popular frameworks for\non-device ML inference \u2014 Google\u2019s TensorFlow Lite, Facebook\u2019s PyTorch\nMobile, NVIDIA\u2019s TensorRT \u2014 offer post-training quantization for free\nwith a few lines of code.\n\nScaling Bert: Key Improvements\n\nHE Thvougtput (inferences per second) from utilizing 32 cores @ Latency in ms (SOth percentie\n\n4000 L\nEC) mS\n3000 x0\nmm m\nin\n1000 100\n100 18\n0 0\nScenario #1 Scenario #2 Scerano #3 Scenario #4\n\u2018Baseline Bert\u201d \u2018Srale Mode\u2019 \u2018Snails pts\u2019 \u201cSmo Weights\u2019\nPyTorch Bert + PyTorch Distibert + (Pytorch Distibet + (Pytorch Distibert +\nFaxed Shape input} Fed Shape put Dynamic Shape input) Dynamic Shape put +\n\nQuantizaton\n\nFigure 6-10. Latency improvement by various model compression methods. Experiment done by\nQuoc Le and Kip Kaehler at Roblox.\n\nThe biggest performance boost they got came from quantization.\n\nConverting 32-bit floating points to 8-bit integers reduces the latency\n7x and increases throughput 8x.\n\nThe results here seem very promising to improve latency, however, they\nshould be taken with a grain of salt since there\u2019s no mention of changes\nin output quality after each performance improvement.\n\nEarlier in the chapter, we mentioned that to enable online prediction for an\nML system, two components are required: a model with fast inference and a\n\nScaling Bert: Key Improvements\n\nHE Thvougtput (inferences per second) from utilizing 32 cores @ Latency in ms (SOth percentie\n\nom m\nm\n1000 10\n100 18\n0 0\nScenario #1 Scenario #2 Scerano #3 Scenario #4\n\u2018Baseline Bert\u201d \u2018Srale Mode\" \u2018Sralls ipa\u2019 \u201cSmaller Weights\u2019\nPyTorch Bert + PyTorch Distiber + (Pytorch Distibert + (Pytorch Distibert +\nFaxed Shape input} Fiaed Shape put Dynamic Shape input) Dynamic Shape input +\n\nQuantizaton|\n\nFigure 6-10. Latency improvement by various model compression methods. Experiment done by\nQuoc Le and Kip Kaehler at Roblox.\n\nThe biggest performance boost they got came from quantization.\n\nConverting 32-bit floating points to 8-bit integers reduces the latency\n7x and increases throughput 8x.\n\nThe results here seem very promising to improve latency, however, they\nshould be taken with a grain of salt since there\u2019s no mention of changes\nin output quality after each performance improvement.\n\nEarlier in the chapter, we mentioned that to enable online prediction for an\nML system, two components are required: a model with fast inference and a\n\n(near) real-time pipeline. We\u2019ve discussed various methods to optimize a\nmodel\u2019s inference speed. In the next part, we\u2019ll discuss a new paradigm that\nhas quickly gained traction in the last five years: stream processing, and\nhow it enables us to build ML systems that can respond in real-time and\nnear real-time. To understand stream processing, we\u2019ll contrast it to the\nolder paradigm: batch processing. Let\u2019s dive in!\n\nML on the Cloud and on the Edge\n\nAnother decision you'll want to consider is where your model\u2019s\ncomputation will happen: on the cloud or on the edge. On the cloud means a\nlarge chunk of computation is done on the cloud, either public clouds or\nprivate clouds. On the edge means a large chunk of computation is done on\nthe consumer devices \u2014 such as browsers, phones, laptops, smartwatches,\ncars, security cameras, robots, embedded devices, F PGAs*\", and ASICs*2\n\u2014 which are also known as edge devices.\n\nThe easiest way is to package your model up and deploy it via a managed\ncloud service such as AWS or GCP, and this is how many companies deploy\nwhen they get started in ML. Cloud services have done an incredible job to\nmake it easy for companies to bring ML models into production.\n\nHowever, there are many downsides to cloud deployment. The first is cost.\nML models can be compute-intensive, and compute is expensive. Even\nback in 2018, big companies like Pinterest, Infor, Intuit, etc. were already\nspending hundreds of millions of dollars in cloud bills every year [1, 2].\nThat number for small and medium companies can be between $50K - 2M a\nyear. A mistake in handling cloud services can cause startups to go\nbankrupt [1, 2].\n\nAs their cloud bills climb, more and more companies are looking for ways\nto push their computations to edge devices. The more computation is done\non the edge, the less is required on the cloud, and the less they\u2019ll have to\npay for servers.\n\nOther than help with controlling costs, there are many properties that make\nedge computing appealing. The first is that it allows your applications to\nrun where cloud computing cannot. When your models are on public\nclouds, they rely on stable Internet connections to send data to the cloud\nand back. Edge computing allows your models to work in situations where\nthere are no Internet connections or where the connections are unreliable,\nsuch as in rural areas or developing countries. I\u2019ve worked with several\ncompanies and organizations that have strict no-Internet policies, which\nmeans that whichever applications we wanted to sell them must not rely on\nInternet connections.\n\nSecond, when your models are already on consumers\u2019 devices, you can\nworry less about network latency. Requiring data transfer over the\nnetwork (sending data to the model on the cloud to make predictions then\nsending predictions back to the users) might make some use cases\nimpossible. In many cases, network latency is a bigger bottleneck than\ninference latency. For example, you might be able to reduce the inference\nlatency of ResNet50 from 30ms to 20ms, but the network latency can go up\nto seconds, depending on where you are and what services you\u2019re trying to\nuse.\n\nPutting your models on the edge is also appealing when handling\nsensitive user data. ML on the cloud means that your systems might have\nto send user data over networks, making it susceptible to being intercepted.\nCloud computing also often means storing data of many users in the same\nplace, which means a breach can affect many people. Nearly 80% of\ncompanies experienced a cloud data breach in the past 18 months,\naccording to Security magazine.\n\nEdge computing makes it easier to comply with regulations, like GDPR,\nabout how user data can be transferred or stored. While edge computing\nmight reduce privacy concerns, it doesn\u2019t eliminate them altogether. In\nsome cases, edge computing might make it easier for attackers to steal user\ndata, such as they can just take the device with them.\n\nTo move computation to the edge, the edge devices have to be powerful\nenough to handle the computation, have enough memory to store ML\nmodels and load them into memory, as well as have enough battery or be\nconnected to an energy source to power the application for a reasonable\namount of time. Running a full-sized BERT on your phone, if your phone is\ncapable of running BERT, is a very quick way to kill its battery.\n\nBecause of the many benefits that edge computing has over cloud\ncomputing, companies are in a race to develop edge devices optimized for\ndifferent ML use cases. Established companies including Google, Apple,\nTesla have all announced their plans to make their own chips. Meanwhile,\nML hardware startups have raised billions of dollars to develop better AI\nchips (see Figure 6-11). It\u2019s projected that by 2025, the number of active\nedge devices worldwide will be over 30 billion.\n\nHardware startup\nSambaNova\nGraphcore\n\nGrog\n\nNuvia\n\nWave Computing\nCambricon\nCerebras\n\nHailo\n\nHabana Labs\nKneron\nProphesee\nSyntiant\n\nGroq\n\nEdgeQ\nLeapMind\n\nRaised ($M) Year founded Location\n\n1100\n682\n362\n293\n203\n200\n2\n88\n5\nB\n65\n65\n62\n53\n50\n\n2017 Bay Area\n2016 UK\n\n2016 Bay Area\n2019 Bay Area\n2008 Bay Area\n2016 China\n2016 Bay Area\n2017 Israel\n2016 Israel\n2015 San Diego\n2014 France\n2017 LA\n\n2016 Bay Area\n2018 Bay Area\n2012 Japan\n\n\nFigure 6-11. Notable hardware startups that have raised money as of September 2021. Fund-raising\ninformation by CrunchBase.\n\nWith so many new offerings for hardware to run ML models on, one\nquestion arises: how do we make your model run on arbitrary hardware\nefficiently? See Figure 6-12. In the following section, we\u2019ll discuss how to\ncompile and optimize a model to run it on a certain hardware backend. In\nthe process, we\u2019ll introduce important concepts that you might encounter\nwhen handling models on the edge including intermediate representations\n(IRs) and compilers.\n\nCompiling and Optimizing Models for Edge Devices\n\nFor a model built with a certain framework, such as TensorFlow or Pytorch,\nto run on a hardware backend, that framework has to be supported by the\nhardware vendor. For example, even though TPUs were released publicly in\nFebruary 2018, it wasn\u2019t until September 2020 that PyTorch was supported\non TPUs. Before then, if you wanted to use a TPU, you'd have to use a\nframework that TPUs supported.\n\nProviding support for a framework on a hardware backend is time-\nconsuming and engineering-intensive. Mapping from ML workloads to a\nhardware backend requires understanding and taking advantage of that\nhardware\u2019s design, and different hardware backends have different memory\nlayouts and compute primitives, as shown in Figure 6-13.\n\nFor example, the compute primitive of CPUs used to be a number (scalar),\nthe compute primitive of GPUs used to be a one-dimensional vector,\nwhereas the compute primitive of TPUs is a two-dimensional vector\n(tensor)*, Performing a convolution operator will be very different with 1-\ndimensional vectors compared to 2-dimensional vectors. Similarly, you\u2019d\nneed to take into account different L1, L2, and L3 layouts and buffer sizes\nto use them efficiently.\n\nMemory Subsystem Architecture\n\nsaan EL RF LR RE\n\nimply managed\n\nmixed\n\nCompute Primitive\nAGE RRREOE\n\nTP\n\nlon} Accum,\nRegister\nFile\n\nex) managed\n\nvector\n\ntensor\n\nFigure 6-13. Different compute primitives and memory layouts for CPU, GPU, and TPU. Image by\nChen et al., 2018.\n\nBecause of this challenge, framework developers tend to focus on providing\nsupport to only a handful of server-class hardware, and hardware vendors\ntend to offer their own kernel libraries for a narrow range of frameworks.\nDeploying ML models to new hardware requires significant manual effort.\n\nInstead of targeting new compilers and libraries for every new hardware\nbackend, what if we create a middle man to bridge frameworks and\nplatforms? Framework developers will no longer have to support every type\nof hardware, only need to translate their framework code into this middle\nman. Hardware vendors can then support one middle man instead of\nmultiple frameworks.\n\nThis type of \u201cmiddle man\u201d is called an intermediate representation (IR). IRs\nlie at the core of how compilers work. From the original code for a model,\ncompilers generate a series of high- and low-level intermediate\nrepresentations before generating the code native to a hardware backend so\nthat it can run on that hardware backend, as shown in Figure 6-14.\n\nThis process is also called \u201clowering\u201d, as in you \u201clower\u201d your high-level\nframework code into low-level hardware-native code. It\u2019s not \u201ctranslating\u201d\nbecause there\u2019s no one-to-one mapping between them.\n\nDifferent IR levels\n\n( Pyfoch\nt TensorFlow\n\nof. wp 2 a nin wea) oh ee\n\nMy:\n\u00bb gta Computation graphs -Hend-tuned Language agnostic\nHarowareagnostc, \u00ab= ML-based Eg: LM, NYC\nhxnet Eq: MAK,\nTencorfloulite\nTensorkT\n\nIntermediate Representations (\n{9]\n\nFigure 6-14. A series of high- and low-level IRs between the original model code to machine code\nthat can run on a given hardware backend\n\nHigh-level IRs are usually computation graphs of your ML models. For\nthose familiar with TensorFlow, the computation graphs here are similar to\nthe computation graphs you have encountered in TensorFlow 1.0, before\nTensorFlow switched to eager execution. In TensorFlow 1.0, TensorFlow\nfirst built the computation graph of your model before running it. This\ncomputation graph allows TensorFlow to understand your model\u2019s structure\nto optimize its runtime.\n\nModel Optimization\n\nAfter you\u2019ve \u201clowered\u201d your code to run your models into the hardware of\nyour choice, an issue you might run into is performance. The generated\nmachine code might be able to run on a hardware backend, but it might not\nbe able to do so efficiently. The generated code may not take advantage of\ndata locality and hardware caches, or it may not leverage advanced features\nsuch as vector or parallel operations that could speed code up.\n\nA typical ML workflow consists of many frameworks and libraries. For\nexample, you might use pandas/dask/ray to extract features from your data.\nYou might use NumPy to perform vectorization. You might use a tree\nmodel like LightGBM to generate features, then make predictions using an\nensemble of models built with various frameworks like sklearn,\nTensorFlow, or HuggingFace\u2019s transformers.\n\nEven though individual functions in these frameworks might be optimized,\nthere\u2019s little to no optimization across frameworks. A naive way of moving\ndata across these functions for computation can cause an order of\nmagnitude slowdown in the whole workflow. A study by researchers at\nStanford DAWN lab found that typical ML workloads using NumPy,\nPandas and TensorFlow run 23 times slower in one thread compared to\nhand-optimized code (Palkar et al., 18).\n\nIn many companies, what usually happens is that data scientists and ML\nengineers develop models that seem to be working fine in development.\n\nconsists of \u2014 convolution, loops, cross-entropy \u2014 and find a way to speed\nit up.\n\nThere are two ways to optimize your ML models: locally and globally.\nLocally is when you optimize an operator or a set of operators of your\nmodel. Globally is when you optimize the entire computation graph end-to-\nend.\n\nThere are standard local optimization techniques that are known to speed up\nyour model, most of them making things run in parallel or reducing\nmemory access on chips. Here are four of the common techniques.\n\n\u00a2 vectorization: given a loop or a nested loop, and instead of\nexecuting it one item at a time, use hardware primitives to operate\non multiple elements contiguous in memory.\n\nparallelization: given an input array (or n-dimensional array),\ndivide it into different, independent work chunks, and do the\noperation on each chunk individually.\n\nloop tiling: change the data accessing order in a loop to leverage\nhardware\u2019s memory layout and cache. This kind of optimization is\nhardware dependent. A good access pattern on CPUs is not a good\naccess pattern on GPUs. A visualization of loop tiling is shown in\nFigure 6-16.\n\noperator fusion: fuse multiple operators into one to avoid\nredundant memory access. For example, two operations on the\nsame array require two loops over that array. In a fused case, it\u2019s\njust a single loop. An example of operator fusion is shown in\nFigure 6-17.\n\nconsists of \u2014 convolution, loops, cross-entropy \u2014 and find a way to speed\nit up.\n\nThere are two ways to optimize your ML models: locally and globally.\nLocally is when you optimize an operator or a set of operators of your\nmodel. Globally is when you optimize the entire computation graph end-to-\nend.\n\nThere are standard local optimization techniques that are known to speed up\nyour model, most of them making things run in parallel or reducing\nmemory access on chips. Here are four of the common techniques.\n\n\u00a2 vectorization: given a loop or a nested loop, and instead of\nexecuting it one item at a time, use hardware primitives to operate\non multiple elements contiguous in memory.\n\nparallelization: given an input array (or n-dimensional array),\ndivide it into different, independent work chunks, and do the\noperation on each chunk individually.\n\nloop tiling: change the data accessing order in a loop to leverage\nhardware\u2019s memory layout and cache. This kind of optimization is\nhardware dependent. A good access pattern on CPUs is not a good\naccess pattern on GPUs. A visualization of loop tiling is shown in\nFigure 6-16.\n\noperator fusion: fuse multiple operators into one to avoid\nredundant memory access. For example, two operations on the\nsame array require two loops over that array. In a fused case, it\u2019s\njust a single loop. An example of operator fusion is shown in\nFigure 6-17.\n\nfor( 4 in Lin)\n{fa,d] = 5 * Bld,\nfor( 4 in Lin)\ntapofit] = Afi,t] + topl[i,t)\na fir Linn)\n4 t Rfid) = tapofiyt]} (i,t)\n\nfon( din Lin)\nALA] = ALi] + SUL) * Ct)\n\nFigure 6-17. An example of an operator fusion. Example by Matthias Boehm.\n\nTo obtain a much bigger speedup, you\u2019d need to leverage higher-level\nstructures of your computation graph. For example, given a convolution\nneural network with the computation graph can be fused vertically or\nhorizontally to reduce memory access and speed up the model, as shown in\nFigure 6-18.\n\nFigure 6-18. Vertical and horizontal fusion of the computation graph of a convolution neural\nnetwork. Illustration by TensorRT.\n\nUsing ML to optimize ML models\n\nAs hinted by the previous section with the vertical and horizontal fusion for\na convolutional neural network, there are many possible ways to execute a\ngiven computation graph. For example, given 3 operators A, B, and C, you\ncan fuse A with B, fuse B with C, or fuse A, B, and C altogether.\n\nTraditionally, framework and hardware vendors hire optimization engineers\nwho, based on their experience, come up with heuristics on how to best\nexecute the computation graph of a model. For example, NVIDIA might\nhave an engineer or a team of engineers who focuses exclusively on how to\nmake ResNet-50 run really fast on their DGX A100 server.*4\n\nThere are a couple of drawbacks to hand-designed heuristics. First, they\u2019re\nnon-optimal. There\u2019s no guarantee that the heuristics an engineer comes up\nwith are the best possible solution. Second, they are non-adaptive.\nRepeating the process on a new framework or a new hardware architecture\nrequires an enormous amount of effort.\n\nThis is complicated by the fact that model optimization is dependent on the\noperators its computation graph consists of. Optimizing a convolution\nneural network is different from optimizing a recurrent neural network,\nwhich is different from optimizing a transformer. Hardware vendors like\nNVIDIA and Google focus on optimizing popular models like ResNet50\nand BERT for their hardware. But what if you, as an ML researcher, come\nup with a new model architecture? You might need to optimize it yourself to\nshow that it\u2019s fast first before it\u2019s adopted and optimized by hardware\nvendors.\n\nIf you don\u2019t have ideas for good heuristics, one possible solution might be\nto try all possible ways to execute a computation graph, record the time\nthey need to run, then pick the best one? However, given a combinatorial\nnumber of possible paths, exploring them all would be intractable. Luckily,\napproximating the solutions to intractable problems is what ML is good at.\nWhat if we use ML to narrow down the search space so we don\u2019t have to\n\nFigure 6-18. Vertical and horizontal fusion of the computation graph of a convolution neural\nnetwork. Illustration by TensorRT.\n\nUsing ML to optimize ML models\n\nAs hinted by the previous section with the vertical and horizontal fusion for\na convolutional neural network, there are many possible ways to execute a\ngiven computation graph. For example, given 3 operators A, B, and C, you\ncan fuse A with B, fuse B with C, or fuse A, B, and C altogether.\n\nTraditionally, framework and hardware vendors hire optimization engineers\nwho, based on their experience, come up with heuristics on how to best\nexecute the computation graph of a model. For example, NVIDIA might\nhave an engineer or a team of engineers who focuses exclusively on how to\nmake ResNet-50 run really fast on their DGX A100 server.*4\n\nThere are a couple of drawbacks to hand-designed heuristics. First, they\u2019re\nnon-optimal. There\u2019s no guarantee that the heuristics an engineer comes up\nwith are the best possible solution. Second, they are non-adaptive.\nRepeating the process on a new framework or a new hardware architecture\nrequires an enormous amount of effort.\n\nThis is complicated by the fact that model optimization is dependent on the\noperators its computation graph consists of. Optimizing a convolution\nneural network is different from optimizing a recurrent neural network,\nwhich is different from optimizing a transformer. Hardware vendors like\nNVIDIA and Google focus on optimizing popular models like ResNet50\nand BERT for their hardware. But what if you, as an ML researcher, come\nup with a new model architecture? You might need to optimize it yourself to\nshow that it\u2019s fast first before it\u2019s adopted and optimized by hardware\nvendors.\n\nIf you don\u2019t have ideas for good heuristics, one possible solution might be\nto try all possible ways to execute a computation graph, record the time\nthey need to run, then pick the best one? However, given a combinatorial\nnumber of possible paths, exploring them all would be intractable. Luckily,\napproximating the solutions to intractable problems is what ML is good at.\nWhat if we use ML to narrow down the search space so we don\u2019t have to\n\nexplore that many paths, and predict how long a path will take so that we\ndon\u2019t have to wait for the entire computation graph to finish executing?\n\nTo estimate how much time a path through a computation graph will take to\nrun turns out to be difficult, as it requires making a lot of assumptions about\nthat graph. It\u2019s much easier to focus on a small part of the graph.\n\nIf you use PyTorch on GPUs, you might have seen\ntorch.backends.cudnn.benchmark=True. When this is set to True,\ncuDNN autotune will be enabled. cuDNN autotune searches over a\npredetermined set of options to execute a convolution operator and then\nchooses the fastest way. cuDNN autotune, despite its effectiveness, only\nworks for convolution operators. A much more general solution is\nautoTVM, which is part of the open-source compiler stack TVM.\nautoTVM works with subgraphs instead of just an operator, so the search\nspaces it works with are much more complex. The way autoTVM works is\nquite complicated, but in simple terms:\n\n1. It first breaks your computation graph into subgraphs.\n2. It predicts how big each subgraph is.\n\n3. It allocates time to search for the best possible path for each\nsubgraph.\n\n4. It stitches the best possible way to run each subgraph together to\nexecute the entire graph.\n\nautoTVM measures the actual time it takes to run each path it goes down,\nwhich gives it ground truth data to train a cost model to predict how long a\nfuture path will take. The pro of this approach is that because the model is\ntrained using the data generated during runtime, it can adapt to any type of\nhardware it runs on. The con is that it takes more time for the cost model to\nstart improving. Figure 6-19 shows the performance gain that autoTVM\ngave compared to cuDNN for the model ResNet-50 on NVIDIA TITAN X.\n\n1.0\n\n012)\n\n3\n\n1\n\nd\n\n0 Hoo\n\n)\n\n505i\n\ni | \u2014 TVM: Mba Model\ngy. TVM: Blackbox Genetic Algorithm\n\nTVM: Random Search\n=== Baseline: cuDNN\n\n0 10 0 30 40 50 60 70 800\nNumber of Trials\n\nFigure 6-19. Speed up achieved by autoTVM over cuDNN for ResNet-50 on NVIDIA TITAN X. It\ntakes ~70 trials for autoTVM to outperform cuDNN. Experiment by Chen et al.\n\nWhile the results of ML-powered compilers are impressive, they come with\na catch: they can be slow. You go through all the possible paths and find the\nmost optimized ones. This process can take hours, even days for complex\nML models. However, it\u2019s a one-time operation, and the results of your\noptimization search can be cached and used to both optimize existing\nmodels and provide a starting point for future tuning sessions. You optimize\nyour model once for one hardware backend then run it on multiple devices\nof that same backend. This sort of optimization is ideal when you have a\nmodel ready for production, and target hardware to run inference on.\n\nML in Browsers\n\nWe\u2019 ve been talking about how compilers can help us generate machine-\nnative code run models on certain hardware backends. It is, however,\npossible to generate code that can run on just any hardware backends by\nrunning that code in browsers. If you can run your model in a browser, you\ncan run your model on any device that supports browsers: Macbooks,\nChromebooks, iPhones, Android phones, and more. You wouldn\u2019t need to\ncare what chips those devices use. If Apple decides to switch from Intel\nchips to ARM chips, it\u2019s not your problem.\n\nWhen talking about browsers, many people think of JavaScript. There are\ntools that can help you compile your models into JavaScript, such as\nTensorFlow.js, Synaptic, and brain.js. However, JavaScript is slow, and its\ncapacity as a programming language is limited for complex logics such as\nextracting features from data.\n\nA much more promising approach is WebAssembly(WASM). WASM is an\nopen standard that allows you to run executable programs in browsers.\nAfter you\u2019ve built your models in sklearn, PyTorch, TensorF low, or\nwhatever frameworks you've used, instead of compiling your models to run\non specific hardware, you can compile your model to WASM. You get back\nan executable file that you can just use with JavaScript.\n\nWASM is one of the most exciting technological trends I\u2019ve seen in the last\ncouple of years. It\u2019s performant, easy to use, and has an ecosystem that is\ngrowing like wildfire [1, 2]. As of September 2021, it\u2019s supported by 93%\nof devices worldwide.\n\nThe main drawback of WASM is that because WASM runs in browsers, it\u2019s\nslow. Even though WASM is already much faster than JavaScript, it\u2019s still\nslow compared to running code natively on devices (such as iOS or\nAndroid apps). A study by Jangda et al. showed that applications compiled\nto WASM run slower than native applications by an average of 45% (on\nFirefox) to 55% (on Chrome).\n\nSummary\n\nCongratulations, you\u2019ve finished possibly the most technical chapter in this\nbook! The chapter is technical because deploying ML models is an\nengineering challenge, not an ML challenge.\n\nWe\u2019ve discussed different ways to deploy a model, comparing online\nprediction with batch prediction, and ML on the edge with ML on the\ncloud. Each way has its own challenges. Online prediction makes your\nmodel more responsive to users\u2019 changing preferences but you have to\nworry about inference latency. Batch prediction is a workaround for when\nyour models take too long to generate predictions, but it makes your model\nless flexible.\n\nSimilarly, doing inference on the cloud is easy to set up, but it becomes\nimpractical with network latency and cloud cost. Doing inference on the\nedge requires having edge devices with sufficient compute power, memory,\nand battery.\n\nHowever, I believe that most of these challenges are due to the limitations\nof the hardware that ML models run on. As hardware becomes more\npowerful and optimized for ML, I believe that ML systems will transition to\nmaking online prediction on-device, illustrated in Figure 6-20.\n\nless\n\nhardware constraint\n\nhigh\n\nbatch ecesliction\n\noe ning fi\n\n\u2018\n\u2018\n\u2018\n\u2019\n\n,\n,\n\u2019\n\u2019\n\u2018\n\u2018\n\u2019\n\u2019\n\u2019\n,\n'\n\u2019\n\u2018\n\u2018\n,\n\u2018\n\u2019\n\nlth prot\n\ncoud conpting\nNe\n\nmode] inference latency\n\n,\n\\ \u2019\n\u2019\n,\n\u2018\n\u2019\n\u2019\n\u2019\n\u2018\n\n\u2018online preslction\neye conputing\n\nry\ny\nwd\n\nonline ecesliction\nceva computing\n9y Google Asia\n\nFigure 6-20. As hardware becomes more powerful, ML models will move to online and on the edge.\n\nIn addition to making predictions on-device, people are also working on\ntechniques that enable ML model training over edge devices, such as the\nefforts around federated learning. And in addition to online predictions,\npeople are also working to allow models to continually learn in production.\nWe'll cover continual learning and federated learning in the next chapter.\n",
                        "extracted-code": ""
                    }
                ]
            },
            {
                "chapter_id": 6,
                "chapter_name": "Chapter 7. Why Machine Learning Systems Fail\nin Production",
                "chapter_path": "./screenshots-images-2/chapter_6",
                "sections": [
                    {
                        "section_id": 6.1,
                        "section_name": "Chapter 7. Why Machine Learning Systems Fail\nin Production",
                        "section_path": "./screenshots-images-2/chapter_6/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_6/section_1/092c075a-623c-4b54-b732-5138f7827c04.png",
                            "./screenshots-images-2/chapter_6/section_1/699b33e1-1769-4e29-964a-6012bb3cbc1b.png",
                            "./screenshots-images-2/chapter_6/section_1/a10d7ac3-a237-4170-87d7-655895e433c8.png",
                            "./screenshots-images-2/chapter_6/section_1/290058d3-8bd9-4a73-908b-4491cdb41f19.png",
                            "./screenshots-images-2/chapter_6/section_1/7aaa1548-6b7f-4ab3-85e8-beff991ed856.png",
                            "./screenshots-images-2/chapter_6/section_1/5a84bb94-a399-4d2d-bfe9-b282b09433dc.png",
                            "./screenshots-images-2/chapter_6/section_1/3e94800d-4310-4eec-9b42-e94e517f33f7.png",
                            "./screenshots-images-2/chapter_6/section_1/4b048fbf-5a35-46f1-b561-a3ffd1fcc334.png",
                            "./screenshots-images-2/chapter_6/section_1/60400822-5d4e-4318-8865-e3175c5ac3e7.png",
                            "./screenshots-images-2/chapter_6/section_1/f7efa53b-7679-4930-a25c-17f3474bdf80.png",
                            "./screenshots-images-2/chapter_6/section_1/d9038394-53c9-4c44-af6e-7f9fed470155.png",
                            "./screenshots-images-2/chapter_6/section_1/600124c8-8102-451d-b9db-a9f26641172d.png",
                            "./screenshots-images-2/chapter_6/section_1/0b25031d-b177-4615-b39a-1c725fe67699.png",
                            "./screenshots-images-2/chapter_6/section_1/336e58e7-9688-4177-8966-45c0ed2eba7d.png",
                            "./screenshots-images-2/chapter_6/section_1/960dfb01-4a3e-4426-b023-003f98beac83.png",
                            "./screenshots-images-2/chapter_6/section_1/305c52a0-ceba-4508-92f0-f25994002794.png",
                            "./screenshots-images-2/chapter_6/section_1/d03c4438-2b14-4fff-b4ce-51769855b011.png",
                            "./screenshots-images-2/chapter_6/section_1/e9c1b3d1-7db3-46bc-9808-fdfe3efeb434.png",
                            "./screenshots-images-2/chapter_6/section_1/8c8f9aa2-4897-4523-b9e8-1c5413e55649.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Let\u2019s start the chapter with a story I was told by an executive that many readers might be able to relate to. About\ntwo years ago, his company hired a consulting firm to develop an ML model to help them predict how many of\neach grocery item they'd need next week, so they could restock the items accordingly. The consulting firm took six\nmonths to develop the model. When the consulting firm handed the model over, his company deployed it and was\nvery happy with its performance. They could finally boast to their investors that they were an Al-powered\ncompany.\n\nHowever, a year later, their numbers went down. The demand for some items was consistently being\noverestimated, which caused the extra items to expire. At the same time, the demand for some items was\nconsistently being underestimated, leading to lost sales. Initially, his inventory team manually changed the model\u2019s\npredictions to correct the patterns they noticed, but eventually, the model\u2019s predictions had become so bad that they\ncould no longer use it. They had three options: pay the same consulting firm an obscene amount of money to\nupdate the model, pay another consulting firm even more money because this firm would need time to get up to\nspeed, or hire an in-house team to maintain the model onwards.\n\nHis company learned the hard way an important lesson that the rest of the industry is also discovering: deploying a\nmodel isn\u2019t the end of the process. A model\u2019s performance degrades over time in production. Once a model has\nbeen deployed, we still have to continually monitor its performance to detect issues as well as deploy updates to\nfix these issues.\n\nIn this chapter and the next, we'll cover the necessary topics to help you keep a model in production after it\u2019s been\ndeployed. We'll start by covering reasons why ML models that perform great during development fail in\nproduction. Then, we'll deepdive into one especially prevalent and thorny issue that affects almost all ML models\nin production: data distribution shifts. This occurs when the data distribution in production differs and diverges\nfrom the data distribution the model was exposed to during training. In Chapter 8, we'll continue the discussion,\ncovering monitoring for distribution shifts and how to continually update your models in production to adapt to\nshifts in data distributions. Because this chapter contains definitions of different types of distribution shifts, it\u2019s\nmore theoretical than the rest of the book.\n\nBefore we dive into this chapter, however, I want to emphasize that detecting data distribution shifts, monitoring\nmodel performance, updating models, and evaluating them in production are all so much easier if your tasks have\nnatural ground truth labels. We'll start by discussing what natural ground truth labels mean. When discussing many\ntopics throughout this chapter and the next, two of the first things we should consider are whether a task has\nnatural ground truth labels and when these natural labels will become available.\n\nNatural Labels and Feedback Loop\n\nTasks with natural ground truth labels are tasks where the model\u2019s predictions can be automatically evaluated or\npartially evaluated by the system. An example is the model that estimates time of arrival on Google Maps. By the\nend of a trip, Google Maps knows how long the trip actually took, and thus can evaluate the accuracy of the\npredicted time of arrival.\n\nNatural labels are ideal for evaluating a model's performance. However, even if your task doesn\u2019t inherently have\nnatural labels, it's possible to set up your system in a way that allows you to collect some feedback on your model.\nFor example, if you're building a translation system like Google Translate, you can have the option for the\ncommunity to submit alternative translations for bad translations. Newsfeed ranking is not a task with inherent\nlabels, but by adding the like button and other reactions to each newsfeed item, Facebook is able to collect\nfeedback on their ranking algorithm.\n\nFor tasks with natural ground truth labels, the time it takes from when a prediction is served until when the\nfeedback on it is provided is the feedback loop length.\n\nTasks with short feedback loops are tasks where ground truth labels are generally available within minutes. The\ncanonical example of this type of task is recommender systems. The goal of a recommender system is to\nrecommend users items they would like. Whether a user clicks on the recommended item or not can be seen as the\nfeedback for that recommendation. A recommendation that gets clicked on can be presumed to be a good\nrecommendation (i.e. the label is POSITIVE) and a recommendation that doesn\u2019t get clicked on can be presumed\nto be bad (i.e. the label is NEGATIVE). Many tasks can be framed as recommendation tasks. For example, you can\nframe the task of predicting ads\u2019 click-through rates as recommending the most relevant ads to users based on their\nactivity histories and profiles.\n\nHowever, not all recommender systems have short feedback loops. Depending on the nature of the item to be\nrecommended, the delay until labels can be seconds to hours, and in some extreme cases, days or weeks. If the\nrecommended items are subreddits to subscribe to on Reddit, people to follow on Twitter, videos to watch next on\nTiktok, etc., the time between when the item is recommended until it\u2019s clicked on, if it\u2019s clicked on at all, is\nseconds. If you work with longer content types like blog posts or articles or YouTube videos, it can be minutes,\neven hours. However, if you build a system to recommend clothes for users like the one Stitch Fix has, you\nwouldn't get feedback until users have received the items and tried them on, which could be weeks later.\n\nUnless next to each recommended item, there\u2019s a prompt that says: \u201cDo you like this recommendation? Yes / No\u201d,\nrecommender systems don\u2019t have explicit negative labels. Even if you add that prompt, there\u2019s no guarantee that\nusers will respond to it. Typically, a recommendation is presumed to be bad if there\u2019s a lack of positive feedback.\nAfter a certain time window, if there is no click, the label is presumed to be negative. Choosing the right window\nlength requires thorough consideration, as it involves the speed and accuracy tradeoff. A short window length\nmeans that you can capture labels faster, which allows you to use these labels for monitoring and continual\nlearning. These two topics will be discussed in the next chapter. However, a short window length also means that\nyou might prematurely label an item as no click before it\u2019s being clicked on.\n\nNo matter how long you set your window length to be, there might still be premature negative labels. In early\n2021, a study by the Ads team at Twitter found that even though the majority of clicks on ads happen within the\nfirst 5 minutes, some clicks happen hours after when the ad is shown. This means that this type of label tends to\ngive an underestimate of the actual click-through rate. If you only record 1000 clicks, the actual number of clicks\nmight be a bit over 1000 clicks.\n\nFor tasks with long feedback loops, natural labels might not arrive for weeks or even months. Fraud detection is an\nexample of a task with long feedback loops. For a certain period of time after a transaction, users can dispute\nwhether that transaction is fraudulent or not. For example, when a customer read their credit card\u2019s statement and\nsaw a transaction they didn\u2019t recognize, they might dispute with their bank, giving the bank the feedback to label\nthat transaction as fraudulent. A typical dispute window is a month to three months. After the dispute window has\npassed, if there\u2019s no dispute from the user, you can presume the transaction to be legitimate.\n\nLabels with long feedback loops are helpful for reporting a model's performance on quarterly or yearly business\nreports. However, they are not very helpful if you want to detect issues with your models as soon as possible. If\n\nthere\u2019s a problem with your fraud detection model and it takes you months to catch, by the time the problem is\nfixed, all the fraudulent transactions your faulty model let through might have caused a small business to go\n\nbankrupt.\n\nCauses of ML System Failures\n\nBefore we identify the cause of ML system failures, let's briefly discuss what an ML system failure is. A failure\nhappens when one or more expectations of the system is violated. In traditional software, we mostly care about a\nsystem's operational expectations: whether the system executes its logics within the expected operational metrics\nsuch as the expected latency and throughput.\n\nFor an ML system, we care about both its operational metrics and its ML performance metrics. For example,\nconsider an English-French machine translation system. Its operational expectation might be that given an English\nsentence, the system returns a French translation within a second latency. Its ML performance expectation is that\nthe returned translation is an accurate translation of the original English sentence 99% of the time.\n\nIf you enter an English sentence into the system and don\u2019t get back a translation, the first expectation is violated,\nso this is a system failure.\n\nIf you get back a translation that isn\u2019t correct, it\u2019s not necessarily a system failure because the accuracy expectation\nallows some margin of error. However, if you keep entering different English sentences into the system and keep\ngetting back wrong translations, the second expectation is violated, which makes it a system failure.\n\nOperational expectation violations are easier to detect, as they're usually accompanied by an operational breakage\nsuch as a timeout, a 404 error on a webpage, an out of memory error, a segmentation fault, etc. However, ML\nperformance expectation violations are harder to detect as it requires measuring and monitoring the performance of\nML models in production. In the example of the English-French machine translation system above, detecting\nwhether the returned translations are correct 99% of the time is difficult if we don\u2019t know what the correct\ntranslations are supposed to be. There are countless examples of Google Translate\u2019s painfully wrong translations\nbeing used by users because they aren\u2019t aware that these are wrong translations. For this reason, we say that ML\nsystems often fail silently.\n\nTo effectively detect and fix ML system failures in production, it\u2019s useful to understand why a model, after proving\nto work well during development, would fail in production. We'll examine two types of failures: Software system\nfailures and ML-specific failures. Software system failures are failures that would have happened to non-ML\nsystems. Here are some examples of software system failures.\n\n+ Dependency failure: a software package or a codebase that your system depends on breaks, which leads\nyour system to break. This failure mode is common when the dependency is maintained by a third party,\nand especially common if the third-party that maintains the dependency no longer exists\u2019.\n\n+ Deployment failure: failures caused by deployment errors, such as when you accidentally deploy the\nbinaries of an older version of your model instead of the current version, or when your systems don\u2019t have\nthe right permissions to read or write certain files.\n\n+ Hardware failures: when the hardware that you use to deploy your model, such as CPUs or GPUs,\ndoesn\u2019t behave the way it should. For example, the CPUs you use might overheat and break down\u2019.\n\n+ Downtime or crashing: if a component of your system runs from a server somewhere, such as AWS ora\nhosted service, and that server is down, your system will also be down.\n\nJust because some failures are not specific to ML doesn\u2019t mean it\u2019s not important for ML engineers to understand.\nIn 2020, Daniel Papasian and Todd Underwood, two ML engineers at Google, looked at 96 cases where a large\nML pipeline at Google broke. They reviewed data from over the previous 15 years to determine the causes and\nfound out that 60 out of these 96 failures happened due to causes not directly related to ML*. Most of the issues\nare related to distributed systems e.g. where the workflow scheduler or orchestrator makes a mistake, or related to\n\nthe data pipeline e.g. where data from multiple sources is joined incorrectly or the wrong data structures are being\nused.\n\nAddressing software system failures requires not ML skills, but traditional software engineering skills, and\naddressing them is beyond the scope of this book. Because of the importance of traditional software engineering\nskills in deploying ML systems, the majority of ML engineering is engineering, not ML\u2018. For readers interested in\nlearning how to make ML systems reliable from the software engineering perspective, | highly recommend the\nbook Reliable Machine Learning, also published by O'Reilly with Todd Underwood as one of the authors.\n\nA reason for the prevalence of software system failures is that because ML adoption in the industry is still nascent,\ntooling around ML production is limited and best practices are not yet well developed or standardized. However,\nas toolings and best practices for ML production mature, there are reasons to believe that the proportion of\nsoftware system failures will decrease and the proportion of ML-specific failures will increase.\n\nML-specific failures are failures specific to ML systems. Examples include data collection and processing\nproblems, poor hyperparameters, changes in the training pipeline not correctly replicated in the inference pipeline\nand vice versa, data distribution shifts that cause a model\u2019s performance to deteriorate over time, edge cases, and\ndegenerate feedback loop.\n\nIn this chapter, we'll focus on addressing ML-specific failures. Even though they account for a small portion of\nfailures, they can be more dangerous than non-ML failures as they\u2019re hard to detect and fix, and can prevent ML\nsystems from being used altogether. We've covered data problems in great detail in Chapter 4: Training Data,\nhyperparameter tuning in Chapter 5: Model Development, and the danger of having two separate pipelines for\ntraining and inference in Chapter 6: Deployment. In this chapter, we'll discuss three new but very common\nproblems that arise after a model has been deployed: changing data distribution, edge cases, and degenerate\nfeedback loops.\n\nProduction Data Differing From Training Data\n\nWhen we say that an ML model learns from the training data, it means that the model learns the underlying\ndistribution of the training data with the goal of leveraging this learned distribution to generate accurate\npredictions for unseen data \u2014 data that it didn\u2019t see during training. We'll go into what this means mathematically\nin the Data Distribution Shifts section below. When the model is able to generate accurate predictions for unseen\ndata, we say that this model \u201cgeneralizes to unseen data.5\u201d The test data that we use to evaluate a model during\ndevelopment is supposed to represent unseen data, and the model's performance on the test data is supposed to\ngive us an idea of how well the model will generalize.\n\nOne of the first things I learned in ML courses is that it\u2019s essential for the training data and the unseen data to come\nfrom the same distribution. The assumption is that the unseen data comes from a stationary distribution that is the\nsame as the training data distribution. If the unseen data comes from a different distribution, the model might not\ngeneralize well\u00ae.\n\nThis assumption is incorrect in most cases for two reasons. First, the underlying distribution of the real-world data\nis unlikely to be the same as the underlying distribution of the training data. Curating a training dataset that can\naccurately represent the data that a model will encounter in production turns out to be very difficult\u201d. Real-world\ndata is multi-faceted, and in many cases, virtually infinite, whereas training data is finite and constrained by the\ntime, compute, and human resources available during the dataset creation and processing. There are many different\nselection and sampling biases, as discussed in Chapter 3, that can happen and make real-world data diverge from\ntraining data. The divergence can be something as minor as real-world data using a different type of encoding of\nemojis. This type of divergence leads to a common failure mode known as the train-serving skew: a model that\ndoes great in development but performs poorly when deployed.\n\nSecond, the real world isn\u2019t stationary. Things change. Data distributions shift. In 2019, when people searched for\nWuhan, they likely wanted to get travel information, but since COVID-19, when people search for Wuhan, they\nlikely want to know about the place where COVID-19 originated. Another common failure mode is that a model\n\ndoes great when first deployed, but its performance degrades over time as the data distribution changes. This\nfailure mode needs to be continually monitored and detected for as long as a model remains in production.\n\nWhen I use COVID-19 as an example that causes data shifts, some people have the impression that data shifts only\nhappen because of unusual events, which implies they don\u2019t happen often. Data shifts happen all the time,\nsuddenly, gradually, or seasonally. They can happen suddenly because of a specific event, such as when your\nexisting competitors change their pricing policies and you have to update your price predictions in response, or\nwhen you launch your product in a new region, or when a celebrity mentions your product which causes a surge in\nnew users, and so on. They can happen gradually because social norms, cultures, languages, trends, industries, and\nmore just change over time. They can also happen due to seasonal variations, such as people might be more likely\nto request rideshares in the winter when it\u2019s cold and snowy than in the spring.\n\nWhen talking about data shifts, many people imagine that they are due to external changes, such as natural\ndisasters, holiday seasons, or user behaviors. But in reality, due to the complexity of ML systems and the poor\npractices in deploying them, a large percentage of what might look like data shifts on monitoring dashboards are\ncaused by internal errors\u00ae, such as bugs in the data pipeline, missing values incorrectly filled in, inconsistencies\nbetween the features extracted during training and inference, features standardized using statistics from the wrong\nsubset of data, wrong model version, or bugs in the app interface that forces users to change their behaviors.\n\nSince this is an error mode that affects almost all ML models, we'll cover this in detail in the section Data\nDistribution Shifts.\n\nEdge Cases\n\nImagine there existed a self-driving car that can drive you safely 99.9% of the time, but the other 0.1% of the time,\nit might get into a catastrophic accident that can leave you permanently injured or even dead\u00ae. Would you use that\ncar?\n\nIf you're tempted to say no, you're not alone. An ML model that performs well on most cases but fails on a small\nnumber of cases might not be usable if these failures cause catastrophic consequences. For this reason, major self-\ndriving car companies are focusing on making their systems work on edge cases \"01112,\n\nEdge cases are the data samples so extreme that they cause the model to make catastrophic mistakes. Even though\nedge cases generally refer to data samples drawn from the same distribution, if there is a sudden increase in the\nnumber of data samples in which your model doesn\u2019t perform well on, it could be an indication that the underlying\ndata distribution has shifted.\n\nAutonomous vehicles are often used to illustrate how edge cases can prevent an ML system from being deployed.\nBut this is also true for any safety-critical application such as medical diagnosis, traffic control, eDiscovery\", etc.\nIt can also be true for non-safety-critical applications. Imagine a customer service chatbot that gives reasonable\nresponses to most of the requests, but sometimes, it spits out outrageously racist or sexist content. This chatbot will\nbe a brand risk for any company that wants to use it, thus rendering it unusable.\n\nEDGE CASES AND OUTLIERS\n\nYou might wonder about the differences between an outlier and an edge case. The definition of what makes an\nedge case varies by discipline. In ML, because of its recent adoption in production, edge cases are still being\ndiscovered, which makes their definition contentious.\n\nIn this book, outliers refer to data: an example that differs significantly from other examples. Edge cases refer\nto performance: an example where a model performs significantly worse than other examples. An outlier can\ncause a model to perform unusually poorly, which makes it an edge case. However, not all outliers are edge\ncases. For example, a person jay-walking on a highway is an outlier, but it\u2019s not an edge case if your self-\ndriving car can accurately detect that person and decide on a motion response appropriately.\n\nDuring model development, outliers can negatively affect your model\u2019s performance, as shown in Figure 7-1.\nIn many cases, it might be beneficial to remove outliers as it helps your model to leam better decision\nboundaries and generalize better to unseen data. However, during inference, you don\u2019t usually have the option\nto remove or ignore the queries that differ significantly from other queries. You can choose to transform it \u2014\nfor example, when you enter \u201cmechin learnin\u201d into Google search, Google might ask if you mean \u201cmachine\nlearning\u201d. But most likely, you'll want to develop a model so that it can perform well even on unexpected\ninputs.\n\n51 tly\n\nFigure 7-1. The image on the left shows the decision boundary when there's no outlier. The image an the right shows the decision boundary when\nthere's ome oullier, which is very different from the decision boundary in the first case, and probably less accurate.\n\nDegenerate Feedback Loop\n\nIn the Natural Labels and Feedback Loop section earlier in this chapter, we discussed a feedback loop as the time it\ntook from when a prediction is shown until the time feedback on the prediction is provided. The feedback can be\nused to extract natural labels to evaluate the model's performance.\n\nA degenerate feedback loop can happen when the predictions themselves influence the feedback, which is then\nused to train the next iteration of the model. More formally, a degenerate feedback loop is created when a system\u2019s\noutputs are used to create or process the same system's inputs, which, in turn, influence the system\u2019s future\noutputs. In ML, a system\u2019s predictions can influence how users interact with the system, and because users\u2019\ninteractions with the system are sometimes used as the inputs to the same system, degenerate feedback loops can\noccur and cause unintended consequences. Degenerate feedback loops are especially common in tasks with natural\nlabels from users, such as recommender systems and ads click-through-rate prediction.\n\nEach new video is randomly assigned an initial pool of traffic (which can be up to hundreds of impressions). This\npool of traffic is used to evaluate each video\u2019s unbiased quality to determine whether it should be moved to a\nbigger pool of traffic or to be marked as irrelevant\u2019,\n\nRandomization has been shown to improve diversity but at the cost of user experience '\u00ae. Showing our users\ncompletely random items might cause users to lose interest in our product. An intelligent exploration strategy, such\nas those discussed in the Contextual Bandits section in Chapter 8, can help increase item diversity with acceptable\nprediction accuracy loss. Schnabel et al. uses a small amount of randomization and causal inference techniques to\nestimate the unbiased value of each song. They were able to show that this algorithm was able to correct a\nrecommendation system to make recommendations fair to creators.\n\nWe've also discussed that degenerative feedback loops are caused by users\u2019 feedback on predictions, and users\u2019\nfeedback on a prediction is biased based on where it is shown. Consider the recommender system example above\nwhere each time you recommend 5 songs to users. You realize that the top recommended song is much more likely\nto be clicked on compared to the other 4 songs. You are unsure whether your model is exceptionally good at\npicking the top song, or whether users click on any song as long as it\u2019s recommended on top.\n\nIf the position in which a prediction is shown affects its feedback in any way, you might want to encode the\nposition information using positional features. Positional features can be numerical (e.g. positions are 1, 2, 3,..) or\nboolean (e.g. whether a prediction is shown in the first position or not). Note that \u201cpositional features\u201d are different\nfrom \u201cpositional embeddings\u201d mentioned in Chapter 4.\n\nHere is a naive example to show how to use positional features. During training, you add \u201cwhether a song is\nrecommended first\u201d as a feature to your training data, as shown in Table 7-1. This feature allows your model to\nlearn how much being a top recommendation influences how likely a song is clicked on.\n\nEach new video is randomly assigned an initial pool of traffic (which can be up to hundreds of impressions). This\npool of traffic is used to evaluate each video\u2019s unbiased quality to determine whether it should be moved to a\nbigger pool of traffic or to be marked as irrelevant'\u2019,\n\nRandomization has been shown to improve diversity but at the cost of user experience '\u00ae. Showing our users\ncompletely random items might cause users to lose interest in our product. An intelligent exploration strategy, such\nas those discussed in the Contextual Bandits section in Chapter 8, can help increase item diversity with acceptable\nprediction accuracy loss. Schnabel et al. uses a small amount of randomization and causal inference techniques to\nestimate the unbiased value of each song. They were able to show that this algorithm was able to correct a\nrecommendation system to make recommendations fair to creators.\n\nWe've also discussed that degenerative feedback loops are caused by users\u2019 feedback on predictions, and users\u2019\nfeedback on a prediction is biased based on where it is shown. Consider the recommender system example above\nwhere each time you recommend 5 songs to users. You realize that the top recommended song is much more likely\nto be clicked on compared to the other 4 songs. You are unsure whether your model is exceptionally good at\npicking the top song, or whether users click on any song as long as it\u2019s recommended on top.\n\nIf the position in which a prediction is shown affects its feedback in any way, you might want to encode the\nposition information using positional features. Positional features can be numerical (e.g. positions are 1, 2, 3,..) or\nboolean (e.g. whether a prediction is shown in the first position or not). Note that \u201cpositional features\u201d are different\nfrom \u201cpositional embeddings\u201d mentioned in Chapter 4.\n\nHere is a naive example to show how to use positional features. During training, you add \u201cwhether a song is\nrecommended first\u201d as a feature to your training data, as shown in Table 7-1. This feature allows your model to\nlearn how much being a top recommendation influences how likely a song is clicked on.\n\nPunk\n\nYear\n\n2019\n\nLady Gaga\n\nFunk Overlord\n\nlistens32\n\nHstenr32\n\nIst Position\n\nFalse\n\nNo\n\n3 Beat It Rock 1989 Michael Jackson \u2014_fancypants False No\n\n4 In Bloom Rock 1991 Nirvana fancypants \u2018True Yes\n\n5 Shallow Pop 2020 Lady Gaga listens32 \u2018True Yes\n\nDuring inference, you want to predict whether a user will click on a song regardless of where the song is\nrecommended, so you might want to set the Ist Position feature to be False. Then you look at the model\u2019s\npredictions for various songs for each user and can choose the order in which to show each song.\n\nThis is a naive example because doing this alone might not be enough to combat degenerative feedback loops. A\nmore sophisticated approach would be to use two different models. The first model predicts the probability that the\nuser will see and consider a recommendation taking into account the position of that recommendation. The second\nmodel then predicts the probability that the user will click on the item given that they saw and considered it. The\nsecond model doesn\u2019t concern positions at all.\n\nData Distribution Shifts\n\nIn the previous section, we've discussed common causes for ML system failures. In this section, we'll zero in onto\none especially sticky cause of failures: data distribution shifts, or data shifts for short. Data distribution shift refers\nto the phenomenon in supervised learning when the data a model works with changes over time, which causes this\nmodel\u2019s predictions to become less accurate as time passes. The distribution of the data the model is trained on is\ncalled the source distribution. The distribution of the data the model runs inference on is called the target\ndistribution.\n\nEven though discussions around data distribution shift have only become common in recent years with the\ngrowing adoption of ML in the industry, data distribution shift in systems that leamed from data has been studied\nas early as in 1986'\u00b0. There\u2019s also a book on dataset distribution shifts by Quifionero-Candela et al. published by\nMIT Press in 2008.\n\nTypes of Data Distribution Shifts\n\nWhile data distribution shift is often used interchangeably with concept drift and covariate shift and occasionally\nlabel shift, these are three distinct subtypes of data shift. To understand what they mean, we first need to define a\ncouple of mathematical notations.\n\nLet\u2019s call the inputs to a model X and its outputs Y. We know that in supervised learning, the training data can be\nviewed as a set of samples from the joint distribution P(X, Y) and then ML usually models P(Y|X). This joint\ndistribution P(X, Y) can be decomposed in two ways:\n\n1. P(X, Y) = P(Y[X)P(X)\n2. P(X, Y) = P(XIY)P(Y)\n\nP(Y|X) denotes the conditional probability of an output given an input \u2014 for example, the probability of an email\nbeing spam given the content of the email. P(X) denotes the probability density of the input. P(Y) denotes the\nprobability density of the output. Label shift, covariate shift, and concept drift are defined as follows.\n\nseconds before a cough or they might not start until the middle of coughing. They might also contain a wide\nvariety of background noise. Your model\u2019s performance on phone recordings won't be very good.\n\nIf you know in advance how the real-world input distribution will differ from your training input distribution, you\ncan leverage techniques such as importance weighting to train your model to work for the real world data.\nImportance weighting consists of two steps: estimate the density ratio between the real-world input distribution\nand the training input distribution, then weight the training data according to this ratio, and train an ML model on\nthis weighted data*\u00ae8,\n\nHowever, because we don\u2019t know in advance how the distribution will change in the real-world, it\u2019s very difficult\nto preemptively train your models to make them robust to new, unknown distributions. There has been research\nthat attempts to help models learn representations of latent variables that are invariant across data distributions, but\nI'm not aware of their adoption in the industry.\n\nLabel Shift\n\nLabel shift, also known as prior shift, prior probability shift or target shift, is when P(Y) changes but P(X|Y)\nremains the same. You can think of this as the case when the output distribution changes but for a given output, the\ninput distribution stays the same.\n\nRemember that covariate shift is when the input distribution changes. When the input distribution changes, the\noutput distribution also changes, resulting in both covariate shift and label shift happening at the same time.\nConsider the breast cancer example for covariate shift above. Because there are more women over 40 in our\ntraining data than in our inference data, the percentage of POSITIVE labels is higher during training. However, if\nyou randomly select person A with breast cancer from your training data and person B with breast cancer from\nyour inference data, A and B have the same probability of being over 40. This means that P(X|Y), or probability of\nage over 40 given having breast cancer, is the same. So this is also a case of label shift.\n\nHowever, not all covariate shifts result in label shifts. It\u2019s a subtle point, so we'll consider another example.\nImagine that there is now a preventive drug that every woman takes that helps reduce their chance of getting breast\ncancer. The probability P(Y|X) reduces for women of all ages, so it\u2019s no longer a case of covariate shift. However,\ngiven a person with breast cancer, the age distribution remains the same, so this is still a case of label shift.\n\nBecause label shift is closely related to covariate shift, methods for detecting and adapting models to label shifts\nare similar to covariate shift adaptation methods. We'll discuss them more in the Handling Data Shifts section\nbelow.\n\nConcept Drift\n\nConcept drift, also known as posterior shift, is when the input distribution remains the same but the conditional\ndistribution of the output given an input changes. You can think of this as \u201csame input, different output\u201d. Consider\nyou're in charge of a model that predicts the price of a house based on its features. Before COVID-19, a 3\nbedroom apartment in San Francisco could cost $2,000,000. However, at the beginning of COVID-19, many\npeople left San Francisco, so the same house would cost only $1,500,000. So even though the distribution of house\nfeatures remains the same, the conditional distribution of the price of a house given its features has changed.\n\nIn many cases, concept drifts are cyclic or seasonal. For example, rideshare\u2019s prices will fluctuate on weekdays\nversus weekends, and flight tickets rise during holiday seasons. Companies might have different models to deal\nwith cyclic and seasonal drifts. For example, they might have one model to predict rideshare prices on weekdays\nand another model for weekends.\n\nGeneral Data Distribution Shifts\n\nThere are other types of changes in the real-world that, even though not well-studied in research, can still degrade\nyour models\u2019 performance.\n\nOne is feature change, such as when new features are added, older features are removed, or the set of all possible\nvalues of a feature changes*\u2019. For example, your model was using years for the \u201cage\u201d feature, but now it uses\n\nseconds before a cough or they might not start until the middle of coughing. They might also contain a wide\nvariety of background noise. Your model\u2019s performance on phone recordings won't be very good.\n\nIf you know in advance how the real-world input distribution will differ from your training input distribution, you\ncan leverage techniques such as importance weighting to train your model to work for the real world data.\nImportance weighting consists of two steps: estimate the density ratio between the real-world input distribution\nand the training input distribution, then weight the training data according to this ratio, and train an ML model on\nthis weighted data*\u00ae8,\n\nHowever, because we don\u2019t know in advance how the distribution will change in the real-world, it\u2019s very difficult\nto preemptively train your models to make them robust to new, unknown distributions. There has been research\nthat attempts to help models learn representations of latent variables that are invariant across data distributions, but\nI'm not aware of their adoption in the industry.\n\nLabel Shift\n\nLabel shift, also known as prior shift, prior probability shift or target shift, is when P(Y) changes but P(X|Y)\nremains the same. You can think of this as the case when the output distribution changes but for a given output, the\ninput distribution stays the same.\n\nRemember that covariate shift is when the input distribution changes. When the input distribution changes, the\noutput distribution also changes, resulting in both covariate shift and label shift happening at the same time.\nConsider the breast cancer example for covariate shift above. Because there are more women over 40 in our\ntraining data than in our inference data, the percentage of POSITIVE labels is higher during training. However, if\nyou randomly select person A with breast cancer from your training data and person B with breast cancer from\nyour inference data, A and B have the same probability of being over 40. This means that P(X|Y), or probability of\nage over 40 given having breast cancer, is the same. So this is also a case of label shift.\n\nHowever, not all covariate shifts result in label shifts. It\u2019s a subtle point, so we'll consider another example.\nImagine that there is now a preventive drug that every woman takes that helps reduce their chance of getting breast\ncancer. The probability P(Y|X) reduces for women of all ages, so it\u2019s no longer a case of covariate shift. However,\ngiven a person with breast cancer, the age distribution remains the same, so this is still a case of label shift.\n\nBecause label shift is closely related to covariate shift, methods for detecting and adapting models to label shifts\nare similar to covariate shift adaptation methods. We'll discuss them more in the Handling Data Shifts section\nbelow.\n\nConcept Drift\n\nConcept drift, also known as posterior shift, is when the input distribution remains the same but the conditional\ndistribution of the output given an input changes. You can think of this as \u201csame input, different output\u201d. Consider\nyou're in charge of a model that predicts the price of a house based on its features. Before COVID-19, a 3\nbedroom apartment in San Francisco could cost $2,000,000. However, at the beginning of COVID-19, many\npeople left San Francisco, so the same house would cost only $1,500,000. So even though the distribution of house\nfeatures remains the same, the conditional distribution of the price of a house given its features has changed.\n\nIn many cases, concept drifts are cyclic or seasonal. For example, rideshare\u2019s prices will fluctuate on weekdays\nversus weekends, and flight tickets rise during holiday seasons. Companies might have different models to deal\nwith cyclic and seasonal drifts. For example, they might have one model to predict rideshare prices on weekdays\nand another model for weekends.\n\nGeneral Data Distribution Shifts\n\nThere are other types of changes in the real-world that, even though not well-studied in research, can still degrade\nyour models\u2019 performance.\n\nOne is feature change, such as when new features are added, older features are removed, or the set of all possible\nvalues of a feature changes?\u201d. For example, your model was using years for the \u201cage\u201d feature, but now it uses\n\nmonths, so the range of this feature values has drifted. One time, our team realized that our model\u2019s performance\nplummeted because a bug in our pipeline caused a feature to become NaNs.\n\nLabel schema change is when the set of possible values for Y change. With label shift, P(Y) changes but P(X|Y)\nremains the same. With label schema change, both P(Y) and P(X|Y) change. A schema describes the structure of\nthe data, so the label schema of a task describes the structure of the labels of that task. For example, a dictionary\nthat maps from a class to an integer value, such as {\u201cPOSITIVE\u201d: 0, \u201cNEGATIVE\u201d: 1}, is a schema.\n\nWith regression tasks, label schema change could happen because of changes in the possible range of label values.\nImagine you're building a model to predict someone's credit score. Originally, you used a credit score system that\nranged from 300 to 850, but you switched to a new system that ranges from 250 to 900.\n\nWith classification tasks, label schema change could happen because you have new classes. For example, suppose\nyou are building a model to diagnose diseases and there\u2019s a new disease to diagnose. Classes can also become\noutdated or more fine-grained. Imagine that you're in charge of a sentiment analysis model for tweets that mention\nyour brand. Originally, your model predicted only 3 classes: POSITIVE, NEGATIVE, and NEUTRAL. However,\nyour marketing department realized the most damaging tweets are the angry ones, so they wanted to break the\nNEGATIVE class into two classes: SAD and ANGRY. Instead of having three classes, your task now has four\nclasses. When the number of classes changes, your model\u2019s structure might change,?\u00ae and you might need to both\nrelabel your data and retrain your model from scratch. Label schema change is especially common with high-\ncardinality tasks \u2014 tasks with a high number of classes \u2014 such as product or documentation categorization.\n\nThere's no rule that says that only one type of shift should happen at one time. A model might suffer from multiple\ntypes of drift, which makes handling them a lot more difficult.\n\nHandling Data Distribution Shifts\n\nHow companies handle data shifts depends on how sophisticated their ML infrastructure setups are. At one end of\nthe spectrum, we have companies that have just started with ML and are still working on getting ML models into\nproduction, so they might not have gotten to the point where data shifts are catastrophic to them. However, at some\npoint in the future \u2014 maybe 3 months, maybe 6 months \u2014 they might realize that their initial deployed models\nhave degraded to the point that they do more harm than good. They will then need to adapt their models to the\nshifted distributions or to replace them with other solutions.\n\nAt the same time, many companies assume that data shifts are inevitable, so they periodically retrain their models\n\u2014 once a quarter, once a month, once a week, or even once a day \u2014 regardless of the extent of the shift. How to\ndetermine the optimal frequency to retrain your models is an important decision that many companies still\ndetermine based on gut feelings instead of experimental data2\u00ae. We'll discuss more about the retraining frequency\nin the Continual Learning section in chapter 8.\n\nMany companies want to handle data shifts in a more targeted way, which consists of two parts: first, detecting the\nshift and second, addressing the shift.\n\nDetecting Data Distribution Shifts\n\nData distribution shifts are only a problem if they cause your model\u2019s performance to degrade. So the first idea\nmight be to monitor your model\u2019s accuracy-related metrics* in production to see whether they have changed.\n\u201cChange\u201d here usually means \u201cdecrease\u201d, but if my model\u2019s accuracy suddenly goes up or fluctuates significantly\nfor no reason that I\u2019m aware of, I'd want to investigate.\n\nAccuracy-related metrics work by comparing the model\u2019s predictions to ground truth labels**. During model\ndevelopment, you have access to ground truth, but in production, you don\u2019t always have access to ground truth,\nand even if you do, ground truth labels will be delayed, as discussed in the section Natural Labels and Feedback\nLoop above. Having access to ground truth within a reasonable time window will vastly help with giving you\nvisibility into your model\u2019s performance.\n\nWhen ground truth labels are unavailable or too delayed to be useful, we can monitor other distributions of interest\ninstead. The distributions of interest are the input distribution P(X), the label distribution P(Y), and the conditional\ndistributions P(X!Y) and P(Y[X).\n\nWhile we don\u2019t need to know the ground truth labels Y to monitor the input distribution, monitoring the label\ndistribution and both of the conditional distributions require knowing Y. In research, there have been efforts to\nunderstand and detect label shifts without labels from the target distribution. One such effort is Black Box Shift\nEstimation by Lipton et al., 2018. However, in the industry, most drift detection methods focus on detecting\nchanges in the input distribution, especially the distributions of features as we'll discuss in detail in Chapter 8.\n\nStatistical methods\n\nIn industry, a simple method many companies use to detect whether the two distributions are the same is to\ncompare their metrics like mean, median, variance, quantiles, skewness, kurtosis, etc. For example, you can\ncompute the median and variance of the values of a feature during inference and compare them to the metrics\ncomputed during training. As of October 2021, even TensorFlow\u2019s built-in data validation tools use only summary\nstatistics to detect the skew between the training and serving data and shifts between different days of training\ndata. This is a good start, but these metrics aren\u2019t sufficient. Mean, median, and variance are only useful with the\ndistributions for which the mean/median/variance are useful summaries. If those metrics differ significantly, the\ninference distribution might have shifted from the training distribution. However, if those metrics are similar,\nthere\u2019s no guarantee that there's no shift.\n\nA more sophisticated solution is to use a two-sample hypothesis test, shortened as two-sample test. It\u2019s a test to\ndetermine whether the difference between two populations (two sets of data) is statistically significant. If the\ndifference is statistically significant, then the probability that the difference is a random fluctuation due to\nsampling variability is very low, and therefore, the difference is caused by the fact that these two populations come\nfrom two distinct distributions. If you consider the data from yesterday to be the source population and the data\nfrom today to be the target population and they are statistically different, it\u2019s likely that the underlying data\ndistribution has shifted between yesterday and today.\n\nA caveat is that just because the difference is statistically significant doesn\u2019t mean that it is practically important.\nHowever, a good heuristic is that if you are able to detect the difference from a relatively small sample, then it is\nprobably a serious difference. If it takes a huge sample, then it is probably not worth worrying about.\n\nA basic two-sample test is the Kolmogorov-Smirnov test, also known as K-S or KS test\u00ae, It\u2019s a nonparametric\nstatistical test, which means it doesn\u2019t require any parameters of the underlying distribution to work. It doesn\u2019t\nmake any assumption about the underlying distribution, which means it can work for any distribution. However,\none major drawback of the KS test is that it can only be used for one-dimensional data. If your model\u2019s predictions\nand labels are one-dimensional (scalar numbers), then the KS test is useful to detect label or prediction shifts.\nHowever, it won\u2019t work for high-dimensional data, and features are usually high-dimensional\u2019. K-S tests can also\nbe expensive and produce too many false positive alerts\u21224.\n\nAnother test is Least-Squares Density Difference, an algorithm that is based on the least squares density-difference\nestimation method**, There is also MMD, Maximum Mean Discrepancy, (Gretton et al. 2012) a kernel-based\ntechnique for multivariate two-sample testing and its variant Learned Kernel MMD (Liu et al., 2020). MMD is\npopular in research, but as of writing this book, I\u2019m not aware of any company that is using it in the industry. alibi-\ndetect is a great open-source package with the implementations of many drift detection algorithms, as shown in\nFigure 7-2.\n\nFigure 7-2. Some drift detection algorithms implemented by Image screenshotted from its GitHub repository\n\nBecause two-sample tests often work better on low-dimensional data than on high-dimensional data, it's highly\nrecommended that you reduce the dimensionality of your data before performing a two-sample test on them\u201d.\n\nTime scale windows for detecting shifts\n\nNot all types of shifts are equal \u2014 some are harder to detect than others. For example, shifts happen at different\nrates, and abrupt changes are easier to detect than slow, gradual changes*\u2019. Shifts can also happen across two\ndimensions: spatial or temporal. Spatial shifts are shifts that happen across access points, such as your application\ngets a new group of users or your application is now served on a different type of device. Temporal shifts are shifts\nthat happen over time. To detect temporal shifts, a common approach is to treat input data to ML applications as\ntime series data*\u00ae\n\nWhen dealing with temporal shifts, the time scale window of the data we look at affects the shifts we can detect. If\nyour data has a weekly cycle, then a time scale of less than a week won't detect the cycle. Consider the data in\n\ngure 7-3. If we use data from day 9 to day 14 as the base distribution, then day 15 looks like a shift. However, if\nwe use data from day | to day 14 as the base distribution, then all data points from day 15 are likely being\ngenerated by that same distribution. As illustrated by this example, detecting temporal shifts is hard when shifts are\nconfounded by seasonal variation.\n\n\nDay! Day? Day3 Day\u00e9 DayS Daye Day? DayS Day9 Dayi0 Daylt Dayi? Duyi3 Dayls Day is\n\nFigure 7-3. Whether a distribution has drifted over time depends on the time scale window specified. If we use data from day 9 to day 14 as the base\ndistribution, then day 18 looks like a drifi. However, if we use data from day ! to day 14 as the base distribution, then all data points from day 15 are\nlikely being generated by that same distribution.\n\nWhen computing running statistics over time, it's important to differentiate between cumulative and sliding\nstatistics. Sliding statistics are computed within a single time scale window, e.g. an hour. Cumulative statistics are\ncontinually updated with more data. This means for each the beginning of each time scale window, the sliding\naccuracy is reset, whereas the cumulative sliding accuracy is not. Because cumulative statistics contain\ninformation from previous time windows, they might obscure what happens in a specific time window. Figure 7-4\nshows an example of how cumulative accuracy can hide the sudden dip in accuracy between the hour 16 and 18.\n\nsooo Example cumulative and sliding accuracy on a given day\n\nws\n\nwo\n\n\u00b0 2 \u2018 6 cy] \u00a9 2 \u201c \u00b0 Fy 2\nTine Os)\n\nFigure 7-4. Cumulative accuracy hides the sudden dip in accuracy between the hour 16 and 18. This image is based on an example from MadeWithML.\n\nWorking with data in the temporal space makes things so much more complicated, requiring knowledge of time\nseries analysis techniques such as time series decompositions that are beyond the scope of this book. For readers\ninterested in time series decomposition, here's a great case study by Lyft engineering on how they decompose their\ntime series data to deal with the seasonality of the market.\n\nAs of today, many companies use the distribution of the training data as the base distribution and monitor the\nproduction data distribution at a certain granularity level, such as hourly and daily. The shorter your time scale\n\nwindow, the faster you'll be able to detect changes in your data distribution. However, too short a time scale\nwindow can lead to false alarms of shifts, like the example in Figure 7-3.\n\nSome platforms, especially those dealing with real-time data analytics such as monitoring, provide a merge\noperation that allows merging statistics from shorter time scale windows to create statistics for larger time scale\nwindows. For example, you can compute the data statistics you care about hourly, then merge these hourly\nstatistics chunks into daily views.\n\nMore advanced monitoring platforms even attempt a root cause analysis (RCA) feature that automatically analyzes\nmetrics across various time window sizes to detect exactly the time window where a change in data happened*\u00ae.\n\nAddressing Data Distribution Shifts\n\nTo make a model work with a new distribution in production, there are three main approaches. The first is the\napproach that currently dominates research: train models using massive datasets. The hope here is that if the\ntraining dataset is large enough, the model will be able to learn such a comprehensive distribution that whatever\ndata points the model will encounter in production will likely come from this distribution.\n\nThe second approach, less popular in research, is to adapt a trained model to a target distribution without requiring\nnew labels. Zhang et al. (2013) used causal interpretations together with kernel embedding of conditional and\nmarginal distributions to correct models\u2019 predictions for both covariate shifts and label shifts without using labels\nfrom the target distribution. Similarly, Zhao et al. (2020) proposed domain-invariant representation learning: an\nunsupervised domain adaptation technique that can learn data representations invariant to changing distributions.\nHowever, this area of research is heavily underexplored and hasn't found wide adoption in industry*\u00b0.\n\nThe third approach is what is usually done in the industry today: retrain your model using the labeled data from the\ntarget distribution. However, retraining your model is not so straightforward. Retraining can mean retraining your\nmodel from scratch on both the old and new data or continuing training the existing model on new data. The latter\napproach is also called fine-tuning.\n\nIf you want to fine-tune your model, the question might be what data to use: data from the last hour, last 24 hours,\nlast week, or last 6 months. A common practice is to fine-tune your model from the point when data has started to\ndrift. Another is to fine-tune your model using the data gathered from the last fine-tuning. You might need to run\nexperiments to figure out which retraining solution works best for you\u2019.\n\nFine-tuning on only new data is obviously preferred because it requires less computing resources and runs faster\nthan retraining a model from scratch on both the old and new data. However, depending on their setups, many\ncompanies find that fine-tuning doesn\u2019t give their models good-enough performance, and therefore have to fall\nback to retraining from scratch.\n\nIn this book, we use \u201cretraining\u201d to refer to both training from scratch and fine-tuning. We'll discuss more about\nthe value of data freshness and the retraining frequency in the Continual Learning section in the next chapter.\n\nReaders familiar with data shift literature might often see data shifts mentioned along with domain adaptation and\ntransfer learning. If you consider a distribution to be a domain, then the question of how to adapt your model to\nnew distributions is similar to the question of how to adapt your model to different domains.\n\nSimilarly, if you consider learning a joint distribution P(X, Y) as a task, then adapting a model trained on one joint\ndistribution for another joint distribution can be framed as a form of transfer learning. As discussed in chapter 3,\ntransfer learning refers to the family of methods where a model developed for a task is reused as the starting point\nfor a model on a second task. The difference is that with transfer learning, you don\u2019t retrain the base model from\nscratch for the second task. However, to adapt your model to a new distribution, you might need to retrain your\nmodel from scratch.\n\nAddressing data distribution shifts doesn\u2019t have to start after the shifts have happened. It\u2019s possible to design your\nsystem to make it more robust to shifts. A system uses multiple features, and different features shift at different\nrates. Consider that you're building a model to predict whether a user will download an app. You might be tempted\nto use that app\u2019s ranking in the app store as a feature since higher ranking apps tend to be downloaded more.\n\nHowever, app ranking changes very quickly. You might want to instead bucket each app\u2019s ranking into general\ncategories such as top 10, between 11 - 100, between 101 - 1000, between 1001 - 10,000, and so on. At the same\ntime, an app\u2019s categories might change a lot less frequently, but might have less power to predict whether a user\nwill download that app. When choosing features for your models, you might want to consider the trade-off\nbetween the performance and the stability of a feature: a feature might be really good for accuracy but deteriorate\nquickly, forcing you to train your model more often.\n\nYou might also want to design your system to make it easier for it to adapt to shifts. For example, housing prices\nmight change a lot faster in major cities like San Francisco than in rural Arizona, so a housing price prediction\nmodel serving rural Arizona might need to be updated less frequently than a model serving San Francisco. If you\nuse the same model to serve both markets, you'll have to use data from both markets to update your model at the\nrate demanded by San Francisco. However, if you use a separate model for each market, you can update each of\nthem only when necessary.\n\nBefore we move on to the next chapter, I just want to reiterate that not all performance degradation of models in\nproduction requires ML solutions. Many ML failures today, not software system failures and ML-specific failures,\nare still caused by human errors. If your model failure is caused by human errors, you'd first need to find those\nerrors to fix them. Detecting a data shift is hard, but determining what causes a shift can be even harder.\n\nSummary\n\nThis might have been the most challenging chapter for me to write in this book. The reason is that despite the\nimportance of understanding how and why ML systems fail in production, the literature surrounding it is limited.\n\u2018We usually think of research preceding production, but this is an area of ML where research is still trying to catch\nup with production.\n\nIn the study of ML outages, the only study I could find comes from Google where Daniel Papasian and Todd\nUnderwood looked at 96 cases of ML outage over the previous 15 years. Even for that case, the number of outages\nwas relatively small and came from within one company. However, with the standardization of tools, more vendors\nhave insights into how their customers\u2019 models failed, which means there's potential for a larger scale study of ML.\nfailures across companies and systematic categorization of ML failures.\n\nIn this chapter, we discussed three major causes of ML failures: data distribution shifts, edge cases, and\ndegenerative feedback loops. The first two causes are related to data, whereas the last cause is related to system\ndesign because it happens when the system\u2019s outputs influence the same system's input. The last cause can be\nespecially dangerous since it can cause ML models to perpetuate biases embedded in the training data.\n\nAfter discussing causes of ML failures, we went deeper into different types of data distribution shifts. Data\ndistribution shifts happen in two scenarios: when the data distribution in production is different from the data\nduring training, which is called the train-serving skew, and when the distribution shifts over time in production. We\nalso discussed how to detect and address shifts. While it might be possible to address a data distribution shift\nwithout retraining the model, in the industry, the most common solution is to retrain the model.\n\nEven though this is a growing subfield of ML research, the research community hasn't yet found a standard\nnarrative. Different papers call the same phenomena by different names, and occasionally there is confusion on\nwhat exactly is concept drift or covariate shift. Many studies are still based on the assumption that we know in\nadvance how the distribution will shift or have the labels for the data from both the source distribution and the\ntarget distribution. However, in reality, we don\u2019t know what the future data will be like, and it can be too costly or\nslow or infeasible to obtain labels for new data.\n\nFiguring out how ML models fail in production is the easy step. The difficult step is how to monitor our systems\nfor failures and how to adapt our systems to changing environments, which we'll discuss in the next chapter:\nMonitoring and Continual Learning.\n",
                        "extracted-code": ""
                    }
                ]
            }
        ]
    }
}