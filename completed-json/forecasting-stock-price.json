{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "MASTER: Market-Guided Stock Transformer for Stock Price Forecasting",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "MASTER: Market-Guided Stock Transformer for Stock Price Forecasting",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/64ca1a6c-8528-4a2d-a4e1-8ffd569b5ef2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Abstract\n\nStock price forecasting has remained an extremely challeng-\ning problem for many decades due to the high volatility of the\nstock market. Recent efforts have been devoted to modeling\ncomplex stock correlations toward joint stock price forecast-\ning. Existing works share a common neural architecture that\nlearns temporal patterns from individual stock series and then\nmixes up temporal representations to establish stock correla-\ntions. However, they only consider time-aligned stock cor-\nrelations stemming from all the input stock features, which\nsuffer from two limitations. First, stock correlations often oc-\ncur momeatarily and in a cross-time manner. Second, the fea-\nture effectiveness is dynamic with market variation, which af-\nfects both the stock sequential patterns and their correlations.\nTo address the limitations, this paper introduces MASTER, a\nMArkert-Guided Stock TransformER, which models the mo-\nmentary and cross-time stock correlation and leverages mar-\nket information for automatic feature selection. MASTER el-\negantly tackles the complex stock correlation by alternatively\nengaging in intra-stock and inter-stock information aggrega-\ntion. Experiments show the superiority of MASTER com-\npared with previous works and visualize the captured realistic\nstock correlation to provide valuable insights.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.2,
                        "section_name": "Introduction",
                        "section_path": "./screenshots-images-2/chapter_1/section_2",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_2/cc47067d-7d02-43b9-a325-5badf53c4568.png",
                            "./screenshots-images-2/chapter_1/section_2/a90d549a-c7c8-4211-bb8b-603ebf78aa29.png",
                            "./screenshots-images-2/chapter_1/section_2/d4836dc0-e4f6-40bb-900a-9d5c07814651.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Stock price forecasting, which utilizes historical data col-\nlected from the stock market to predict future trends, is a\nvital technique for profitable stock investment. Unlike sta-\ntionary time series that often exhibit regular patterns such\nas periodicity and steady trends, the dynamics in the stock\nprice series are intricate because stock prices fluctuate sub-\nject to multiple factors, including macroeconomic factors,\ncapital flows, investor sentiments, and events. The mixing\nof factors interweaves the stock market as a correlated net-\nwork, making it difficult to precisely predict the individual\nbehavior of stocks without taking other stocks into account.\n\nMost previous works (Feng et al. 2019; Xu et al. 2021;\nWang et al. 2021, 2022; Wang, Qu, and Chen 2022) in the\nfield of stock correlation have relied on predefined concepts,\nrelationships, or rules and established a static correlation\ngraph, e.g., stocks in the same industry are connected to each\n\n| FBR esis mp\n\u201cll SSN tees\n\nstock sequences representations _carrelation module predictions\n\nFigure 1: The framework of existing works. The dashed lines\nrepresent the underlying momentary and cross-time stock\ncorrelations, which reside between some (stock, , time),\n(stock, time) pairs.\n\nother. While these methods provide insights into the rela-\ntions between stocks, they do not account for the real-time\ncorrelation of stocks. For example, different stocks within\nthe same industry can experience opposite price movements\non a particular day. Additionally, the pre-defined relation-\nships may not be generalizable to new stocks in an evolv-\ning market where events such as company listing, delist-\ning, or changes in the main business happen normally. An-\nother line of research (Yoo et al. 2021) follows the Trans-\nformer architecture (Vaswani et al. 2017), and use the self-\nattention mechanism to compute dynamic stock correlations.\nThis data-driven manner is more flexible and applicable to\nthe time-varying stock sets in the market. Despite differ-\nent schemes for establishing stock correlations, the exist-\ning methods generally follow a common two-step compu-\ntation flow. As depicted in Figure 1, the first step is using a\nsequential encoder to summarize the historical sequence of\nstock features, and obtain stock representation, and the sec-\nond step is to refine each stock representation by aggregating\ninformation from correlated stocks using graph encoders or\nattention mechanism. However, such a flow suffers from two\nlimitations.\n\nFirst, existing works distill an overall stock representation\nand blur the time-specific details of stock sequence, lead-\ning to weakness in modeling the de-facto stock correlations,\nwhich often occurs momentarily and in a cross-time man-\nner (Bennett, Cucuringu, and Reinert 2022). To be specific,\nthe stock correlation is highly dynamic and may reside in\nmisaligned time steps rather than holding true through the\nwhole lookback period. This is because the dominating fac-\ntors of stock prices constantly change, and different stocks\nmay react to the same factors with different delays. For in-\n\nstance, upstream companies\u2019 stock prices may react faster to\na shortage of raw materials than those of downstream com-\npanies, and individual stocks exhibit a lot of catch-up and\nfall-behind behaviors.\n\nSince the stock correlation may underlie between every\nstock pair and time pair, a straightforward way to simu-\nlate the momentary and cross-time correlation is to gather\nthe 7 x |S| feature vectors for pair-wise attention computa-\ntion, where 7 is the lookback window length and S is the\nstock set. However, in addition to the increased computa-\ntional complexity, this approach faces practical difficulties\nbecause the stock forecasting task is in intense data hunger.\nIntuitively, there are only around 250 trading days per year,\nproducing limited observations on stocks. When the model\nadopts such a large attention field with insufficient training\nsamples, it often struggles to optimize and may even fall\ninto suboptimal solutions. Although clustering approaches\nlike local sensitive hashing (Kitaev, Kaiser, and Levskaya\n2020) have been proposed to reduce the size of the attention\nfield, they are sensitive to initialization, which is a fatal issue\nin a data-hungry domain like stock forecasting. To address\nthese challenges, we propose a novel stock transformer ar-\nchitecture specifically designed for stock price forecasting.\nRather than directly modeling the r x |S| attention field or\nusing clustering-based approximation methods, our model\naggregates information from different time steps and differ-\nent stocks alternately to model realistic stock correlation and\nfacilitate model learning.\n\nAnother limitation of existing works is that they ignore\nthe impact of varying market status. In long-term practice\nwith the market variation, one essential observation by in-\nvestors is that the features come into effect and expire dy-\nnamically. The effectiveness of features has an influence on\nboth the intra-stock sequential pattern and the stock correla-\ntion. For instance, in a bull market, the correlations among\nstocks are more significant due to the investors\u2019 optimism.\nTraditional investors repeatedly conduct statistical examina-\ntion on to select effective feature, which is exhaustive and\nface a gap when integrated with learning-based methods.\nTo save the human efforts, we are motivated to equip our\nstock transformer with a novel gating mechanism, which in-\ncorporates the market information to perform automatically\nfeature selection. We name the proposed method MASTER,\nstanding for MArket-Guided Stock TransformER. To sum-\nmarize, our main contributions are as follows.\n\nWe propose a novel stock transformer for stock price\nforecasting to effectively capture the stock correlation. To\nthe best of our knowledge, we are the first to mine the\nmomentary and cross-time stock correlation with learning-\nbased methods.\n\nWe introduce a novel gating mechanism that integrates\nmarket information to automatically select relevant features\nand adapt to varying market scenarios.\n\nWe conducted experiments to validate the designs of our\nproposed method and demonstrated its superiority compared\nto baselines. The visualization results provided valuable in-\nsights into the real-time dynamics of stock correlations.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.3,
                        "section_name": "Methodology",
                        "section_path": "./screenshots-images-2/chapter_1/section_3",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_3/ef354085-6a26-4bff-b9f6-1285f095447b.png",
                            "./screenshots-images-2/chapter_1/section_3/11c38089-1319-4df0-8411-eaa634ac641a.png",
                            "./screenshots-images-2/chapter_1/section_3/bd2b0d33-ec1b-4ff7-a390-3ddd40adddde.png",
                            "./screenshots-images-2/chapter_1/section_3/f8986c6e-46d0-400b-a24a-185f744513b7.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Problem Formulation\n\nThe indicators of each stock u \u20ac S are collected at every\ntime step \u00a2 \u20ac [1,7] to form the feature vector r,, \u20ac R\u201d.\nFollowing existing works on stock market analysis (Feng\net al. 2018; Sawhney et al. 2020; Huynh et al. 2023), we\nfocus on the prediction of the change in stock price rather\nthan the absolute value. The retum ratio, which is the rel-\native close price change in d days, is Fu = (\u00a2u,r+a \u2014\nCu.r+1)/Cur+i+ Where e+ is the closing price of stock u\nat time step \u00a2, and d represents the predetermined prediction\ninterval. The return ratio normalizes the market price vari-\nety between different stocks in comparison to the absolute\nprice change. Since stock investment is to rank and select\nthe most profitable stocks, we perform daily Z-score normal-\nization of return ratio to encode the label with the rankings,\nr, = Norms(?,), as in previous work (Yang et al. 2020).\n\nDefinition 1 (Stock Price Forecasting) Given stock fea-\ntures {uy\u00a2}ues,te[i.r) the stock price forecasting is to\njointly predict the future normalized return ratio {r.}ues-\n\nOverview\n\nFigure 2 depicts the architecture of our proposed method\nMASTER, which consists of five steps. (1) Market-Guided\nGating. We construct a vector representing the current mar-\nket status m, and leverage it to rescale feature vectors by\na gating mechanism, achieving market-guided feature selec-\ntion. (2) Intra-Stock Aggregation. Within the sequence of\neach stock, at each time step, we aggregate information from\nother time steps to generate a local embedding that preserves\nthe temporal local details of the stock while collecting all\nimportant signals along the time axis. The local embedding\nhy will serve as relays and transport the collected signals\nto other stocks in subsequent modules. (3) Inter-Stock Ag-\ngregation. At each time step, we compute stock correlation\nwith attention mechanism, and each stock further aggregates\nthe local embedding of other stocks. The aggregated infor-\nmation 2.,\u00a2, which we refer to as temporal embedding, con-\ntains not only the information of the momentarily correlated\nstocks at t, but also preserves the personal information of\nu. (4) Temporal Aggregation. For each stock, the last tem-\nporal embedding queries from all historical temporal em-\nbedding and produce a comprehensive stock embedding e,,.\n(5) Prediction. The comprehensive stock embedding is sent\nto prediction layers for label prediction. We elaborate on\nthe details of MASTER step by step in the following sub-\nsections.\n\nMarket-Guided Gating\n\nMarket Status Representation \u2014 First, we propose to com-\nbine information from two aspects into a vector m, to give\nan abundant description of the current market status. (1)\nMarket index price. The market index price is a weighted\naverage of the prices of a group of stocks S\u2019 by their share\nof market capitalization. S\u2019 is typically composed of top\ncompanies with the most market capitalization, represent-\ning a particular market or sector, and may differ from user-\n\n1. Market-Guided Gating\n\n\u2018Time Step \u00a2\nzu 5\n\u201c\nfog\n\u2018\nYej\u2014 Stock w\n\nFigure 2: Overview of the MASTER framework.\n\ninterested stocks in investing S. We include both the cur-\nrent market index price at 7 and the historical market index\nprices, which is described by the average and standard de-\nviation in the past d\u2019 days to reveal the price fluctuations.\nHere, d\u2019 specifies the referable interval length to introduce\nhistorical market information in applications. (2) Market in-\ndex trading volume. The trading volumes of S\u2019 reveals the\ninvestors involvement, reflecting the activity of the market.\nWe include the average and standard deviation of market in-\ndex trading volume in the past d\u2019 days, to reveal the actual\nsize of the market. S' and d\u2019 are identical to the aforemen-\ntioned definitions. Now we present the market-guided stock\nprice forecasting task.\n\nDefinition 2 (Market-Guided Stock Price Forecasting)\nGiven {u,\u00a2}ues.te[i,r) and the constructed market status\nvector m,, market-guided stock price forecasting is to\njointly predict the future normalized return ratio {r,}ues-\n\nGating Mechanism The gating mechanism generates one\nscaling coefficient for each feature dimension to enlarge or\nshrink the magnitude of the feature, thereby emphasizing\nor diminishing the amount of information from the feature\nflowing to the subsequent modules. The gating mechanism\nis learned by the model training, and the coefficient is opti-\nmized by how much the feature contributes to improve fore-\ncasting performance, thus reflect the feature effectiveness.\n\nGiven the market status representation m,,|m,| = F\u2019,\nwe first use a single linear layer to transform m, into the\nfeature dimension F = |2,, \u00a2|. Then, we perform Softmax\nalong the feature dimension to obtain a distribution.\n\na(m,) = F + softmaxs(Wam, +a),\n\nwhere W,,, b,, are learnable matrix and bias, {9 is the temper-\nature hyperparameter controlling the sharpness of the output\ndistribution. Softmax compels a competition among features\nto distinguish the effective ones and ineffective ones. Here,\na smaller temperature 3 encourages the distribution to focus\non certain dimension and the gating effect is stronger while a\nlarger 3 makes the distribution incline to even and the gating\neffect is weaker. Note that we enlarge the value at each di-\nmension by F times as the scaling coefficient. This operation\n\ncompare the generated distribution with a uniform distribu-\ntion where each dimension is 1/F, to determine whether to\nenlarge or shrink the value. The intuition to generate coef-\nficients from m, is that the effectiveness of features are in-\nfluenced by market status. For example, if the model learns\nmoving average (MA) factor is useful during volatile mar-\nket periods, it will emphasize MA when the market becomes\nvolatile again. Under the same m,, a are shared for {x7 \u00a2},\nu \u20ac S,t \u20ac [1,7], in that we incorporate the most recent mar-\nket status to perform unified feature selection. The rescaled\nfeature vectors are Z,, = a(m,) o \u00a9, \u00a2, where o is the\nHadamard product.\n\nIntra-Stock Aggregation\n\nIn MASTER, we use intra-stock aggregation followed by\ninter-stock aggregation to break down the large and complex\nattention field. Although the entire market is complicated\nwith diverse behaviours of individual stocks, the patterns of\na specific stock tend to be relatively continuous. Therefore,\nwe perform intra-stock aggregation first due to its smaller at-\ntention field and simpler distribution. In our proposed intra-\nstock aggregation, the feature at each time step aggregate in-\nformation from other time steps and form a local embedding.\nCompared with existing works which initially mix the fea-\nture sequence into one representation (Yoo et al. 2021), we\nmaintain a sequence of local embedding which are advised\nwith the important signals in sequence through intra-stock\naggregation while reserve the local details.\n\nWe first send the rescaled feature vectors to a feature en-\ncoder and transform them into the embedding space, y,, \u00a2 =\nf(\u00aeu.), \\Yue| = D. We simply use a single linear layer as\nJ(-). Then, we apply a bi-directional sequential encoder to\nobtain the local output at each time step t. Inspired by the\nsuccess of transformer-based models in modeling sequential\npatterns, we instantiate the sequential encoder with a single-\nlayer transformer encoder (Vaswani et al. 2017). Each fea-\nture vector at a particular time step is treated as a token, and\nwe add a fixed D-dimensional sinusoidal positional encod-\ning p: to mark the chronically order in the look back window.\n\nYu = Sleep LN(f(@ue) + Peds\nwhere || denotes the concatenation of vectors and LN the\n\nlayer normalization. Then, the feature embedding at each\ntime step queries from all time steps in the stock sequence.\nWe introduce multi-head attention mechanisms, denoted as\nMHA(;), with V, heads to perform different aggregations in\nparallel. We also utilize feed forward layers, FFN(-), to fuse\nthe information obtained from the multi-head attention.\nQ.=WhY., KL=WkY, Vi= Wi,\n\nHD, = |leeriftue = FFN'(MHA' (Qi, KL, Vit) + Yu),\nwhere FFN is a two-layer MLP with ReLU activation and\nresidual connection. As a result, the local embedding h,, \u00a2\nboth reserve the local details and encode indicative signals\nfrom other time steps.\n\nInter-Stock Aggregation\n\nThen, we consider aggregating information from correlated\nstocks. Compared with existing works that distill an over-\nall stock correlation, we establish a series of momentary\nstock correlation corresponding to every time step. Instead\nof using pre-defined relationships that face a mismatch with\nthe proximity of real-time stock movements, we propose to\nmine the asymmetric and dynamic inter-stock correlation\nvia attention-mechanism. The quality of the correlation will\nbe measured by its contribution to improving the forecast-\ning performance, and automatically optimized by the model\ntraining process.\n\nSpecifically, at each time step, we gather the local embed-\nding of all stocks H? = |/,,cshy,. and perform multi-head\nattention mechanism with N> heads.\n\nQ?=W3H?, K?=WRH?, V2 =WH?,\n\n2, = |Iucs%ue = FFN\"(MHA*(Q?, K?, V2) + H2).\nWith the residual connection of FFN, the temporal embed-\nding 2u,+ is encoded with both the information from momen-\ntarily correlated stocks and the personal information of stock\nu itself. Our stock transformer is able to model the cross-\ntime correlation of stocks, as shown in Figure 2 (Right). The\nlocal details of y, ; can first be conveyed to h, ; by the intra-\nstock aggregation of stock v, and then transmitted to 2,\nby inter-stock aggregation at time step i, hence modeling\nthe correlation from any (v, j) to (w, i). We further visualize\nand explain the captured cross-time correlation in the exper-\niments section.\n\nTemporal Aggregation\n\nIn contrast with existing works which obtain one embedding\nfor each stock after modeling stock correlation (Feng et al.\n2019), our approach involves producing a series of temporal\nembedding z,;,\u00a2 \u20ac [1,7]. Each z,\u00a2 is encoded with in-\nformation from stocks that are momentarily correlated with\n(u, t). To summarize the obtained temporal embeddings and\nobtain a comprehensive stock embedding e,,, we employ a\ntemporal attention layer along the time axis. We use the lat-\nest temporal embedding z,, - as the query vector, and com-\npute the attention score A,, ; in a hidden space with transfor-\nmation matrix W,,\n\nexp(2eWrzu.r)\ndat = Soo = Au,t2u,t+\nDieter) PCT Wazu,r) x\u00bb\n\nPrediction and Training\n\nFinally, the stock embedding e. is fed into a predictor 9(-)\nfor label regression. We use a single linear layer as the pre-\ndictor, and the forecasting quality is measured by the MSE\nloss. In each batch, MASTER is jointly optimized for all\nu \u20ac S ona particular prediction date. And a training epoch\nis composed of multiple batches correspond to different pre-\ndiction dates in the training set.\n\nFu =9(eu), L= SO MSE(ra, Fu).\n\nues\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.4,
                        "section_name": "Discussions",
                        "section_path": "./screenshots-images-2/chapter_1/section_4",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_4/4974376d-4a92-4341-87d6-dd07b232ad63.png",
                            "./screenshots-images-2/chapter_1/section_4/404c3ee8-bdfd-4fa2-90b9-5bf1fa918515.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "Relationships with Existing Works Modeling stock cor-\nrelations has long been an indispensable research direc-\ntion for stock price prediction. Today, many researchers\nand quantitative analysts, still opt for linear models, sup-\nport vector machines, and tree-based methods for stock price\nforecasting (Nugroho, Adji, and Fauziati 2014; Chen and\nGuestrin 2016; Kamble 2017; Xie et al. 2013; Li et al. 2015;\nPiccolo 1990). The aggregation of correlation information\nwithin and between stocks is often achieved through fea-\nture engineering, which relies heavily on manual expertise\nand constantly faces the risk of factor decay. Inspired by\nthe success of neural sequential data analysis, researchers\nare driven to take into account the stock feature sequences\nand learn the temporal correlation automatically. They de-\nsign various sequential models, such as RNN-based (Feng\net al. 2019; Sawhney et al. 2021; Yoo et al. 2021; Huynh\net al. 2023), CNN-based (Wang et al. 2021), and attention-\nbased models(Liu et al. 2019; Ding et al. 2020), to mine the\ninternal temporal dynamics of a stock. Recent research focus\non the modeling of stock correlation, which add a correlation\nmodule in posterior to the sequential model as illustrated in\nFigure 1. They propose to use graph-based (Feng et al. 2019;\nXu et al. 2021; Wang et al. 2021, 2022), hypergraph-based\n(Sawhney et al. 2021; Huynh et al. 2023) and attention-\nbased (Yoo et al. 2021; Xiang et al. 2022) modules to build\nthe overall stock correlation and perform joint prediction.\nOur MASTER is dedicated to momentary and cross-time\nstock correlation mining. To do so, we develop a novel\nmodel architecture as in Figure 2 that is genuinely different\nfrom all existing methods. Furthermore, MASTER is spe-\ncialized for stock price forecasting, which is distinct in data\nform and task properties from existing transformer-based\nmodels in spatial-temporal data (Bulat et al. 2021; Cong\net al. 2021; Xu et al. 2020; Li et al. 2023) or multivariate\ntime series domains (Zhang and Yan 2022; Nie et al. 2022).\n\nComplexity Analysis We now analyze the computation\ncomplexity of our proposed method. Let M = |S|, the\nmarket-guided gating rescale M x 7 feature vectors of\ndimension F. In intra-stock aggregation, the calculation\namount of pair-wise attention is 7? for each stock at\neach attention head. In inter-stock aggregation, the calcu-\nlation amount is M? at each time step and each atten-\ntion head. In temporal aggregation, we compute 7 atten-\ntion scores for each stock. The overall computation com-\nplexity is O(F Mr + NiMr?D? + N2M?rD? + MrD*),\n\nwhere M >> r. Therefore, MASTER is of O(N2M?7D*)\ntime complexity. Compared with directly operating on the\nM x 7 attention field with N attention heads, which is\nin O(NM?r*D*), we reduce the computation cost by\nabout 7 times and achieve modeling cross-time correla-\ntions between stocks more efficiently. The overall param-\neters to be trained in MASTER are transformation matri-\nces Wh,Whk, Wy, W3,Wk, We, Wa, which is in shape\nD x D, and parameters in MLP layers \u00ab, f,FFN',FFN?\nand g.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.5,
                        "section_name": "Experiments",
                        "section_path": "./screenshots-images-2/chapter_1/section_5",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_5/22a1f759-bf16-4034-b73e-c26a1d33231b.png",
                            "./screenshots-images-2/chapter_1/section_5/b6aedd37-534e-47bd-9bc1-8d1a88f6f663.png",
                            "./screenshots-images-2/chapter_1/section_5/7fec84df-d137-4ef5-817a-fcfe4c6ff0d0.png",
                            "./screenshots-images-2/chapter_1/section_5/39e62ca4-116f-4824-8ef2-0e15557f9cc4.png",
                            "./screenshots-images-2/chapter_1/section_5/13bcbbc7-fd26-46fa-9317-71675732f4c5.png",
                            "./screenshots-images-2/chapter_1/section_5/d58d0036-a31e-43bb-9f05-d8abc83455c2.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In this section, we conduct experiments to answer the fol-\nlowing four research questions:\n\n* RQI How is the overall performance of MASTER com-\npared with state-of-the-art methods?\n\n* RQ2 Is the proposed stock transformer architecture ef-\nfective for stock price forecasting?\n\n* RQ3 How do hyper-parameter configurations affect the\nperformance of MASTER?\n\n* RQ4 What insights on the stock correlation can we get\nthrough visualizing the attention map?\n\nDatasets We evaluate our framework on the Chinese stock\nmarket with CSI300 and CSI800 stock sets. CSI300 and\nCSI800 are two stock sets containing 300 and 800 stocks\nwith the highest capital value on the Shanghai Stock Ex-\nchange and the Shenzhen Stock Exchange. The dataset\ncontains daily information ranging from 2008 to 2022 of\nCSI300 and CSI800. We use the data from QI 2008 to QI\n2020 as the training set, data in Q2 2020 as the validation\nset, and the last ten quarters, ie., Q3 2020 to Q4 2022, are\nreserved as the test set. We apply the public Alphal58 in-\ndicators (Yang et al. 2020) to extract stock features from\nthe collected data. The lookback window length 7 and pre-\ndiction interval d are set as 8 and 5 respectively. For mar-\nket representation, we constructed 63 features with CSI300,\nCSIS00 and CSI800 market indices, and refereable interval\nd\u2019 = 5, 10, 20, 30, 60.\n\nBaselines We compare the performance of MASTER with\nseveral stock price forecasting baselines from different cate-\ngories. e XGBoost (Chen and Guestrin 2016): A decision-\ntree based method. According to the leaderboard of Qlib\nplatform (Yang et al. 2020), it is one of the strongest base-\nlines. e LSTM (Graves and Graves 2012), GRU (Cho et al.\n2014), TCN (Bai, Kolter, and Koltun 2018), and Trans-\nformer (Vaswani et al. 2017): Sequential baselines that lever-\nage vanilla LSTM/GRU/Temporal convolutional network/-\nTransformer along the time axis for stock price forecasting.\n\u00a2 GAT (Velitkovi\u00e9 et al. 2017): A graph-based baseline,\nwhich first use sequential encoder to gain stock presenta-\ntion and then aggregate information by graph attention net-\nworks', e DTML (Yoo et al. 2021): A state-of-the-art stock\ncorrelation mining method, which follows the framework in\nFigure 1. DTML adopts the attention-mechanism to mine\n\nthe dynamic correlation among stocks and also incorporates\nthe market information into the modeling.\n\nEvaluation We adopt both ranking metrics and portfolio-\nbased metrics to give a thorough evaluation of the model\nperformance. Four ranking metrics, Information Coefficient\n(IC), Rank Information Coefficient (RankIC), Information\nRatio based IC (ICIR) and Information Ratio based RankIC\n(RankICIR) are considered. IC and RankIC are the Pearson\ncoefficient and Spearman coefficient averaged at a daily fre-\nquency. ICIR and RankICIR are normalized metrics of IC\nand RankIC by dividing the standard deviation. Those met-\nrics are commonly used in literature (e.g., Xu et al. 2021 and\nYang et al. 2020) to describe the performance of the forecast-\ning results from the value and rank perspectives. Further-\nmore, we employ two portfolio-based metrics to compare\nthe investment profit and risk of each method. We simulate\ndaily trading using a simple strategy that selects the top 30\nstocks with the highest return ratio and reports the Excess\nAnnualized Return (AR) and Information Ratio (IR) met-\nrics. AR measures the annual expected excess return gener-\nated by the investment, while IR measures the risk-adjusted\nperformance of an investment.\n\nImplementation We implemented MASTER? with Py-\n\u2018Torch and build our methods based on the open-source quan-\ntitative investment platform Qlib (Yang et al. 2020). For\nDTML, we implement it based on the original paper since\nthere is no official implementation publicly. For other base-\nlines, we use their Qlib implementations. For hyperparame-\nters of each baseline method, the layer number and model\nsize are tuned from {1,2,3} and {128,256,512} respec-\ntively. The learning rate [r is tuned among {10~*}e(3,4.5,6}+\nand we selected the best hyperparameters based on the IC\nperformance in the validation stage. For hyperparameters of\nMASTER, we tune the model size D and learning rate Ir\namong the same range as the baselines, and the final selec-\ntion is D=256, Ir=10~S for all datasets; we set N,=4, N2=2\nfor all datasets and S=5 and \u00a7=2 for CSI300 and CSI800\nrespectively. More implementation details of baseline meth-\nods are summarized in the supplementary materials. Each\nmodel is trained for at most 40 epochs with early stopping.\nAll the experiments are conducted on a server equipped with\nIntel(R) Xeon(R) Platinum 8163 CPU, 128GB Memory, and\na Tesla V100-SXM2 GPU (16GB Memory). Each experi-\nment was repeated 5 times with random initialization and\nthe average performance was reported.\n\nOverall Performance (RQ1)\n\nThe overall performance are reported in Table 1 MAS-\nTER achieves the best results on 6/8 of the ranking met-\nrics, and consistently outperforms all benchmarks in the\nportfolio-based metrics. In particular, MASTER achieve\n13% improvements in ranking metrics and 47% improve-\nments in portfolio-based metrics compared to the second-\nbest results on the average sense. Note that ranking matrics\nare computed with the whole set and portfolio-based metrics\n\nDataset | Model 1 Ic Ick RankIC RankICIR AR IR\nXGBoost 0.0514\u00a30.001 0.370.010.0504 0.001 0.36 \u00a30.01 | 0.234003 1.9403\nLSTM 0.049 + 0.001 0.05140.002 0.414003 | 0204004 20+04\nGRU 0.052 + 0.004 0.05240.005 0.344004 | 0.194004 T5203\n\nCSI300 TCN 0.050 + 0.002 0.33 + 0.04 0.049 + 0.002 0.31 + 0.04 0.18 + 0.05 14405\nTransformer | 0.04740.007 0.3940.040.0514.0.002 0.42 40.04 | 0.2240.06 20404\nGAT 0.05440.002 0.360.020.0414 0.002 0.25 \u00a30.02 | 0.194003 13403\nDTML 0.049\u00a30.006 0.3340.04 0.0524.0.005 0.3340.04 | 0.214003 17403\nMASTER | 0.064\" + 0.006 0.42+0.04 0.0767\u00a30.005 0.49+0.04 | 0.27+0.05 2.4404\nXGBoost 0.040 + 0.000 0.37 + 0.01 0.047 + 0.000 0.42 + 0.01 0.08 + 0.02 0640.2\nLSTM 0.028 40.002 0.320.020.0839 0.002 0.41 \u00a30.03 | 0.094002 0.9402\nGRU 0.039 + 0.002 0.36 + 0.05 0.044 + 0.003 0.39 + 0.07 0.07 + 0.04 0640.3\n\ncsisoo | TN 0.038 40.002 0.8340.04 0.0454.0.002 0.3840.05 | 0.054004 04403\n\u2018Transformer 0.040 + 0.003 0.43 + 0.03 0.048 + 0.003 0.51 + 0.05 0.13 + 0.04 11403\nGAT 0.043 +0.002 0.394 0.020.042 4.0.002 0.35 40.02 | 0.104004 0.7403\nDTML 0.039 + 0.004 0.29 + 0.03 0.053 + 0.008 0.37 + 0.06 0.16 + 0.03 1340.2\nMASTER | 0.052\"+0.006 0.40+0.06 0.06640.007  0.4840.06 | 0.287 \u00a30.02 2.3\u00b0\u00a30.3\n\nTable 1: Overall performance comparison. The best results are in bold and the second-best results are underlined. And * denotes\nstatistically significant improvement (measured by t-test with p-value < 0.01) over all baselines.\n\nModel | Ic IcIR RankIC RankICIR_ | AR IR\n\n(MA)STER 0.064+0.003 0.43+0.02 0.074+0.004 0.48+0.04 | 0.25+0.03 2140.3\n(MA)STER-Bi | 0.058+0.005 0.88+ 0.04 0.066 + 0.008 =\u2014-0.41+ 0.05 | 0.19+0.03 16+0.2\nNaive 0.041+0.008 0.304005 0.04620.007 0.32+0.04 | 0.18+0.05 16406\nClustering 0.044+0.003 = 0.36+0.02 0.049+ 0.005 = 0.39 + 0.04 | 0.184004 L7+03\n\nTable 2: Experiments on CS1300 to validate the effectiveness of proposed stock transformer architecture. The best results are in\n\nbold and the second-best results are underlined.\n\nmostly consider the 30 top-performed stocks. The achieve-\nments in both types of metrics imply that MASTER is of\ngood predicting ability on the whole stock set without sacri-\nficing the accuracy of the important stocks. The significant\nimprovements cast light on the importance of stock correla-\ntion modeling, so each stock can also benefit from the his-\ntorical signals of other momentarily correlated stocks. We\nalso observe all methods gain better performance on CS1300\nover CSI800. We believe it is because CSI300 consists of\ncompanies with larger capitalization whose stock prices are\nmore predictable. When compared to the existing stock cor-\nrelation method (i.e, DTML), MASTER outperforms in all\n6 metrics, which tells our proposed Market-Guided Gat-\ning and aggregation techniques are more efficient in mining\ncross-stock information than existing literature.\n\nStock Transformer Architecture (RQ2)\n\nWe validate the effectiveness of our specialized stock trans-\nformer architecture by experiments on four settings. (1)\n(MA)STER, which is our stock transformer without the gat-\ning. (2) (MA)STER-Bi, in which we substitute the single-\nlayer transformer encoder with a bi-directional LSTM to\nevince that the effectiveness of our proposed architecture\nis not coupled with strong sequential encoders. (3) Naive,\nwhich directly performs information aggregation among 7 x\n|S| tokens. (4) Clustering, in which we adapt the Local Sen-\nsitive Hashing (Kitaev, Kaiser, and Levskaya 2020) to allo-\n\ncate all tokens into 10 buckets by similarity and perform ag-\ngregation within each group, which is a classic task-agnostic\ntechnique to reduce the scale of the attention field. For a fair\ncomparison, in (3) and (4), we first use the same transformer\nencoder to extract token embedding and then use the same\nmulti-head attention mechanism as in our stock transformer,\nso the only difference is the attention field. Due to resource\nlimits, we only conduct experiment on CSI300 dataset. The\nresults in Table 2 illustrate the efficacy of our tailored stock\ntransformer architecture, which performs intra-stock aggre-\ngation and inter-stock aggregation alternatively.\n\nAblation Study (RQ3)\n\nFirst, we conduct ablation study on (NV, ,. V2) combination.\n\u2018The results of CS1300 are shown in Figure 3 and the results\non CSI800 are similar. The difference among head combina-\ntions is not significant compared with the inherent variance\nunder each setting. In the studied range, most settings con-\nsistently performed better than the baselines.\n\nSecond, we study the influence of temperature \u00a7 in the\ngating mechanism. As explained before, a smaller 9 forces\na stronger feature selection while a larger 8 turns off the gat-\ning effect. Figure 4 shows the performance with varying 8.\nThe CSI300 is a relatively easier dataset where most fea-\ntures are quite effective, so the temperature is expected to\nbe larger to relax the feature selection, while more powerful\nfeature selection intervention is needed for the sophisticated\n\nIc cir Rankic\n\nRarkiCla AR IR\n\nFigure 3: The average and standard deviation of metrics with different (Ni, N2) combinations on CS1300.\n\nankle\n\naa\n\nFigure 4: MASTER performance with varying \u00a7. The horizontal dash lines are performance without market-guided gating.\n\ncnecysi601857) ae\nIeACSHEDII3E 002\nCATLISZ 500750) ua\nFigure 5: The correlation towards three target stocks on Aug\n19th, 2022. The y-axis is time steps in the lookback win-\n\ndow and the x-axis is source stocks. Avg. denotes the evenly\ndistributed value.\n\nCRACK CATE CATLA CNPC. 190 CNPC A ICRC 19th CREE eMC 78h\n\nTijasers faaesers\n\n3\nTisaseoe\n\nFigure 6: Cross-time correlation of stock pairs on Aug 19th\nand 25th, 2022. The x-axis is the source time steps and the\ny-axis is the target time steps.\n\nCSI800 dataset whose 9 of the best performance is smaller.\n\nVisualization of Attention Maps (RQ4)\n\nWe show how MASTER captures the momentary and cross-\ntime stock correlation that previous methods are not ex-\npressive enough to model. Figure 5 shows the inter-stock\nattention map at different time steps in the lookback win-\ndow. We choose three representative stocks as the target\nand sample 100 random stocks as sources for visualiza-\ntion. The highlighted part is scattered instead of exhibit-\ning neat strips, implying that the correlation is momentary\nrather than long-standing. Also, the inter-stock correlation\nis sparse, with only a few stocks having strong correlations\ntoward the target stocks. Figure 6 displays the correlation\nbetween stock pairs to show how the correlation resides in\ntime. From source stock v to target stock u, we compute\nTucvli, fj] = S}/i, j]S?[u, v] as the r x 7 correlation map,\nwhile S* and S* are the intra-stock and inter-stock atten-\ntion map. First, the highlighted blocks are not centered on\nthe diagonal, because the stock correlation is usually cross-\ntime rather than temporally aligned. Second, the left two\nfigures are totally different, illustrating that correlation is\n\nhighly asymmetric between u \u00a2~ v and uv \u00a2 u. Third, the\nimportance of mined correlation changes slowly when the\nlookback window slides to forecast on different dates. For\nexample, blocked regions in the right two figures correspond\nto the same absolute time scope of different prediction dates,\nwhose patterns are to a certain degree similar.\n",
                        "extracted-code": ""
                    },
                    {
                        "section_id": 1.6,
                        "section_name": "Conclusion",
                        "section_path": "./screenshots-images-2/chapter_1/section_6",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_6/bc7f9e42-d4aa-42d9-bc76-5d7961801049.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "We introduce a novel method MASTER for stock price\nforecasting, which models the realistic stock correlation\nand guide feature selection with market information. MAS-\nTER consists of five steps, market-guided gating, intra-stock\naggregation, inter-stock aggregation, temporal aggregation,\nand prediction. Experiments on the Chinese market with 2\nstock universe shows that MASTER achieves averagely 13%\nimprovements on ranking metrics and 47% on portfolio-\nbased metrics compared with all baselines. Visualization of\nattention maps reveals the de-facto momentary and cross-\ntime stock correlation. In conclusion, we provide a more\ngranular perspective for studying stock correlation, while\nalso indicating an effective application of market informa-\ntion. Future work can explore to mine stock correlations of\nhigher quality and study other uses of market information.\n",
                        "extracted-code": ""
                    }
                ]
            }
        ]
    }
}