{
    "New item": {
        "chapters": [
            {
                "chapter_id": 1,
                "chapter_name": "Chapter 1. The Need for\nMachine Learning Design\nPatterns",
                "chapter_path": "./screenshots-images-2/chapter_1",
                "sections": [
                    {
                        "section_id": 1.1,
                        "section_name": "Chapter 1. The Need for\nMachine Learning Design\nPatterns",
                        "section_path": "./screenshots-images-2/chapter_1/section_1",
                        "images": [
                            "./screenshots-images-2/chapter_1/section_1/c8b16cd1-6793-418c-93db-96cc2a35cad7.png",
                            "./screenshots-images-2/chapter_1/section_1/54a3c1ce-1d3b-45fb-82e9-f0a398c1d161.png",
                            "./screenshots-images-2/chapter_1/section_1/b43a2910-77b7-4e85-91e9-5f304af65b5f.png",
                            "./screenshots-images-2/chapter_1/section_1/bbf3d636-c365-4596-be22-5a946aa3f726.png",
                            "./screenshots-images-2/chapter_1/section_1/308f7003-bd46-49da-9a84-928e3b692209.png",
                            "./screenshots-images-2/chapter_1/section_1/d46ca50c-f2aa-4615-bf5d-11665743509a.png",
                            "./screenshots-images-2/chapter_1/section_1/184851f6-44f3-42f9-ba17-f0b06328a3d5.png",
                            "./screenshots-images-2/chapter_1/section_1/0264aec1-9d26-48e6-8783-4541e0b4a666.png",
                            "./screenshots-images-2/chapter_1/section_1/c9b197c9-fe6d-4d88-a195-53e8c44b3ed0.png",
                            "./screenshots-images-2/chapter_1/section_1/cd7c3927-7259-46c6-b003-a3f94a84a8b2.png",
                            "./screenshots-images-2/chapter_1/section_1/d196ec15-92a3-4a5f-9abd-b28888f884a1.png",
                            "./screenshots-images-2/chapter_1/section_1/13f2f95f-5997-4615-8ba8-96f61b062852.png",
                            "./screenshots-images-2/chapter_1/section_1/d744e4e4-c555-4765-94cf-dc62d739f411.png",
                            "./screenshots-images-2/chapter_1/section_1/b189bb21-1f6a-4163-8e3c-5da60c93c5c5.png",
                            "./screenshots-images-2/chapter_1/section_1/ce77bdfc-903e-49a1-addf-4d6e881f7478.png",
                            "./screenshots-images-2/chapter_1/section_1/feeb18c4-a598-4237-ae46-9cdbc14a9ceb.png",
                            "./screenshots-images-2/chapter_1/section_1/e013a917-a61d-4c19-8a7f-88d982171919.png",
                            "./screenshots-images-2/chapter_1/section_1/0b1628f6-0d0b-4370-9013-52e7629e0e9e.png",
                            "./screenshots-images-2/chapter_1/section_1/1501d8a1-4337-46e2-84c5-ee0e54359e1b.png",
                            "./screenshots-images-2/chapter_1/section_1/be301a05-888f-4817-8708-a895586f9413.png",
                            "./screenshots-images-2/chapter_1/section_1/37313c93-d558-49fc-98d6-75ed082f2ca9.png",
                            "./screenshots-images-2/chapter_1/section_1/aab44c57-4ce4-49a4-a259-983606fa6c76.png",
                            "./screenshots-images-2/chapter_1/section_1/add562b9-7edb-457b-9445-6a561b087d3d.png",
                            "./screenshots-images-2/chapter_1/section_1/cb790e08-0f44-47bf-8e1a-7871d56f6cdf.png",
                            "./screenshots-images-2/chapter_1/section_1/067693a2-ed26-4f83-acdf-4a62143196e3.png",
                            "./screenshots-images-2/chapter_1/section_1/d07138c1-6fb0-4401-8bcf-359415e0c7bb.png",
                            "./screenshots-images-2/chapter_1/section_1/b976086e-a792-4cf0-a1a1-5203e338d916.png",
                            "./screenshots-images-2/chapter_1/section_1/1ad0561e-c44a-4787-b692-6b683ebf52f1.png"
                        ],
                        "code_images": [],
                        "status": "images tested ok",
                        "errors": [],
                        "extracted-text": "In engineering disciplines, design patterns capture best practices and\nsolutions to commonly occurring problems. They codify the knowledge\nand experience of experts into advice that all practitioners can follow. This\nbook is a catalog of machine learning design patterns that we have\nobserved in the course of working with hundreds of machine learning\nteams.\n\nWhat Are Design Patterns?\n\nThe idea of patterns, and a catalog of proven patterns, was introduced in\nthe field of architecture by Christopher Alexander and five coauthors in a\nhugely influential book titled A Pattern Language (Oxford University\nPress, 1977). In their book, they catalog 253 patterns, introducing them\nthis way:\n\nEach pattern describes a problem which occurs over and over again in\nour environment, and then describes the core of the solution to that\nproblem, in such a way that you can use this solution a million times\nover, without ever doing it the same way twice.\n\nEach solution is stated in such a way that it gives the essential field of\nrelationships needed to solve the problem, but in a very general and\nabstract way\u2014so that you can solve the problem for yourself, in your\n\nown way, by adapting it to your preferences, and the local conditions at\nthe place where you are making it.\n\nFor example, a couple of the patterns that incorporate human details when\nbuilding a home are Light on Two Sides of Every Room and Six-Foot\nBalcony. Think of your favorite room in your home, and your least-\nfavorite room. Does your favorite room have windows on two walls?\nWhat about your least-favorite room? According to Alexander:\n\nRooms lit on two sides, with natural light, create less glare around\npeople and objects; this lets us see things more intricately; and most\nimportant, it allows us to read in detail the minute expressions that\nflash across people\u2019s faces....\n\nHaving a name for this pattern saves architects from having to continually\nrediscover this principle. Yet where and how you get two light sources in\nany specific local condition is up to the architect\u2019s skill. Similarly, when\ndesigning a balcony, how big should it be? Alexander recommends 6 feet\nby 6 feet as being enough for 2 (mismatched!) chairs and a side table, and\n12 feet by 12 feet if you want both a covered sitting space and a sitting\nspace in the sun.\n\nErich Gamma, Richard Helm, Ralph Johnson, and John Vlissides brought\nthe idea to software by cataloging 23 object-oriented design patterns in a\n1994 book entitled Design Patterns: Elements of Reusable Object-\nOriented Software (Addison-Wesley, 1995). Their catalog includes\npatterns such as Proxy, Singleton, and Decorator and led to lasting impact\non the field of object-oriented programming. In 2005 the Association of\nComputing Machinery (ACM) awarded their annual Programming\nLanguages Achievement Award to the authors, recognizing the impact of\ntheir work \u201con programming practice and programming language design.\u201d\n\nBuilding production machine learning models is increasingly becoming an\nengineering discipline, taking advantage of ML methods that have been\nproven in research settings and applying them to business problems. As\nmachine learning becomes more mainstream, it is important that\npractitioners take advantage of tried-and-proven methods to address\nrecurring problems.\n\nOne benefit of our jobs in the customer-facing part of Google Cloud is that\nit brings us in contact with a wide variety of machine learning and data\nscience teams and individual developers from around the world. At the\nsame time, we each work closely with internal Google teams solving\ncutting-edge machine learning problems. Finally, we have been fortunate\nto work with the TensorFlow, Keras, BigQuery ML, TPU, and Cloud AI\nPlatform teams that are driving the democratization of machine learning\nresearch and infrastructure. All this gives us a rather unique perspective\nfrom which to catalog the best practices we have observed these teams\ncarrying out.\n\nThis book is a catalog of design patterns or repeatable solutions to\ncommonly occurring problems in ML engineering. For example, the\nTransform pattern (Chapter 6) enforces the separation of inputs, features,\nand transforms and makes the transformations persistent in order to\nsimplify moving an ML model to production. Similarly, Keyed\nPredictions, in Chapter 5, is a pattern that enables the large-scale\ndistribution of batch predictions, such as for recommendation models.\n\nFor each pattern, we describe the commonly occurring problem that is\nbeing addressed and then walk through a variety of potential solutions to\nthe problem, the trade-offs of these solutions, and recommendations for\nchoosing between these solutions. Implementation code for these solutions\n\nis provided in SQL (useful if you are carrying out preprocessing and other\nETL in Spark SQL, BigQuery, and so on), scikit-learn, and/or Keras with\na TensorFlow backend.\n\nMachine Learning Terminology\n\nBecause machine learning practitioners today may have different areas of\nprimary expertise\u2014software engineering, data analysis, DevOps, or\nstatistics\u2014there can be subtle differences in the way that different\npractitioners use certain terms. In this section, we define terminology that\nwe use throughout the book.\n\nModels and Frameworks\n\nAt its core, machine learning is a process of building models that learn\nfrom data. This is in contrast to traditional programming where we write\nexplicit rules that tell programs how to behave. Machine learning models\nare algorithms that learn patterns from data. To illustrate this point,\nimagine we are a moving company and need to estimate moving costs for\npotential customers. In traditional programming, we might solve this with\nan if statement:\n\nif num_bedrooms == 2 and num_bathrooms == 2:\nestimate = 1500\n\nelif num_bedrooms == 3 and sq_ft > 2000:\nestimate = 2500\n\nYou can imagine how this will quickly get complicated as we add more\nvariables (number of large furniture items, amount of clothing, fragile\nitems, and so on) and try to handle edge cases. More to the point, asking\nfor all this information ahead of time from customers can cause them to\nabandon the estimation process. Instead, we can train a machine learning\nmodel to estimate moving costs based on past data on previous households\nour company has moved.\n\nThroughout the book, we primarily use feed-forward neural network\n\nmodels in our examples, but we\u2019ll also reference linear regression models,\ndecision trees, clustering models, and others. Feed-forward neural\nnetworks, which we will commonly shorten as neural networks, are a type\nof machine learning algorithm whereby multiple layers, each with many\nneurons, analyze and process information and then send that information\nto the next layer, resulting in a final layer that produces a prediction as\noutput. Though they are in no way identical, neural networks are often\ncompared to the neurons in our brain because of the connectivity between\nnodes and the way they are able to generalize and form new predictions\nfrom the data they process. Neural networks with more than one hidden\nlayer (layers other than the input and output layer) are classified as deep\nlearning (see Figure 1-1).\n\nMachine learning models, regardless of how they are depicted visually, are\nmathematical functions and can therefore be implemented from scratch\nusing a numerical software package. However, ML engineers in industry\ntend to employ one of several open source frameworks designed to\nprovide intuitive APIs for building models. The majority of our examples\nwill use TensorFlow, an open source machine learning framework created\nby Google with a focus on deep learning models. Within the TensorFlow\nlibrary, we\u2019ll be using the Keras API in our examples, which can be\nimported through tensorflow.keras. Keras is a higher-\nlevel API for building neural networks. While Keras\nsupports many backends, we\u2019ll be using its TensorFlow backend. In other\nexamples, we\u2019ll be using scikit-learn, XGBoost, and PyTorch, which are\nother popular open source frameworks that provide utilities for preparing\nyour data, along with APIs for building linear and deep models. Machine\nlearning continues to become more accessible, and one exciting\ndevelopment is the availability of machine learning models that can be\nexpressed in SQL. We\u2019ll use BigQuery ML as an example of this,\n\nespecially in situations where we want to combine data preprocessing and\nmodel creation.\n\nVacrelarig\n\nFigure 1-1. A breakdown of different types of machine learning, with a few examples of each. Note\nthat although it is not included in this diagram, neural networks like autoencoders can also be used\nfor unsupervised learning.\n\nConversely, neural networks with only an input and output layer are\nanother subset of machine learning known as linear models. Linear\nmodels represent the patterns they\u2019ve learned from data using a linear\nfunction. Decision trees are machine learning models that use your data to\ncreate a subset of paths with various branches. These branches\napproximate the results of different outcomes from your data. Finally,\nclustering models look for similarities between different subsets of your\ndata and use these identified patterns to group data into clusters.\n\nMachine learning problems (see Figure 1-1) can be broken into two types:\nsupervised and unsupervised learning. Supervised learning defines\nproblems where you know the ground truth label for your data in advance.\nFor example, this could include labeling an image as \u201ccat\u201d or labeling a\nbaby as being 2.3 kg at birth. You feed this labeled data to your model in\nhopes that it can learn enough to label new examples. With unsupervised\nlearning, you do not know the labels for your data in advance, and the\ngoal is to build a model that can find natural groupings of your data (called\nclustering), compress the information content (dimensionality reduction),\nor find association rules. The majority of this book will focus on\nsupervised learning because the vast majority of machine learning models\nused in production are supervised.\n\nWith supervised learning, problems can typically be defined as either\nclassification or regression. Classification models assign your input data a\nlabel (or labels) from a discrete, predefined set of categories. Examples of\nclassification problems include determining the type of pet breed in an\nimage, tagging a document, or predicting whether or not a transaction is\n\nfraudulent. Regression models assign continuous, numerical values to your\ninputs. Examples of regression models include predicting the duration of a\nbike trip, a company\u2019s future revenue, or the price of a product.\n\nData and Feature Engineering\n\nData is at the heart of any machine learning problem. When we talk about\ndatasets, we\u2019re referring to the data used for training, validating, and\ntesting a machine learning model. The bulk of your data will be training\ndata: the data fed to your model during the training process. Validation\ndata is data that is held out from your training set and used to evaluate\nhow the model is performing after each training epoch (or pass through the\ntraining data). The performance of the model on the validation data is used\nto decide when to stop the training run, and to choose hyperparameters,\nsuch as the number of trees in a random forest model. Test data is data that\nis not used in the training process at all and is used to evaluate how the\ntrained model performs. Performance reports of the machine learning\nmodel must be computed on the independent test data, rather than the\ntraining or validation tests. It\u2019s also important that the data be split in such\na way that all three datasets (training, test, validation) have similar\nstatistical properties.\n\nThe data you use to train your model can take many forms depending on\nthe model type. We define structured data as numerical and categorical\ndata. Numerical data includes integer and float values, and categorical data\nincludes data that can be divided into a finite set of groups, like type of car\nor education level. You can also think of structured data as data you would\ncommonly find in a spreadsheet. Throughout the book, we\u2019ll use the term\ntabular data interchangeably with structured data. Unstructured data, on\nthe other hand, includes data that cannot be represented as neatly. This\n\ntypically includes free-form text, images, video, and audio.\n\nNumeric data can often be fed directly to a machine learning model, where\nother data requires various data preprocessing before it\u2019s ready to be sent\nto a model. This preprocessing step typically includes scaling numerical\nvalues, or converting nonnumerical data into a numerical format that can\nbe understood by your model. Another term for preprocessing is feature\nengineering. We\u2019|l use these two terms interchangeably throughout the\nbook.\n\nThere are various terms used to describe data as it goes through the feature\nengineering process. Input describes a single column in your dataset\nbefore it has been processed, and feature describes a single column after it\nhas been processed. For example, a timestamp could be your input, and\nthe feature would be day of the week. To convert the data from timestamp\nto day of the week, you\u2019ll need to do some data preprocessing. This\npreprocessing step can also be referred to as data transformation.\n\nAn instance is an item you\u2019d like to send to your model for prediction. An\ninstance could be a row in your test dataset (without the label column), an\nimage you want to classify, or a text document to send to a sentiment\nanalysis model. Given a set of features about the instance, the model will\ncalculate a predicted value. In order to do that, the model is trained on\ntraining examples, which associate an instance with a label. A training\nexample refers to a single instance (row) of data from your dataset that\nwill be fed to your model. Building on the timestamp use case, a full\ntraining example might include: \u201cday of week,\u201d \u201ccity,\u201d and \u201ctype of car.\u201d\nA label is the output column in your dataset\u2014the item your model is\npredicting. Label can refer both to the target column in your dataset (also\ncalled a ground truth label) and the output given by your model (also\n\ncalled a prediction). A sample label for the training example outlined\nabove could be \u201ctrip duration\u201d\u2014in this case, a float value denoting\nminutes.\n\nOnce you\u2019ve assembled your dataset and determined the features for your\nmodel, data validation is the process of computing statistics on your data,\nunderstanding your schema, and evaluating the dataset to identify\nproblems like drift and training-serving skew. Evaluating various statistics\non your data can help you ensure the dataset contains a balanced\nrepresentation of each feature. In cases where it\u2019s not possible to collect\nmore data, understanding data balance will help you design your model to\naccount for this. Understanding your schema involves defining the data\ntype for each feature and identifying training examples where certain\nvalues may be incorrect or missing. Finally, data validation can identify\ninconsistencies that may affect the quality of your training and test sets.\nFor example, maybe the majority of your training dataset contains\nweekday examples while your test set contains primarily weekend\nexamples.\n\nThe Machine Learning Process\n\nThe first step in a typical machine learning workflow is training\u2014the\nprocess of passing training data to a model so that it can learn to identify\npatterns. After training, the next step in the process is testing how your\nmodel performs on data outside of your training set. This is known as\nmodel evaluation. You might run training and evaluation multiple times,\nperforming additional feature engineering and tweaking your model\narchitecture. Once you are happy with your model\u2019s performance during\nevaluation, you\u2019ll likely want to serve your model so that others can\naccess it to make predictions. We use the term serving to refer to accepting\n\nincoming requests and sending back predictions by deploying the model as\na microservice. The serving infrastructure could be in the cloud, on-\npremises, or on-device.\n\nThe process of sending new data to your model and making use of its\noutput is called prediction. This can refer both to generating predictions\nfrom local models that have not yet been deployed as well as getting\npredictions from deployed models. For deployed models, we\u2019ll refer both\nto online and batch prediction. Online prediction is used when you want to\nget predictions on a few examples in near real time. With online\nprediction, the emphasis is on low latency. Batch prediction, on the other\nhand, refers to generating predictions on a large set of data offline. Batch\nprediction jobs take longer than online prediction and are useful for\nprecomputing predictions (such as in recommendation systems) and in\nanalyzing your model\u2019s predictions across a large sample of new data.\n\nThe word prediction is apt when it comes to forecasting future values,\nsuch as in predicting the duration of a bicycle ride or predicting whether a\nshopping cart will be abandoned. It is less intuitive in the case of image\nand text classification models. If an ML model looks at a text review and\noutputs that the sentiment is positive, it\u2019s not really a \u201cprediction\u201d (there is\nno future outcome). Hence, you will also see word inference being used to\nrefer to predictions. The statistical term inference is being repurposed\nhere, but it\u2019s not really about reasoning.\n\nOften, the processes of collecting training data, feature engineering,\ntraining, and evaluating your model are handled separately from the\nproduction pipeline. When this is the case, you\u2019ll reevaluate your solution\nwhenever you decide you have enough additional data to train a new\nversion of your model. In other situations, you may have new data being\n\ningested continuously and need to process this data immediately before\nsending it to your model for training or prediction. This is known as\nstreaming. To handle streaming data, you\u2019! need a multistep solution for\nperforming feature engineering, training, evaluation, and predictions. Such\nmultistep solutions are called ML pipelines.\n\nData and Model Tooling\n\nThere are various Google Cloud products we\u2019ll be referencing that provide\ntooling for solving data and machine learning problems. These products\nare merely one option for implementing the design patterns referenced in\nthis book and are not meant to be an exhaustive list. All of the products\nincluded here are serverless, allowing us to focus more on implementing\nmachine learning design patterns instead of the infrastructure behind them.\n\nBigQuery is an enterprise data warehouse designed for analyzing large\ndatasets quickly with SQL. We\u2019ll use BigQuery in our examples for data\ncollection and feature engineering. Data in BigQuery is organized by\nDatasets, and a Dataset can have multiple Tables. Many of our examples\nwill use data from Google Cloud Public Datasets, a set of free, publicly\navailable data hosted in BigQuery. Google Cloud Public Datasets consists\nof hundreds of different datasets, including NOAA weather data since\n1929, Stack Overflow questions and answers, open source code from\nGitHub, natality data, and more. To build some of the models in our\nexamples, we\u2019ll use BigQuery Machine Learning (or BigQuery ML).\nBigQuery ML is a tool for building models from data stored in BigQuery.\nWith BigQuery ML, we can train, evaluate, and generate predictions on\nour models using SQL. It supports classification and regression models,\nalong with unsupervised clustering models. It\u2019s also possible to import\npreviously trained TensorFlow models to BigQuery ML for prediction.\n\nCloud AI Platform includes a variety of products for training and serving\ncustom machine learning models on Google Cloud. In our examples, we\u2019ll\nbe using AI Platform Training and AI Platform Prediction. AI Platform\nTraining provides infrastructure for training machine learning models on\nGoogle Cloud. With AI Platform Prediction, you can deploy your trained\nmodels and generate predictions on them using an API. Both services\nsupport TensorFlow, scikit-Learn, and XGBoost models, along with\ncustom containers for models built with other frameworks. We\u2019ll also\nreference Explainable AI, a tool for interpreting the results of your\nmodel\u2019s predictions, available for models deployed to AI Platform.\n\nRoles\n\nWithin an organization, there are many different job roles relating to data\nand machine learning. Below we\u2019ll define a few common ones referenced\nfrequently throughout the book. This book is targeted primarily at data\nscientists, data engineers, and ML engineers, so let\u2019s start with those.\n\nA data scientist is someone focused on collecting, interpreting, and\nprocessing datasets. They run statistical and exploratory analysis on data.\nAs it relates to machine learning, a data scientist may work on data\ncollection, feature engineering, model building, and more. Data scientists\noften work in Python or R in a notebook environment, and are usually the\nfirst to build out an organization\u2019s machine learning models.\n\nA data engineer is focused on the infrastructure and workflows powering\nan organization\u2019s data. They might help manage how a company ingests\ndata, data pipelines, and how data is stored and transferred. Data engineers\nimplement infrastructure and pipelines around data.\n\nMachine learning engineers do similar tasks to data engineers, but for ML\n\nmodels. They take models developed by data scientists, and manage the\ninfrastructure and operations around training and deploying those models.\nML engineers help build production systems to handle updating models,\nmodel versioning, and serving predictions to end users.\n\nThe smaller the data science team at a company and the more agile the\nteam is, the more likely it is that the same person plays multiple roles. If\nyou are in such a situation, it is very likely that you read the above three\ndescriptions and saw yourself partially in all three categories. You might\ncommonly start out a machine learning project as a data engineer and\nbuild data pipelines to operationalize the ingest of data. Then, you\ntransition to the data scientist role and build the ML model(s). Finally, you\nput on the ML engineer hat and move the model to production. In larger\norganizations, machine learning projects may move through the same\nphases, but different teams might be involved in each phase.\n\nResearch scientists, data analysts, and developers may also build and use\nAI models, but these job roles are not a focus audience for this book.\n\nResearch scientists focus primarily on finding and developing new\nalgorithms to advance the discipline of ML. This could include a variety\nof subfields within machine learning, like model architectures, natural\nlanguage processing, computer vision, hyperparameter tuning, model\ninterpretability, and more. Unlike the other roles discussed here, research\nscientists spend most of their time prototyping and evaluating new\napproaches to ML, rather than building out production ML systems.\n\nData analysts evaluate and gather insights from data, then summarize\nthese insights for other teams within their organization. They tend to work\nin SQL and spreadsheets, and use business intelligence tools to create data\n\nvisualizations to share their findings. Data analysts work closely with\nproduct teams to understand how their insights can help address business\nproblems and create value. While data analysts focus on identifying trends\nin existing data and deriving insights from it, data scientists are concerned\nwith using that data to generate future predictions and in automating or\nscaling out the generation of insights. With the increasing democratization\nof machine learning, data analysts can upskill themselves to become data\nscientists.\n\nDevelopers are in charge of building production systems that enable end\nusers to access ML models. They are often involved in designing the APIs\nthat query models and return predictions in a user-friendly format via a\nweb or mobile application. This could involve models hosted in the cloud,\nor models served on-device. Developers utilize the model serving\ninfrastructure implemented by ML Engineers to build applications and\nuser interfaces for surfacing predictions to model users.\n\nFigure 1-2 illustrates how these different roles work together throughout\nan organization\u2019s machine learning model development process.\n\nResearch\nsets\n\nDeveloper\n\nFigure 1-2. There are many different job roles related to data and machine learning, and these\nroles collaborate on the ML workflow, from data ingestion to model serving and the end user\ninterface. For example, the data engineer works on data ingestion and data validation and\ncollaborates closely with data scientists.\n\nCommon Challenges in Machine Learning\n\nWhy do we need a book about machine learning design patterns? The\nprocess of building out ML systems presents a variety of unique\nchallenges that influence ML design. Understanding these challenges will\nhelp you, an ML practitioner, develop a frame of reference for the\nsolutions introduced throughout the book.\n\nData Quality\n\nMachine learning models are only as reliable as the data used to train\nthem. If you train a machine learning model on an incomplete dataset, on\ndata with poorly selected features, or on data that doesn\u2019t accurately\nrepresent the population using the model, your model\u2019s predictions will be\na direct reflection of that data. As a result, machine learning models are\noften referred to as \u201cgarbage in, garbage out.\u201d Here we\u2019ll highlight four\nimportant components of data quality: accuracy, completeness,\nconsistency, and timeliness.\n\nData accuracy refers to both your training data\u2019s features and the ground\ntruth labels corresponding with those features. Understanding where your\ndata came from and any potential errors in the data collection process can\nhelp ensure feature accuracy. After your data has been collected, it\u2019s\nimportant to do a thorough analysis to screen for typos, duplicate entries,\nmeasurement inconsistencies in tabular data, missing features, and any\nother errors that may affect data quality. Duplicates in your training\ndataset, for example, can cause your model to incorrectly assign more\nweight to these data points.\n\nAccurate data labels are just as important as feature accuracy. Your model\nrelies solely on the ground truth labels in your training data to update its\nweights and minimize loss. As a result, incorrectly labeled training\nexamples can cause misleading model accuracy. For example, let\u2019s say\nyou\u2019re building a sentiment analysis model and 25% of your \u201cpositive\u201d\ntraining examples have been incorrectly labeled as \u201cnegative.\u201d Your\nmodel will have an inaccurate picture of what should be considered\nnegative sentiment, and this will be directly reflected in its predictions.\n\nTo understand data completeness, let\u2019s say you\u2019 re training a model to\n\nidentify cat breeds. You train the model on an extensive dataset of cat\nimages, and the resulting model is able to classify images into 1 of 10\npossible categories (\u201cBengal,\u201d \u201cSiamese,\u201d and so forth) with 99%\naccuracy. When you deploy your model to production, however, you find\nthat in addition to uploading cat photos for classification, many of your\nusers are uploading photos of dogs and are disappointed with the model\u2019s\nresults. Because the model was trained only to identify 10 different cat\nbreeds, this is all it knows how to do. These 10 breed categories are,\nessentially, the model\u2019s entire \u201cworld view.\u201d No matter what you send the\nmodel, you can expect it to slot it into one of these 10 categories. It may\neven do so with high confidence for an image that looks nothing like a cat.\nAdditionally, there\u2019s no way your model will be able to return \u201cnot a cat\u201d\nif this data and label weren\u2019t included in the training dataset.\n\nAnother aspect of data completeness is ensuring your training data\ncontains a varied representation of each label. In the cat breed detection\nexample, if all of your images are close-ups of a cat\u2019s face, your model\nwon\u2019t be able to correctly identify an image of a cat from the side, or a\nfull-body cat image. To look at a tabular data example, if you are building\na model to predict the price of real estate in a specific city but only include\ntraining examples of houses larger than 2,000 square feet, your resulting\nmodel will perform poorly on smaller houses.\n\nThe third aspect of data quality is data consistency. For large datasets, it\u2019s\ncommon to divide the work of data collection and labeling among a group\nof people. Developing a set of standards for this process can help ensure\nconsistency across your dataset, since each person involved in this will\ninevitably bring their own biases to the process. Like data completeness,\ndata inconsistencies can be found in both data features and labels. For an\nexample of inconsistent features, let\u2019s say you\u2019re collecting atmospheric\n\ndata from temperature sensors. If each sensor has been calibrated to\ndifferent standards, this will result in inaccurate and unreliable model\npredictions. Inconsistencies can also refer to data format. If you\u2019re\ncapturing location data, some people may write out a full street address as\n\u201cMain Street\u201d and others may abbreviate it as \u201cMain St.\u201d Measurement\nunits, like miles and kilometers, can also differ around the world.\n\nIn regards to labeling inconsistencies, let\u2019s return to the text sentiment\nexample. In this case, it\u2019s likely people will not always agree on what is\nconsidered positive and negative when labeling training data. To solve\nthis, you can have multiple people labeling each example in your dataset,\nthen take the most commonly applied label for each item. Being aware of\npotential labeler bias, and implementing systems to account for it, will\nensure label consistency throughout your dataset. We\u2019ll explore the\nconcept of bias in the \u201cDesign Pattern 30: Fairness Lens\u201d in Chapter 7.\n\nTimeliness in data refers to the latency between when an event occurred\nand when it was added to your database. If you\u2019re collecting data on\napplication logs, for example, an error log might take a few hours to show\nup in your log database. For a dataset recording credit card transactions, it\nmight take one day from when the transaction occurred before it is\nreported in your system. To deal with timeliness, it\u2019s useful to record as\nmuch information as possible about a particular data point, and make sure\nthat information is reflected when you transform your data into features\nfor a machine learning model. More specifically, you can keep track of the\ntimestamp of when an event occurred and when it was added to your\ndataset. Then, when performing feature engineering, you can account for\nthese differences accordingly.\n\nReproducibility\n\nIn traditional programming, the output of a program is reproducible and\nguaranteed. For example, if you write a Python program that reverses a\nstring, you know that an input of the word \u201cbanana\u201d will always return an\noutput of \u201cananab.\u201d Similarly, if there\u2019s a bug in your program causing it\nto incorrectly reverse strings containing numbers, you could send the\nprogram to a colleague and expect them to be able to reproduce the error\nwith the same inputs you used (unless the bug has something to do with\nthe program maintaining some incorrect internal state, differences in\narchitecture such as floating point precision, or differences in execution\nsuch as threading).\n\nMachine learning models, on the other hand, have an inherent element of\nrandomness. When training, ML model weights are initialized with\nrandom values. These weights then converge during training as the model\niterates and learns from the data. Because of this, the same model code\ngiven the same training data will produce slightly different results across\ntraining runs. This introduces a challenge of reproducibility. If you train a\nmodel to 98.1% accuracy, a repeated training run is not guaranteed to\nreach the same result. This can make it difficult to run comparisons across\nexperiments.\n\nIn order to address this problem of repeatability, it\u2019s common to set the\nrandom seed value used by your model to ensure that the same\nrandomness will be applied each time you run training. In TensorFlow,\nyou can do this by running tf . random. set_seed( value) at the\nbeginning of your program.\n\nAdditionally, in scikit-learn, many utility functions for shuffling your data\nalso allow you to set a random seed value:\n\nfrom sklearn.utils import shuffle\n\ndata = shuffle(data, random_state=value)\n\nKeep in mind that you\u2019ll need to use the same data and the same random\nseed when training your model to ensure repeatable, reproducible results\nacross different experiments.\n\nTraining an ML model involves several artifacts that need to be fixed in\norder to ensure reproducibility: the data used, the splitting mechanism\nused to generate datasets for training and validation, data preparation and\nmodel hyperparameters, and variables like the batch size and learning rate\nschedule.\n\nReproducibility also applies to machine learning framework dependencies.\nIn addition to manually setting a random seed, frameworks also implement\nelements of randomness internally that are executed when you call a\nfunction to train your model. If this underlying implementation changes\nbetween different framework versions, repeatability is not guaranteed. As\na concrete example, if one version of a framework\u2019s train( ) method\nmakes 13 calls to rand( ), and a newer version of the same framework\nmakes 14 calls, using different versions between experiments will cause\nslightly different results, even with the same data and model code.\nRunning ML workloads in containers and standardizing library versions\ncan help ensure repeatability. Chapter 6 introduces a series of patterns for\nmaking ML processes reproducible.\n\nFinally, reproducibility can refer to a model\u2019s training environment. Often,\ndue to large datasets and complexity, many models take a significant\namount of time to train. This can be accelerated by employing distribution\nstrategies like data or model parallelism (see Chapter 5). With this\nacceleration, however, comes an added challenge of repeatability when\n\nyou rerun code that makes use of distributed training.\n\nData Drift\n\nWhile machine learning models typically represent a static relationship\nbetween inputs and outputs, data can change significantly over time. Data\ndrift refers to the challenge of ensuring your machine learning models stay\nrelevant, and that model predictions are an accurate reflection of the\nenvironment in which they\u2019re being used.\n\nFor example, let\u2019s say you\u2019re training a model to classify news article\nheadlines into categories like \u201cpolitics,\u201d \u201cbusiness,\u201d and \u201ctechnology.\u201d If\nyou train and evaluate your model on historical news articles from the\n20th century, it likely won\u2019t perform as well on current data. Today, we\nknow that an article with the word \u201csmartphone\u201d in the headline is\nprobably about technology. However, a model trained on historical data\nwould have no knowledge of this word. To solve for drift, it\u2019s important to\ncontinually update your training dataset, retrain your model, and modify\nthe weight your model assigns to particular groups of input data.\n\nTo see a less-obvious example of drift, look at the NOAA dataset of\nsevere storms in BigQuery. If we were training a model to predict the\nlikelihood of a storm in a given area, we would need to take into account\nthe way weather reporting has changed over time. We can see in Figure 1-\n3 that the total number of severe storms recorded has been steadily\nincreasing since 1950.\n\n=m J toms\n\n20k\n20k\n20k\n\n10k\n\n(\nDH OAS OHOADA 6 A REN\nPPPREEEREEREPEERER ES ESENS\n\nFigure 1-3. Number of severe storms reported in a year, as recorded by NOAA from 1950 to 2011.\n\nFrom this trend, we can see that training a model on data before 2000 to\ngenerate predictions on storms today would lead to inaccurate predictions.\nIn addition to the total number of reported storms increasing, it\u2019s also\nimportant to consider other factors that may have influenced the data in\nFigure 1-3. For example, the technology for observing storms has\nimproved over time, most dramatically with the introduction of weather\nradars in the 1990s. In the context of features, this may mean that newer\ndata contains more information about each storm, and that a feature\navailable in today\u2019s data may not have been observed in 1950. Exploratory\ndata analysis can help identify this type of drift and can inform the correct\nwindow of data to use for training. Section , \u201cDesign Pattern 23: Bridged\nSchema\u201d provides a way to handle datasets in which the availability of\nfeatures improves over time.\n\nScale\n\nThe challenge of scaling is present throughout many stages of a typical\nmachine learning workflow. You\u2019ll likely encounter scaling challenges in\ndata collection and preprocessing, training, and serving. When ingesting\nand preparing data for a machine learning model, the size of the dataset\nwill dictate the tooling required for your solution. It is often the job of data\nengineers to build out data pipelines that can scale to handle datasets with\nmillions of rows.\n\nFor model training, ML engineers are responsible for determining the\nnecessary infrastructure for a specific training job. Depending on the type\nand size of the dataset, model training can be time consuming and\ncomputationally expensive, requiring infrastructure (like GPUs) designed\nspecifically for ML workloads. Image models, for instance, typically\n\nrequire much more training infrastructure than models trained entirely on\ntabular data.\n\nIn the context of model serving, the infrastructure required to support a\nteam of data scientists getting predictions from a model prototype is\nentirely different from the infrastructure necessary to support a production\nmodel getting millions of prediction requests every hour. Developers and\nML engineers are typically responsible for handling the scaling challenges\nassociated with model deployment and serving prediction requests.\n\nMost of the ML patterns in this book are useful without regard to\norganizational maturity. However, several of the patterns in Chapters 6\nand 7 address resilience and reproducibility challenges in different ways,\nand the choice between them will often come down to the use case and the\nability of your organization to absorb complexity.\n\nMultiple Objectives\n\nThough there is often a single team responsible for building a machine\nlearning model, many teams across an organization will make use of the\nmodel in some way. Inevitably, these teams may have different ideas of\nwhat defines a successful model.\n\nTo understand how this may play out in practice, let\u2019s say you\u2019re building\na model to identify defective products from images. As a data scientist,\nyour goal may be to minimize your model\u2019s cross-entropy loss. The\nproduct manager, on the other hand, may want to reduce the number of\ndefective products that are misclassified and sent to customers. Finally, the\nexecutive team\u2019s goal might be to increase revenue by 30%. Each of these\ngoals vary in what they are optimizing for, and balancing these differing\nneeds within an organization can present a challenge.\n\nAs a data scientist, you could translate the product team\u2019s needs into the\ncontext of your model by saying false negatives are five times more costly\nthan false positives. Therefore, you should optimize for recall over\nprecision to satisfy this when designing your model. You can then find a\nbalance between the product team\u2019s goal of optimizing for precision and\nyour goal of minimizing the model\u2019s loss.\n\nWhen defining the goals for your model, it\u2019s important to consider the\nneeds of different teams across an organization, and how each team\u2019s\nneeds relate back to the model. By analyzing what each team is optimizing\nfor before building out your solution, you can find areas of compromise in\norder to optimally balance these multiple objectives.\n\nSummary\n\nDesign patterns are a way to codify the knowledge and experience of\nexperts into advice that all practitioners can follow. The design patterns in\nthis book capture best practices and solutions to commonly occurring\nproblems in designing, building, and deploying machine learning systems.\nThe common challenges in machine learning tend to revolve around data\nquality, reproducibility, data drift, scale, and having to satisfy multiple\nobjectives.\n\nWe tend to use different ML design patterns at different stages of the ML\nlife cycle. There are patterns that are useful in problem framing and\nassessing feasibility. The majority of patterns address either development\nor deployment, and quite a few patterns address the interplay between\nthese stages.\n",
                        "extracted-code": ""
                    }
                ]
            }
        ]
    }
}